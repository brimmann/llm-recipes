{"answers": {"text": ["Denver Broncos", "Denver Broncos", "Denver Broncos"], "answer_start": [177, 177, 177]}, "prediction_text": "Denver Broncos"}
{"answers": {"text": ["Carolina Panthers", "Carolina Panthers", "Carolina Panthers"], "answer_start": [249, 249, 249]}, "prediction_text": "Carolina Panthers"}
{"answers": {"text": ["Santa Clara, California", "Levi's Stadium", "Levi's Stadium in the San Francisco Bay Area at Santa Clara, California."], "answer_start": [403, 355, 355]}, "prediction_text": "San Francisco Bay Area"}
{"answers": {"text": ["Denver Broncos", "Denver Broncos", "Denver Broncos"], "answer_start": [177, 177, 177]}, "prediction_text": "Denver Broncos"}
{"answers": {"text": ["gold", "gold", "gold"], "answer_start": [488, 488, 521]}, "prediction_text": "Gold"}
{"answers": {"text": ["\"golden anniversary\"", "gold-themed", "\"golden anniversary"], "answer_start": [487, 521, 487]}, "prediction_text": "Gold."}
{"answers": {"text": ["February 7, 2016", "February 7", "February 7, 2016"], "answer_start": [334, 334, 334]}, "prediction_text": "February 7."}
{"answers": {"text": ["American Football Conference", "American Football Conference", "American Football Conference"], "answer_start": [133, 133, 133]}, "prediction_text": "American Football Conference (AFC)"}
{"answers": {"text": ["\"golden anniversary\"", "gold-themed", "gold"], "answer_start": [487, 521, 521]}, "prediction_text": "Gold."}
{"answers": {"text": ["American Football Conference", "American Football Conference", "American Football Conference"], "answer_start": [133, 133, 133]}, "prediction_text": "American Football Conference (AFC)"}
{"answers": {"text": ["February 7, 2016", "February 7", "February 7, 2016"], "answer_start": [334, 334, 334]}, "prediction_text": "February 7."}
{"answers": {"text": ["Denver Broncos", "Denver Broncos", "Denver Broncos"], "answer_start": [177, 177, 177]}, "prediction_text": "Denver Broncos"}
{"answers": {"text": ["Levi's Stadium", "Levi's Stadium", "Levi's Stadium in the San Francisco Bay Area at Santa Clara"], "answer_start": [355, 355, 355]}, "prediction_text": "Levi's Stadium"}
{"answers": {"text": ["Santa Clara", "Santa Clara", "Santa Clara"], "answer_start": [403, 403, 403]}, "prediction_text": "San Francisco Bay Area"}
{"answers": {"text": ["Super Bowl L", "L", "Super Bowl L"], "answer_start": [693, 704, 693]}, "prediction_text": "Super Bowl L"}
{"answers": {"text": ["2015", "the 2015 season", "2015"], "answer_start": [116, 112, 116]}, "prediction_text": "2015"}
{"answers": {"text": ["2015", "2016", "2015"], "answer_start": [116, 346, 116]}, "prediction_text": "2015"}
{"answers": {"text": ["Santa Clara", "Santa Clara", "Santa Clara"], "answer_start": [403, 403, 403]}, "prediction_text": "San Francisco Bay Area"}
{"answers": {"text": ["Levi's Stadium", "Levi's Stadium", "Levi's Stadium"], "answer_start": [355, 355, 355]}, "prediction_text": "Levi's Stadium"}
{"answers": {"text": ["24\u201310", "24\u201310", "24\u201310"], "answer_start": [267, 267, 267]}, "prediction_text": "24-10"}
{"answers": {"text": ["February 7, 2016", "February 7, 2016", "February 7, 2016"], "answer_start": [334, 334, 334]}, "prediction_text": "February 7."}
{"answers": {"text": ["2015", "2016", "2016"], "answer_start": [116, 346, 346]}, "prediction_text": "2015"}
{"answers": {"text": ["Denver Broncos", "Denver Broncos", "Denver Broncos"], "answer_start": [177, 177, 177]}, "prediction_text": "Denver Broncos"}
{"answers": {"text": ["Carolina Panthers", "Carolina Panthers", "Carolina Panthers"], "answer_start": [249, 249, 249]}, "prediction_text": "Carolina Panthers"}
{"answers": {"text": ["Denver Broncos", "Denver Broncos", "Denver Broncos"], "answer_start": [177, 177, 177]}, "prediction_text": "Denver Broncos"}
{"answers": {"text": ["2015", "the 2015 season", "2015"], "answer_start": [116, 112, 116]}, "prediction_text": "2015"}
{"answers": {"text": ["Denver Broncos", "Denver Broncos", "Denver Broncos"], "answer_start": [177, 177, 177]}, "prediction_text": "Denver Broncos"}
{"answers": {"text": ["Santa Clara, California.", "Levi's Stadium", "Levi's Stadium"], "answer_start": [403, 355, 355]}, "prediction_text": "Levi's Stadium in the San Francisco Bay Area."}
{"answers": {"text": ["Super Bowl", "Super Bowl", "Super Bowl"], "answer_start": [0, 0, 0]}, "prediction_text": "Super Bowl 50"}
{"answers": {"text": ["Denver Broncos", "Denver Broncos", "Denver Broncos"], "answer_start": [177, 177, 177]}, "prediction_text": "Denver Broncos"}
{"answers": {"text": ["Cam Newton", "Cam Newton", "Cam Newton"], "answer_start": [77, 77, 77]}, "prediction_text": "Cam Newton"}
{"answers": {"text": ["8", "eight", "eight"], "answer_start": [467, 601, 601]}, "prediction_text": "8"}
{"answers": {"text": ["1995", "1995", "1995"], "answer_start": [291, 291, 291]}, "prediction_text": "1995"}
{"answers": {"text": ["Arizona Cardinals", "the Arizona Cardinals", "Arizona Cardinals"], "answer_start": [152, 148, 152]}, "prediction_text": "Arizona Cardinals"}
{"answers": {"text": ["New England Patriots", "the New England Patriots", "New England Patriots"], "answer_start": [372, 368, 372]}, "prediction_text": "New England Patriots"}
{"answers": {"text": ["Arizona Cardinals", "the Arizona Cardinals", "Arizona Cardinals"], "answer_start": [152, 148, 152]}, "prediction_text": "Arizona Cardinals"}
{"answers": {"text": ["New England Patriots", "the New England Patriots", "New England Patriots"], "answer_start": [372, 368, 372]}, "prediction_text": "New England Patriots"}
{"answers": {"text": ["New England Patriots", "the New England Patriots", "New England Patriots"], "answer_start": [372, 368, 372]}, "prediction_text": "Broncos"}
{"answers": {"text": ["four", "four", "four"], "answer_start": [575, 575, 575]}, "prediction_text": "4"}
{"answers": {"text": ["Cam Newton", "Cam Newton", "Cam Newton"], "answer_start": [77, 77, 77]}, "prediction_text": "Cam Newton"}
{"answers": {"text": ["15\u20131", "15\u20131", "15\u20131"], "answer_start": [48, 48, 48]}, "prediction_text": "15-1"}
{"answers": {"text": ["Cam Newton", "Cam Newton", "Cam Newton"], "answer_start": [77, 77, 77]}, "prediction_text": "Cam Newton"}
{"answers": {"text": ["12\u20134", "12\u20134", "12\u20134"], "answer_start": [344, 344, 344]}, "prediction_text": "12-4."}
{"answers": {"text": ["4", "four", "four"], "answer_start": [170, 575, 575]}, "prediction_text": "4"}
{"answers": {"text": ["New England Patriots", "the New England Patriots", "New England Patriots"], "answer_start": [372, 368, 372]}, "prediction_text": "New England Patriots"}
{"answers": {"text": ["Cam Newton", "Cam Newton", "Cam Newton"], "answer_start": [77, 77, 77]}, "prediction_text": "Cam Newton"}
{"answers": {"text": ["Arizona Cardinals", "the Arizona Cardinals", "Arizona Cardinals"], "answer_start": [152, 148, 152]}, "prediction_text": "Arizona Cardinals"}
{"answers": {"text": ["2", "second", "second"], "answer_start": [345, 227, 227]}, "prediction_text": "8"}
{"answers": {"text": ["New England Patriots", "the New England Patriots", "New England Patriots"], "answer_start": [372, 368, 372]}, "prediction_text": "New England Patriots"}
{"answers": {"text": ["Cam Newton", "Cam Newton", "Cam Newton"], "answer_start": [77, 77, 77]}, "prediction_text": "Cam Newton"}
{"answers": {"text": ["New England Patriots", "the New England Patriots", "New England Patriots"], "answer_start": [372, 368, 372]}, "prediction_text": "New England Patriots"}
{"answers": {"text": ["Arizona Cardinals", "the Arizona Cardinals", "Arizona Cardinals"], "answer_start": [152, 148, 152]}, "prediction_text": "Arizona Cardinals"}
{"answers": {"text": ["Cam Newton", "Cam Newton", "Cam Newton"], "answer_start": [77, 77, 77]}, "prediction_text": "Cam Newton"}
{"answers": {"text": ["Arizona Cardinals", "the Arizona Cardinals", "Arizona Cardinals"], "answer_start": [152, 148, 152]}, "prediction_text": "Arizona Cardinals"}
{"answers": {"text": ["1995.", "1995", "1995"], "answer_start": [291, 291, 291]}, "prediction_text": "1995"}
{"answers": {"text": ["Von Miller", "Von Miller", "Miller"], "answer_start": [248, 248, 252]}, "prediction_text": "Von Miller"}
{"answers": {"text": ["2", "two", "two"], "answer_start": [314, 328, 328]}, "prediction_text": "2\u00bd sacks."}
{"answers": {"text": ["Broncos", "The Broncos", "Broncos"], "answer_start": [4, 0, 4]}, "prediction_text": "Broncos"}
{"answers": {"text": ["linebacker Von Miller", "Von Miller", "Miller"], "answer_start": [237, 248, 252]}, "prediction_text": "Von Miller"}
{"answers": {"text": ["five solo tackles", "five", "five"], "answer_start": [295, 295, 295]}, "prediction_text": "Five."}
{"answers": {"text": ["Newton was limited by Denver's defense", "Newton", "Newton"], "answer_start": [67, 67, 67]}, "prediction_text": "Newton"}
{"answers": {"text": ["seven", "seven", "seven"], "answer_start": [124, 124, 124]}, "prediction_text": "Seven times."}
{"answers": {"text": ["Von Miller", "The Broncos", "Miller"], "answer_start": [248, 0, 252]}, "prediction_text": "Von Miller"}
{"answers": {"text": ["three", "three", "three"], "answer_start": [156, 156, 156]}, "prediction_text": "Three."}
{"answers": {"text": ["two", "two", "two"], "answer_start": [328, 328, 328]}, "prediction_text": "2\u00bd sacks."}
{"answers": {"text": ["Von Miller", "Von Miller", "Miller"], "answer_start": [248, 248, 252]}, "prediction_text": "Von Miller"}
{"answers": {"text": ["linebacker", "linebacker", "linebacker"], "answer_start": [237, 237, 237]}, "prediction_text": "Linebacker"}
{"answers": {"text": ["5", "five", "five"], "answer_start": [45, 295, 295]}, "prediction_text": "Five."}
{"answers": {"text": ["2", "two", "two"], "answer_start": [314, 328, 328]}, "prediction_text": "2\u00bd sacks."}
{"answers": {"text": ["Von Miller", "Von Miller", "Von Miller"], "answer_start": [248, 248, 248]}, "prediction_text": "Von Miller"}
{"answers": {"text": ["5", "five", "five"], "answer_start": [45, 295, 295]}, "prediction_text": "Five solos."}
{"answers": {"text": ["seven", "seven", "seven"], "answer_start": [124, 124, 124]}, "prediction_text": "Seven times."}
{"answers": {"text": ["three", "three", "three"], "answer_start": [156, 156, 156]}, "prediction_text": "3 times."}
{"answers": {"text": ["a fumble", "a fumble", "fumble"], "answer_start": [183, 183, 185]}, "prediction_text": "Fumble."}
{"answers": {"text": ["Von Miller", "Von Miller", "Von Miller"], "answer_start": [248, 248, 248]}, "prediction_text": "Von Miller"}
{"answers": {"text": ["linebacker", "linebacker", "linebacker"], "answer_start": [237, 237, 237]}, "prediction_text": "Linebacker"}
{"answers": {"text": ["seven", "seven", "seven"], "answer_start": [124, 124, 124]}, "prediction_text": "7 times."}
{"answers": {"text": ["three", "three", "three"], "answer_start": [156, 156, 156]}, "prediction_text": "Three."}
{"answers": {"text": ["Von Miller", "Von Miller", "Von Miller"], "answer_start": [248, 248, 248]}, "prediction_text": "Von Miller"}
{"answers": {"text": ["five", "five", "five"], "answer_start": [295, 295, 295]}, "prediction_text": "Five solos."}
{"answers": {"text": ["CBS", "CBS", "CBS"], "answer_start": [0, 0, 0]}, "prediction_text": "CBS"}
{"answers": {"text": ["$5 million", "$5 million", "$5 million"], "answer_start": [67, 67, 67]}, "prediction_text": "$5 million"}
{"answers": {"text": ["Coldplay", "Coldplay", "Coldplay"], "answer_start": [194, 194, 194]}, "prediction_text": "Coldplay"}
{"answers": {"text": ["Beyonc\u00e9 and Bruno Mars", "Beyonc\u00e9 and Bruno Mars", "Beyonc\u00e9 and Bruno Mars"], "answer_start": [233, 233, 233]}, "prediction_text": "Coldplay, Beyonc\u00e9, Bruno Mars."}
{"answers": {"text": ["Super Bowl XLVII", "Super Bowl XLVII", "XLVII"], "answer_start": [275, 275, 286]}, "prediction_text": "Super Bowl XLVIII"}
{"answers": {"text": ["CBS", "CBS", "CBS"], "answer_start": [0, 0, 0]}, "prediction_text": "CBS"}
{"answers": {"text": ["$5 million", "$5 million", "$5 million"], "answer_start": [67, 67, 67]}, "prediction_text": "$5 million"}
{"answers": {"text": ["Beyonc\u00e9", "Beyonc\u00e9", "Beyonc\u00e9"], "answer_start": [233, 233, 233]}, "prediction_text": "Bruno Mars"}
{"answers": {"text": ["Bruno Mars", "Bruno Mars", "Mars"], "answer_start": [245, 245, 251]}, "prediction_text": "Bruno Mars"}
{"answers": {"text": ["Coldplay", "Coldplay", "Coldplay"], "answer_start": [194, 194, 194]}, "prediction_text": "Coldplay"}
{"answers": {"text": ["CBS", "CBS", "CBS"], "answer_start": [0, 0, 0]}, "prediction_text": "CBS"}
{"answers": {"text": ["$5 million", "$5 million", "$5 million"], "answer_start": [67, 67, 67]}, "prediction_text": "$5 million"}
{"answers": {"text": ["Bruno Mars", "Bruno Mars", "Bruno Mars,"], "answer_start": [245, 245, 245]}, "prediction_text": "Coldplay"}
{"answers": {"text": ["third", "third", "third"], "answer_start": [355, 355, 355]}, "prediction_text": "3rd."}
{"answers": {"text": ["CBS", "CBS", "CBS"], "answer_start": [0, 0, 0]}, "prediction_text": "CBS"}
{"answers": {"text": ["$5 million", "$5 million", "$5 million"], "answer_start": [67, 67, 67]}, "prediction_text": "$5 million"}
{"answers": {"text": ["Coldplay", "Coldplay", "Coldplay"], "answer_start": [194, 194, 194]}, "prediction_text": "Coldplay"}
{"answers": {"text": ["Beyonc\u00e9 and Bruno Mars", "Beyonc\u00e9 and Bruno Mars", "Beyonc\u00e9 and Bruno Mars"], "answer_start": [233, 233, 233]}, "prediction_text": "Beyonc\u00e9 and Bruno Mars"}
{"answers": {"text": ["CBS", "CBS", "CBS"], "answer_start": [0, 0, 0]}, "prediction_text": "CBS"}
{"answers": {"text": ["Coldplay", "Coldplay", "Coldplay"], "answer_start": [194, 194, 194]}, "prediction_text": "Coldplay"}
{"answers": {"text": ["Beyonc\u00e9 and Bruno Mars", "Beyonc\u00e9 and Bruno Mars", "Beyonc\u00e9 and Bruno Mars"], "answer_start": [233, 233, 233]}, "prediction_text": "Beyonc\u00e9 and Bruno Mars"}
{"answers": {"text": ["Super Bowl XLVII", "Super Bowl XLVII", "Super Bowl XLVII"], "answer_start": [275, 275, 275]}, "prediction_text": "Super Bowl XLVII"}
{"answers": {"text": ["$5 million", "$5 million", "$5 million for a 30-second"], "answer_start": [67, 67, 67]}, "prediction_text": "$5 million"}
{"answers": {"text": ["Coldplay", "Coldplay", "Coldplay"], "answer_start": [194, 194, 194]}, "prediction_text": "Coldplay"}
{"answers": {"text": ["Beyonc\u00e9 and Bruno Mars", "Beyonc\u00e9 and Bruno Mars", "Beyonc\u00e9 and Bruno Mars"], "answer_start": [233, 233, 233]}, "prediction_text": "Beyonc\u00e9 and Bruno Mars"}
{"answers": {"text": ["Bruno Mars", "Coldplay", "Coldplay"], "answer_start": [245, 194, 194]}, "prediction_text": "Bruno Mars"}
{"answers": {"text": ["Roger Goodell", "Roger Goodell", "Goodell"], "answer_start": [32, 32, 38]}, "prediction_text": "Roger Goodell"}
{"answers": {"text": ["the 50th Super Bowl", "the 50th", "50th"], "answer_start": [85, 85, 89]}, "prediction_text": "50th Super Bowl"}
{"answers": {"text": ["2012", "2012", "2012"], "answer_start": [9, 9, 9]}, "prediction_text": "2012"}
{"answers": {"text": ["Roger Goodell", "Roger Goodell", "Goodell"], "answer_start": [32, 32, 38]}, "prediction_text": "Roger Goodell"}
{"answers": {"text": ["early 2012", "In early 2012", "2012"], "answer_start": [3, 0, 9]}, "prediction_text": "2012"}
{"answers": {"text": ["Roger Goodell", "Roger Goodell", "Goodell"], "answer_start": [32, 32, 38]}, "prediction_text": "Roger Goodell"}
{"answers": {"text": ["Roger Goodell", "Roger Goodell", "Goodell"], "answer_start": [32, 32, 38]}, "prediction_text": "Roger Goodell"}
{"answers": {"text": ["Roger Goodell", "Roger Goodell", "Goodell"], "answer_start": [32, 32, 38]}, "prediction_text": "Roger Goodell"}
{"answers": {"text": ["spectacular", "an important game for us as a league", "spectacular"], "answer_start": [106, 141, 106]}, "prediction_text": "Spectacular"}
{"answers": {"text": ["spectacular", "spectacular", "spectacular"], "answer_start": [106, 106, 106]}, "prediction_text": "Spectacular"}
{"answers": {"text": ["2012", "2012", "2012"], "answer_start": [9, 9, 9]}, "prediction_text": "2012"}
{"answers": {"text": ["New Orleans' Mercedes-Benz Superdome", "New Orleans' Mercedes-Benz Superdome", "Mercedes-Benz Superdome"], "answer_start": [56, 56, 69]}, "prediction_text": "Mercedes-Benz Superdome"}
{"answers": {"text": ["Miami's Sun Life Stadium", "Miami's Sun Life Stadium", "Sun Life Stadium"], "answer_start": [94, 94, 102]}, "prediction_text": "Mercedes-Benz Superdome"}
{"answers": {"text": ["San Francisco Bay Area's Levi's Stadium", "San Francisco Bay Area's Levi's Stadium", "Levi's Stadium"], "answer_start": [128, 128, 153]}, "prediction_text": "San Francisco Bay Area"}
{"answers": {"text": ["Sun Life Stadium", "Sun Life Stadium", "Sun Life Stadium"], "answer_start": [102, 102, 102]}, "prediction_text": "Sun Life Stadium"}
{"answers": {"text": ["Levi's Stadium", "Levi's Stadium", "Levi's Stadium"], "answer_start": [153, 153, 153]}, "prediction_text": "Levi's Stadium"}
{"answers": {"text": ["Levi's Stadium", "Levi's Stadium", "Levi's Stadium."], "answer_start": [153, 153, 153]}, "prediction_text": "Levi's Stadium"}
{"answers": {"text": ["Mercedes-Benz Superdome", "Mercedes-Benz Superdome", "Mercedes-Benz Superdome"], "answer_start": [69, 69, 69]}, "prediction_text": "Mercedes-Benz Superdome"}
{"answers": {"text": ["Sun Life Stadium", "Sun Life Stadium", "Sun Life Stadium"], "answer_start": [102, 102, 102]}, "prediction_text": "Sun Life Stadium"}
{"answers": {"text": ["New Orleans' Mercedes-Benz Superdome, Miami's Sun Life Stadium, and the San Francisco Bay Area's Levi's Stadium", "New Orleans' Mercedes-Benz Superdome, Miami's Sun Life Stadium, and the San Francisco Bay Area's Levi's Stadium.", "New Orleans' Mercedes-Benz Superdome, Miami's Sun Life Stadium, and the San Francisco Bay Area's Levi's Stadium."], "answer_start": [56, 56, 56]}, "prediction_text": "Mercedes-Benz Superdome, Sun Life Stadium, and Levi's Stadium."}
{"answers": {"text": ["three", "three", "three"], "answer_start": [43, 43, 43]}, "prediction_text": "3 sites."}
{"answers": {"text": ["New Orleans", "New Orleans", "New Orleans'"], "answer_start": [56, 56, 56]}, "prediction_text": "New Orleans"}
{"answers": {"text": ["Sun Life Stadium", "Sun Life Stadium", "Sun Life Stadium"], "answer_start": [102, 102, 102]}, "prediction_text": "Sun Life Stadium"}
{"answers": {"text": ["San Francisco", "San Francisco", "San Francisco Bay Area's"], "answer_start": [128, 128, 128]}, "prediction_text": "San Francisco Bay Area"}
{"answers": {"text": ["Levi's Stadium.", "Levi's Stadium", "Levi's Stadium."], "answer_start": [153, 153, 153]}, "prediction_text": "Levi's Stadium"}
{"answers": {"text": ["Sun Life Stadium", "Sun Life Stadium", "Sun Life Stadium"], "answer_start": [102, 102, 102]}, "prediction_text": "Levi's Stadium"}
{"answers": {"text": ["Mercedes-Benz Superdome", "Mercedes-Benz Superdome", "Mercedes-Benz Superdome,"], "answer_start": [69, 69, 69]}, "prediction_text": "Mercedes-Benz Superdome"}
{"answers": {"text": ["Levi's Stadium.", "Levi's Stadium", "Levi's Stadium."], "answer_start": [153, 153, 153]}, "prediction_text": "Levi's Stadium"}
{"answers": {"text": ["October 16, 2012", "October 16, 2012,", "October 16, 2012"], "answer_start": [24, 24, 24]}, "prediction_text": "October 16, 2012"}
{"answers": {"text": ["10", "10", "10"], "answer_start": [168, 168, 168]}, "prediction_text": "10 times."}
{"answers": {"text": ["Super Bowl XLIV", "Super Bowl XLIV", "2010"], "answer_start": [242, 242, 261]}, "prediction_text": "Super Bowl XLIV."}
{"answers": {"text": ["2010", "2010", "2010"], "answer_start": [261, 261, 261]}, "prediction_text": "2010"}
{"answers": {"text": ["1985", "1985", "1985"], "answer_start": [309, 309, 309]}, "prediction_text": "1985"}
{"answers": {"text": ["Sun Life Stadium", "Sun Life Stadium", "Sun Life Stadium"], "answer_start": [70, 70, 70]}, "prediction_text": "Sun Life Stadium"}
{"answers": {"text": ["October 16, 2012", "October 16, 2012", "October 16, 2012,"], "answer_start": [24, 24, 24]}, "prediction_text": "October 16, 2012"}
{"answers": {"text": ["Stanford Stadium", "Stanford Stadium", "Stanford Stadium"], "answer_start": [340, 340, 340]}, "prediction_text": "Stanford Stadium"}
{"answers": {"text": ["May 3, 2013", "May 3, 2013", "May 3, 2013"], "answer_start": [491, 491, 491]}, "prediction_text": "May 3, 2013."}
{"answers": {"text": ["2010", "2010", "2010"], "answer_start": [261, 261, 261]}, "prediction_text": "1985"}
{"answers": {"text": ["two", "10", "10"], "answer_start": [51, 168, 168]}, "prediction_text": "10 times."}
{"answers": {"text": ["Super Bowl XLIV", "Super Bowl XLIV", "2010"], "answer_start": [242, 242, 261]}, "prediction_text": "Super Bowl XLIV."}
{"answers": {"text": ["two", "two", "two"], "answer_start": [51, 51, 51]}, "prediction_text": "2"}
{"answers": {"text": ["Florida legislature", "the Florida legislature", "Florida legislature"], "answer_start": [508, 504, 508]}, "prediction_text": "Florida legislature"}
{"answers": {"text": ["1985", "1985", "1985"], "answer_start": [309, 309, 309]}, "prediction_text": "1985"}
{"answers": {"text": ["New Orleans", "New Orleans", "New Orleans"], "answer_start": [197, 197, 197]}, "prediction_text": "New Orleans"}
{"answers": {"text": ["October 16, 2012", "October 16, 2012", "October 16, 2012,"], "answer_start": [24, 24, 24]}, "prediction_text": "October 16, 2012"}
{"answers": {"text": ["10.", "10", "10 times"], "answer_start": [263, 168, 168]}, "prediction_text": "10 times."}
{"answers": {"text": ["New Orleans", "New Orleans", "New Orleans"], "answer_start": [197, 197, 197]}, "prediction_text": "New Orleans"}
{"answers": {"text": ["1985", "1985", "1985"], "answer_start": [309, 309, 309]}, "prediction_text": "1985"}
{"answers": {"text": ["Florida legislature", "the Florida legislature", "Florida legislature"], "answer_start": [508, 504, 508]}, "prediction_text": "Florida legislature."}
{"answers": {"text": ["May 21, 2013", "May 21, 2013", "May 21, 2013,"], "answer_start": [3, 3, 3]}, "prediction_text": "2014"}
{"answers": {"text": ["NFL owners", "NFL owners", "NFL owners"], "answer_start": [17, 17, 17]}, "prediction_text": "NFL owners"}
{"answers": {"text": ["2014", "in 2014", "2014"], "answer_start": [144, 141, 144]}, "prediction_text": "2014"}
{"answers": {"text": ["$1.2 billion", "$1.2 billion", "$1.2 billion"], "answer_start": [113, 113, 113]}, "prediction_text": "$1.2 billion"}
{"answers": {"text": ["San Diego", "San Diego", "San Diego"], "answer_start": [308, 308, 308]}, "prediction_text": "San Diego"}
{"answers": {"text": ["Boston", "in Boston", "May 21, 2013"], "answer_start": [56, 53, 3]}, "prediction_text": "Boston"}
{"answers": {"text": ["May 21, 2013", "May 21, 2013", "May 21, 2013,"], "answer_start": [3, 3, 3]}, "prediction_text": "May 21, 2013"}
{"answers": {"text": ["$1.2 billion", "$1.2 billion", "$1.2 billion"], "answer_start": [113, 113, 113]}, "prediction_text": "$1.2 billion"}
{"answers": {"text": ["Super Bowl XXXVII", "Super Bowl XXXVII", "XXXVII"], "answer_start": [276, 276, 287]}, "prediction_text": "Super Bowl XXXVII."}
{"answers": {"text": ["San Diego", "San Diego", "San Diego"], "answer_start": [308, 308, 308]}, "prediction_text": "San Diego"}
{"answers": {"text": ["2013", "2013", "2013"], "answer_start": [11, 11, 11]}, "prediction_text": "2014"}
{"answers": {"text": ["2014", "2014", "2014"], "answer_start": [144, 144, 144]}, "prediction_text": "2014"}
{"answers": {"text": ["$1.2 billion", "$1.2 billion", "$1.2 billion"], "answer_start": [113, 113, 113]}, "prediction_text": "$1.2 billion"}
{"answers": {"text": ["1985", "1985", "1985"], "answer_start": [236, 236, 236]}, "prediction_text": "1985"}
{"answers": {"text": ["Super Bowl XXXVII", "Super Bowl XXXVII", "XXXVII"], "answer_start": [276, 276, 287]}, "prediction_text": "XXXVII"}
{"answers": {"text": ["May 21, 2013", "May 21, 2013,", "May 21, 2013"], "answer_start": [3, 3, 3]}, "prediction_text": "2003"}
{"answers": {"text": ["2014", "in 2014", "2014"], "answer_start": [144, 141, 144]}, "prediction_text": "2014"}
{"answers": {"text": ["2003", "in 2003", "2003"], "answer_start": [321, 318, 321]}, "prediction_text": "2003"}
{"answers": {"text": ["Boston", "in Boston", "Boston"], "answer_start": [56, 53, 56]}, "prediction_text": "Boston"}
{"answers": {"text": ["May 21, 2013", "May 21, 2013", "May 21, 2013"], "answer_start": [3, 3, 3]}, "prediction_text": "2014"}
{"answers": {"text": ["2014.", "in 2014", "2014"], "answer_start": [144, 141, 144]}, "prediction_text": "2014"}
{"answers": {"text": ["$1.2 billion", "$1.2 billion", "$1.2 billion"], "answer_start": [113, 113, 113]}, "prediction_text": "$1.2 billion"}
{"answers": {"text": ["2003.", "2003", "2003"], "answer_start": [321, 321, 321]}, "prediction_text": "2003"}
{"answers": {"text": ["John Fox", "John Fox", "Fox"], "answer_start": [636, 636, 641]}, "prediction_text": "John Fox"}
{"answers": {"text": ["ten", "ten", "six"], "answer_start": [138, 138, 219]}, "prediction_text": "Ten teams."}
{"answers": {"text": ["six", "six", "six"], "answer_start": [219, 219, 219]}, "prediction_text": "Ten teams."}
{"answers": {"text": ["Carolina Panthers", "The Carolina Panthers", "Panthers"], "answer_start": [101, 97, 110]}, "prediction_text": "Denver Broncos"}
{"answers": {"text": ["Super Bowl XLVIII", "Super Bowl XLVIII", "XLVIII"], "answer_start": [444, 444, 455]}, "prediction_text": "Super Bowl XLVIII"}
{"answers": {"text": ["John Fox", "John Fox", "Fox"], "answer_start": [636, 636, 641]}, "prediction_text": "John Fox"}
{"answers": {"text": ["eight", "eight", "eight"], "answer_start": [324, 324, 324]}, "prediction_text": "8 appearances."}
{"answers": {"text": ["ten", "ten", "ten"], "answer_start": [138, 138, 138]}, "prediction_text": "Ten teams."}
{"answers": {"text": ["Super Bowl XXXVIII", "Super Bowl XXXVIII", "XXXVIII"], "answer_start": [573, 573, 584]}, "prediction_text": "Super Bowl XLVIII"}
{"answers": {"text": ["six", "ten", "ten"], "answer_start": [219, 138, 138]}, "prediction_text": "Ten teams."}
{"answers": {"text": ["number one", "number one", "one"], "answer_start": [35, 35, 42]}, "prediction_text": "Number one."}
{"answers": {"text": ["number one", "number one", "one"], "answer_start": [35, 35, 42]}, "prediction_text": "4th seed"}
{"answers": {"text": ["Super Bowl XLVIII", "Super Bowl XLVIII", "Super Bowl XLVIII"], "answer_start": [444, 444, 444]}, "prediction_text": "Super Bowl XXXVIII."}
{"answers": {"text": ["Super Bowl XXXVIII.", "Super Bowl XXXVIII", "Super Bowl XXXVIII"], "answer_start": [573, 573, 573]}, "prediction_text": "Super Bowl XXXVIII."}
{"answers": {"text": ["six", "ten", "ten"], "answer_start": [219, 138, 138]}, "prediction_text": "Six."}
{"answers": {"text": ["one", "1", "1"], "answer_start": [42, 251, 251]}, "prediction_text": "0"}
{"answers": {"text": ["four", "four", "four"], "answer_start": [300, 300, 300]}, "prediction_text": "Four."}
{"answers": {"text": ["John Fox", "John Fox", "John Fox"], "answer_start": [636, 636, 636]}, "prediction_text": "John Fox"}
{"answers": {"text": ["DeAngelo Williams", "DeAngelo Williams", "Williams"], "answer_start": [38, 38, 47]}, "prediction_text": "DeAngelo Williams"}
{"answers": {"text": ["Kelvin Benjamin", "Kelvin Benjamin", "Benjamin"], "answer_start": [85, 85, 92]}, "prediction_text": "Kelvin Benjamin"}
{"answers": {"text": ["7", "seventh", "seventh"], "answer_start": [326, 220, 220]}, "prediction_text": "7"}
{"answers": {"text": ["1978", "1978", "1978"], "answer_start": [324, 324, 324]}, "prediction_text": "1978"}
{"answers": {"text": ["Carolina Panthers", "the Panthers", "Carolina"], "answer_start": [137, 695, 330]}, "prediction_text": "Carolina Panthers"}
{"answers": {"text": ["Ten", "Ten", "Ten"], "answer_start": [807, 807, 807]}, "prediction_text": "Ten players."}
{"answers": {"text": ["eight", "eight", "eight"], "answer_start": [892, 892, 892]}, "prediction_text": "Ten."}
{"answers": {"text": ["Kelvin Benjamin", "Kelvin Benjamin", "Benjamin"], "answer_start": [85, 85, 92]}, "prediction_text": "Kelvin Benjamin"}
{"answers": {"text": ["1978", "1978", "1978"], "answer_start": [324, 324, 324]}, "prediction_text": "1978"}
{"answers": {"text": ["2009", "2009", "2009"], "answer_start": [591, 591, 591]}, "prediction_text": "2009"}
{"answers": {"text": ["2011", "2011", "2011"], "answer_start": [623, 623, 623]}, "prediction_text": "2011"}
{"answers": {"text": ["torn ACL", "a torn ACL", "torn ACL"], "answer_start": [106, 104, 106]}, "prediction_text": "Torn ACL."}
{"answers": {"text": ["Kelvin Benjamin", "Kelvin Benjamin", "Benjamin"], "answer_start": [85, 85, 92]}, "prediction_text": "Kelvin Benjamin"}
{"answers": {"text": ["DeAngelo Williams", "DeAngelo Williams", "Williams"], "answer_start": [38, 38, 47]}, "prediction_text": "DeAngelo Williams"}
{"answers": {"text": ["1978", "1978", "1978"], "answer_start": [324, 324, 324]}, "prediction_text": "1978"}
{"answers": {"text": ["Ten", "Ten", "Ten"], "answer_start": [807, 807, 807]}, "prediction_text": "Ten players."}
{"answers": {"text": ["Carolina Panthers", "the Panthers", "Carolina"], "answer_start": [137, 695, 330]}, "prediction_text": "Carolina Panthers"}
{"answers": {"text": ["1978.", "1978", "1978"], "answer_start": [324, 324, 324]}, "prediction_text": "1978"}
{"answers": {"text": ["Carolina Panthers", "the Panthers", "Carolina"], "answer_start": [137, 695, 330]}, "prediction_text": "Carolina Panthers"}
{"answers": {"text": ["Ten", "Ten", "Ten"], "answer_start": [807, 807, 807]}, "prediction_text": "Ten players."}
{"answers": {"text": ["six", "six", "six"], "answer_start": [98, 98, 98]}, "prediction_text": "6"}
{"answers": {"text": ["45", "45", "45"], "answer_start": [279, 279, 279]}, "prediction_text": "35"}
{"answers": {"text": ["10", "10", "10"], "answer_start": [312, 338, 338]}, "prediction_text": "10."}
{"answers": {"text": ["27", "27", "27"], "answer_start": [659, 659, 659]}, "prediction_text": "27"}
{"answers": {"text": ["Greg Olsen", "Greg Olsen", "Olsen"], "answer_start": [444, 444, 449]}, "prediction_text": "Greg Olsen"}
{"answers": {"text": ["45", "45", "45"], "answer_start": [279, 279, 279]}, "prediction_text": "45 total touchdowns."}
{"answers": {"text": ["99.4", "99.4", "99.4."], "answer_start": [396, 396, 396]}, "prediction_text": "45 total touchdowns."}
{"answers": {"text": ["77 passes", "77", "77"], "answer_start": [481, 481, 481]}, "prediction_text": "77 passes."}
{"answers": {"text": ["receivers", "receivers", "receivers"], "answer_start": [419, 693, 693]}, "prediction_text": "Tight end"}
{"answers": {"text": ["Jonathan Stewart", "Jonathan Stewart", "Jonathan Stewart"], "answer_start": [964, 964, 964]}, "prediction_text": "Jonathan Stewart"}
{"answers": {"text": ["six", "six", "six"], "answer_start": [98, 98, 98]}, "prediction_text": "6"}
{"answers": {"text": ["Cam Newton", "Cam Newton", "Cam Newton"], "answer_start": [144, 144, 144]}, "prediction_text": "Cam Newton"}
{"answers": {"text": ["3,837", "3,837", "3,837"], "answer_start": [197, 197, 197]}, "prediction_text": "3,837 yards."}
{"answers": {"text": ["45", "45", "45"], "answer_start": [279, 279, 279]}, "prediction_text": "45"}
{"answers": {"text": ["six", "six", "six"], "answer_start": [98, 1026, 1026]}, "prediction_text": "6"}
{"answers": {"text": ["500", "500", "500"], "answer_start": [52, 52, 52]}, "prediction_text": "500 points."}
{"answers": {"text": ["3,837", "3,837", "3,837"], "answer_start": [197, 197, 197]}, "prediction_text": "3,837 yards."}
{"answers": {"text": ["45", "45", "45"], "answer_start": [279, 279, 279]}, "prediction_text": "35"}
{"answers": {"text": ["99.4.", "99.4", "99.4."], "answer_start": [396, 396, 396]}, "prediction_text": "99.4"}
{"answers": {"text": ["39", "39", "39"], "answer_start": [588, 739, 739]}, "prediction_text": "39 receptions."}
{"answers": {"text": ["308", "308", "308"], "answer_start": [34, 34, 34]}, "prediction_text": "308 points."}
{"answers": {"text": ["136", "136", "136"], "answer_start": [470, 470, 470]}, "prediction_text": "11."}
{"answers": {"text": ["118", "118", "118"], "answer_start": [789, 789, 789]}, "prediction_text": "118 tackles."}
{"answers": {"text": ["four", "four", "four"], "answer_start": [140, 1104, 1104]}, "prediction_text": "Four."}
{"answers": {"text": ["Kawann Short", "Kawann Short", "Short"], "answer_start": [192, 192, 199]}, "prediction_text": "Kawann Short"}
{"answers": {"text": ["24", "24", "24"], "answer_start": [124, 124, 124]}, "prediction_text": "Four."}
{"answers": {"text": ["Kawann Short", "Kawann Short", "Short"], "answer_start": [192, 192, 199]}, "prediction_text": "Kawann Short"}
{"answers": {"text": ["four", "four", "four"], "answer_start": [140, 140, 140]}, "prediction_text": "Four."}
{"answers": {"text": ["four", "5\u00bd", "four"], "answer_start": [140, 695, 705]}, "prediction_text": "4"}
{"answers": {"text": ["Kurt Coleman", "Kurt Coleman", "Coleman"], "answer_start": [900, 900, 905]}, "prediction_text": "Kurt Coleman"}
{"answers": {"text": ["24", "24", "24"], "answer_start": [124, 124, 124]}, "prediction_text": "Four."}
{"answers": {"text": ["Kony Ealy", "Kony Ealy"], "answer_start": [500, 500]}, "prediction_text": "Mario Addison"}
{"answers": {"text": ["Luke Kuechly.", "Luke Kuechly", "Short"], "answer_start": [666, 666, 199]}, "prediction_text": "Mario Addison"}
{"answers": {"text": ["two.", "two", "24"], "answer_start": [284, 1124, 124]}, "prediction_text": "Four."}
{"answers": {"text": ["Gary Kubiak", "Gary Kubiak", "Kubiak"], "answer_start": [291, 291, 332]}, "prediction_text": "John Fox"}
{"answers": {"text": ["Brock Osweiler", "Brock Osweiler", "Osweiler"], "answer_start": [1157, 1157, 1163]}, "prediction_text": "Brock Osweiler"}
{"answers": {"text": ["Indianapolis Colts", "the Indianapolis Colts", "Colts"], "answer_start": [641, 637, 654]}, "prediction_text": "Indianapolis Colts"}
{"answers": {"text": ["San Diego Chargers", "San Diego Chargers", "Chargers"], "answer_start": [1404, 1404, 1414]}, "prediction_text": "San Diego Chargers"}
{"answers": {"text": ["Wade Phillips", "Wade Phillips", "Phillips"], "answer_start": [1640, 1640, 1645]}, "prediction_text": "Wade Phillips"}
{"answers": {"text": ["four", "four", "four"], "answer_start": [201, 238, 238]}, "prediction_text": "Four years."}
{"answers": {"text": ["Gary Kubiak", "Gary Kubiak", "Kubiak"], "answer_start": [291, 291, 332]}, "prediction_text": "Gary Kubiak"}
{"answers": {"text": ["Indianapolis Colts", "the Indianapolis Colts", "1998"], "answer_start": [641, 637, 663]}, "prediction_text": "Indianapolis Colts"}
{"answers": {"text": ["39", "39", "39"], "answer_start": [817, 817, 817]}, "prediction_text": "39"}
{"answers": {"text": ["plantar fasciitis", "a partial tear of the plantar fasciitis"], "answer_start": [678, 946]}, "prediction_text": "Plantar fasciitis."}
{"answers": {"text": ["Gary Kubiak", "Gary Kubiak", "Kubiak"], "answer_start": [291, 291, 332]}, "prediction_text": "Gary Kubiak"}
{"answers": {"text": ["Peyton Manning", "Peyton Manning", "Manning"], "answer_start": [442, 442, 566]}, "prediction_text": "Brock Osweiler"}
{"answers": {"text": ["a plantar fasciitis injury", "a plantar fasciitis injury", "plantar fasciitis"], "answer_start": [676, 676, 678]}, "prediction_text": "Plantar fasciitis."}
{"answers": {"text": ["39", "39", "39"], "answer_start": [817, 817, 817]}, "prediction_text": "39"}
{"answers": {"text": ["four", "4", "4"], "answer_start": [201, 1991, 1991]}, "prediction_text": "13 games."}
{"answers": {"text": ["John Fox", "John Fox", "Fox"], "answer_start": [179, 179, 184]}, "prediction_text": "John Fox"}
{"answers": {"text": ["Peyton Manning", "Peyton Manning", "Manning"], "answer_start": [442, 442, 566]}, "prediction_text": "Peyton Manning"}
{"answers": {"text": ["Gary Kubiak", "Gary Kubiak", "Kubiak"], "answer_start": [291, 291, 296]}, "prediction_text": "John Fox"}
{"answers": {"text": ["left foot.", "left", "left"], "answer_start": [993, 993, 993]}, "prediction_text": "Plantar fasciitis."}
{"answers": {"text": ["Wade Phillips", "Wade Phillips", "Phillips"], "answer_start": [1640, 1640, 1645]}, "prediction_text": "Wade Phillips"}
{"answers": {"text": ["67.9", "67.9", "67.9"], "answer_start": [44, 44, 44]}, "prediction_text": "67.9"}
{"answers": {"text": ["17", "17", "17"], "answer_start": [115, 115, 115]}, "prediction_text": "17"}
{"answers": {"text": ["Demaryius Thomas", "Demaryius Thomas", "Thomas"], "answer_start": [253, 253, 263]}, "prediction_text": "Emmanuel Sanders"}
{"answers": {"text": ["C. J. Anderson", "C. J. Anderson", "Anderson"], "answer_start": [572, 572, 578]}, "prediction_text": "C. J. Anderson"}
{"answers": {"text": ["10", "10", "10"], "answer_start": [178, 178, 178]}, "prediction_text": "9"}
{"answers": {"text": ["67.9", "67.9", "67.9"], "answer_start": [44, 44, 44]}, "prediction_text": "67.9"}
{"answers": {"text": ["2,249", "2,249", "2,249"], "answer_start": [77, 77, 77]}, "prediction_text": "2,249 yards."}
{"answers": {"text": ["nine", "17", "career-low 67.9 passer rating,"], "answer_start": [93, 115, 33]}, "prediction_text": "17"}
{"answers": {"text": ["Demaryius Thomas", "Demaryius Thomas", "Thomas"], "answer_start": [253, 253, 263]}, "prediction_text": "Emmanuel Sanders"}
{"answers": {"text": ["receiver", "receiver", "Thomas"], "answer_start": [244, 244, 263]}, "prediction_text": "Wide receiver"}
{"answers": {"text": ["67.9", "67.9", "67.9"], "answer_start": [44, 44, 44]}, "prediction_text": "67.9%"}
{"answers": {"text": ["17", "17", "17"], "answer_start": [115, 115, 115]}, "prediction_text": "17"}
{"answers": {"text": ["Demaryius Thomas", "Demaryius Thomas", "Thomas"], "answer_start": [253, 253, 263]}, "prediction_text": "Demaryius Thomas"}
{"answers": {"text": ["5", "five", "five"], "answer_start": [290, 761, 761]}, "prediction_text": "5"}
{"answers": {"text": ["67.9", "67.9", "67.9"], "answer_start": [44, 44, 44]}, "prediction_text": "67.9%"}
{"answers": {"text": ["17", "17", "17"], "answer_start": [115, 115, 115]}, "prediction_text": "17"}
{"answers": {"text": ["Emmanuel Sanders", "Emmanuel Sanders", "Sanders"], "answer_start": [345, 345, 354]}, "prediction_text": "Emmanuel Sanders"}
{"answers": {"text": ["C. J. Anderson", "C. J. Anderson", "Anderson"], "answer_start": [572, 572, 578]}, "prediction_text": "C. J. Anderson"}
{"answers": {"text": ["4.7", "4.7", "4.7"], "answer_start": [799, 799, 799]}, "prediction_text": "4.7 yards per carry."}
{"answers": {"text": ["4,530", "4,530", "4,530"], "answer_start": [60, 60, 60]}, "prediction_text": "4,530"}
{"answers": {"text": ["5\u00bd", "5\u00bd", "5\u00bd"], "answer_start": [198, 198, 198]}, "prediction_text": "5\u00bd sacks."}
{"answers": {"text": ["Brandon Marshall", "Brandon Marshall", "Marshall"], "answer_start": [458, 458, 466]}, "prediction_text": "DeMarcus Ware"}
{"answers": {"text": ["three", "three", "three"], "answer_start": [302, 585, 585]}, "prediction_text": "Three."}
{"answers": {"text": ["Linebacker", "Linebacker", "Linebacker"], "answer_start": [309, 309, 309]}, "prediction_text": "Linebacker"}
{"answers": {"text": ["Linebacker", "Linebacker", "Linebacker"], "answer_start": [309, 447, 447]}, "prediction_text": "Cornerback"}
{"answers": {"text": ["Defensive ends", "Defensive ends", "Defensive ends"], "answer_start": [144, 144, 144]}, "prediction_text": "Linebacker"}
{"answers": {"text": ["296", "296", "296"], "answer_start": [138, 138, 138]}, "prediction_text": "296 points."}
{"answers": {"text": ["Von Miller", "Von Miller", "Miller"], "answer_start": [228, 228, 232]}, "prediction_text": "Von Miller"}
{"answers": {"text": ["Brandon Marshall", "Brandon Marshall", "Marshall"], "answer_start": [458, 458, 466]}, "prediction_text": "DeMarcus Ware"}
{"answers": {"text": ["three.", "three", "three"], "answer_start": [302, 585, 585]}, "prediction_text": "Three."}
{"answers": {"text": ["Von Miller", "Von Miller", "Miller"], "answer_start": [228, 228, 232]}, "prediction_text": "Derek Wolfe"}
{"answers": {"text": ["Linebacker Brandon Marshall", "Brandon Marshall", "Marshall"], "answer_start": [447, 458, 466]}, "prediction_text": "DeMarcus Ware"}
{"answers": {"text": ["Derek Wolfe and Malik Jackson", "Derek Wolfe and Malik Jackson"], "answer_start": [159, 159]}, "prediction_text": "Derek Wolfe and Malik Jackson."}
{"answers": {"text": ["Seattle Seahawks", "the Seattle Seahawks", "Seahawks"], "answer_start": [22, 18, 30]}, "prediction_text": "Seattle Seahawks"}
{"answers": {"text": ["Arizona Cardinals", "Arizona Cardinals", "Cardinals"], "answer_start": [249, 249, 257]}, "prediction_text": "Arizona Cardinals"}
{"answers": {"text": ["487", "487", "487"], "answer_start": [315, 315, 315]}, "prediction_text": "487 yards."}
{"answers": {"text": ["seven", "seven", "seven"], "answer_start": [337, 337, 337]}, "prediction_text": "7"}
{"answers": {"text": ["31\u201324", "31\u201324", "31\u201324"], "answer_start": [163, 163, 163]}, "prediction_text": "31-24."}
{"answers": {"text": ["Seattle Seahawks", "the Seattle Seahawks", "Seahawks"], "answer_start": [22, 18, 30]}, "prediction_text": "Seattle Seahawks"}
{"answers": {"text": ["31\u201324", "31\u201324", "31\u201324,"], "answer_start": [163, 163, 163]}, "prediction_text": "49-15"}
{"answers": {"text": ["487", "487", "487"], "answer_start": [315, 315, 315]}, "prediction_text": "487 yards."}
{"answers": {"text": ["Seattle Seahawks", "the Seattle Seahawks", "Seahawks"], "answer_start": [22, 18, 30]}, "prediction_text": "Seattle Seahawks"}
{"answers": {"text": ["Arizona Cardinals", "the Arizona Cardinals", "Cardinals"], "answer_start": [249, 245, 257]}, "prediction_text": "Seattle Seahawks"}
{"answers": {"text": ["seven", "seven", "seven"], "answer_start": [337, 337, 337]}, "prediction_text": "7"}
{"answers": {"text": ["Seattle Seahawks", "the Seattle Seahawks", "Seahawks"], "answer_start": [22, 18, 30]}, "prediction_text": "Seattle Seahawks"}
{"answers": {"text": ["49\u201315", "49\u201315", "49\u201315,"], "answer_start": [297, 297, 297]}, "prediction_text": "49-15"}
{"answers": {"text": ["Arizona Cardinals", "the Seattle Seahawks", "Cardinals"], "answer_start": [249, 18, 257]}, "prediction_text": "Seattle Seahawks"}
{"answers": {"text": ["487", "487"], "answer_start": [315, 315]}, "prediction_text": "487 yards."}
{"answers": {"text": ["Pittsburgh Steelers", "the Pittsburgh Steelers", "Steelers"], "answer_start": [25, 21, 36]}, "prediction_text": "Pittsburgh Steelers"}
{"answers": {"text": ["11", "11", "11"], "answer_start": [88, 88, 88]}, "prediction_text": "11 points."}
{"answers": {"text": ["New England Patriots", "New England Patriots", "Patriots"], "answer_start": [192, 192, 204]}, "prediction_text": "New England Patriots"}
{"answers": {"text": ["20\u201318", "20\u201318", "20\u201318,"], "answer_start": [243, 243, 243]}, "prediction_text": "20-18."}
{"answers": {"text": ["17 seconds", "17 seconds", "17"], "answer_start": [322, 322, 322]}, "prediction_text": "17 seconds."}
{"answers": {"text": ["Broncos", "The Broncos", "Broncos"], "answer_start": [4, 0, 4]}, "prediction_text": "Pittsburgh Steelers"}
{"answers": {"text": ["23\u201316", "23\u201316", ", 23\u201316,"], "answer_start": [70, 70, 68]}, "prediction_text": "11 points."}
{"answers": {"text": ["New England Patriots", "New England Patriots", "Patriots"], "answer_start": [192, 192, 204]}, "prediction_text": "New England Patriots"}
{"answers": {"text": ["17", "17 seconds", "17"], "answer_start": [322, 322, 322]}, "prediction_text": "17 seconds."}
{"answers": {"text": ["Manning", "Manning"], "answer_start": [360, 360]}, "prediction_text": "Manning"}
{"answers": {"text": ["Pittsburgh Steelers", "the Pittsburgh Steelers", "Steelers"], "answer_start": [25, 21, 36]}, "prediction_text": "Pittsburgh Steelers"}
{"answers": {"text": ["11", "11", "11"], "answer_start": [88, 88, 88]}, "prediction_text": "11 points."}
{"answers": {"text": ["New England Patriots", "New England Patriots", "Patriots"], "answer_start": [192, 192, 204]}, "prediction_text": "New England Patriots"}
{"answers": {"text": ["Pittsburgh Steelers", "Pittsburgh Steelers", "Steelers"], "answer_start": [25, 25, 36]}, "prediction_text": "Pittsburgh Steelers"}
{"answers": {"text": ["New England Patriots", "New England Patriots", "Patriots"], "answer_start": [192, 192, 204]}, "prediction_text": "New England Patriots"}
{"answers": {"text": ["17", "17", "17"], "answer_start": [322, 322, 322]}, "prediction_text": "17 seconds."}
{"answers": {"text": ["Thomas Davis", "Thomas Davis,", "Davis"], "answer_start": [39, 39, 46]}, "prediction_text": "Thomas Davis"}
{"answers": {"text": ["a broken arm", "a broken arm", "broken arm"], "answer_start": [143, 143, 145]}, "prediction_text": "Broken arm"}
{"answers": {"text": ["three", "three", "three"], "answer_start": [97, 97, 97]}, "prediction_text": "3"}
{"answers": {"text": ["11", "11", "11"], "answer_start": [56, 56, 56]}, "prediction_text": "11 years."}
{"answers": {"text": ["ACL tears", "ACL tears", "ACL"], "answer_start": [103, 103, 103]}, "prediction_text": "Broken arm."}
{"answers": {"text": ["arm", "arm", "arm"], "answer_start": [152, 152, 152]}, "prediction_text": "Arm"}
{"answers": {"text": ["11", "11", "11"], "answer_start": [56, 56, 56]}, "prediction_text": "11 years."}
{"answers": {"text": ["Super Bowl", "the Super Bowl", "Super Bowl"], "answer_start": [253, 249, 253]}, "prediction_text": "Super Bowl"}
{"answers": {"text": ["three", "three", "three"], "answer_start": [97, 97, 97]}, "prediction_text": "3"}
{"answers": {"text": ["broken arm", "a broken arm", "broken arm"], "answer_start": [145, 143, 145]}, "prediction_text": "Broken arm"}
{"answers": {"text": ["11", "11", "11"], "answer_start": [56, 56, 56]}, "prediction_text": "3"}
{"answers": {"text": ["Thomas Davis", "Thomas Davis", "Davis"], "answer_start": [39, 39, 46]}, "prediction_text": "Thomas Davis"}
{"answers": {"text": ["39", "39", "39", "39"], "answer_start": [172, 172, 172, 172]}, "prediction_text": "39"}
{"answers": {"text": ["John Elway", "John Elway", "Elway", "Elway"], "answer_start": [204, 204, 209, 209]}, "prediction_text": "John Elway"}
{"answers": {"text": ["38", "38", "38", "38"], "answer_start": [275, 275, 275, 275]}, "prediction_text": "38"}
{"answers": {"text": ["Executive Vice President of Football Operations and General Manager", "Executive Vice President of Football Operations and General Manager", "Executive Vice President of Football Operations and General Manager.", "Executive Vice President of Football Operations and General Manager."], "answer_start": [304, 304, 304, 304]}, "prediction_text": "Executive Vice President of Football Operations and General Manager."}
{"answers": {"text": ["Broncos", "the Broncos", "Broncos", "Broncos"], "answer_start": [228, 224, 228, 228]}, "prediction_text": "Broncos"}
{"answers": {"text": ["Broncos", "the Broncos", "Broncos", "Broncos"], "answer_start": [228, 224, 228, 228]}, "prediction_text": "Broncos"}
{"answers": {"text": ["John Elway", "John Elway", "Elway", "Elway"], "answer_start": [204, 204, 209, 209]}, "prediction_text": "John Elway"}
{"answers": {"text": ["38", "38", "38", "38"], "answer_start": [275, 275, 275, 275]}, "prediction_text": "38"}
{"answers": {"text": ["Peyton Manning", "Peyton Manning", "Manning", "Manning"], "answer_start": [0, 0, 7, 7]}, "prediction_text": "John Elway"}
{"answers": {"text": ["two", "two", "two", "two"], "answer_start": [57, 57, 57, 57]}, "prediction_text": "2"}
{"answers": {"text": ["two", "two", "two", "two"], "answer_start": [57, 57, 57, 57]}, "prediction_text": "2"}
{"answers": {"text": ["Peyton Manning", "Peyton Manning", "Manning", "Manning"], "answer_start": [0, 0, 7, 7]}, "prediction_text": "John Elway"}
{"answers": {"text": ["John Elway", "John Elway", "Elway", "Elway"], "answer_start": [204, 204, 209, 209]}, "prediction_text": "John Elway"}
{"answers": {"text": ["Super Bowl XXXIII", "Super Bowl XXXIII", "XXXIII", "XXXIII"], "answer_start": [250, 250, 261, 261]}, "prediction_text": "Super Bowl XXXIII"}
{"answers": {"text": ["Peyton Manning", "Peyton Manning", "Manning", "Manning"], "answer_start": [0, 0, 7, 7]}, "prediction_text": "Peyton Manning"}
{"answers": {"text": ["39.", "39", "39", "39"], "answer_start": [172, 172, 172, 172]}, "prediction_text": "39"}
{"answers": {"text": ["John Elway", "John Elway", "Elway", "Elway"], "answer_start": [204, 204, 209, 209]}, "prediction_text": "John Elway"}
{"answers": {"text": ["1998", "1998", "1998"], "answer_start": [149, 149, 149]}, "prediction_text": "1998"}
{"answers": {"text": ["2011", "2011", "2011"], "answer_start": [198, 198, 198]}, "prediction_text": "2011"}
{"answers": {"text": ["26", "26", "26"], "answer_start": [492, 492, 492]}, "prediction_text": "26 days."}
{"answers": {"text": ["13 years and 48 days", "13 years and 48 days", "13 years"], "answer_start": [443, 443, 443]}, "prediction_text": "13 years and 48 days."}
{"answers": {"text": ["Von Miller", "Von Miller", "Miller"], "answer_start": [306, 306, 310]}, "prediction_text": "Von Miller"}
{"answers": {"text": ["Manning", "Manning", "Manning"], "answer_start": [113, 113, 113]}, "prediction_text": "Manning"}
{"answers": {"text": ["Newton", "Newton", "Newton"], "answer_start": [171, 171, 171]}, "prediction_text": "Von Miller"}
{"answers": {"text": ["26", "26", "26"], "answer_start": [492, 492, 492]}, "prediction_text": "26 days."}
{"answers": {"text": ["quarterback", "quarterbacks", "quarterback"], "answer_start": [43, 427, 43]}, "prediction_text": "Quarterback"}
{"answers": {"text": ["1998", "1998", "1998"], "answer_start": [149, 149, 149]}, "prediction_text": "1998"}
{"answers": {"text": ["2011", "2011", "2011"], "answer_start": [198, 198, 198]}, "prediction_text": "2011"}
{"answers": {"text": ["Von Miller", "Von Miller", "Miller"], "answer_start": [306, 306, 310]}, "prediction_text": "Von Miller"}
{"answers": {"text": ["2011.", "2011", "2011"], "answer_start": [198, 198, 198]}, "prediction_text": "2011"}
{"answers": {"text": ["26", "26", "26"], "answer_start": [492, 492, 492]}, "prediction_text": "26 days."}
{"answers": {"text": ["13 years and 48 days", "13 years and 48 days", "13 years and 48 days"], "answer_start": [443, 443, 443]}, "prediction_text": "13 years and 48 days."}
{"answers": {"text": ["Super Bowl XX", "Super Bowl XX", "XX"], "answer_start": [63, 63, 74]}, "prediction_text": "Super Bowl XX"}
{"answers": {"text": ["Chicago Bears", "the Chicago Bears", "Bears"], "answer_start": [46, 42, 54]}, "prediction_text": "Chicago Bears"}
{"answers": {"text": ["linebacker", "linebacker", "linebacker"], "answer_start": [26, 26, 26]}, "prediction_text": "Linebacker"}
{"answers": {"text": ["Elway", "Elway", "Elway"], "answer_start": [99, 99, 99]}, "prediction_text": "Elway"}
{"answers": {"text": ["Broncos", "the Broncos", "Broncos"], "answer_start": [123, 119, 123]}, "prediction_text": "Chicago Bears"}
{"answers": {"text": ["linebacker", "linebacker", "linebacker"], "answer_start": [26, 26, 26]}, "prediction_text": "Linebacker"}
{"answers": {"text": ["Elway", "Elway", "Elway"], "answer_start": [99, 99, 99]}, "prediction_text": "Elway"}
{"answers": {"text": ["Rivera", "Rivera", "Rivera"], "answer_start": [5, 5, 5]}, "prediction_text": "Kubiak"}
{"answers": {"text": ["Super Bowl XX", "Super Bowl XX", "XX"], "answer_start": [63, 63, 74]}, "prediction_text": "Super Bowl XX"}
{"answers": {"text": ["Justin Tucker", "Justin Tucker", "Tucker"], "answer_start": [320, 320, 327]}, "prediction_text": "Justin Tucker"}
{"answers": {"text": ["Bermuda 419", "Bermuda 419", "hybrid Bermuda 419"], "answer_start": [569, 569, 562]}, "prediction_text": "Bermuda 419."}
{"answers": {"text": ["Ed Mangan", "Ed Mangan", "Mangan"], "answer_start": [625, 625, 628]}, "prediction_text": "Ed Mangan"}
{"answers": {"text": ["Baltimore Ravens", "Baltimore Ravens", "Ravens"], "answer_start": [296, 296, 306]}, "prediction_text": "Baltimore Ravens"}
{"answers": {"text": ["kicker", "kicker", "kicker"], "answer_start": [313, 313, 313]}, "prediction_text": "Kicker"}
{"answers": {"text": ["Justin Tucker", "Justin Tucker", "Tucker"], "answer_start": [320, 320, 327]}, "prediction_text": "Justin Tucker"}
{"answers": {"text": ["kicker", "kicker", "kicker"], "answer_start": [313, 313, 313]}, "prediction_text": "Kicker"}
{"answers": {"text": ["hybrid Bermuda 419 turf", "Bermuda 419 turf", "hybrid Bermuda 419"], "answer_start": [562, 569, 562]}, "prediction_text": "Bermuda 419 turf."}
{"answers": {"text": ["Justin Tucker", "Justin Tucker", "Tucker"], "answer_start": [320, 320, 327]}, "prediction_text": "Justin Tucker"}
{"answers": {"text": ["a new playing surface", "re-sodded", "re-sodded"], "answer_start": [537, 512, 157]}, "prediction_text": "Re-sodded the field with a new playing surface."}
{"answers": {"text": ["a hybrid Bermuda 419 turf.", "Bermuda 419 turf", "hybrid Bermuda 419 turf"], "answer_start": [560, 569, 562]}, "prediction_text": "Bermuda 419 turf."}
{"answers": {"text": ["their cleats", "cleats", "cleats"], "answer_start": [788, 794, 794]}, "prediction_text": "Cleats."}
{"answers": {"text": ["Justin Tucker", "Tucker"], "answer_start": [320, 327]}, "prediction_text": "Justin Tucker"}
{"answers": {"text": ["natural grass", "natural grass", "grass"], "answer_start": [480, 480, 488]}, "prediction_text": "Natural grass stadiums."}
{"answers": {"text": ["Broncos", "the Broncos", "Broncos"], "answer_start": [82, 78, 82]}, "prediction_text": "Broncos"}
{"answers": {"text": ["34\u201319", "34\u201319", "34\u201319"], "answer_start": [392, 392, 392]}, "prediction_text": "34-19."}
{"answers": {"text": ["Atlanta Falcons", "the Atlanta Falcons", "Falcons"], "answer_start": [376, 372, 384]}, "prediction_text": "Atlanta Falcons"}
{"answers": {"text": ["white", "white", "white"], "answer_start": [117, 145, 145]}, "prediction_text": "Silver pants."}
{"answers": {"text": ["Super Bowl XXXIII", "Super Bowl XXXIII", "Super Bowl XXXIII"], "answer_start": [302, 302, 302]}, "prediction_text": "Super Bowl XXXIII."}
{"answers": {"text": ["Super Bowl XXXIII", "Super Bowl XXXIII", "Super Bowl XXXIII"], "answer_start": [302, 302, 302]}, "prediction_text": "Super Bowl XXXIII."}
{"answers": {"text": ["34\u201319", "34\u201319", "34\u201319"], "answer_start": [392, 392, 392]}, "prediction_text": "34-19."}
{"answers": {"text": ["Atlanta Falcons", "the Atlanta Falcons", "Falcons"], "answer_start": [376, 372, 384]}, "prediction_text": "Atlanta Falcons"}
{"answers": {"text": ["white", "matching white jerseys", "white"], "answer_start": [117, 248, 209]}, "prediction_text": "White"}
{"answers": {"text": ["road white jerseys", "matching white", "white"], "answer_start": [112, 136, 117]}, "prediction_text": "White jerseys with matching white pants."}
{"answers": {"text": ["Pittsburgh Steelers", "the Pittsburgh Steelers", "Steelers"], "answer_start": [801, 797, 812]}, "prediction_text": "Pittsburgh Steelers"}
{"answers": {"text": ["Super Bowl XXXIII", "Super Bowl XXXIII", "XXXIII"], "answer_start": [302, 302, 313]}, "prediction_text": "Super Bowl XXXIII."}
{"answers": {"text": ["blue", "blue", "white"], "answer_start": [467, 467, 209]}, "prediction_text": "Blue"}
{"answers": {"text": ["orange", "orange", "orange"], "answer_start": [629, 629, 629]}, "prediction_text": "White jerseys."}
{"answers": {"text": ["black jerseys with silver pants.", "black jerseys with silver pants", "black"], "answer_start": [993, 993, 993]}, "prediction_text": "Black jerseys with silver pants."}
{"answers": {"text": ["San Jose State", "San Jose State", "San Jose State"], "answer_start": [22, 22, 22]}, "prediction_text": "Stanford University"}
{"answers": {"text": ["Stanford University", "Stanford University", "Stanford"], "answer_start": [117, 117, 117]}, "prediction_text": "Stanford University"}
{"answers": {"text": ["San Jose", "San Jose", "San Jose"], "answer_start": [22, 73, 73]}, "prediction_text": "Santa Clara"}
{"answers": {"text": ["Santa Clara", "Santa Clara", "Santa Clara"], "answer_start": [155, 155, 155]}, "prediction_text": "Santa Clara"}
{"answers": {"text": ["San Jose Marriott", "the San Jose Marriott", "San Jose State practice facility and stayed at the San Jose Marriott."], "answer_start": [73, 69, 22]}, "prediction_text": "San Jose Marriott"}
{"answers": {"text": ["Santa Clara Marriott", "the San Jose Marriott", "Marriott"], "answer_start": [155, 69, 82]}, "prediction_text": "Santa Clara Marriott"}
{"answers": {"text": ["San Jose State practice facility", "the San Jose State practice facility", "San Jose State"], "answer_start": [22, 18, 22]}, "prediction_text": "Stanford University"}
{"answers": {"text": ["Stanford University", "at Stanford University", "Stanford"], "answer_start": [117, 114, 117]}, "prediction_text": "Stanford University"}
{"answers": {"text": ["San Jose State practice facility", "the San Jose State practice facility", "San Jose State"], "answer_start": [22, 18, 22]}, "prediction_text": "San Jose State practice facility"}
{"answers": {"text": ["San Jose Marriott.", "the San Jose Marriott", "San Jose Marriott"], "answer_start": [73, 69, 73]}, "prediction_text": "San Jose Marriott"}
{"answers": {"text": ["Stanford University", "at Stanford University", "Stanford"], "answer_start": [117, 114, 117]}, "prediction_text": "Stanford University"}
{"answers": {"text": ["Santa Clara Marriott.", "the Santa Clara Marriott", "Santa Clara Marriott."], "answer_start": [155, 151, 155]}, "prediction_text": "Santa Clara Marriott"}
{"answers": {"text": ["San Jose", "the San Jose State practice facility", "San Jose State"], "answer_start": [22, 18, 22]}, "prediction_text": "San Jose State practice facility"}
{"answers": {"text": ["San Jose Marriott.", "the San Jose Marriott", "San Jose Marriott."], "answer_start": [73, 69, 73]}, "prediction_text": "San Jose Marriott"}
{"answers": {"text": ["Stanford University", "at Stanford University", "Stanford"], "answer_start": [117, 114, 117]}, "prediction_text": "Stanford University"}
{"answers": {"text": ["Santa Clara Marriott.", "the Santa Clara Marriott", "Santa Clara Marriott."], "answer_start": [155, 151, 155]}, "prediction_text": "Santa Clara Marriott"}
{"answers": {"text": ["June 4, 2014", "On June 4, 2014", "2014"], "answer_start": [3, 0, 11]}, "prediction_text": "June 4, 2014."}
{"answers": {"text": ["Super Bowl V", "Super Bowl V", "V"], "answer_start": [129, 129, 140]}, "prediction_text": "Super Bowl V"}
{"answers": {"text": ["Jaime Weston", "Jaime Weston", "Weston"], "answer_start": [339, 339, 345]}, "prediction_text": "Jaime Weston"}
{"answers": {"text": ["Super Bowl XLV", "Super Bowl XLV", "V"], "answer_start": [585, 585, 140]}, "prediction_text": "Super Bowl XLV"}
{"answers": {"text": ["Vince Lombardi", "Vince Lombardi Trophy", "Lombardi"], "answer_start": [699, 699, 705]}, "prediction_text": "Vince Lombardi"}
{"answers": {"text": ["2014", "2014", "2014"], "answer_start": [11, 11, 11]}, "prediction_text": "2014"}
{"answers": {"text": ["Super Bowl LI", "LI", "LI"], "answer_start": [324, 335, 335]}, "prediction_text": "Super Bowl LI."}
{"answers": {"text": ["L", "L", "L"], "answer_start": [23, 272, 272]}, "prediction_text": "Arabic numerals."}
{"answers": {"text": ["gold", "gold", "gold"], "answer_start": [682, 682, 682]}, "prediction_text": "Gold"}
{"answers": {"text": ["June 4, 2014", "On June 4, 2014", "2014"], "answer_start": [3, 0, 11]}, "prediction_text": "June 4, 2014."}
{"answers": {"text": ["Arabic numerals", "Arabic numerals", "Arabic"], "answer_start": [214, 214, 214]}, "prediction_text": "Arabic numerals."}
{"answers": {"text": ["L.", "L", "L"], "answer_start": [272, 272, 272]}, "prediction_text": "Arabic numerals."}
{"answers": {"text": ["gold", "gold", "gold"], "answer_start": [682, 682, 682]}, "prediction_text": "Gold"}
{"answers": {"text": ["Super Bowl LI.", "Super Bowl LI", "Super Bowl LI"], "answer_start": [324, 324, 324]}, "prediction_text": "Super Bowl LI."}
{"answers": {"text": ["Arabic", "Arabic numerals", "Arabic"], "answer_start": [214, 214, 214]}, "prediction_text": "Arabic numerals."}
{"answers": {"text": ["LI.", "LI", "LI"], "answer_start": [335, 335, 335]}, "prediction_text": "Arabic numerals."}
{"answers": {"text": ["gold", "gold", "gold"], "answer_start": [8, 8, 267]}, "prediction_text": "Gold"}
{"answers": {"text": ["week 7", "2015", "week 7,"], "answer_start": [290, 72, 290]}, "prediction_text": "Week 7."}
{"answers": {"text": ["50", "50-yard line", "50"], "answer_start": [232, 232, 232]}, "prediction_text": "50-yard line"}
{"answers": {"text": ["gold", "gold", "gold"], "answer_start": [8, 8, 267]}, "prediction_text": "Gold"}
{"answers": {"text": ["gold", "gold", "Gold"], "answer_start": [8, 8, 357]}, "prediction_text": "Gold"}
{"answers": {"text": ["Golden Super Bowl", "the \"Golden Super Bowl\"", "Golden Super Bowl"], "answer_start": [105, 100, 105]}, "prediction_text": "Gold Super Bowl"}
{"answers": {"text": ["Gold footballs", "Gold footballs", "Gold footballs"], "answer_start": [357, 357, 357]}, "prediction_text": "Gold footballs."}
{"answers": {"text": ["the 50-yard line", "the 50", "50"], "answer_start": [228, 228, 232]}, "prediction_text": "50-yard line."}
{"answers": {"text": ["gold", "gold", "gold"], "answer_start": [8, 8, 8]}, "prediction_text": "Gold"}
{"answers": {"text": ["Moscone Center", "the Moscone Center", "Moscone Center"], "answer_start": [42, 38, 42]}, "prediction_text": "Moscone Center"}
{"answers": {"text": ["San Francisco", "San Francisco", "San Francisco"], "answer_start": [60, 60, 60]}, "prediction_text": "San Francisco"}
{"answers": {"text": ["Ed Lee", "Ed Lee", "Lee"], "answer_start": [419, 419, 422]}, "prediction_text": "Ed Lee"}
{"answers": {"text": ["Jane Kim", "Jane Kim", "Kim"], "answer_start": [549, 549, 554]}, "prediction_text": "Jane Kim"}
{"answers": {"text": ["January 30", "January 30", "January 30"], "answer_start": [116, 116, 116]}, "prediction_text": "January 30."}
{"answers": {"text": ["1 million", "More than 1 million", "1 million"], "answer_start": [302, 292, 302]}, "prediction_text": "1 million"}
{"answers": {"text": ["Ed Lee", "Ed Lee", "Lee"], "answer_start": [419, 419, 422]}, "prediction_text": "Ed Lee"}
{"answers": {"text": ["Moscone Center", "the Moscone Center", "Moscone Center"], "answer_start": [42, 38, 42]}, "prediction_text": "Moscone Center"}
{"answers": {"text": ["Super Bowl City", "Super Bowl City", "Super Bowl City"], "answer_start": [89, 89, 89]}, "prediction_text": "Super Bowl City"}
{"answers": {"text": ["Moscone Center", "at the Moscone Center", "Moscone Center"], "answer_start": [42, 35, 42]}, "prediction_text": "San Francisco"}
{"answers": {"text": ["Super Bowl City", "Super Bowl City\" opened", "\"Super Bowl City"], "answer_start": [89, 89, 88]}, "prediction_text": "Super Bowl City"}
{"answers": {"text": ["Ed Lee", "Ed Lee", "Lee"], "answer_start": [419, 419, 422]}, "prediction_text": "Ed Lee"}
{"answers": {"text": ["Super Bowl City", "\"Super Bowl City\"", "Super Bowl City"], "answer_start": [89, 88, 89]}, "prediction_text": "Super Bowl City"}
{"answers": {"text": ["More than 1 million", "More than 1 million", "1 million"], "answer_start": [292, 292, 302]}, "prediction_text": "1 million"}
{"answers": {"text": ["mayor Ed Lee", "Ed Lee", "mayor Ed Lee"], "answer_start": [413, 419, 413]}, "prediction_text": "Ed Lee"}
{"answers": {"text": ["$5 million.", "0", "$5 million"], "answer_start": [655, 125, 655]}, "prediction_text": "$5 million"}
{"answers": {"text": ["The annual NFL Experience", "The annual NFL Experience", "NFL Experience"], "answer_start": [0, 0, 11]}, "prediction_text": "The annual NFL Experience."}
{"answers": {"text": ["Santa Clara University", "Santa Clara University", "Santa Clara University"], "answer_start": [184, 184, 184]}, "prediction_text": "Santa Clara University"}
{"answers": {"text": ["$2 million", "$2 million", "$2 million"], "answer_start": [23, 23, 23]}, "prediction_text": "$2 million"}
{"answers": {"text": ["a week", "a week", "week"], "answer_start": [77, 77, 79]}, "prediction_text": "Week."}
{"answers": {"text": ["$2 million", "$2 million", "$2 million"], "answer_start": [23, 23, 23]}, "prediction_text": "$2 million"}
{"answers": {"text": ["pep rally", "pep", "pep"], "answer_start": [214, 214, 214]}, "prediction_text": "Beer, wine, and food festival."}
{"answers": {"text": ["city council", "the city council", "city council"], "answer_start": [335, 411, 415]}, "prediction_text": "City council"}
{"answers": {"text": ["Bellomy Field", "Bellomy Field", "Santa Clara Convention Center"], "answer_start": [167, 167, 102]}, "prediction_text": "Bellomy Field at Santa Clara University."}
{"answers": {"text": ["A professional fundraiser", "A professional fundraiser", "professional fundraiser"], "answer_start": [225, 225, 227]}, "prediction_text": "City council"}
{"answers": {"text": ["city council", "the city council", "city council"], "answer_start": [335, 411, 415]}, "prediction_text": "City council"}
{"answers": {"text": ["$2 million", "$2 million", "$2 million"], "answer_start": [23, 23, 23]}, "prediction_text": "$2 million"}
{"answers": {"text": ["city council", "the city council", "city council"], "answer_start": [335, 411, 335]}, "prediction_text": "City council"}
{"answers": {"text": ["Monday", "February 1, 2016", "Monday"], "answer_start": [108, 189, 108]}, "prediction_text": "February 1."}
{"answers": {"text": ["Tuesday", "Tuesday afternoon prior to the game", "Tuesday"], "answer_start": [54, 54, 54]}, "prediction_text": "Tuesday afternoon."}
{"answers": {"text": ["SAP Center", "SAP Center", "SAP Center"], "answer_start": [209, 209, 209]}, "prediction_text": "SAP Center"}
{"answers": {"text": ["San Jose", "San Jose", "San Jose."], "answer_start": [223, 223, 223]}, "prediction_text": "San Jose"}
{"answers": {"text": ["the Golden Gate Bridge", "the Golden Gate Bridge", "Golden Gate Bridge."], "answer_start": [362, 362, 366]}, "prediction_text": "Golden Gate Bridge"}
{"answers": {"text": ["Tuesday", "Tuesday afternoon prior to the game", "Tuesday"], "answer_start": [54, 54, 54]}, "prediction_text": "Tuesday"}
{"answers": {"text": ["Monday", "Monday", "Monday"], "answer_start": [108, 108, 108]}, "prediction_text": "Monday"}
{"answers": {"text": ["Super Bowl Opening Night", "Super Bowl Opening Night", "Super Bowl Opening Night."], "answer_start": [141, 141, 141]}, "prediction_text": "Super Bowl Opening Night"}
{"answers": {"text": ["SAP Center", "SAP Center", "SAP Center"], "answer_start": [209, 209, 209]}, "prediction_text": "SAP Center"}
{"answers": {"text": ["San Jose", "San Jose", "San Jose."], "answer_start": [223, 223, 223]}, "prediction_text": "San Jose"}
{"answers": {"text": ["the Tuesday afternoon prior to the game", "Tuesday afternoon prior to the game", "Tuesday"], "answer_start": [50, 54, 54]}, "prediction_text": "Tuesday afternoon."}
{"answers": {"text": ["Super Bowl Opening Night.", "Super Bowl Opening Night", "Super Bowl Opening Night."], "answer_start": [141, 141, 141]}, "prediction_text": "Super Bowl Opening Night"}
{"answers": {"text": ["SAP Center in San Jose.", "at SAP Center in San Jose", "San Jose"], "answer_start": [209, 206, 223]}, "prediction_text": "Tuesday afternoon."}
{"answers": {"text": ["the Golden Gate Bridge.", "the Golden Gate Bridge", "Golden Gate Bridge."], "answer_start": [362, 362, 366]}, "prediction_text": "Golden Gate Bridge"}
{"answers": {"text": ["Monday", "Monday", "Monday"], "answer_start": [108, 108, 108]}, "prediction_text": "Monday"}
{"answers": {"text": ["Super Bowl Opening Night.", "Super Bowl Opening Night", "Super Bowl Opening Night"], "answer_start": [141, 141, 141]}, "prediction_text": "Media day."}
{"answers": {"text": ["SAP Center in San Jose.", "SAP Center", "SAP Center"], "answer_start": [209, 209, 209]}, "prediction_text": "Monday evening."}
{"answers": {"text": ["Golden Gate Bridge.", "the Golden Gate", "Golden Gate Bridge."], "answer_start": [366, 362, 366]}, "prediction_text": "Golden Gate Bridge"}
{"answers": {"text": ["February 1, 2016", "February 1, 2016", "February 1, 2016"], "answer_start": [189, 189, 189]}, "prediction_text": "February 1, 2016"}
{"answers": {"text": ["Business Connect", "Business Connect", "Business Connect"], "answer_start": [164, 164, 164]}, "prediction_text": "Business Connect"}
{"answers": {"text": ["$40 million", "over $40 million", "$40 million"], "answer_start": [339, 334, 339]}, "prediction_text": "$40 million"}
{"answers": {"text": ["Dignity Health", "Dignity Health", "Dignity"], "answer_start": [426, 426, 426]}, "prediction_text": "Dignity Health"}
{"answers": {"text": ["Gap", "Gap", "Gap"], "answer_start": [408, 408, 408]}, "prediction_text": "Apple"}
{"answers": {"text": ["Chevron", "Chevron", "Chevron"], "answer_start": [413, 413, 413]}, "prediction_text": "Apple"}
{"answers": {"text": ["Super Bowl 50 Host Committee", "the Super Bowl 50 Host Committee", "Super Bowl 50 Host Committee"], "answer_start": [24, 20, 24]}, "prediction_text": "Super Bowl 50 Host Committee"}
{"answers": {"text": ["over $40 million", "over $40 million", "over $40 million"], "answer_start": [334, 334, 334]}, "prediction_text": "$40 million"}
{"answers": {"text": ["sponsors", "through sponsors", "sponsors"], "answer_start": [359, 351, 359]}, "prediction_text": "Sponsors."}
{"answers": {"text": ["Business Connect", "Business Connect", "Business Connect"], "answer_start": [164, 164, 164]}, "prediction_text": "Business Connect"}
{"answers": {"text": ["Business Connect", "Business Connect", "Business Connect"], "answer_start": [164, 164, 164]}, "prediction_text": "Business Connect"}
{"answers": {"text": ["over $40 million", "over $40 million", "$40 million"], "answer_start": [334, 334, 339]}, "prediction_text": "$40 million."}
{"answers": {"text": ["25", "25 percent", "25"], "answer_start": [102, 102, 102]}, "prediction_text": "25%"}
{"answers": {"text": ["the 50 fund", "the 50 fund", "50 fund"], "answer_start": [200, 200, 204]}, "prediction_text": "50 Fund"}
{"answers": {"text": ["25 percent", "25 percent", "25"], "answer_start": [102, 102, 102]}, "prediction_text": "25%"}
{"answers": {"text": ["50 fund", "the 50 fund", "50 fund"], "answer_start": [204, 200, 204]}, "prediction_text": "50 Fund"}
{"answers": {"text": ["the most giving Super Bowl ever", "giving Super Bowl", "giving"], "answer_start": [50, 59, 59]}, "prediction_text": "giving Super Bowl ever"}
{"answers": {"text": ["25 percent", "25 percent of all money", "25"], "answer_start": [102, 102, 102]}, "prediction_text": "25%"}
{"answers": {"text": ["the 50 fund", "the 50 fund", "50 fund"], "answer_start": [200, 200, 204]}, "prediction_text": "50 Fund"}
{"answers": {"text": ["25 percent", "25 percent", "25"], "answer_start": [102, 102, 102]}, "prediction_text": "25%"}
{"answers": {"text": ["50 fund", "the 50 fund", "50 fund"], "answer_start": [204, 200, 204]}, "prediction_text": "50 Fund"}
{"answers": {"text": ["Vince Lombardi", "Vince Lombardi", "Lombardi"], "answer_start": [19, 19, 25]}, "prediction_text": "Vince Lombardi"}
{"answers": {"text": ["18", "18", "18"], "answer_start": [135, 135, 135]}, "prediction_text": "18 karats."}
{"answers": {"text": ["66", "66 lb", "33"], "answer_start": [213, 213, 184]}, "prediction_text": "66 lb (30 kg)"}
{"answers": {"text": ["Tiffany & Co.", "Tiffany & Co", "Tiffany & Co."], "answer_start": [283, 283, 283]}, "prediction_text": "Tiffany & Co."}
{"answers": {"text": ["Tiffany & Co.", "Tiffany & Co", "Tiffany & Co"], "answer_start": [283, 283, 283]}, "prediction_text": "Tiffany & Co."}
{"answers": {"text": ["Vince Lombardi Trophy", "the Vince Lombardi Trophy", "Vince Lombardi Trophy"], "answer_start": [19, 15, 19]}, "prediction_text": "Vince Lombardi Trophy"}
{"answers": {"text": ["18-karat gold-plated", "18-karat gold", "gold"], "answer_start": [135, 135, 144]}, "prediction_text": "Gold"}
{"answers": {"text": ["Tiffany & Co", "Tiffany & Co", "Tiffany & Co"], "answer_start": [283, 283, 283]}, "prediction_text": "Tiffany & Co."}
{"answers": {"text": ["the Vince Lombardi Trophy", "the Vince Lombardi Trophy", "Vince Lombardi Trophy"], "answer_start": [15, 15, 19]}, "prediction_text": "Vince Lombardi Trophy"}
{"answers": {"text": ["Tiffany & Co.", "Tiffany & Co", "Tiffany & Co."], "answer_start": [283, 283, 283]}, "prediction_text": "Tiffany & Co."}
{"answers": {"text": ["CBS", "CBS", "CBS"], "answer_start": [48, 48, 48]}, "prediction_text": "CBS"}
{"answers": {"text": ["Phil Simms", "Phil Simms", "Phil Simms"], "answer_start": [188, 188, 188]}, "prediction_text": "Phil Simms"}
{"answers": {"text": ["Tracy Wolfson", "Tracy Wolfson", "Tracy Wolfson"], "answer_start": [224, 224, 224]}, "prediction_text": "Tracy Wolfson"}
{"answers": {"text": ["36", "36", "36"], "answer_start": [384, 400, 400]}, "prediction_text": "36"}
{"answers": {"text": ["5K", "5K", "5K"], "answer_start": [629, 629, 629]}, "prediction_text": "5K."}
{"answers": {"text": ["three", "three", "three"], "answer_start": [84, 84, 84]}, "prediction_text": "3"}
{"answers": {"text": ["sidelines", "the sidelines", "sidelines"], "answer_start": [263, 259, 263]}, "prediction_text": "Evan Washburn."}
{"answers": {"text": ["360-degree", "360", "360"], "answer_start": [462, 384, 462]}, "prediction_text": "360 degrees."}
{"answers": {"text": ["5K resolution", "5K", "5K"], "answer_start": [629, 629, 629]}, "prediction_text": "5K."}
{"answers": {"text": ["sidelines", "the sidelines", "sidelines"], "answer_start": [263, 259, 263]}, "prediction_text": "Evan Washburn was located on the sidelines."}
{"answers": {"text": ["CBS", "CBS", "CBS"], "answer_start": [48, 48, 48]}, "prediction_text": "CBS"}
{"answers": {"text": ["three", "three", "three"], "answer_start": [84, 84, 84]}, "prediction_text": "3"}
{"answers": {"text": ["CBS", "CBS", "CBS"], "answer_start": [48, 48, 48]}, "prediction_text": "CBS"}
{"answers": {"text": ["Jim Nantz and Phil Simms", "Jim Nantz and Phil Simms", "Jim Nantz and Phil Simms"], "answer_start": [174, 174, 174]}, "prediction_text": "Jim Nantz and Phil Simms"}
{"answers": {"text": ["Tracy Wolfson and Evan Washburn", "Tracy Wolfson and Evan Washburn", "Tracy Wolfson and Evan Washburn"], "answer_start": [224, 224, 224]}, "prediction_text": "Tracy Wolfson and Evan Washburn."}
{"answers": {"text": ["5K", "5K", "5K"], "answer_start": [629, 629, 629]}, "prediction_text": "5K."}
{"answers": {"text": ["cameras", "cameras", "cameras"], "answer_start": [339, 339, 339]}, "prediction_text": "EyeVision 360."}
{"answers": {"text": ["ESPN Deportes", "ESPN Deportes", "ESPN Deportes"], "answer_start": [22, 22, 22]}, "prediction_text": "CBS"}
{"answers": {"text": ["John Sutcliffe", "John Sutcliffe", "Sutcliffe"], "answer_start": [617, 617, 622]}, "prediction_text": "John Sutcliffe"}
{"answers": {"text": ["Alvaro Martin", "Alvaro Martin", "Alvaro Martin"], "answer_start": [563, 563, 563]}, "prediction_text": "Alvaro Martin"}
{"answers": {"text": ["December 28, 2015", "December 28, 2015", "December 28, 2015,"], "answer_start": [3, 3, 3]}, "prediction_text": "December 28, 2015"}
{"answers": {"text": ["Spanish", "Spanish", "Spanish"], "answer_start": [122, 122, 122]}, "prediction_text": "Spanish"}
{"answers": {"text": ["CBS", "ESPN Deportes", "ESPN Deportes"], "answer_start": [86, 22, 22]}, "prediction_text": "CBS"}
{"answers": {"text": ["ESPN Deportes", "ESPN Deportes", "ESPN Deportes"], "answer_start": [22, 22, 22]}, "prediction_text": "ESPN Deportes"}
{"answers": {"text": ["John Sutcliffe.", "John Sutcliffe", "Sutcliffe"], "answer_start": [617, 617, 622]}, "prediction_text": "John Sutcliffe"}
{"answers": {"text": ["ESPN Deportes", "ESPN Deportes", "ESPN Deportes"], "answer_start": [22, 22, 22]}, "prediction_text": "CBS"}
{"answers": {"text": ["Alvaro Martin and Raul Allegre", "Alvaro Martin and Raul Allegre", "Alvaro Martin and Raul Allegre"], "answer_start": [563, 563, 563]}, "prediction_text": "Alvaro Martin and Raul Allegre."}
{"answers": {"text": ["John Sutcliffe.", "John Sutcliffe", "Sutcliffe"], "answer_start": [617, 617, 622]}, "prediction_text": "John Sutcliffe"}
{"answers": {"text": ["NFL Mobile", "NFL Mobile", "NFL Mobile service."], "answer_start": [304, 304, 304]}, "prediction_text": "NFL Mobile"}
{"answers": {"text": ["WatchESPN", "WatchESPN", "WatchESPN"], "answer_start": [387, 387, 387]}, "prediction_text": "WatchESPN."}
{"answers": {"text": ["CBSSports.com", "CBSSports.com", "CBSSports"], "answer_start": [45, 45, 45]}, "prediction_text": "CBSSports.com"}
{"answers": {"text": ["Xbox One", "Xbox One", "Xbox One"], "answer_start": [108, 108, 108]}, "prediction_text": "Xbox One"}
{"answers": {"text": ["10", "10", "10"], "answer_start": [104, 104, 104]}, "prediction_text": "Windows 10."}
{"answers": {"text": ["CBSSports.com", "CBSSports.com", "CBSSports.com"], "answer_start": [45, 45, 45]}, "prediction_text": "CBSSports.com"}
{"answers": {"text": ["Xbox One", "Xbox One", "Xbox"], "answer_start": [108, 108, 108]}, "prediction_text": "Xbox One"}
{"answers": {"text": ["Verizon Wireless customers", "Verizon", "Verizon"], "answer_start": [269, 187, 187]}, "prediction_text": "Verizon Wireless customers."}
{"answers": {"text": ["NFL Mobile service", "the CBS Sports apps", "NFL"], "answer_start": [304, 64, 304]}, "prediction_text": "NFL Mobile"}
{"answers": {"text": ["Verizon", "Verizon", "Verizon"], "answer_start": [187, 187, 187]}, "prediction_text": "Verizon Communications"}
{"answers": {"text": ["NFL Mobile service.", "NFL Mobile", "NFL"], "answer_start": [304, 304, 304]}, "prediction_text": "NFL Mobile"}
{"answers": {"text": ["digital streams of the game", "digital streams", "digital streams"], "answer_start": [13, 13, 13]}, "prediction_text": "Digital streams of the game."}
{"answers": {"text": ["Verizon", "Verizon", "Verizon"], "answer_start": [187, 187, 187]}, "prediction_text": "Verizon Communications"}
{"answers": {"text": ["WatchESPN.", "through WatchESPN", "WatchESPN"], "answer_start": [387, 379, 387]}, "prediction_text": "WatchESPN."}
{"answers": {"text": ["The Late Show with Stephen Colbert", "The Late Show with Stephen Colbert", "Late Show with Stephen Colbert"], "answer_start": [186, 186, 190]}, "prediction_text": "The Late Show with Stephen Colbert"}
{"answers": {"text": ["The Late Late Show with James Corden", "The Late Late Show with James Corden", "Late Late Show with James Corden"], "answer_start": [323, 323, 327]}, "prediction_text": "The Late Late Show with James Corden"}
{"answers": {"text": ["The Late Show with Stephen Colbert", "The Late Show with Stephen Colbert", "Colbert"], "answer_start": [186, 186, 213]}, "prediction_text": "The Late Show with Stephen Colbert"}
{"answers": {"text": ["The Late Late Show with James Corden", "The Late Late Show with James Corden", "Late Late Show with James Corden"], "answer_start": [323, 323, 327]}, "prediction_text": "The Late Show with Stephen Colbert"}
{"answers": {"text": ["The Late Show with Stephen Colbert", "The Late Show with Stephen Colbert", "Late Show with Stephen Colbert"], "answer_start": [186, 186, 190]}, "prediction_text": "The Late Show with Stephen Colbert"}
{"answers": {"text": ["late local programming", "late local programming", "late local programming"], "answer_start": [263, 263, 263]}, "prediction_text": "Late local programming."}
{"answers": {"text": ["The Late Late Show with James Corden.", "The Late Late Show with James Corden", "The Late Late Show with James Corden."], "answer_start": [323, 323, 323]}, "prediction_text": "The Late Late Show with James Corden."}
{"answers": {"text": ["$5,000,000", "$5,000,000", "$5,000,000,"], "answer_start": [55, 55, 55]}, "prediction_text": "$5,000,000"}
{"answers": {"text": ["Anheuser-Busch InBev", "Anheuser-Busch InBev", "Anheuser-Busch InBev"], "answer_start": [492, 492, 492]}, "prediction_text": "Anheuser-Busch InBev"}
{"answers": {"text": ["Doritos", "Doritos", "Doritos"], "answer_start": [648, 770, 648]}, "prediction_text": "Doritos"}
{"answers": {"text": ["20th", "the 20th", "20th"], "answer_start": [910, 906, 910]}, "prediction_text": "20th anniversary."}
{"answers": {"text": ["$5,000,000", "$5,000,000", "$5,000,000"], "answer_start": [55, 55, 55]}, "prediction_text": "$5,000,000"}
{"answers": {"text": ["Anheuser-Busch InBev", "Anheuser-Busch InBev", "Anheuser-Busch InBev"], "answer_start": [492, 492, 492]}, "prediction_text": "Anheuser-Busch InBev"}
{"answers": {"text": ["Doritos", "Doritos", "Doritos"], "answer_start": [648, 770, 648]}, "prediction_text": "Doritos"}
{"answers": {"text": ["Nintendo", "Nintendo", "Nintendo"], "answer_start": [829, 829, 829]}, "prediction_text": "Nintendo"}
{"answers": {"text": ["The Pok\u00e9mon Company", "The Pok\u00e9mon Company", "The Pok\u00e9mon Company"], "answer_start": [842, 842, 842]}, "prediction_text": "The Pok\u00e9mon Company"}
{"answers": {"text": ["Anheuser-Busch InBev", "Anheuser-Busch InBev", "Anheuser-Busch InBev"], "answer_start": [492, 492, 492]}, "prediction_text": "Doritos"}
{"answers": {"text": ["Doritos", "Doritos", "Doritos"], "answer_start": [648, 648, 648]}, "prediction_text": "Doritos"}
{"answers": {"text": ["Anheuser-Busch InBev", "Anheuser-Busch InBev", "Anheuser-Busch InBev"], "answer_start": [492, 492, 492]}, "prediction_text": "Doritos"}
{"answers": {"text": ["Doritos", "Doritos", "Doritos"], "answer_start": [648, 648, 648]}, "prediction_text": "Doritos"}
{"answers": {"text": ["Crash the Super Bowl", "Crash the Super Bowl", "Crash the Super Bowl"], "answer_start": [699, 699, 699]}, "prediction_text": "Crash the Super Bowl."}
{"answers": {"text": ["\"Small Business Big Game\"", "Small Business Big Game", "Small Business Big Game"], "answer_start": [23, 24, 24]}, "prediction_text": "Small Business Big Game"}
{"answers": {"text": ["Death Wish Coffee", "Death Wish Coffee", "Death Wish Coffee"], "answer_start": [67, 67, 157]}, "prediction_text": "Death Wish Coffee"}
{"answers": {"text": ["30-second", "30-second", "30-second"], "answer_start": [91, 91, 91]}, "prediction_text": "30 seconds."}
{"answers": {"text": ["nine", "nine", "nine"], "answer_start": [184, 184, 184]}, "prediction_text": "Nine."}
{"answers": {"text": ["Death Wish Coffee", "Death Wish Coffee", "Death Wish Coffee"], "answer_start": [67, 67, 157]}, "prediction_text": "Death Wish Coffee"}
{"answers": {"text": ["nine", "nine", "nine"], "answer_start": [184, 184, 184]}, "prediction_text": "Nine."}
{"answers": {"text": ["QuickBooks.", "QuickBooks", "QuickBooks"], "answer_start": [145, 0, 0]}, "prediction_text": "Death Wish Coffee"}
{"answers": {"text": ["Death Wish Coffee", "Death Wish Coffee", "Death Wish Coffee"], "answer_start": [67, 67, 157]}, "prediction_text": "Death Wish Coffee"}
{"answers": {"text": ["ten", "nine", "nine"], "answer_start": [198, 184, 184]}, "prediction_text": "9"}
{"answers": {"text": ["QuickBooks.", "QuickBooks", "QuickBooks"], "answer_start": [145, 0, 0]}, "prediction_text": "QuickBooks"}
{"answers": {"text": ["Death Wish Coffee", "Death Wish Coffee", "Death Wish Coffee"], "answer_start": [67, 67, 157]}, "prediction_text": "Death Wish Coffee"}
{"answers": {"text": ["Jason Bourne", "Jason Bourne", "Jason Bourne"], "answer_start": [438, 438, 438]}, "prediction_text": "The Secret Life of Pets."}
{"answers": {"text": ["Gods of Egypt", "Gods of Egypt", "Gods of Egypt,"], "answer_start": [261, 261, 261]}, "prediction_text": "Gods of Egypt"}
{"answers": {"text": ["Teenage Mutant Ninja Turtles: Out of the Shadows", "Teenage Mutant Ninja Turtles: Out of the Shadows", "Teenage Mutant Ninja Turtles: Out of the Shadows"], "answer_start": [295, 295, 295]}, "prediction_text": "The Secret Life of Pets."}
{"answers": {"text": ["Resurgence", "Resurgence", "Resurgence"], "answer_start": [210, 210, 210]}, "prediction_text": "Resurgence"}
{"answers": {"text": ["Gods of Egypt", "Gods of Egypt", "Gods of Egypt"], "answer_start": [261, 261, 261]}, "prediction_text": "Deadpool"}
{"answers": {"text": ["Teenage Mutant Ninja Turtles: Out of the Shadows", "Teenage Mutant Ninja Turtles: Out of the Shadows", "Teenage Mutant Ninja Turtles"], "answer_start": [295, 295, 295]}, "prediction_text": "The Jungle Book."}
{"answers": {"text": ["Jason Bourne", "Jason Bourne", "Jason Bourne"], "answer_start": [438, 438, 438]}, "prediction_text": "Jason Bourne"}
{"answers": {"text": ["Captain America: Civil War", "Captain America: Civil War", "Captain America: Civil War"], "answer_start": [471, 471, 471]}, "prediction_text": "The Secret Life of Pets."}
{"answers": {"text": ["Independence Day", "Independence Day", "Independence Day: Resurgence"], "answer_start": [192, 192, 192]}, "prediction_text": "X-Men: Apocalypse"}
{"answers": {"text": ["Universal", "Universal", "Universal"], "answer_start": [49, 369, 369]}, "prediction_text": "Disney"}
{"answers": {"text": ["Fox", "Fox", "Disney"], "answer_start": [13, 150, 455]}, "prediction_text": "Lionsgate"}
{"answers": {"text": ["Westwood One", "Westwood One", "Westwood One"], "answer_start": [0, 0, 0]}, "prediction_text": "Westwood One"}
{"answers": {"text": ["Kevin Harlan", "Kevin Harlan", "Kevin Harlan"], "answer_start": [64, 64, 64]}, "prediction_text": "Kevin Harlan"}
{"answers": {"text": ["Jim Gray", "Jim Gray", ". Jim Gray"], "answer_start": [208, 208, 206]}, "prediction_text": "Jim Gray"}
{"answers": {"text": ["Boomer Esiason", "Boomer Esiason", "Boomer Esiason"], "answer_start": [104, 104, 104]}, "prediction_text": "James Lofton"}
{"answers": {"text": ["James Lofton", "James Lofton", "James Lofton and Mark Malone"], "answer_start": [156, 156, 156]}, "prediction_text": "James Lofton"}
{"answers": {"text": ["two", "Boomer Esiason and Dan Fouts"], "answer_start": [3, 104]}, "prediction_text": "Dan Fouts, James Lofton, Mark Malone."}
{"answers": {"text": ["Kevin Harlan", "Kevin Harlan", "Kevin Harlan"], "answer_start": [64, 64, 64]}, "prediction_text": "Kevin Harlan"}
{"answers": {"text": ["Westwood One", "Westwood One", "Westwood One"], "answer_start": [0, 0, 0]}, "prediction_text": "Kevin Harlan"}
{"answers": {"text": ["Kevin Harlan", "Kevin Harlan", "Kevin Harlan"], "answer_start": [64, 64, 64]}, "prediction_text": "Kevin Harlan"}
{"answers": {"text": ["Jim Gray", "Jim Gray", "Jim Gray"], "answer_start": [208, 208, 208]}, "prediction_text": "Jim Gray"}
{"answers": {"text": ["Kevin Harlan", "Kevin Harlan", "Kevin Harlan"], "answer_start": [64, 64, 64]}, "prediction_text": "Kevin Harlan"}
{"answers": {"text": ["Boomer Esiason and Dan Fouts", "Boomer Esiason and Dan Fouts", "Boomer Esiason and Dan Fouts"], "answer_start": [104, 104, 104]}, "prediction_text": "James Lofton, Mark Malone, Dan Fouts."}
{"answers": {"text": ["James Lofton and Mark Malone", "James Lofton and Mark Malone", "James Lofton and Mark Malone"], "answer_start": [156, 156, 156]}, "prediction_text": "James Lofton, Mark Malone, Dan Fouts."}
{"answers": {"text": ["pre-game and halftime coverage.", "halftime", "anchor the pre-game and halftime coverage"], "answer_start": [233, 246, 222]}, "prediction_text": "Pre-game and halftime coverage."}
{"answers": {"text": ["North America", "North America", "North America"], "answer_start": [44, 44, 44]}, "prediction_text": "North America"}
{"answers": {"text": ["KRFX", "KRFX", "KRFX"], "answer_start": [137, 137, 137]}, "prediction_text": "KRFX"}
{"answers": {"text": ["Dave Logan", "KOA (850 AM) and KRFX (103.5 FM)", "Dave Logan"], "answer_start": [179, 120, 179]}, "prediction_text": "Dave Logan"}
{"answers": {"text": ["1110 AM", "1110 AM", "(1110 AM)"], "answer_start": [268, 268, 267]}, "prediction_text": "1110 AM."}
{"answers": {"text": ["Chester, South Carolina", "Chester", "Chester, South Carolina"], "answer_start": [476, 476, 476]}, "prediction_text": "Chester, South Carolina"}
{"answers": {"text": ["Mick Mixon", "Mick Mixon", "Mick Mixon"], "answer_start": [303, 303, 303]}, "prediction_text": "Eugene Robinson"}
{"answers": {"text": ["Dave Logan", "Dave Logan", "Dave Logan"], "answer_start": [179, 179, 179]}, "prediction_text": "Dave Logan"}
{"answers": {"text": ["Ed McCaffrey", "Ed McCaffrey", "Ed McCaffrey"], "answer_start": [210, 210, 210]}, "prediction_text": "Ed McCaffrey"}
{"answers": {"text": ["WBT", "WBT (1110 AM)", "WBT (1110 AM)"], "answer_start": [263, 263, 263]}, "prediction_text": "WBT-FM (99.3 FM)"}
{"answers": {"text": ["Mick Mixon", "Mick Mixon", "Mick Mixon"], "answer_start": [303, 303, 303]}, "prediction_text": "Eugene Robinson"}
{"answers": {"text": ["KOA (850 AM) and KRFX (103.5 FM)", "KOA (850 AM) and KRFX (103.5 FM)", "KOA (850 AM) and KRFX (103.5 FM)"], "answer_start": [120, 120, 120]}, "prediction_text": "KOA (850 AM) and KRFX (103.5 FM)"}
{"answers": {"text": ["WBT-FM (99.3 FM)", "WBT (1110 AM)", "WBT-FM"], "answer_start": [440, 263, 440]}, "prediction_text": "WBT-FM (99.3 FM)"}
{"answers": {"text": ["BBC Radio 5", "Radio 5 Live", "BBC Radio 5 Live"], "answer_start": [23, 27, 23]}, "prediction_text": "BBC Radio 5 Live"}
{"answers": {"text": ["5 Live Sports Extra", "5 Live Sports Extra", "5 Live Sports Extra"], "answer_start": [44, 44, 44]}, "prediction_text": "5 Live Sports Extra"}
{"answers": {"text": ["Darren Fletcher", "Darren Fletcher", "Darren Fletcher"], "answer_start": [159, 159, 159]}, "prediction_text": "Darren Fletcher"}
{"answers": {"text": ["BBC", "BBC", "BBC"], "answer_start": [23, 23, 92]}, "prediction_text": "BBC"}
{"answers": {"text": ["Greg Brady", "Greg Brady", "Greg Brady"], "answer_start": [147, 147, 147]}, "prediction_text": "Greg Brady"}
{"answers": {"text": ["Bart Starr", "Bart Starr", "Starr"], "answer_start": [148, 148, 153]}, "prediction_text": "Bart Starr"}
{"answers": {"text": ["Chuck Howley", "Chuck Howley", "Howley"], "answer_start": [193, 193, 199]}, "prediction_text": "Chuck Howley"}
{"answers": {"text": ["Peyton Manning", "Peyton Manning", "Manning"], "answer_start": [254, 254, 261]}, "prediction_text": "Bart Starr"}
{"answers": {"text": ["2001", "2001", "2001"], "answer_start": [484, 484, 484]}, "prediction_text": "2001"}
{"answers": {"text": ["Peyton Manning", "Peyton Manning", "Manning"], "answer_start": [254, 254, 261]}, "prediction_text": "Peyton Manning"}
{"answers": {"text": ["39", "39", "39"], "answer_start": [92, 92, 92]}, "prediction_text": "39"}
{"answers": {"text": ["Peyton Manning", "Peyton Manning", "Manning"], "answer_start": [254, 254, 261]}, "prediction_text": "Peyton Manning"}
{"answers": {"text": ["Harvey Martin", "Harvey Martin", "Martin"], "answer_start": [431, 431, 438]}, "prediction_text": "Harvey Martin"}
{"answers": {"text": ["43", "43", "43"], "answer_start": [102, 102, 102]}, "prediction_text": "39"}
{"answers": {"text": ["39", "39", "39"], "answer_start": [92, 92, 92]}, "prediction_text": "39"}
{"answers": {"text": ["Bart Starr", "Bart Starr", "Starr"], "answer_start": [148, 148, 153]}, "prediction_text": "Bart Starr"}
{"answers": {"text": ["Peyton Manning", "Peyton Manning", "Manning"], "answer_start": [254, 254, 261]}, "prediction_text": "Peyton Manning"}
{"answers": {"text": ["Harvey Martin", "Harvey Martin", "Harvey Martin"], "answer_start": [431, 431, 431]}, "prediction_text": "Harvey Martin"}
{"answers": {"text": ["Six", "Six", "Six"], "answer_start": [0, 0, 0]}, "prediction_text": "Six"}
{"answers": {"text": ["the national anthem", "the national anthem", "national anthem"], "answer_start": [69, 69, 73]}, "prediction_text": "\"America\""}
{"answers": {"text": ["Academy Award", "Academy Award", "Academy Award"], "answer_start": [27, 96, 96]}, "prediction_text": "Academy Award"}
{"answers": {"text": ["the national anthem", "the national anthem", "national anthem"], "answer_start": [69, 69, 73]}, "prediction_text": "American Sign Language (ASL) translation."}
{"answers": {"text": ["American Sign Language", "American Sign Language", "American Sign Language"], "answer_start": [140, 140, 140]}, "prediction_text": "ASL"}
{"answers": {"text": ["Lady Gaga", "Lady Gaga", "Lady Gaga"], "answer_start": [49, 49, 49]}, "prediction_text": "Lady Gaga"}
{"answers": {"text": ["Marlee Matlin", "Marlee Matlin", "Marlee Matlin"], "answer_start": [117, 117, 117]}, "prediction_text": "Marlee Matlin"}
{"answers": {"text": ["Lady Gaga", "Lady Gaga", "Lady Gaga"], "answer_start": [49, 49, 49]}, "prediction_text": "Lady Gaga"}
{"answers": {"text": ["Marlee Matlin", "Marlee Matlin", "Marlee Matlin"], "answer_start": [117, 117, 117]}, "prediction_text": "Marlee Matlin"}
{"answers": {"text": ["Lady Gaga", "Lady Gaga", "Lady Gaga"], "answer_start": [49, 49, 49]}, "prediction_text": "Lady Gaga"}
{"answers": {"text": ["Six", "Six", "Six"], "answer_start": [0, 0, 0]}, "prediction_text": "Six."}
{"answers": {"text": ["Marlee Matlin", "Marlee Matlin", "Marlee Matlin"], "answer_start": [117, 117, 117]}, "prediction_text": "Marlee Matlin"}
{"answers": {"text": ["December 3", "December 3", "December 3"], "answer_start": [112, 112, 112]}, "prediction_text": "December 3."}
{"answers": {"text": ["British", "British", "British"], "answer_start": [185, 185, 185]}, "prediction_text": "British"}
{"answers": {"text": ["Super Bowl XLVII", "Super Bowl XLVII", "XLVII"], "answer_start": [306, 306, 317]}, "prediction_text": "Super Bowl XLVII"}
{"answers": {"text": ["\"Hymn for the Weekend\"", "Hymn for the Weekend", "Hymn for the Weekend"], "answer_start": [382, 383, 383]}, "prediction_text": "Hymn for the Weekend"}
{"answers": {"text": ["Super Bowl XLVIII", "Super Bowl XLVIII", "XLVIII"], "answer_start": [467, 467, 478]}, "prediction_text": "Super Bowl XLVIII."}
{"answers": {"text": ["Coldplay.", "Coldplay", "Coldplay"], "answer_start": [204, 204, 204]}, "prediction_text": "Coldplay"}
{"answers": {"text": ["Pepsi", "Pepsi", "Pepsi"], "answer_start": [234, 234, 234]}, "prediction_text": "Pepsi"}
{"answers": {"text": ["\"Hymn for the Weekend\"", "Hymn for the Weekend", "Hymn for the Weekend"], "answer_start": [382, 383, 383]}, "prediction_text": "\"Hymn for the Weekend\""}
{"answers": {"text": ["Coldplay.", "Coldplay", "Coldplay"], "answer_start": [204, 204, 204]}, "prediction_text": "Coldplay"}
{"answers": {"text": ["Beyonc\u00e9", "Beyonc\u00e9", "Beyonc\u00e9"], "answer_start": [279, 279, 279]}, "prediction_text": "Bruno Mars"}
{"answers": {"text": ["Hymn for the Weekend", "Hymn for the Weekend", "Mars"], "answer_start": [383, 383, 443]}, "prediction_text": "\"Hymn for the Weekend\""}
{"answers": {"text": ["Bruno Mars", "Bruno Mars", "Bruno Mars,"], "answer_start": [437, 437, 437]}, "prediction_text": "Bruno Mars"}
{"answers": {"text": ["Denver", "Denver", "Denver"], "answer_start": [0, 0, 0]}, "prediction_text": "Denver"}
{"answers": {"text": ["Andre Caldwell", "Andre Caldwell", "Caldwell"], "answer_start": [160, 160, 166]}, "prediction_text": "Andre Caldwell"}
{"answers": {"text": ["Ronnie Hillman", "Ronnie Hillman", "Hillman"], "answer_start": [366, 366, 373]}, "prediction_text": "Ronnie Hillman"}
{"answers": {"text": ["Brandon McManus", "Brandon McManus", "McManus"], "answer_start": [536, 536, 544]}, "prediction_text": "Thomas Davis"}
{"answers": {"text": ["C. J. Anderson", "C. J. Anderson", "Anderson"], "answer_start": [197, 197, 203]}, "prediction_text": "Thomas Davis"}
{"answers": {"text": ["18", "18", "22-yard"], "answer_start": [89, 89, 134]}, "prediction_text": "18 yards."}
{"answers": {"text": ["Shaq Thompson", "Shaq Thompson", "Thompson"], "answer_start": [344, 344, 349]}, "prediction_text": "Shaq Thompson"}
{"answers": {"text": ["Brandon McManus", "Brandon McManus", "McManus"], "answer_start": [536, 536, 544]}, "prediction_text": "Thomas Davis"}
{"answers": {"text": ["a deficit.", "a deficit", "deficit"], "answer_start": [654, 654, 656]}, "prediction_text": "A deficit."}
{"answers": {"text": ["Denver", "Carolina", "Denver"], "answer_start": [0, 634, 0]}, "prediction_text": "Denver"}
{"answers": {"text": ["Owen Daniels", "Owen Daniels", "Daniels"], "answer_start": [115, 115, 120]}, "prediction_text": "Owen Daniels"}
{"answers": {"text": ["C. J. Anderson", "C. J. Anderson", "Anderson"], "answer_start": [197, 197, 203]}, "prediction_text": "Owen Daniels"}
{"answers": {"text": ["Brandon McManus", "Brandon McManus", "McManus"], "answer_start": [536, 536, 544]}, "prediction_text": "Ronnie Hillman"}
{"answers": {"text": ["a deficit.", "a deficit", "deficit"], "answer_start": [654, 654, 656]}, "prediction_text": "A deficit."}
{"answers": {"text": ["Mike Carey", "Mike Carey", "Carey"], "answer_start": [219, 219, 224]}, "prediction_text": "Mike Carey"}
{"answers": {"text": ["Cam Newton", "Cam Newton", "Newton"], "answer_start": [45, 45, 49]}, "prediction_text": "Cam Newton"}
{"answers": {"text": ["Von Miller", "Von Miller", "Miller"], "answer_start": [389, 389, 393]}, "prediction_text": "Von Miller"}
{"answers": {"text": ["Malik Jackson", "Malik Jackson", "Jackson"], "answer_start": [462, 462, 468]}, "prediction_text": "Malik Jackson"}
{"answers": {"text": ["Super Bowl XXVIII", "Super Bowl XXVIII", "XXVIII"], "answer_start": [624, 624, 635]}, "prediction_text": "Super Bowl XXVIII"}
{"answers": {"text": ["Jerricho Cotchery", "Jerricho Cotchery", "Cotchery"], "answer_start": [92, 92, 101]}, "prediction_text": "Jerricho Cotchery"}
{"answers": {"text": ["Mike Carey", "Mike Carey", "Carey"], "answer_start": [219, 219, 224]}, "prediction_text": "Mike Carey"}
{"answers": {"text": ["Von Miller", "Von Miller", "Miller"], "answer_start": [389, 389, 393]}, "prediction_text": "Von Miller"}
{"answers": {"text": ["Malik Jackson", "Malik Jackson", "Jackson"], "answer_start": [462, 462, 468]}, "prediction_text": "Von Miller"}
{"answers": {"text": ["1993", "1993", "1993"], "answer_start": [660, 660, 660]}, "prediction_text": "1993"}
{"answers": {"text": ["Mike Carey", "Mike Carey", "Carey"], "answer_start": [219, 219, 224]}, "prediction_text": "Mike Carey"}
{"answers": {"text": ["Von Miller", "Von Miller", "Miller"], "answer_start": [389, 389, 393]}, "prediction_text": "Von Miller"}
{"answers": {"text": ["Malik Jackson", "Malik Jackson", "Jackson"], "answer_start": [462, 462, 468]}, "prediction_text": "Malik Jackson"}
{"answers": {"text": ["Super Bowl XXVIII", "the end of the 1993 season", "1993"], "answer_start": [624, 645, 660]}, "prediction_text": "1993"}
{"answers": {"text": ["Jonathan Stewart", "Jonathan Stewart", "Stewart"], "answer_start": [171, 171, 180]}, "prediction_text": "Jonathan Stewart"}
{"answers": {"text": ["Brad Nortman", "Brad Nortman", "Nortman"], "answer_start": [352, 352, 357]}, "prediction_text": "Newton"}
{"answers": {"text": ["28", "28", "28"], "answer_start": [270, 373, 373]}, "prediction_text": "61 yards."}
{"answers": {"text": ["61", "61", "61"], "answer_start": [612, 612, 612]}, "prediction_text": "61 yards."}
{"answers": {"text": ["33", "33", "33"], "answer_start": [805, 805, 805]}, "prediction_text": "33 yards."}
{"answers": {"text": ["51", "51", "51"], "answer_start": [125, 125, 125]}, "prediction_text": "51 yards."}
{"answers": {"text": ["Jonathan Stewart", "Jonathan Stewart", "Stewart"], "answer_start": [171, 171, 180]}, "prediction_text": "Jordan Norwood"}
{"answers": {"text": ["11:28", "11:28", "11:28"], "answer_start": [267, 267, 267]}, "prediction_text": "11:28."}
{"answers": {"text": ["Jordan Norwood", "Jordan Norwood", "Norwood"], "answer_start": [328, 328, 516]}, "prediction_text": "Jordan Norwood"}
{"answers": {"text": ["33", "33", "33"], "answer_start": [805, 805, 805]}, "prediction_text": "33 yards."}
{"answers": {"text": ["Jonathan Stewart", "Jonathan Stewart", "Stewart"], "answer_start": [171, 171, 180]}, "prediction_text": "Jonathan Stewart"}
{"answers": {"text": ["field goal", "field goal", "field goal"], "answer_start": [813, 813, 813]}, "prediction_text": "Mario Addison's 61-yard return."}
{"answers": {"text": ["Darian Stewart", "Darian Stewart", "Stewart"], "answer_start": [96, 96, 103]}, "prediction_text": "Darian Stewart"}
{"answers": {"text": ["linebacker", "linebacker", "linebacker"], "answer_start": [118, 118, 118]}, "prediction_text": "Safety"}
{"answers": {"text": ["Kony Ealy", "Kony Ealy", "Ealy"], "answer_start": [249, 249, 254]}, "prediction_text": "Danny Trevathan"}
{"answers": {"text": ["Newton", "Newton", "Newton"], "answer_start": [571, 571, 571]}, "prediction_text": "DeMarcus Ware"}
{"answers": {"text": ["DeMarcus Ware", "DeMarcus Ware", "Ware"], "answer_start": [592, 592, 601]}, "prediction_text": "DeMarcus Ware"}
{"answers": {"text": ["Mike Tolbert", "Mike Tolbert", "Tolbert"], "answer_start": [39, 39, 44]}, "prediction_text": "Mike Tolbert"}
{"answers": {"text": ["Kony Ealy", "Kony Ealy", "Ealy"], "answer_start": [249, 249, 254]}, "prediction_text": "Danny Trevathan"}
{"answers": {"text": ["19", "19", "19"], "answer_start": [336, 336, 336]}, "prediction_text": "19 yards."}
{"answers": {"text": ["DeMarcus Ware", "DeMarcus Ware", "Ware"], "answer_start": [592, 592, 601]}, "prediction_text": "Darian Stewart"}
{"answers": {"text": ["Mike Tolbert", "Mike Tolbert", "Tolbert"], "answer_start": [39, 39, 44]}, "prediction_text": "Mike Tolbert"}
{"answers": {"text": ["Danny Trevathan", "Danny Trevathan", "Stewart"], "answer_start": [129, 129, 103]}, "prediction_text": "Danny Trevathan"}
{"answers": {"text": ["Kony Ealy", "Kony Ealy", "Ealy"], "answer_start": [249, 249, 254]}, "prediction_text": "Kony Ealy"}
{"answers": {"text": ["punt", "Newton was sacked", "sacked"], "answer_start": [474, 571, 582]}, "prediction_text": "With 11 seconds left."}
{"answers": {"text": ["DeMarcus Ware", "DeMarcus Ware", "Ware"], "answer_start": [592, 592, 601]}, "prediction_text": "DeMarcus Ware"}
{"answers": {"text": ["Ted Ginn Jr.", "Ted Ginn Jr", "Ted Ginn Jr"], "answer_start": [118, 118, 118]}, "prediction_text": "Ted Ginn Jr."}
{"answers": {"text": ["Graham Gano", "Graham Gano", "Gano"], "answer_start": [285, 285, 292]}, "prediction_text": "T. J. Ward"}
{"answers": {"text": ["44", "44", "44"], "answer_start": [319, 319, 319]}, "prediction_text": "33 yards."}
{"answers": {"text": ["McManus", "McManus", "McManus"], "answer_start": [459, 459, 459]}, "prediction_text": "Graham Gano"}
{"answers": {"text": ["T. J. Ward", "T. J. Ward", "Ward"], "answer_start": [775, 775, 781]}, "prediction_text": "T. J. Ward"}
{"answers": {"text": ["Ted Ginn Jr.", "Ted Ginn Jr.", "Ted Ginn Jr"], "answer_start": [118, 118, 118]}, "prediction_text": "Ted Ginn Jr."}
{"answers": {"text": ["the uprights", "the uprights", "uprights"], "answer_start": [301, 301, 305]}, "prediction_text": "The uprights."}
{"answers": {"text": ["T. J. Ward.", "T. J. Ward", "Ward"], "answer_start": [775, 775, 781]}, "prediction_text": "T. J. Ward"}
{"answers": {"text": ["Trevathan", "Trevathan", "Trevathan"], "answer_start": [832, 832, 832]}, "prediction_text": "Trevathan"}
{"answers": {"text": ["Ted Ginn Jr.", "Ted Ginn Jr"], "answer_start": [118, 118]}, "prediction_text": "Ted Ginn Jr."}
{"answers": {"text": ["26-yard line", "26"], "answer_start": [238, 238]}, "prediction_text": "26 yards."}
{"answers": {"text": ["Graham Gano", "Graham Gano", "Gano"], "answer_start": [285, 285, 292]}, "prediction_text": "T. J. Ward"}
{"answers": {"text": ["Emmanuel Sanders", "Emmanuel Sanders", "Sanders"], "answer_start": [401, 401, 410]}, "prediction_text": "T. J. Ward"}
{"answers": {"text": ["Ealy", "Ealy", "Ealy"], "answer_start": [144, 144, 144]}, "prediction_text": "Ealy"}
{"answers": {"text": ["39", "39", "39"], "answer_start": [363, 363, 363]}, "prediction_text": "39 yards."}
{"answers": {"text": ["Devin Funchess", "Devin Funchess", "Funchess"], "answer_start": [300, 300, 306]}, "prediction_text": "Devin Funchess"}
{"answers": {"text": ["Stewart", "Stewart", "Stewart"], "answer_start": [336, 336, 336]}, "prediction_text": "Devin Funchess"}
{"answers": {"text": ["41-yard line.", "41", "41"], "answer_start": [112, 112, 112]}, "prediction_text": "41 yard line."}
{"answers": {"text": ["Ealy", "Ealy", "Ealy"], "answer_start": [144, 144, 144]}, "prediction_text": "Ealy"}
{"answers": {"text": ["50-yard line.", "on the 50-yard line", "50"], "answer_start": [263, 256, 263]}, "prediction_text": "50-yard line."}
{"answers": {"text": ["punts.", "punts", "punts"], "answer_start": [482, 482, 482]}, "prediction_text": "Punts."}
{"answers": {"text": ["Ealy", "Ealy", "Ealy"], "answer_start": [144, 144, 144]}, "prediction_text": "Ealy"}
{"answers": {"text": ["50-yard line.", "41", "50"], "answer_start": [263, 112, 263]}, "prediction_text": "50 yards."}
{"answers": {"text": ["39-yard", "39", "39"], "answer_start": [363, 363, 363]}, "prediction_text": "39 yards."}
{"answers": {"text": ["three", "three", "The next three drives"], "answer_start": [444, 444, 435]}, "prediction_text": "3"}
{"answers": {"text": ["24", "their own 24", "24"], "answer_start": [65, 55, 65]}, "prediction_text": "24-yard line."}
{"answers": {"text": ["Newton", "Newton", "Newton"], "answer_start": [202, 202, 434]}, "prediction_text": "Josh Norman"}
{"answers": {"text": ["Josh Norman", "Josh Norman", "Norman"], "answer_start": [620, 620, 625]}, "prediction_text": "Josh Norman"}
{"answers": {"text": ["Anderson", "Anderson", "Anderson"], "answer_start": [674, 674, 674]}, "prediction_text": "Josh Norman"}
{"answers": {"text": ["Bennie Fowler", "Bennie Fowler", "Fowler"], "answer_start": [748, 748, 755]}, "prediction_text": "Bennie Fowler"}
{"answers": {"text": ["Miller", "Miller", "Miller"], "answer_start": [167, 167, 167]}, "prediction_text": "Ward"}
{"answers": {"text": ["wards", "Ward", "Ward"], "answer_start": [275, 302, 302]}, "prediction_text": "Ward"}
{"answers": {"text": ["Newton", "Newton", "Newton"], "answer_start": [202, 434, 434]}, "prediction_text": "Josh Norman"}
{"answers": {"text": ["Josh Norman", "Josh Norman", "Norman"], "answer_start": [620, 620, 625]}, "prediction_text": "Josh Norman"}
{"answers": {"text": ["3:08", "3:08", "3:08"], "answer_start": [820, 820, 820]}, "prediction_text": "3:08."}
{"answers": {"text": ["4:51", "4:51", "4:51"], "answer_start": [5, 5, 5]}, "prediction_text": "4:51 left."}
{"answers": {"text": ["Miller", "Miller", "Miller"], "answer_start": [167, 167, 167]}, "prediction_text": "Ward"}
{"answers": {"text": ["wards", "Ward", "Ward"], "answer_start": [275, 302, 302]}, "prediction_text": "Josh Norman"}
{"answers": {"text": ["three", "three", "three"], "answer_start": [571, 571, 571]}, "prediction_text": "3 plays."}
{"answers": {"text": ["Anderson", "Anderson", "Anderson"], "answer_start": [674, 674, 674]}, "prediction_text": "Bennie Fowler"}
{"answers": {"text": ["five", "2\u00bd", "2"], "answer_start": [296, 308, 308]}, "prediction_text": "2\u00bd sacks."}
{"answers": {"text": ["zero", "zero", "zero"], "answer_start": [75, 75, 75]}, "prediction_text": "Zero."}
{"answers": {"text": ["four", "four", "four"], "answer_start": [236, 472, 472]}, "prediction_text": "Four."}
{"answers": {"text": ["Thomas Davis", "Thomas Davis", "Davis"], "answer_start": [1056, 1056, 1063]}, "prediction_text": "Thomas Davis"}
{"answers": {"text": ["one", "one", "one"], "answer_start": [54, 612, 612]}, "prediction_text": "0"}
{"answers": {"text": ["one", "one", "one"], "answer_start": [54, 54, 54]}, "prediction_text": "0"}
{"answers": {"text": ["zero", "zero", "zero"], "answer_start": [75, 75, 75]}, "prediction_text": "Zero."}
{"answers": {"text": ["Anderson", "Anderson", "Anderson"], "answer_start": [155, 155, 155]}, "prediction_text": "Sanders"}
{"answers": {"text": ["Sanders", "Sanders", "Sanders"], "answer_start": [92, 92, 92]}, "prediction_text": "Sanders"}
{"answers": {"text": ["Thomas Davis", "Thomas Davis", "Davis"], "answer_start": [1056, 1056, 1063]}, "prediction_text": "Thomas Davis"}
{"answers": {"text": ["Sanders", "Sanders", "Sanders"], "answer_start": [92, 92, 92]}, "prediction_text": "Sanders"}
{"answers": {"text": ["Anderson", "Anderson", "Anderson"], "answer_start": [155, 155, 155]}, "prediction_text": "Sanders"}
{"answers": {"text": ["all four", "all four", "four"], "answer_start": [468, 468, 472]}, "prediction_text": "Four."}
{"answers": {"text": ["one", "one", "one"], "answer_start": [54, 612, 612]}, "prediction_text": "4"}
{"answers": {"text": ["four", "four", "four"], "answer_start": [236, 955, 955]}, "prediction_text": "Four."}
{"answers": {"text": ["194", "194", "194"], "answer_start": [136, 136, 136]}, "prediction_text": "194 yards."}
{"answers": {"text": ["11", "11", "11"], "answer_start": [164, 164, 164]}, "prediction_text": "11"}
{"answers": {"text": ["Baltimore Ravens", "the Baltimore Ravens", "Ravens"], "answer_start": [308, 304, 318]}, "prediction_text": "Baltimore Ravens"}
{"answers": {"text": ["Jordan Norwood", "Jordan Norwood", "Norwood"], "answer_start": [593, 593, 600]}, "prediction_text": "Jordan Norwood"}
{"answers": {"text": ["Manning", "Manning", "Manning"], "answer_start": [897, 1069, 1069]}, "prediction_text": "John Taylor"}
{"answers": {"text": ["194", "194", "194"], "answer_start": [136, 136, 136]}, "prediction_text": "194 yards."}
{"answers": {"text": ["11", "11", "11"], "answer_start": [164, 164, 164]}, "prediction_text": "11"}
{"answers": {"text": ["Chicago Bears", "the Chicago Bears", "Bears"], "answer_start": [508, 504, 516]}, "prediction_text": "Jordan Norwood."}
{"answers": {"text": ["Broncos", "Denver", "Denver"], "answer_start": [451, 726, 68]}, "prediction_text": "Baltimore Ravens"}
{"answers": {"text": ["21", "21", "21"], "answer_start": [158, 158, 158]}, "prediction_text": "11."}
{"answers": {"text": ["11", "11", "11"], "answer_start": [164, 164, 164]}, "prediction_text": "11"}
{"answers": {"text": ["The Broncos", "Denver", "Denver"], "answer_start": [447, 68, 68]}, "prediction_text": "Baltimore Ravens"}
{"answers": {"text": ["Chicago Bears", "the Chicago Bears", "Bears"], "answer_start": [508, 504, 516]}, "prediction_text": "Kony Ealy"}
{"answers": {"text": ["two", "two", "two"], "answer_start": [814, 1199, 1199]}, "prediction_text": "2"}
{"answers": {"text": ["Nobel Prize", "Nobel Prize", "Nobel Prize"], "answer_start": [188, 188, 188]}, "prediction_text": "Nobel Prize"}
{"answers": {"text": ["1745", "1745", "1745"], "answer_start": [517, 517, 517]}, "prediction_text": "1745"}
{"answers": {"text": ["Maria Sk\u0142odowska-Curie", "Maria Sk\u0142odowska-Curie", "Maria Sk\u0142odowska-Curie"], "answer_start": [49, 49, 49]}, "prediction_text": "Maria Sk\u0142odowska-Curie"}
{"answers": {"text": ["Famous musicians", "musicians", "Famous musicians"], "answer_start": [201, 208, 201]}, "prediction_text": "W\u0142adys\u0142aw Szpilman"}
{"answers": {"text": ["seven months old", "seven months old", "seven months old"], "answer_start": [404, 404, 404]}, "prediction_text": "Seven months old."}
{"answers": {"text": ["100", "over 100", "100"], "answer_start": [92, 87, 92]}, "prediction_text": "100"}
{"answers": {"text": ["Krasi\u0144ski Palace Garden", "Krasi\u0144ski Palace Garden", "Krasi\u0144ski Palace Garden"], "answer_start": [579, 579, 579]}, "prediction_text": "Krasi\u0144ski Palace Garden"}
{"answers": {"text": ["The Saxon Garden", "Saxon Garden", "The Saxon Garden"], "answer_start": [0, 4, 0]}, "prediction_text": "Saxon Garden"}
{"answers": {"text": ["east end", "east", "east"], "answer_start": [176, 176, 176]}, "prediction_text": "East end."}
{"answers": {"text": ["\u0141azienki", "\u0141azienki Park", "\u0141azienki Park"], "answer_start": [728, 728, 728]}, "prediction_text": "Wilan\u00f3w Palace Park"}
{"answers": {"text": ["15 kilometres", "15 kilometres", "About 15 kilometres"], "answer_start": [109, 109, 103]}, "prediction_text": "15 kilometers (9 miles)"}
{"answers": {"text": ["otter, beaver and hundreds of bird species", "otter, beaver and hundreds of bird species", "otter, beaver and hundreds of bird species."], "answer_start": [287, 287, 287]}, "prediction_text": "Otter, beaver, birds."}
{"answers": {"text": ["13", "13", "13"], "answer_start": [10, 10, 10]}, "prediction_text": "13"}
{"answers": {"text": ["several", "several", "several"], "answer_start": [346, 346, 346]}, "prediction_text": "15 kilometers (9 miles)"}
{"answers": {"text": ["to clean them", "clean them", "to clean them of plants and sediments"], "answer_start": [590, 593, 590]}, "prediction_text": "Cleaning plants and sediments."}
{"answers": {"text": ["city", "Demographically", "city"], "answer_start": [41, 0, 41]}, "prediction_text": "Demographically."}
{"answers": {"text": ["833,500", "833,500", "833,500"], "answer_start": [476, 476, 476]}, "prediction_text": "833,500"}
{"answers": {"text": ["around 34%", "34", "34%"], "answer_start": [301, 308, 308]}, "prediction_text": "34%"}
{"answers": {"text": ["Jewish", "Jewish", "Jewish"], "answer_start": [171, 171, 171]}, "prediction_text": "Jewish minority"}
{"answers": {"text": ["migration and urbanisation", "internal migration and urbanisation", "internal migration and urbanisation."], "answer_start": [731, 722, 722]}, "prediction_text": "Internal migration and urbanisation."}
{"answers": {"text": ["Warsaw University of Technology", "Warsaw University of Technology", "Warsaw University of Technology"], "answer_start": [169, 169, 169]}, "prediction_text": "Warsaw University of Technology"}
{"answers": {"text": ["2,000", "2,000", "2,000"], "answer_start": [318, 318, 318]}, "prediction_text": "2,000"}
{"answers": {"text": ["Medical University of Warsaw", "National Defence University", "Medical University of Warsaw"], "answer_start": [388, 492, 388]}, "prediction_text": "National Defence University"}
{"answers": {"text": ["1816", "1816", "1816"], "answer_start": [44, 44, 44]}, "prediction_text": "1816"}
{"answers": {"text": ["Fryderyk Chopin University of Music", "Fryderyk Chopin University of Music", "Fryderyk Chopin University of Music"], "answer_start": [574, 574, 574]}, "prediction_text": "National Defence University"}
{"answers": {"text": ["1816", "1816", "1816"], "answer_start": [63, 63, 63]}, "prediction_text": "1816"}
{"answers": {"text": ["over two million", "over two million", "over two million"], "answer_start": [80, 80, 80]}, "prediction_text": "2 million."}
{"answers": {"text": ["architects", "architects", "architects"], "answer_start": [133, 133, 133]}, "prediction_text": "Architect"}
{"answers": {"text": ["Irena Bajerska", "Irena Bajerska", "Irena Bajerska"], "answer_start": [284, 284, 284]}, "prediction_text": "Marek Budzy\u0144ski and Zbigniew Badowski."}
{"answers": {"text": ["10,000 m2", "more than 10,000 m2", "10,000 m2"], "answer_start": [421, 411, 421]}, "prediction_text": "5,111 m2 (55,014.35 sq ft)"}
{"answers": {"text": ["infrastructure", "infrastructure", "infrastructure"], "answer_start": [48, 48, 48]}, "prediction_text": "infrastructure"}
{"answers": {"text": ["Three-Year Plan", "Three-Year Plan", "Three-Year Plan"], "answer_start": [187, 187, 187]}, "prediction_text": "Three-Year Plan to rebuild Poland."}
{"answers": {"text": ["solid economic growth", "solid economic growth", "economic growth"], "answer_start": [379, 379, 385]}, "prediction_text": "Solid economic growth, foreign investment, funding from the European Union."}
{"answers": {"text": ["improved markedly", "improved markedly", "improved markedly"], "answer_start": [585, 585, 585]}, "prediction_text": "Improved."}
{"answers": {"text": ["Warsaw", "Warsaw", "Warsaw"], "answer_start": [7, 7, 7]}, "prediction_text": "Warsaw"}
{"answers": {"text": ["Children's Memorial Health Institute", "Children's Memorial Health Institute", "Children's Memorial Health Institute"], "answer_start": [113, 113, 113]}, "prediction_text": "Children's Memorial Health Institute (CMHI)"}
{"answers": {"text": ["Maria Sk\u0142odowska-Curie Institute of Oncology", "Maria Sk\u0142odowska-Curie Institute of Oncology", "Maria Sk\u0142odowska-Curie Institute of Oncology"], "answer_start": [269, 269, 269]}, "prediction_text": "Maria Sk\u0142odowska-Curie Institute of Oncology"}
{"answers": {"text": ["700", "700", "700"], "answer_start": [451, 451, 451]}, "prediction_text": "700 beds."}
{"answers": {"text": ["developed", "developed", "developed"], "answer_start": [595, 595, 595]}, "prediction_text": "Developed."}
{"answers": {"text": ["musical", "musical", "musical"], "answer_start": [19, 19, 19]}, "prediction_text": "Musical venues"}
{"answers": {"text": ["events and festivals", "events and festivals", "events and festivals"], "answer_start": [289, 289, 289]}, "prediction_text": "Events and festivals."}
{"answers": {"text": ["in the Palace of Culture and Science", "Palace of Culture and Science", "Palace of Culture and Science"], "answer_start": [233, 240, 240]}, "prediction_text": "Palace of Culture and Science."}
{"answers": {"text": ["Warsaw", "Warsaw", "Warsaw"], "answer_start": [271, 271, 271]}, "prediction_text": "Warsaw Summer Jazz Days"}
{"answers": {"text": ["festivals", "events"], "answer_start": [300, 321]}, "prediction_text": "Jazz Jamboree"}
{"answers": {"text": ["Ogr\u00f3d Saski", "Ogr\u00f3d Saski", "Ogr\u00f3d Saski"], "answer_start": [11, 11, 11]}, "prediction_text": "Ogr\u00f3d Saski"}
{"answers": {"text": ["Saxon Garden", "the Saxon Garden", "Saxon Garden"], "answer_start": [28, 24, 28]}, "prediction_text": "Ogr\u00f3d Saski (the Saxon Garden)"}
{"answers": {"text": ["1870 to 1939", "1870 to 1939", "1870 to 1939"], "answer_start": [84, 84, 84]}, "prediction_text": "1870-1939"}
{"answers": {"text": ["Momus", "Momus", "Momus"], "answer_start": [161, 161, 161]}, "prediction_text": "Momus"}
{"answers": {"text": ["Wojciech Bogus\u0142awski Theatre", "Wojciech Bogus\u0142awski Theatre", "Wojciech Bogus\u0142awski Theatre"], "answer_start": [251, 251, 251]}, "prediction_text": "Wojciech Bogus\u0142awski Theatre"}
{"answers": {"text": ["Wianki", "Wianki", "Wianki"], "answer_start": [157, 157, 157]}, "prediction_text": "Wianki"}
{"answers": {"text": ["thousands", "thousands", "thousands"], "answer_start": [66, 66, 66]}, "prediction_text": "Maidens."}
{"answers": {"text": ["Midsummer\u2019s Night", "Midsummer\u2019s Night", "Midsummer\u2019s Night"], "answer_start": [117, 117, 117]}, "prediction_text": "Midsummer\u2019s Night."}
{"answers": {"text": ["when they would be married", "when they would be married, and to whom", "when they would be married,"], "answer_start": [405, 405, 405]}, "prediction_text": "Marriage."}
{"answers": {"text": ["the fern", "fern", "fern"], "answer_start": [685, 689, 689]}, "prediction_text": "Fern flower"}
{"answers": {"text": ["art posters", "art posters", "art posters"], "answer_start": [140, 140, 140]}, "prediction_text": "Art posters."}
{"answers": {"text": ["60", "60", "60"], "answer_start": [239, 239, 239]}, "prediction_text": "60"}
{"answers": {"text": ["prestigious", "prestigious", "prestigious"], "answer_start": [260, 260, 260]}, "prediction_text": "prestigious museums"}
{"answers": {"text": ["some paintings", "paintings", "paintings"], "answer_start": [467, 472, 472]}, "prediction_text": "Works."}
{"answers": {"text": ["arms", "history of arms", "arms"], "answer_start": [586, 575, 586]}, "prediction_text": "Arms."}
{"answers": {"text": ["Warsaw Uprising Museum", "Warsaw Uprising Museum", "Warsaw Uprising Museum"], "answer_start": [79, 79, 79]}, "prediction_text": "Warsaw Uprising Museum"}
{"answers": {"text": ["Katy\u0144", "Katy\u0144 Museum", "Katy\u0144 Museum"], "answer_start": [113, 113, 113]}, "prediction_text": "Katy\u0144 Museum"}
{"answers": {"text": ["stereoscopic", "stereoscopic theatre", "stereoscopic"], "answer_start": [248, 248, 248]}, "prediction_text": "Stereoscopic theatre"}
{"answers": {"text": ["Museum of Independence", "Museum of Independence", "Museum of Independence"], "answer_start": [300, 300, 300]}, "prediction_text": "Warsaw Historical Museum"}
{"answers": {"text": ["60", "60", "60"], "answer_start": [471, 471, 471]}, "prediction_text": "60 rooms."}
{"answers": {"text": ["Royal Ujazd\u00f3w Castle", "Royal Ujazd\u00f3w Castle", "Royal Ujazd\u00f3w"], "answer_start": [17, 17, 17]}, "prediction_text": "Ujazd\u00f3w Castle"}
{"answers": {"text": ["about 500", "500", "about 500"], "answer_start": [201, 207, 201]}, "prediction_text": "500 projects a year."}
{"answers": {"text": ["Zach\u0119ta National Gallery of Art", "Zach\u0119ta National Gallery of Art", "Zach\u0119ta National Gallery of Art"], "answer_start": [228, 228, 228]}, "prediction_text": "Zach\u0119ta National Gallery of Art"}
{"answers": {"text": ["Polish and international artists", "modern art by Polish and international artists", "Polish and international artists a"], "answer_start": [395, 381, 395]}, "prediction_text": "Modern art."}
{"answers": {"text": ["last weekend of September", "last weekend of September", "last weekend of September"], "answer_start": [510, 510, 510]}, "prediction_text": "September."}
{"answers": {"text": ["Polonia Warsaw", "Polonia Warsaw", "Polonia Warsaw"], "answer_start": [20, 20, 20]}, "prediction_text": "Polonia Warsaw"}
{"answers": {"text": ["1946", "1946", "1946"], "answer_start": [175, 175, 175]}, "prediction_text": "1946"}
{"answers": {"text": ["twice", "twice", "twice"], "answer_start": [197, 197, 197]}, "prediction_text": "2 times."}
{"answers": {"text": ["at Konwiktorska Street", "Konwiktorska Street", "Konwiktorska Street"], "answer_start": [244, 247, 247]}, "prediction_text": "Konwiktorska Street"}
{"answers": {"text": ["disastrous financial situation", "their disastrous financial situation", "disastrous financial situation."], "answer_start": [388, 382, 388]}, "prediction_text": "Financial situation."}
{"answers": {"text": ["syrenka", "syrenka", "syrenka"], "answer_start": [13, 13, 13]}, "prediction_text": "Warsaw"}
{"answers": {"text": ["The mermaid", "mermaid", "The mermaid"], "answer_start": [0, 4, 0]}, "prediction_text": "Mermaid (syrenka)"}
{"answers": {"text": ["since at least the mid-14th century", "mid-14th century", "at least the mid-14th century"], "answer_start": [150, 169, 156]}, "prediction_text": "Since at least the mid-14th century."}
{"answers": {"text": ["1390", "1390", "1390"], "answer_start": [245, 245, 245]}, "prediction_text": "1390"}
{"answers": {"text": ["a sword", "sword", "sword"], "answer_start": [496, 498, 498]}, "prediction_text": "A sword."}
{"answers": {"text": ["legend", "The best-known legend", "legend"], "answer_start": [70, 55, 70]}, "prediction_text": "A mermaid."}
{"answers": {"text": ["depths of the oceans and seas", "depths of the oceans and seas", "depths of the oceans and seas"], "answer_start": [171, 171, 171]}, "prediction_text": "The depths of the oceans and seas."}
{"answers": {"text": ["coast of Denmark", "coast of Denmark", "coast of Denmark"], "answer_start": [237, 237, 237]}, "prediction_text": "Denmark"}
{"answers": {"text": ["Warszowa", "village of Warszowa", "Warszowa"], "answer_start": [463, 452, 463]}, "prediction_text": "Warszowa"}
{"answers": {"text": ["captured", "captured the mermaid", "captured"], "answer_start": [620, 620, 620]}, "prediction_text": "Captured her."}
{"answers": {"text": ["Warsaw", "Warsaw", "Warsaw"], "answer_start": [47, 47, 84]}, "prediction_text": "Warsaw"}
{"answers": {"text": ["1916", "1916", "1916"], "answer_start": [117, 117, 117]}, "prediction_text": "1916"}
{"answers": {"text": ["the Art Deco style", "Art Deco style in painting and art", "Art Deco style"], "answer_start": [204, 208, 208]}, "prediction_text": "Art Deco style."}
{"answers": {"text": ["poet", "poet", "poet"], "answer_start": [273, 273, 273]}, "prediction_text": "Poet"}
{"answers": {"text": ["Isaac Bashevis Singer", "Isaac Bashevis Singer", "Isaac Bashevis Singer"], "answer_start": [444, 444, 444]}, "prediction_text": "Isaac Bashevis Singer"}
{"answers": {"text": ["Economist Intelligence Unit", "Economist Intelligence Unit", "Economist Intelligence Unit"], "answer_start": [12, 12, 12]}, "prediction_text": "Economist Intelligence Unit"}
{"answers": {"text": ["2012", "2012", "2012"], "answer_start": [3, 3, 3]}, "prediction_text": "2012"}
{"answers": {"text": ["wide variety of industries", "FMCG manufacturing, metal processing, steel and electronic manufacturing and food processing", "FMCG manufacturing"], "answer_start": [345, 393, 393]}, "prediction_text": "FMCG manufacturing, metal processing, steel, electronic manufacturing, food processing."}
{"answers": {"text": ["Stock", "Warsaw Stock Exchange", "Warsaw Stock Exchange"], "answer_start": [611, 604, 604]}, "prediction_text": "Frontex"}
{"answers": {"text": ["Frontex", "Frontex", "Frontex"], "answer_start": [698, 698, 698]}, "prediction_text": "Frontex"}
{"answers": {"text": ["1313", "1313", "1313"], "answer_start": [64, 64, 64]}, "prediction_text": "1313"}
{"answers": {"text": ["Krak\u00f3w", "Krak\u00f3w", "Krak\u00f3w"], "answer_start": [85, 85, 85]}, "prediction_text": "Krak\u00f3w"}
{"answers": {"text": ["1596", "1596", "1596"], "answer_start": [390, 390, 390]}, "prediction_text": "1806"}
{"answers": {"text": ["King Sigismund III Vasa", "King Sigismund III Vasa", "King Sigismund III Vasa"], "answer_start": [325, 325, 325]}, "prediction_text": "King Sigismund III Vasa"}
{"answers": {"text": ["survived many wars, conflicts and invasions", "it has survived many wars, conflicts and invasions", "because it has survived many wars, conflicts and invasions throughout its long history"], "answer_start": [1250, 1243, 1235]}, "prediction_text": "Because it has survived many wars, conflicts, and invasions throughout its long history."}
{"answers": {"text": ["Roman Catholic", "Roman Catholic", "Roman Catholic"], "answer_start": [26, 26, 26]}, "prediction_text": "Roman Catholic archdiocese"}
{"answers": {"text": ["Polish Academy of Sciences", "Polish Academy of Sciences", "Polish Academy of Sciences"], "answer_start": [159, 159, 159]}, "prediction_text": "Polish Academy of Sciences"}
{"answers": {"text": ["a UNESCO World Heritage Site", "UNESCO World Heritage Site", "UNESCO World Heritage Site"], "answer_start": [365, 367, 367]}, "prediction_text": "UNESCO World Heritage Site"}
{"answers": {"text": ["architectural", "architectural", "architectural"], "answer_start": [406, 406, 406]}, "prediction_text": "King Sigismund's Column"}
{"answers": {"text": ["luxurious parks and royal gardens", "luxurious parks and royal gardens", "luxurious parks and royal gardens"], "answer_start": [905, 905, 905]}, "prediction_text": "Luxurious parks and royal gardens."}
{"answers": {"text": ["Warszawa", "Warszawa", "Warszawa"], "answer_start": [40, 40, 40]}, "prediction_text": "Warszawa"}
{"answers": {"text": ["belonging to Warsz", "belonging to Warsz", "belonging to Warsz"], "answer_start": [133, 133, 133]}, "prediction_text": "Belonging to Warsz."}
{"answers": {"text": ["12th/13th-century nobleman", "shortened form of the masculine name of Slavic origin Warcis\u0142aw", "nobleman"], "answer_start": [469, 168, 487]}, "prediction_text": "Slavic origin Warcis\u0142aw."}
{"answers": {"text": ["a village", "village"], "answer_start": [506, 508]}, "prediction_text": "Village"}
{"answers": {"text": ["miasto sto\u0142eczne Warszawa", "miasto sto\u0142eczne Warszawa", "miasto sto\u0142eczne Warszawa"], "answer_start": [668, 668, 668]}, "prediction_text": "miasto sto\u0142eczne Warszawa (English: \"The Capital City of Warsaw\")"}
{"answers": {"text": ["Jazd\u00f3w", "Jazd\u00f3w", "Jazd\u00f3w"], "answer_start": [142, 142, 142]}, "prediction_text": "Jazd\u00f3w"}
{"answers": {"text": ["The Prince of P\u0142ock", "Prince of P\u0142ock", "Prince of P\u0142ock"], "answer_start": [284, 288, 288]}, "prediction_text": "Prince of P\u0142ock"}
{"answers": {"text": ["1300", "1300", "1300"], "answer_start": [390, 390, 390]}, "prediction_text": "1300."}
{"answers": {"text": ["1413", "1413", "1413"], "answer_start": [536, 536, 536]}, "prediction_text": "1413"}
{"answers": {"text": ["1526", "1526", "1526"], "answer_start": [706, 706, 706]}, "prediction_text": "1526"}
{"answers": {"text": ["General Sejm", "General Sejm", "General Sejm"], "answer_start": [58, 58, 58]}, "prediction_text": "General Sejm"}
{"answers": {"text": ["1569", "1569", "1569"], "answer_start": [87, 87, 87]}, "prediction_text": "1569"}
{"answers": {"text": ["religious freedom", "religious freedom in the Polish\u2013Lithuanian Commonwealth", "religious freedom"], "answer_start": [175, 175, 175]}, "prediction_text": "Religious freedom."}
{"answers": {"text": ["Due to its central location", "its central location", "central location"], "answer_start": [232, 239, 243]}, "prediction_text": "King Sigismund III Vasa moved his court from Krak\u00f3w to Warsaw in 1596."}
{"answers": {"text": ["1596", "1596", "1596"], "answer_start": [474, 474, 474]}, "prediction_text": "1596"}
{"answers": {"text": ["until 1796", "until 1796"], "answer_start": [66, 66]}, "prediction_text": "1796-1806"}
{"answers": {"text": ["Prussia", "Kingdom of Prussia", "Prussia"], "answer_start": [116, 105, 116]}, "prediction_text": "Prussia"}
{"answers": {"text": ["Napoleon's", "Napoleon's", "Napoleon's"], "answer_start": [193, 193, 193]}, "prediction_text": "Napoleon's army"}
{"answers": {"text": ["1815", "1815", "1815"], "answer_start": [320, 320, 320]}, "prediction_text": "1815"}
{"answers": {"text": ["1816", "1816", "1816"], "answer_start": [496, 496, 496]}, "prediction_text": "1816"}
{"answers": {"text": ["from 4 August 1915 until November 1918", "4 August 1915 until November 1918", "from 4 August 1915 until November 1918"], "answer_start": [31, 36, 31]}, "prediction_text": "4 August 1915 to November 1918."}
{"answers": {"text": ["areas controlled by Russia in 1914", "areas controlled by Russia in 1914", "areas controlled by Russia"], "answer_start": [148, 148, 148]}, "prediction_text": "areas controlled by Russia in 1914."}
{"answers": {"text": ["underground leader Pi\u0142sudski", "Pi\u0142sudski", "Pi\u0142sudski"], "answer_start": [227, 246, 246]}, "prediction_text": "Pi\u0142sudski"}
{"answers": {"text": ["1920", "1920", "1920"], "answer_start": [411, 411, 411]}, "prediction_text": "1920"}
{"answers": {"text": ["the Red Army", "Red Army", "Red Army"], "answer_start": [542, 546, 546]}, "prediction_text": "Red Army"}
{"answers": {"text": ["September 1939", "1 September 1939", "1 September 1939"], "answer_start": [41, 39, 39]}, "prediction_text": "1 September 1939"}
{"answers": {"text": ["a German Nazi colonial administration", "German Nazi", "German Nazi colonial administration"], "answer_start": [204, 206, 206]}, "prediction_text": "German Nazi colonial administration."}
{"answers": {"text": ["some 30% of the city", "30%", "30%"], "answer_start": [367, 372, 372]}, "prediction_text": "Several hundred thousand."}
{"answers": {"text": ["April 1943", "19 April 1943", "19 April 1943"], "answer_start": [595, 592, 592]}, "prediction_text": "19 April 1943"}
{"answers": {"text": ["almost a month", "almost a month", "almost a month"], "answer_start": [733, 733, 733]}, "prediction_text": "Almost a month."}
{"answers": {"text": ["the Red Army", "Red Army", "e Red Army"], "answer_start": [14, 18, 16]}, "prediction_text": "Red Army"}
{"answers": {"text": ["Stalin was hostile to the idea of an independent Poland", "Stalin was hostile to the idea of an independent Poland", "Stalin was hostile to the idea of an independent Poland"], "answer_start": [111, 111, 111]}, "prediction_text": "To try to seize control of Warsaw from the Germans before the Red Army arrived."}
{"answers": {"text": ["August 1944", "the Red Army was nearing the city", "1 August 1944"], "answer_start": [346, 362, 344]}, "prediction_text": "1 August 1944"}
{"answers": {"text": ["63 days", "63", "63 days"], "answer_start": [519, 519, 519]}, "prediction_text": "63 days."}
{"answers": {"text": ["between 150,000 and 200,000", "between 150,000 and 200,000", "between 150,000 and 200,000"], "answer_start": [756, 756, 756]}, "prediction_text": "150,000 to 200,000."}
{"answers": {"text": ["\"Bricks for Warsaw\"", "Bricks for Warsaw", "Bricks for Warsaw"], "answer_start": [83, 84, 84]}, "prediction_text": "Bricks for Warsaw campaign"}
{"answers": {"text": ["prefabricated", "prefabricated", "prefabricated"], "answer_start": [137, 137, 137]}, "prediction_text": "Large prefabricated housing projects."}
{"answers": {"text": ["an Eastern Bloc city", "an Eastern Bloc city"], "answer_start": [262, 262]}, "prediction_text": "Bricks for Warsaw campaign"}
{"answers": {"text": ["Palace of Culture and Science", "Palace of Culture and Science", "Palace of Culture and Science"], "answer_start": [296, 296, 296]}, "prediction_text": "Palace of Culture and Science"}
{"answers": {"text": ["UNESCO's World Heritage list", "UNESCO's World Heritage", "UNESCO's World Heritage"], "answer_start": [612, 612, 612]}, "prediction_text": "UNESCO's World Heritage list."}
{"answers": {"text": ["John Paul II", "John Paul II", "John Paul II"], "answer_start": [0, 0, 0]}, "prediction_text": "John Paul II"}
{"answers": {"text": ["growing anti-communist fervor", "anti-communist fervor", "growing anti-communist fervor"], "answer_start": [131, 139, 131]}, "prediction_text": "The budding solidarity movement."}
{"answers": {"text": ["less than a year", "less than a year", "less than a year"], "answer_start": [177, 177, 177]}, "prediction_text": "Less than a year."}
{"answers": {"text": ["Victory Square", "Victory Square", "1979"], "answer_start": [244, 244, 171]}, "prediction_text": "Victory Square"}
{"answers": {"text": ["incentive for the democratic changes", "incentive for the democratic changes", "democratic changes"], "answer_start": [507, 507, 525]}, "prediction_text": "Renew the face of Poland."}
{"answers": {"text": ["about 300", "300", "300"], "answer_start": [35, 41, 41]}, "prediction_text": "190 mi."}
{"answers": {"text": ["325", "325", "325"], "answer_start": [142, 142, 142]}, "prediction_text": "523 km (325 mi)"}
{"answers": {"text": ["Vistula River", "Vistula", "Vistula"], "answer_start": [198, 198, 198]}, "prediction_text": "Vistula River"}
{"answers": {"text": ["452.8 ft", "115.7 metres", "115.7 metres"], "answer_start": [866, 400, 400]}, "prediction_text": "115.7 meters (379.6 ft)"}
{"answers": {"text": ["at the right bank of the Vistula", "at the right bank of the Vistula, by the eastern border of Warsaw", "by the eastern border"], "answer_start": [633, 633, 667]}, "prediction_text": "75.6 meters (248.0 ft) (at the right bank of the Vistula, by the eastern border of Warsaw)."}
{"answers": {"text": ["two", "two", "two"], "answer_start": [21, 21, 21]}, "prediction_text": "2"}
{"answers": {"text": ["Vistula Valley", "geomorphologic", "Vistula Valley"], "answer_start": [91, 30, 91]}, "prediction_text": "Vistula Valley"}
{"answers": {"text": ["moraine", "moraine", "moraine"], "answer_start": [301, 301, 301]}, "prediction_text": "moraine plateau"}
{"answers": {"text": ["Vistula River", "Vistula River", "Vistula River"], "answer_start": [163, 163, 163]}, "prediction_text": "Vistula River"}
{"answers": {"text": ["Warsaw Escarpment", "Warsaw Escarpment", "Warsaw Escarpment"], "answer_start": [541, 541, 541]}, "prediction_text": "Warsaw Escarpment"}
{"answers": {"text": ["moraine", "moraine", "moraine"], "answer_start": [10, 10, 10]}, "prediction_text": "Warsaw plateau"}
{"answers": {"text": ["former flooded terraces", "former flooded terraces", "peat swamps or small ponds"], "answer_start": [222, 222, 806]}, "prediction_text": "Former flooded terraces."}
{"answers": {"text": ["valleys", "valleys and ground depressions", "valleys"], "answer_start": [341, 341, 341]}, "prediction_text": "valleys and ground depressions with water systems coming from the Vistula old \u2013 riverbed."}
{"answers": {"text": ["plain Vistula terraces", "highest terrace", "highest terrace"], "answer_start": [637, 843, 843]}, "prediction_text": "Peat swamps or small ponds."}
{"answers": {"text": ["pine", "pine", "pine"], "answer_start": [893, 893, 893]}, "prediction_text": "Pine forest."}
{"answers": {"text": ["turbulent history of the city", "turbulent history of the city and country", "turbulent history of the city"], "answer_start": [54, 54, 54]}, "prediction_text": "turbulent history of the city and country."}
{"answers": {"text": ["During the Second World War", "Second World War", "During the Second World War"], "answer_start": [97, 108, 97]}, "prediction_text": "Second World War."}
{"answers": {"text": ["After liberation", "After liberation", "After liberation"], "answer_start": [199, 199, 199]}, "prediction_text": "1950s and 1960s"}
{"answers": {"text": ["Leopold Kronenberg Palace", "Leopold Kronenberg Palace", "Leopold Kronenberg Palace"], "answer_start": [514, 514, 514]}, "prediction_text": "Leopold Kronenberg Palace"}
{"answers": {"text": ["typical of Eastern bloc countries", "basic design typical of Eastern bloc countries", "design typical of Eastern bloc countries"], "answer_start": [598, 585, 591]}, "prediction_text": "Eastern bloc countries"}
{"answers": {"text": ["Gothic", "Gothic", "Gothic"], "answer_start": [0, 0, 0]}, "prediction_text": "Gothic architecture."}
{"answers": {"text": ["14th century", "14th century", "14th century"], "answer_start": [168, 168, 168]}, "prediction_text": "14th century."}
{"answers": {"text": ["Masovian gothic", "Gothic architecture", "Masovian gothic"], "answer_start": [232, 0, 232]}, "prediction_text": "Gothic architecture."}
{"answers": {"text": ["Renaissance", "Renaissance", "Renaissance"], "answer_start": [432, 432, 432]}, "prediction_text": "Gothic"}
{"answers": {"text": ["mannerist architecture", "mannerist architecture", "mannerist"], "answer_start": [631, 631, 631]}, "prediction_text": "The Royal Castle Curia Maior."}
{"answers": {"text": ["17th century", "17th century", "17th century."], "answer_start": [98, 98, 98]}, "prediction_text": "17th century."}
{"answers": {"text": ["1688\u20131692", "1677\u20131683", "1688\u20131692"], "answer_start": [245, 180, 245]}, "prediction_text": "1688-1692"}
{"answers": {"text": ["rococo", "rococo", "rococo"], "answer_start": [289, 289, 289]}, "prediction_text": "Neoclassical architecture."}
{"answers": {"text": ["neoclassical architecture", "neoclassical architecture", "neoclassical architecture"], "answer_start": [423, 423, 423]}, "prediction_text": "Neoclassical architecture."}
{"answers": {"text": ["1775\u20131795", "1696", "1775\u20131795"], "answer_start": [663, 213, 663]}, "prediction_text": "1775-1795"}
{"answers": {"text": ["bourgeois", "bourgeois", "bourgeois"], "answer_start": [28, 28, 28]}, "prediction_text": "bourgeois architecture"}
{"answers": {"text": ["not restored by the communist authorities", "were not restored", "not restored by the communist authorities after the war"], "answer_start": [77, 72, 77]}, "prediction_text": "Not restored."}
{"answers": {"text": ["socialist realism", "socialist realism", "socialist realism"], "answer_start": [229, 229, 229]}, "prediction_text": "Socialist realism."}
{"answers": {"text": ["Warsaw University of Technology building", "Warsaw University of Technology building", "Warsaw University of Technology"], "answer_start": [352, 352, 352]}, "prediction_text": "Warsaw University of Technology building (1899\u20131902)"}
{"answers": {"text": ["the most distinctive buildings", "the most distinctive buildings", "most distinctive buildings"], "answer_start": [712, 712, 716]}, "prediction_text": "Warsaw University of Technology building."}
{"answers": {"text": ["many places", "many places"], "answer_start": [15, 15]}, "prediction_text": "Pawiak"}
{"answers": {"text": ["Pawiak", "Pawiak", "Pawiak"], "answer_start": [71, 71, 71]}, "prediction_text": "Pawiak"}
{"answers": {"text": ["The Warsaw Citadel", "Warsaw Citadel", "The Warsaw Citadel"], "answer_start": [242, 246, 242]}, "prediction_text": "Warsaw Citadel"}
{"answers": {"text": ["children", "children who served as messengers and frontline troops in the Warsaw Uprising", "children"], "answer_start": [506, 506, 506]}, "prediction_text": "Children."}
{"answers": {"text": ["Warsaw Uprising Monument", "Warsaw Uprising Monument", "Warsaw Uprising Monument"], "answer_start": [606, 606, 606]}, "prediction_text": "Warsaw Uprising Monument"}
{"answers": {"text": ["green", "green", "green"], "answer_start": [6, 6, 6]}, "prediction_text": "Green spaces"}
{"answers": {"text": ["New Orangery", "New Orangery", "New Orangery"], "answer_start": [197, 197, 197]}, "prediction_text": "New Orangery."}
{"answers": {"text": ["Pole Mokotowskie", "Mokot\u00f3w", "Pole Mokotowskie"], "answer_start": [315, 360, 315]}, "prediction_text": "Botanic Garden"}
{"answers": {"text": ["Park Ujazdowski", "Park Ujazdowski", "Park Ujazdowski"], "answer_start": [428, 428, 428]}, "prediction_text": "Park Ujazdowski"}
{"answers": {"text": ["1927", "1927", "1927"], "answer_start": [715, 715, 715]}, "prediction_text": "1927"}
{"answers": {"text": ["location of Warsaw", "location of Warsaw", "location"], "answer_start": [104, 104, 104]}, "prediction_text": "The flora of Warsaw is rich in species due to the location of Warsaw within the border region of several big floral regions comprising substantial proportions of close-to-wilderness areas (natural forests, wetlands along the Vistula) as well as arable land, meadows, and forests."}
{"answers": {"text": ["within the borders of Warsaw", "Warsaw", "within the borders of Warsaw"], "answer_start": [357, 379, 357]}, "prediction_text": "Warsaw"}
{"answers": {"text": ["Masovian Primeval Forest", "Masovian Primeval Forest", "Masovian Primeval Forest"], "answer_start": [416, 416, 416]}, "prediction_text": "Masovian Primeval Forest"}
{"answers": {"text": ["Kabaty", "Kabaty Forest", "Kabaty"], "answer_start": [630, 630, 630]}, "prediction_text": "Bielany Forest"}
{"answers": {"text": ["two", "two", "two"], "answer_start": [689, 689, 689]}, "prediction_text": "2"}
{"answers": {"text": ["1,300,000", "1,300,000", "1,300,000"], "answer_start": [12, 12, 12]}, "prediction_text": "1,300,000"}
{"answers": {"text": ["420,000", "420,000", "420,000"], "answer_start": [65, 65, 65]}, "prediction_text": "420,000"}
{"answers": {"text": ["1951", "1951", "1951"], "answer_start": [295, 295, 295]}, "prediction_text": "1951"}
{"answers": {"text": ["as better", "thought of themselves as better", "better"], "answer_start": [715, 693, 718]}, "prediction_text": "Better because they lived in the capital."}
{"answers": {"text": ["residency registration", "residency registration", "residency registration"], "answer_start": [909, 909, 909]}, "prediction_text": "Residency registration."}
{"answers": {"text": ["multi-cultural", "multi-cultural city", "multi-cultural"], "answer_start": [44, 44, 44]}, "prediction_text": "Multi-cultural city"}
{"answers": {"text": ["711,988", "711,988", "711,988"], "answer_start": [102, 102, 102]}, "prediction_text": "711,988"}
{"answers": {"text": ["56.2%", "35.7", "56.2%"], "answer_start": [122, 144, 122]}, "prediction_text": "56.2%"}
{"answers": {"text": ["2.8%", "2.8", "2.8%"], "answer_start": [189, 189, 189]}, "prediction_text": "2.8%"}
{"answers": {"text": ["1944", "1944", "1944"], "answer_start": [488, 488, 488]}, "prediction_text": "1944"}
{"answers": {"text": ["a commune", "commune", "commune"], "answer_start": [52, 54, 54]}, "prediction_text": "commune (gmina)"}
{"answers": {"text": ["counties or powiats", "counties or powiats", "counties or powiats"], "answer_start": [421, 421, 421]}, "prediction_text": "Counties or powiats."}
{"answers": {"text": ["Krak\u00f3w", "Krak\u00f3w"], "answer_start": [1085, 1085]}, "prediction_text": "Wola, Ursyn\u00f3w."}
{"answers": {"text": ["Warsaw City Council", "Warsaw City Council", "Warsaw City Council"], "answer_start": [54, 54, 54]}, "prediction_text": "Warsaw City Council (Rada Miasta)"}
{"answers": {"text": ["60", "60", "60"], "answer_start": [105, 105, 105]}, "prediction_text": "60"}
{"answers": {"text": ["every four years", "four years", "every four years."], "answer_start": [154, 160, 154]}, "prediction_text": "Every four years."}
{"answers": {"text": ["committees", "committees", "committees"], "answer_start": [239, 239, 239]}, "prediction_text": "committees"}
{"answers": {"text": ["30 days", "30", "30"], "answer_start": [475, 475, 475]}, "prediction_text": "30 days."}
{"answers": {"text": ["President", "President", "President"], "answer_start": [30, 30, 30]}, "prediction_text": "President"}
{"answers": {"text": ["Jan Andrzej Menich", "Jan Andrzej Menich", "Jan Andrzej Menich"], "answer_start": [246, 246, 246]}, "prediction_text": "Jan Andrzej Menich"}
{"answers": {"text": ["1695\u20131696", "1695\u20131696", "1695\u20131696"], "answer_start": [266, 266, 266]}, "prediction_text": "1695-1696"}
{"answers": {"text": ["the City council", "City council", "City council"], "answer_start": [416, 420, 420]}, "prediction_text": "City council."}
{"answers": {"text": ["Centrum", "Centrum", "Centrum"], "answer_start": [486, 486, 486]}, "prediction_text": "Centrum"}
{"answers": {"text": ["\u015ar\u00f3dmie\u015bcie", "\u015ar\u00f3dmie\u015bcie", "\u015ar\u00f3dmie\u015bcie"], "answer_start": [36, 36, 36]}, "prediction_text": "\u015ar\u00f3dmie\u015bcie"}
{"answers": {"text": ["304,016", "304,016"], "answer_start": [186, 186]}, "prediction_text": "304,016"}
{"answers": {"text": ["emerging market", "emerging market", "emerging market"], "answer_start": [470, 470, 470]}, "prediction_text": "Emerging market."}
{"answers": {"text": ["12%", "12%", "12%"], "answer_start": [620, 620, 620]}, "prediction_text": "12%"}
{"answers": {"text": ["191.766 billion PLN", "191.766 billion PLN", "191.766 billion PLN"], "answer_start": [892, 892, 892]}, "prediction_text": "191.766 billion PLN."}
{"answers": {"text": ["1817", "1817", "1817"], "answer_start": [49, 49, 49]}, "prediction_text": "1817"}
{"answers": {"text": ["World War II", "World War II", "World War II."], "answer_start": [82, 82, 82]}, "prediction_text": "World War II"}
{"answers": {"text": ["April 1991", "April 1991", "April 1991"], "answer_start": [121, 121, 121]}, "prediction_text": "1991"}
{"answers": {"text": ["374", "374", "374"], "answer_start": [361, 361, 361]}, "prediction_text": "374"}
{"answers": {"text": ["Polish United Workers' Party", "Polish United Workers' Party", "Polish United Workers' Party"], "answer_start": [573, 573, 573]}, "prediction_text": "PZPR"}
{"answers": {"text": ["1951", "1951", "1951"], "answer_start": [39, 39, 39]}, "prediction_text": "1951"}
{"answers": {"text": ["Polonez", "125p", "Polonez"], "answer_start": [235, 196, 235]}, "prediction_text": "Warszawa"}
{"answers": {"text": ["Daewoo", "Daewoo", "Daewoo"], "answer_start": [440, 440, 440]}, "prediction_text": "Daewoo"}
{"answers": {"text": ["AvtoZAZ", "AvtoZAZ", "AvtoZAZ"], "answer_start": [585, 585, 585]}, "prediction_text": "AvtoZAZ"}
{"answers": {"text": ["Chevrolet Aveo", "Aveo", "Aveo"], "answer_start": [649, 703, 703]}, "prediction_text": "February 2011"}
{"answers": {"text": ["Warszawa", "Warszawa", "Warszawa"], "answer_start": [16, 16, 16]}, "prediction_text": "Warszawa [var\u02c8\u0282ava] ( listen)"}
{"answers": {"text": ["Warsaw", "Warsaw", "Warsaw"], "answer_start": [0, 0, 0]}, "prediction_text": "Warsaw"}
{"answers": {"text": ["Vistula River", "Vistula River", "Vistula"], "answer_start": [129, 129, 129]}, "prediction_text": "Vistula River"}
{"answers": {"text": ["2.666 million residents", "2.666 million", "2.666 million"], "answer_start": [371, 371, 371]}, "prediction_text": "2,355.39 square kilometers."}
{"answers": {"text": ["9th", "9th", "9th"], "answer_start": [419, 419, 419]}, "prediction_text": "9th."}
{"answers": {"text": ["Warsaw", "Warsaw", "Warsaw"], "answer_start": [0, 0, 0]}, "prediction_text": "Warsaw"}
{"answers": {"text": ["Vistula", "Vistula River", "Vistula"], "answer_start": [129, 129, 129]}, "prediction_text": "Vistula River"}
{"answers": {"text": ["roughly 260 kilometres", "260 kilometres", "260 kilometres"], "answer_start": [167, 175, 175]}, "prediction_text": "260 km."}
{"answers": {"text": ["2.666 million", "2.666 million", "2.666 million"], "answer_start": [371, 371, 371]}, "prediction_text": "2,355.39 square kilometers."}
{"answers": {"text": ["9th", "9th", "9th"], "answer_start": [419, 419, 419]}, "prediction_text": "9th"}
{"answers": {"text": ["France", "France", "France", "France"], "answer_start": [159, 159, 159, 159]}, "prediction_text": "France"}
{"answers": {"text": ["10th and 11th centuries", "in the 10th and 11th centuries", "10th and 11th centuries", "10th and 11th centuries"], "answer_start": [94, 87, 94, 94]}, "prediction_text": "10th century."}
{"answers": {"text": ["Denmark, Iceland and Norway", "Denmark, Iceland and Norway", "Denmark, Iceland and Norway", "Denmark, Iceland and Norway"], "answer_start": [256, 256, 256, 256]}, "prediction_text": "Denmark, Iceland, Norway."}
{"answers": {"text": ["Rollo", "Rollo", "Rollo", "Rollo"], "answer_start": [308, 308, 308, 308]}, "prediction_text": "Rollo"}
{"answers": {"text": ["10th century", "the first half of the 10th century", "10th", "10th"], "answer_start": [671, 649, 671, 671]}, "prediction_text": "10th century."}
{"answers": {"text": ["William the Conqueror", "William the Conqueror", "William the Conqueror"], "answer_start": [1022, 1022, 1022]}, "prediction_text": "William the Conqueror"}
{"answers": {"text": ["Richard I", "Richard I", "Richard I"], "answer_start": [573, 573, 573]}, "prediction_text": "Richard I"}
{"answers": {"text": ["Catholic", "Catholic orthodoxy", "Catholic"], "answer_start": [230, 230, 230]}, "prediction_text": "Catholic"}
{"answers": {"text": ["Viking", "Norseman, Viking", "Norseman, Viking"], "answer_start": [341, 331, 331]}, "prediction_text": "Normans/Normanz."}
{"answers": {"text": ["9th century", "9th century", "9th century"], "answer_start": [309, 309, 309]}, "prediction_text": "9th century."}
{"answers": {"text": ["911", "911", "911"], "answer_start": [244, 244, 244]}, "prediction_text": "911"}
{"answers": {"text": ["King Charles III", "King Charles III", "King Charles III"], "answer_start": [324, 324, 324]}, "prediction_text": "King Charles III of West Francia"}
{"answers": {"text": ["Seine", "Epte", "Seine"], "answer_start": [711, 524, 711]}, "prediction_text": "Seine"}
{"answers": {"text": ["Rollo", "Rollo", "Rollo"], "answer_start": [7, 7, 7]}, "prediction_text": "Rollo"}
{"answers": {"text": ["Catholicism", "Catholicism", "Catholicism"], "answer_start": [121, 121, 121]}, "prediction_text": "Catholicism (Christianity)"}
{"answers": {"text": ["north", "the north", "north"], "answer_start": [327, 323, 327]}, "prediction_text": "North of France."}
{"answers": {"text": ["fighting horsemen", "fighting horsemen", "fighting horsemen"], "answer_start": [428, 428, 428]}, "prediction_text": "Fighting horsemen."}
{"answers": {"text": ["999", "In 999", "999"], "answer_start": [233, 230, 233]}, "prediction_text": "999."}
{"answers": {"text": ["Archangel Michael", "the Archangel Michael", "Archangel Michael"], "answer_start": [621, 617, 621]}, "prediction_text": "Monte Gargano"}
{"answers": {"text": ["Monte Gargano", "at Monte Gargano", "Monte Gargano"], "answer_start": [642, 639, 642]}, "prediction_text": "Monte Gargano"}
{"answers": {"text": ["Drogo", "Drogo", "Drogo"], "answer_start": [627, 627, 627]}, "prediction_text": "Drogo"}
{"answers": {"text": ["William Iron Arm", "William Iron Arm", "William Iron Arm"], "answer_start": [432, 432, 432]}, "prediction_text": "Drogo"}
{"answers": {"text": ["Saracens", "the Saracens", "Saracens"], "answer_start": [76, 72, 76]}, "prediction_text": "Robert Guiscard"}
{"answers": {"text": ["1130", "1130", "1130"], "answer_start": [252, 252, 252]}, "prediction_text": "1130"}
{"answers": {"text": ["Squillace", "Squillace", "Squillace"], "answer_start": [536, 536, 536]}, "prediction_text": "Squillace"}
{"answers": {"text": ["Kitab Rudjdjar", "Kitab Rudjdjar", "Kitab Rudjdjar"], "answer_start": [837, 837, 837]}, "prediction_text": "Tabula Rogeriana"}
{"answers": {"text": ["The Book of Roger", "The Book of Roger", "The Book of Roger"], "answer_start": [855, 855, 855]}, "prediction_text": "The Book of Roger."}
{"answers": {"text": ["meritocratic", "meritocratic", "meritocratic"], "answer_start": [282, 282, 282]}, "prediction_text": "Meritocratic bureaucracy."}
{"answers": {"text": ["Seljuk Turks", "the Pechenegs, the Bulgars, and especially the Seljuk Turks", "the Seljuk Turks"], "answer_start": [161, 114, 157]}, "prediction_text": "Pechenegs"}
{"answers": {"text": ["1050s", "in the 1050s", "in the 1050s"], "answer_start": [85, 78, 78]}, "prediction_text": "1050s"}
{"answers": {"text": ["1060s", "In the 1060s", "In the 1060s"], "answer_start": [292, 285, 285]}, "prediction_text": "1060s"}
{"answers": {"text": ["Alexius Komnenos", "Alexius Komnenos", "Alexius Komnenos"], "answer_start": [522, 522, 522]}, "prediction_text": "Byzantine general Alexius Komnenos"}
{"answers": {"text": ["Afranji", "Afranji", "Afranji"], "answer_start": [539, 539, 539]}, "prediction_text": "Afranji"}
{"answers": {"text": ["Oursel", "Oursel", "Oursel"], "answer_start": [256, 256, 256]}, "prediction_text": "Oursel"}
{"answers": {"text": ["Turkish forces", "Turkish forces", "Turkish forces"], "answer_start": [20, 20, 20]}, "prediction_text": "Turkish forces."}
{"answers": {"text": ["Norman mercenary", "an Italo-Norman named Raoul", "descended from an Italo-Norman named Raoul"], "answer_start": [45, 217, 202]}, "prediction_text": "Norman mercenary."}
{"answers": {"text": ["Robert Guiscard", "Robert Guiscard", "Robert Guiscard"], "answer_start": [0, 0, 0]}, "prediction_text": "Robert Guiscard"}
{"answers": {"text": ["1082", "February 1082", "February 1082"], "answer_start": [1315, 1306, 1306]}, "prediction_text": "February 1085."}
{"answers": {"text": ["30,000", "30,000", "30,000"], "answer_start": [492, 492, 492]}, "prediction_text": "30,000."}
{"answers": {"text": ["Deabolis", "Deabolis", "Deabolis"], "answer_start": [302, 718, 718]}, "prediction_text": "Deabolis"}
{"answers": {"text": ["Bohemond", "Bohemond", "Bohemond"], "answer_start": [79, 79, 79]}, "prediction_text": "Bohemond"}
{"answers": {"text": ["Deabolis", "the river Deabolis", "Deabolis"], "answer_start": [302, 292, 302]}, "prediction_text": "Deabolis"}
{"answers": {"text": ["1185", "in 1185", "1185"], "answer_start": [86, 83, 86]}, "prediction_text": "1185"}
{"answers": {"text": ["Dyrrachium", "Dyrrachium", "Dyrrachium"], "answer_start": [125, 205, 205]}, "prediction_text": "Dyrrachium"}
{"answers": {"text": ["the Adriatic", "the Adriatic", "Adriatic"], "answer_start": [257, 257, 261]}, "prediction_text": "Adriatic"}
{"answers": {"text": ["King Ethelred II", "Ethelred II", "King Ethelred II"], "answer_start": [360, 365, 360]}, "prediction_text": "King Ethelred II"}
{"answers": {"text": ["Duke Richard II", "Duke Richard II", "Duke Richard II"], "answer_start": [327, 327, 327]}, "prediction_text": "Duke Richard II"}
{"answers": {"text": ["Normandy", "Normandy", "Normandy"], "answer_start": [423, 423, 423]}, "prediction_text": "Normandy"}
{"answers": {"text": ["Sweyn Forkbeard", "Sweyn Forkbeard", "Sweyn Forkbeard"], "answer_start": [480, 480, 480]}, "prediction_text": "Sweyn Forkbeard"}
{"answers": {"text": ["Harthacnut", "Harthacnut", "Harthacnut"], "answer_start": [115, 115, 115]}, "prediction_text": "Harthacnut"}
{"answers": {"text": ["1041", "in 1041", "1041"], "answer_start": [71, 68, 71]}, "prediction_text": "1041"}
{"answers": {"text": ["Robert of Jumi\u00e8ges", "Robert of Jumi\u00e8ges", "Robert of Jumi\u00e8ges"], "answer_start": [382, 382, 382]}, "prediction_text": "Robert of Jumi\u00e8ges"}
{"answers": {"text": ["Battle of Hastings", "the Battle of Hastings", "at the Battle of Hastings"], "answer_start": [85, 81, 78]}, "prediction_text": "Battle of Hastings"}
{"answers": {"text": ["William II", "Duke William II", "Duke William II"], "answer_start": [14, 9, 9]}, "prediction_text": "Duke William II"}
{"answers": {"text": ["1066", "In 1066", "1066"], "answer_start": [3, 0, 3]}, "prediction_text": "1066"}
{"answers": {"text": ["Anglo-Saxons", "the Anglo-Saxons", "Anglo-Saxons"], "answer_start": [161, 157, 161]}, "prediction_text": "Anglo-Saxons"}
{"answers": {"text": ["Modern English", "Modern English", "Modern English"], "answer_start": [629, 629, 629]}, "prediction_text": "Anglo-Saxon language"}
{"answers": {"text": ["1169", "1169", "1169"], "answer_start": [101, 101, 101]}, "prediction_text": "1169"}
{"answers": {"text": ["Ireland", "Ireland", "Ireland"], "answer_start": [379, 379, 379]}, "prediction_text": "Ireland"}
{"answers": {"text": ["Irish", "Irish", "Irish"], "answer_start": [37, 220, 220]}, "prediction_text": "Irish culture."}
{"answers": {"text": ["Edgar", "Edgar", "Edgar Atheling"], "answer_start": [75, 157, 75]}, "prediction_text": "Edgar Atheling"}
{"answers": {"text": ["King Malcolm III of Scotland", "King Malcolm III", "King Malcolm III"], "answer_start": [120, 120, 120]}, "prediction_text": "Malcolm III"}
{"answers": {"text": ["1072", "1072", "1072"], "answer_start": [300, 300, 300]}, "prediction_text": "1072"}
{"answers": {"text": ["Duncan", "Duncan", "Duncan"], "answer_start": [440, 440, 440]}, "prediction_text": "Duncan"}
{"answers": {"text": ["Sybilla of Normandy", "Sybilla of Normandy", "Sybilla"], "answer_start": [271, 271, 271]}, "prediction_text": "Sybilla of Normandy"}
{"answers": {"text": ["Norman", "Norman", "Norman"], "answer_start": [336, 336, 336]}, "prediction_text": "Normans"}
{"answers": {"text": ["Hereford", "Hereford", "Hereford"], "answer_start": [158, 158, 158]}, "prediction_text": "Hereford"}
{"answers": {"text": ["the Welsh", "the Welsh", "the Welsh"], "answer_start": [227, 227, 227]}, "prediction_text": "Welsh"}
{"answers": {"text": ["Edward the Confessor", "Edward the Confessor", "Edward the Confessor"], "answer_start": [90, 90, 90]}, "prediction_text": "Edward the Confessor"}
{"answers": {"text": ["Wales", "Wales", "Wales"], "answer_start": [299, 299, 299]}, "prediction_text": "Wales"}
{"answers": {"text": ["1018", "1064", "1018"], "answer_start": [221, 345, 221]}, "prediction_text": "1064"}
{"answers": {"text": ["William of Montreuil", "William of Montreuil", "William of Montreuil"], "answer_start": [380, 380, 380]}, "prediction_text": "William of Montreuil"}
{"answers": {"text": ["1097", "1097", "1097"], "answer_start": [267, 267, 267]}, "prediction_text": "1097"}
{"answers": {"text": ["Tancred", "Tancred", "Tancred"], "answer_start": [100, 100, 100]}, "prediction_text": "Tancred"}
{"answers": {"text": ["Jerusalem", "Jerusalem", "Jerusalem"], "answer_start": [390, 390, 390]}, "prediction_text": "Jerusalem"}
{"answers": {"text": ["380 years", "380 years", "380 years"], "answer_start": [189, 189, 189]}, "prediction_text": "380 years."}
{"answers": {"text": ["a storm", "a storm", "a storm"], "answer_start": [99, 99, 99]}, "prediction_text": "Storm dispersal of the fleet."}
{"answers": {"text": ["Berengaria", "Berengaria", "Berengaria"], "answer_start": [218, 218, 218]}, "prediction_text": "Berengaria"}
{"answers": {"text": ["1191", "1191", "1191"], "answer_start": [9, 9, 9]}, "prediction_text": "1191"}
{"answers": {"text": ["Isaac Komnenos", "Isaac", "Isaac Komnenos"], "answer_start": [421, 522, 421]}, "prediction_text": "Isaac Komnenos"}
{"answers": {"text": ["Conrad of Montferrat", "Conrad of Montferrat", "Conrad of Montferrat"], "answer_start": [188, 188, 188]}, "prediction_text": "Conrad of Montferrat"}
{"answers": {"text": ["silver", "silver", "silver"], "answer_start": [565, 565, 565]}, "prediction_text": "Silver chains."}
{"answers": {"text": ["Guy de Lusignan", "Guy de Lusignan", "Guy de Lusignan"], "answer_start": [85, 508, 508]}, "prediction_text": "Guy de Lusignan"}
{"answers": {"text": ["Richard the Lion-Heart", "Richard the Lion-Heart", "Richard the Lion-Heart"], "answer_start": [19, 19, 19]}, "prediction_text": "Richard the Lion-Heart"}
{"answers": {"text": ["12 May 1191", "12 May 1191", "12 May 1191"], "answer_start": [147, 147, 147]}, "prediction_text": "12 May 1191"}
{"answers": {"text": ["double coronation", "double", "double"], "answer_start": [359, 359, 359]}, "prediction_text": "Double coronation."}
{"answers": {"text": ["1489", "1489", "1489"], "answer_start": [419, 419, 419]}, "prediction_text": "1489"}
{"answers": {"text": ["Knights Templar", "the Knights Templar", "the Knights Templar"], "answer_start": [290, 286, 286]}, "prediction_text": "Knights Templar"}
{"answers": {"text": ["Africa", "Africa", "Africa"], "answer_start": [219, 219, 219]}, "prediction_text": "Africa"}
{"answers": {"text": ["Bethencourt", "Bethencourt", "Bethencourt"], "answer_start": [0, 0, 0]}, "prediction_text": "Henry III of Castile"}
{"answers": {"text": ["Enrique P\u00e9rez de Guzm\u00e1n", "Enrique P\u00e9rez de Guzm\u00e1n", "Enrique P\u00e9rez de Guzm\u00e1n"], "answer_start": [172, 172, 172]}, "prediction_text": "Enrique P\u00e9rez de Guzm\u00e1n"}
{"answers": {"text": ["Maciot de Bethencourt", "Maciot de Bethencourt", "Maciot de Bethencourt"], "answer_start": [116, 116, 116]}, "prediction_text": "Enrique P\u00e9rez de Guzm\u00e1n"}
{"answers": {"text": ["Channel Islands", "the Channel Islands", "the Channel Islands"], "answer_start": [155, 151, 151]}, "prediction_text": "Channel Islands"}
{"answers": {"text": ["two", "two", "two"], "answer_start": [212, 212, 212]}, "prediction_text": "2"}
{"answers": {"text": ["Romanesque", "Romanesque", "Romanesque"], "answer_start": [135, 135, 135]}, "prediction_text": "Romanesque"}
{"answers": {"text": ["rounded", "rounded", "rounded"], "answer_start": [332, 332, 332]}, "prediction_text": "Round arches"}
{"answers": {"text": ["Early Gothic", "Early Gothic", "Early Gothic"], "answer_start": [108, 108, 108]}, "prediction_text": "Anglo-Saxon"}
{"answers": {"text": ["Anglo-Saxon", "Anglo-Saxon", "Anglo-Saxon"], "answer_start": [79, 79, 79]}, "prediction_text": "Anglo-Saxon"}
{"answers": {"text": ["Sicily", "Sicily", "Kingdom of Sicily"], "answer_start": [328, 328, 317]}, "prediction_text": "Sicily"}
{"answers": {"text": ["early 11th century", "11th century", "in the early 11th century"], "answer_start": [129, 135, 122]}, "prediction_text": "11th century."}
{"answers": {"text": ["dukes", "the dukes", "dukes"], "answer_start": [152, 422, 426]}, "prediction_text": "The dukes."}
{"answers": {"text": ["16th century", "the 16th century", "in the 16th century"], "answer_start": [35, 31, 28]}, "prediction_text": "16th century"}
{"answers": {"text": ["embroidery", "embroidery", "embroidery"], "answer_start": [104, 104, 104]}, "prediction_text": "Embroidery."}
{"answers": {"text": ["Bayeux Tapestry", "the Bayeux Tapestry", "the Bayeux Tapestry"], "answer_start": [49, 45, 45]}, "prediction_text": "Bayeux Tapestry"}
{"answers": {"text": ["Odo", "Odo", "Odo"], "answer_start": [139, 139, 139]}, "prediction_text": "Odo"}
{"answers": {"text": ["mosaics", "mosaics", "mosaics"], "answer_start": [466, 466, 466]}, "prediction_text": "Mosaics."}
{"answers": {"text": ["11th", "the 11th", "11th"], "answer_start": [97, 93, 97]}, "prediction_text": "11th century."}
{"answers": {"text": ["William of Volpiano and John of Ravenna", "William of Volpiano and John of Ravenna", "William of Volpiano and John of Ravenna"], "answer_start": [234, 234, 234]}, "prediction_text": "William of Volpiano and John of Ravenna."}
{"answers": {"text": ["southern Italy", "southern Italy", "southern Italy"], "answer_start": [179, 179, 179]}, "prediction_text": "Italy"}
{"answers": {"text": ["Latin monastery at Sant'Eufemia.", "a Latin monastery at Sant'Eufemia", "Sant'Eufemia"], "answer_start": [259, 257, 278]}, "prediction_text": "Sant'Eufemia"}
{"answers": {"text": ["Robert Guiscard", "Robert Guiscard", "Robert Guiscard"], "answer_start": [225, 225, 225]}, "prediction_text": "Robert Guiscard"}
{"answers": {"text": ["singing", "singing", "singing"], "answer_start": [32, 32, 330]}, "prediction_text": "Singing"}
{"answers": {"text": ["1856", "1856", "1856"], "answer_start": [54, 54, 54]}, "prediction_text": "1856"}
{"answers": {"text": ["Serbian", "Serbian", "Serbian"], "answer_start": [14, 14, 14]}, "prediction_text": "Serbian"}
{"answers": {"text": ["1943", "1943", "1943"], "answer_start": [71, 71, 71]}, "prediction_text": "1943"}
{"answers": {"text": ["1856", "10 July 1856", "1856"], "answer_start": [54, 46, 54]}, "prediction_text": "10 July 1856"}
{"answers": {"text": ["1943", "1943", "1943"], "answer_start": [71, 71, 71]}, "prediction_text": "1943"}
{"answers": {"text": ["Serbian", "Serbian", "Serbian"], "answer_start": [14, 83, 14]}, "prediction_text": "Serbia"}
{"answers": {"text": ["alternating current", "alternating current", "alternating current"], "answer_start": [237, 237, 237]}, "prediction_text": "Alternating Current Supply System"}
{"answers": {"text": ["1884", "1884", "1884"], "answer_start": [106, 106, 106]}, "prediction_text": "1884"}
{"answers": {"text": ["Thomas Edison", "Thomas Edison", "Thomas Edison"], "answer_start": [123, 123, 123]}, "prediction_text": "Thomas Edison"}
{"answers": {"text": ["George Westinghouse", "George Westinghouse", "George Westinghouse"], "answer_start": [354, 354, 354]}, "prediction_text": "George Westinghouse"}
{"answers": {"text": ["New York City", "New York City", "New York City"], "answer_start": [140, 140, 140]}, "prediction_text": "New York City"}
{"answers": {"text": ["War of Currents", "War of Currents", "War of Currents"], "answer_start": [556, 556, 556]}, "prediction_text": "Current War"}
{"answers": {"text": ["1884", "1884", "1884"], "answer_start": [106, 106, 106]}, "prediction_text": "1884"}
{"answers": {"text": ["Thomas Edison", "Thomas Edison", "Thomas Edison"], "answer_start": [123, 123, 123]}, "prediction_text": "George Westinghouse"}
{"answers": {"text": ["New York City", "New York City", "New York City"], "answer_start": [140, 140, 140]}, "prediction_text": "New York City"}
{"answers": {"text": ["George Westinghouse", "George Westinghouse", "George Westinghouse"], "answer_start": [354, 354, 354]}, "prediction_text": "George Westinghouse"}
{"answers": {"text": ["transformer", "transformer", "transformer"], "answer_start": [325, 325, 325]}, "prediction_text": "AC induction motor and transformer."}
{"answers": {"text": ["1893", "1893", "1893"], "answer_start": [188, 188, 188]}, "prediction_text": "1893"}
{"answers": {"text": ["high-voltage", "high-voltage", "high-voltage"], "answer_start": [91, 91, 91]}, "prediction_text": "High-voltage."}
{"answers": {"text": ["mechanical oscillators/generators, electrical discharge tubes, and early X-ray imaging", "power experiments", "high-voltage, high-frequency power"], "answer_start": [484, 120, 91]}, "prediction_text": "Mechanical oscillators/generators, electrical discharge tubes, X-ray imaging."}
{"answers": {"text": ["Colorado Springs", "Colorado Springs", "Colorado Springs"], "answer_start": [154, 154, 154]}, "prediction_text": "Colorado Springs"}
{"answers": {"text": ["1893", "1893", "1893"], "answer_start": [188, 188, 188]}, "prediction_text": "1893"}
{"answers": {"text": ["boat", "boat", "boat"], "answer_start": [608, 608, 608]}, "prediction_text": "Wardenclyffe Tower"}
{"answers": {"text": ["Wardenclyffe Tower project", "Wardenclyffe Tower", "Wardenclyffe Tower"], "answer_start": [399, 399, 399]}, "prediction_text": "Wardenclyffe Tower"}
{"answers": {"text": ["1943", "1943", "1943"], "answer_start": [399, 399, 399]}, "prediction_text": "1943"}
{"answers": {"text": ["SI unit of magnetic flux density", "the SI unit of magnetic flux density", "SI unit of magnetic flux density"], "answer_start": [529, 525, 529]}, "prediction_text": "SI unit of magnetic flux density."}
{"answers": {"text": ["New York hotels", "New York hotels", "New York hotels"], "answer_start": [334, 334, 334]}, "prediction_text": "New York hotels."}
{"answers": {"text": ["mad scientist", "mad scientist", "mad scientist"], "answer_start": [130, 130, 130]}, "prediction_text": "Mad scientist"}
{"answers": {"text": ["patents", "His patents", "His patents"], "answer_start": [150, 146, 146]}, "prediction_text": "He used his patents."}
{"answers": {"text": ["1943", "1943", "1943"], "answer_start": [399, 399, 399]}, "prediction_text": "1943"}
{"answers": {"text": ["SI unit of magnetic flux density", "the SI unit of magnetic flux density", "SI unit of magnetic flux density"], "answer_start": [529, 525, 529]}, "prediction_text": "tesla"}
{"answers": {"text": ["1990s", "the 1990s", "1990s"], "answer_start": [653, 649, 653]}, "prediction_text": "1990s"}
{"answers": {"text": ["showmanship", "showmanship", "showmanship"], "answer_start": [44, 44, 44]}, "prediction_text": "Showmanship."}
{"answers": {"text": ["Croatia", "Croatia", "Croatia"], "answer_start": [120, 120, 120]}, "prediction_text": "Croatia"}
{"answers": {"text": ["priest", "priest", "Serbian Orthodox priest"], "answer_start": [180, 180, 163]}, "prediction_text": "Serbian Orthodox priest."}
{"answers": {"text": ["eidetic", "eidetic", "eidetic"], "answer_start": [451, 451, 451]}, "prediction_text": "eidetic memory"}
{"answers": {"text": ["his mother's genetics", "his mother", "his mother"], "answer_start": [492, 492, 492]}, "prediction_text": "His mother's genetics and influence."}
{"answers": {"text": ["priest", "priest", "Serbian Orthodox priest"], "answer_start": [180, 180, 163]}, "prediction_text": "Serbian Orthodox priest."}
{"answers": {"text": ["Milutin Tesla", "Milutin Tesla", "Milutin Tesla"], "answer_start": [142, 142, 142]}, "prediction_text": "Milutin Tesla"}
{"answers": {"text": ["\u0110uka Tesla", "\u0110uka Tesla", "\u0110uka Tesla (n\u00e9e Mandi\u0107)"], "answer_start": [204, 204, 204]}, "prediction_text": "\u0110uka Tesla"}
{"answers": {"text": ["making home craft tools, mechanical appliances, and the ability to memorize Serbian epic poems", "making home craft tools, mechanical appliances, and the ability to memorize Serbian epic poems", "making home craft tools, mechanical appliances, and the ability to memorize Serbian epic poems"], "answer_start": [291, 291, 291]}, "prediction_text": "Genetics and influence."}
{"answers": {"text": ["his mother's genetics and influence", "his mother", "his mother's genetics and influence"], "answer_start": [492, 492, 492]}, "prediction_text": "Mother's genetics and influence."}
{"answers": {"text": ["four", "four", "four"], "answer_start": [14, 14, 14]}, "prediction_text": "5"}
{"answers": {"text": ["German", "German", "German"], "answer_start": [270, 270, 270]}, "prediction_text": "German"}
{"answers": {"text": ["1862", "1862", "1862"], "answer_start": [307, 307, 307]}, "prediction_text": "1862"}
{"answers": {"text": ["Dane", "Dane", "Dane"], "answer_start": [69, 69, 69]}, "prediction_text": "Dane"}
{"answers": {"text": ["Milka, Angelina and Marica", "Milka, Angelina and Marica", "Milka, Angelina and Marica"], "answer_start": [93, 93, 93]}, "prediction_text": "Milka, Angelina, and Marica."}
{"answers": {"text": ["killed in a horse-riding accident", "killed in a horse-riding accident", "killed in a horse-riding accident"], "answer_start": [130, 130, 130]}, "prediction_text": "Dane died in a horse-riding accident."}
{"answers": {"text": ["Gospi\u0107, Austrian Empire", "Gospi\u0107", "Gospi\u0107"], "answer_start": [339, 339, 339]}, "prediction_text": "Gospi\u0107, Austrian Empire."}
{"answers": {"text": ["pastor", "a pastor", "pastor"], "answer_start": [397, 395, 397]}, "prediction_text": "Pastor"}
{"answers": {"text": ["Martin Sekuli\u0107", "Martin Sekuli\u0107", "math teacher Martin Sekuli\u0107"], "answer_start": [134, 134, 121]}, "prediction_text": "Martin Sekuli\u0107"}
{"answers": {"text": ["German", "German", "German"], "answer_start": [178, 178, 178]}, "prediction_text": "German"}
{"answers": {"text": ["integral calculus", "integral calculus", "integral calculus"], "answer_start": [278, 278, 278]}, "prediction_text": "Integral calculus."}
{"answers": {"text": ["cheating", "cheating", "cheating"], "answer_start": [360, 360, 360]}, "prediction_text": "Cheating."}
{"answers": {"text": ["1873", "1873", "1873"], "answer_start": [429, 429, 429]}, "prediction_text": "1873"}
{"answers": {"text": ["1870", "In 1870", "1870"], "answer_start": [3, 0, 3]}, "prediction_text": "1870"}
{"answers": {"text": ["to attend school", "to attend school", "attend school at the Higher Real Gymnasium"], "answer_start": [34, 34, 37]}, "prediction_text": "To attend school at the Higher Real Gymnasium."}
{"answers": {"text": ["Martin Sekuli\u0107", "Martin Sekuli\u0107", "math teacher Martin Sekuli\u0107"], "answer_start": [134, 134, 121]}, "prediction_text": "Martin Sekuli\u0107"}
{"answers": {"text": ["German", "German", "German"], "answer_start": [178, 178, 178]}, "prediction_text": "German"}
{"answers": {"text": ["1873", "1873", "1873"], "answer_start": [429, 429, 429]}, "prediction_text": "1873"}
{"answers": {"text": ["cholera", "cholera", "cholera"], "answer_start": [94, 94, 94]}, "prediction_text": "Cholera"}
{"answers": {"text": ["nine months", "nine months", "nine months"], "answer_start": [124, 124, 124]}, "prediction_text": "Nine months."}
{"answers": {"text": ["the best engineering school", "to send him to the best engineering school", "send him to the best engineering school if he recovered"], "answer_start": [235, 220, 223]}, "prediction_text": "A best engineering school."}
{"answers": {"text": ["enter the priesthood", "enter the priesthood", "enter the priesthood"], "answer_start": [337, 337, 337]}, "prediction_text": "Send him to the best engineering school."}
{"answers": {"text": ["Smiljan", "Smiljan", "his birthtown, Smiljan"], "answer_start": [42, 42, 27]}, "prediction_text": "Smiljan"}
{"answers": {"text": ["1873", "1873", "1873"], "answer_start": [3, 3, 3]}, "prediction_text": "1873"}
{"answers": {"text": ["cholera", "cholera", "cholera"], "answer_start": [94, 94, 94]}, "prediction_text": "Cholera"}
{"answers": {"text": ["nine months", "nine months", "nine months"], "answer_start": [124, 124, 124]}, "prediction_text": "Nine months."}
{"answers": {"text": ["enter the priesthood", "the priesthood", "enter the priesthood"], "answer_start": [337, 343, 337]}, "prediction_text": "Send him to the best engineering school."}
{"answers": {"text": ["to send him to the best engineering school", "to send him to the best engineering school", "promised to send him to the best engineering school"], "answer_start": [220, 220, 211]}, "prediction_text": "Send him to the best engineering school."}
{"answers": {"text": ["Tomingaj", "Tomingaj", "Tomingaj"], "answer_start": [97, 97, 97]}, "prediction_text": "Tomingaj"}
{"answers": {"text": ["Mark Twain", "Mark Twain's", "Mark Twain's"], "answer_start": [319, 319, 319]}, "prediction_text": "Mark Twain"}
{"answers": {"text": ["the mountains", "the mountains", "mountains"], "answer_start": [139, 139, 143]}, "prediction_text": "Mountains"}
{"answers": {"text": ["1874", "1874", "1874"], "answer_start": [3, 3, 3]}, "prediction_text": "1874"}
{"answers": {"text": ["hunter's garb", "hunter's garb", "hunter's garb"], "answer_start": [156, 156, 156]}, "prediction_text": "Hunter's garb."}
{"answers": {"text": ["being drafted into the Austro-Hungarian Army", "being drafted", "being drafted into the Austro-Hungarian Army"], "answer_start": [22, 22, 22]}, "prediction_text": "He ran away to Tomingaj."}
{"answers": {"text": ["1874", "In 1874", "1874"], "answer_start": [3, 0, 3]}, "prediction_text": "1874"}
{"answers": {"text": ["he explored the mountains in hunter's garb", "he explored", "explored the mountains in hunter's garb"], "answer_start": [127, 127, 130]}, "prediction_text": "Explored the mountains."}
{"answers": {"text": ["Mark Twain", "Mark Twain", "Mark Twain"], "answer_start": [319, 319, 319]}, "prediction_text": "Mark Twain"}
{"answers": {"text": ["1875", "1875", "1875"], "answer_start": [3, 3, 3]}, "prediction_text": "1875"}
{"answers": {"text": ["Austrian Polytechnic", "Austrian Polytechnic", "Austrian Polytechnic"], "answer_start": [27, 27, 27]}, "prediction_text": "Austrian Polytechnic in Graz, Austria."}
{"answers": {"text": ["1879", "1879", "1879"], "answer_start": [611, 611, 611]}, "prediction_text": "1879"}
{"answers": {"text": ["gambling", "gambling", "gambling"], "answer_start": [1012, 1012, 1012]}, "prediction_text": "Gambling."}
{"answers": {"text": ["no", "not", "He never graduated"], "answer_start": [474, 1436, 1389]}, "prediction_text": "No."}
{"answers": {"text": ["Graz, Austria", "in Graz, Austria", "Graz, Austria"], "answer_start": [51, 48, 51]}, "prediction_text": "Graz, Austria"}
{"answers": {"text": ["1875", "In 1875", "1875"], "answer_start": [3, 0, 3]}, "prediction_text": "1875"}
{"answers": {"text": ["1879", "in 1879", "1879"], "answer_start": [611, 608, 611]}, "prediction_text": "1879"}
{"answers": {"text": ["gambled", "gambling", "gambling"], "answer_start": [1051, 1012, 1012]}, "prediction_text": "Gambled away his allowance."}
{"answers": {"text": ["Tesla would be killed through overwork", "killed through overwork", "would be killed through overwork"], "answer_start": [738, 753, 744]}, "prediction_text": "Death."}
{"answers": {"text": ["left Graz", "left Graz", "left Graz and severed all relations with his family"], "answer_start": [24, 24, 24]}, "prediction_text": "Left Graz."}
{"answers": {"text": ["to hide the fact that he dropped out of school", "hide the fact that he dropped out of school", "to hide the fact that he dropped out of school"], "answer_start": [76, 79, 76]}, "prediction_text": "To hide the fact that he dropped out of school."}
{"answers": {"text": ["a draftsman", "a draftsman", "as a draftsman"], "answer_start": [242, 242, 239]}, "prediction_text": "Draftsman"}
{"answers": {"text": ["return home", "return home", "return home"], "answer_start": [410, 410, 410]}, "prediction_text": "Return home."}
{"answers": {"text": ["nervous breakdown", "nervous breakdown", "nervous breakdown"], "answer_start": [461, 461, 461]}, "prediction_text": "Nervous breakdown."}
{"answers": {"text": ["1878", "December 1878", "to hide the fact that he dropped out of school"], "answer_start": [12, 3, 76]}, "prediction_text": "December 1878"}
{"answers": {"text": ["that he dropped out of school", "the fact that he dropped out of school", "he dropped out of school"], "answer_start": [93, 84, 98]}, "prediction_text": "Graz"}
{"answers": {"text": ["His friends thought that he had drowned in the Mur River.", "drowned in the Mur River", "drowned in the Mur River"], "answer_start": [124, 156, 156]}, "prediction_text": "Nikola suffered a nervous breakdown."}
{"answers": {"text": ["draftsman", "he worked as a draftsman", "draftsman"], "answer_start": [244, 229, 244]}, "prediction_text": "Draftsman"}
{"answers": {"text": ["nervous breakdown", "a nervous breakdown", "nervous breakdown"], "answer_start": [461, 459, 461]}, "prediction_text": "Nervous breakdown."}
{"answers": {"text": ["not having a residence permit", "not having a residence permit", "not having a residence permit"], "answer_start": [70, 70, 70]}, "prediction_text": "Not having a residence permit."}
{"answers": {"text": ["March 1879", "On 24 March 1879", "24 March 1879"], "answer_start": [6, 0, 3]}, "prediction_text": "24 March 1879"}
{"answers": {"text": ["60", "60", "60"], "answer_start": [152, 152, 152]}, "prediction_text": "60"}
{"answers": {"text": ["a stroke"], "answer_start": [239]}, "prediction_text": "Stroke"}
{"answers": {"text": ["taught", "Higher Real Gymnasium", "taught a large class of students"], "answer_start": [274, 326, 274]}, "prediction_text": "taught a large class of students in his old school in Gospi\u0107."}
{"answers": {"text": ["for not having a residence permit.", "for not having a residence permit", "not having a residence permit"], "answer_start": [66, 66, 70]}, "prediction_text": "For not having a residence permit."}
{"answers": {"text": ["1879", "On 24 March 1879,", "24 March 1879"], "answer_start": [12, 0, 3]}, "prediction_text": "1879"}
{"answers": {"text": ["Higher Real Gymnasium", "Higher Real Gymnasium", "Higher Real Gymnasium"], "answer_start": [326, 326, 326]}, "prediction_text": "Higher Real Gymnasium"}
{"answers": {"text": ["stroke", "a stroke", "a stroke"], "answer_start": [241, 239, 239]}, "prediction_text": "Stroke"}
{"answers": {"text": ["Prague", "Prague", "Prague"], "answer_start": [94, 94, 94]}, "prediction_text": "Prague"}
{"answers": {"text": ["arrived too late", "arrived too late to enroll", "he arrived too late to enroll at Charles-Ferdinand University; he never studied Greek, a required subject; and he was illiterate in Czech"], "answer_start": [142, 142, 139]}, "prediction_text": "He was illiterate in Czech."}
{"answers": {"text": ["as an auditor", "as an auditor", "as an auditor"], "answer_start": [369, 369, 369]}, "prediction_text": "Lectures."}
{"answers": {"text": ["Charles-Ferdinand University", "Charles-Ferdinand University", "Charles-Ferdinand University"], "answer_start": [172, 172, 172]}, "prediction_text": "Charles-Ferdinand University"}
{"answers": {"text": ["Prague", "Prague", "Prague"], "answer_start": [94, 94, 94]}, "prediction_text": "Prague"}
{"answers": {"text": ["1880", "In January 1880", "In January 1880"], "answer_start": [11, 0, 0]}, "prediction_text": "January 1880"}
{"answers": {"text": ["Charles-Ferdinand University", "at Charles-Ferdinand University", "Charles-Ferdinand University"], "answer_start": [172, 169, 172]}, "prediction_text": "Charles-Ferdinand University"}
{"answers": {"text": ["two of Tesla's uncles", "Tesla's uncles", "two of Tesla's uncles"], "answer_start": [17, 24, 17]}, "prediction_text": "Uncles"}
{"answers": {"text": ["Budapest", "to Budapest", "Budapest"], "answer_start": [24, 21, 24]}, "prediction_text": "Budapest"}
{"answers": {"text": ["Budapest Telephone Exchange", "the Budapest Telephone Exchange", "Budapest Telephone Exchange"], "answer_start": [304, 85, 89]}, "prediction_text": "Budapest Telephone Exchange"}
{"answers": {"text": ["chief electrician", "chief electrician", "chief electrician"], "answer_start": [378, 378, 378]}, "prediction_text": "Chief electrician"}
{"answers": {"text": ["a telephone repeater or amplifier", "a telephone repeater or amplifier", "a telephone repeater or amplifier"], "answer_start": [521, 521, 521]}, "prediction_text": "A telephone repeater or amplifier."}
{"answers": {"text": ["draftsman", "chief electrician", "draftsman"], "answer_start": [228, 378, 228]}, "prediction_text": "Chief electrician"}
{"answers": {"text": ["1881", "1881", "1881"], "answer_start": [3, 3, 3]}, "prediction_text": "1881"}
{"answers": {"text": ["a telegraph company", "the Budapest Telephone Exchange", "Budapest Telephone Exchange"], "answer_start": [64, 85, 89]}, "prediction_text": "Budapest Telephone Exchange"}
{"answers": {"text": ["Budapest Telephone Exchange", "the Budapest Telephone Exchange", "Budapest Telephone Exchange"], "answer_start": [89, 85, 89]}, "prediction_text": "Budapest Telephone Exchange"}
{"answers": {"text": ["chief electrician", "chief electrician", "chief electrician"], "answer_start": [378, 378, 378]}, "prediction_text": "Chief electrician"}
{"answers": {"text": ["1882", "1882", "1882"], "answer_start": [3, 3, 3]}, "prediction_text": "1882"}
{"answers": {"text": ["France", "in France", "France"], "answer_start": [67, 64, 67]}, "prediction_text": "France"}
{"answers": {"text": ["New York City", "to New York City", "New York City"], "answer_start": [164, 161, 164]}, "prediction_text": "New York City"}
{"answers": {"text": ["Thomas Edison", "Thomas Edison", "Thomas Edison"], "answer_start": [206, 206, 206]}, "prediction_text": "Thomas Edison"}
{"answers": {"text": ["Edison Machine Works", "Edison Machine Works", "Edison Machine Works"], "answer_start": [235, 235, 235]}, "prediction_text": "New York City"}
{"answers": {"text": ["Continental Edison Company", "Continental Edison Company", "Continental Edison Company"], "answer_start": [37, 37, 37]}, "prediction_text": "France"}
{"answers": {"text": ["France", "in France", "France"], "answer_start": [67, 64, 67]}, "prediction_text": "France"}
{"answers": {"text": ["1884", "1884", "1884"], "answer_start": [142, 142, 142]}, "prediction_text": "June 1884"}
{"answers": {"text": ["Thomas Edison", "Thomas Edison", "Thomas Edison"], "answer_start": [206, 206, 206]}, "prediction_text": "Thomas Edison"}
{"answers": {"text": ["Manhattan's lower east side", "Manhattan's lower east side", "New York City"], "answer_start": [259, 259, 164]}, "prediction_text": "Manhattan"}
{"answers": {"text": ["fifty thousand dollars", "fifty thousand dollars", "fifty thousand dollars"], "answer_start": [281, 281, 281]}, "prediction_text": "US$10 a week."}
{"answers": {"text": ["$10 a week raise", "a US$10 a week raise", "a US$10 a week raise over Tesla's US$18 per week salary"], "answer_start": [690, 686, 686]}, "prediction_text": "A US$10 a week raise over his US$18 per week salary."}
{"answers": {"text": ["months", "months", "months"], "answer_start": [490, 490, 490]}, "prediction_text": "months"}
{"answers": {"text": ["fifty thousand dollars", "fifty thousand dollars", "fifty thousand dollars"], "answer_start": [281, 281, 281]}, "prediction_text": "US$10 a week."}
{"answers": {"text": ["American humor.", "Tesla, you don't understand our American humor", "you don't understand our American humor"], "answer_start": [641, 609, 616]}, "prediction_text": "\"Tesla, you don't understand our American humor.\":64  :64  :64  :64  :64  :64  :64  :64  :64  :64  :64  :64  :64  :64  :64  :64  :64  :64  :64  :64  :64  :64  :64  :64  :64  :64  :64  :64  :64  :64  :64  :64  :64  :64  :64  :64  :64  :64  :64  :64  :64  :64  :64  :64  :64  :64"}
{"answers": {"text": ["US$10 a week raise", "a US$10 a week raise", "a US$10 a week raise"], "answer_start": [688, 686, 686]}, "prediction_text": "US$10 a week raise over US$18 per week salary."}
{"answers": {"text": ["Robert Lane and Benjamin Vail", "Robert Lane and Benjamin Vail", "Robert Lane and Benjamin Vail"], "answer_start": [77, 77, 77]}, "prediction_text": "Benjamin Vail"}
{"answers": {"text": ["Tesla Electric Light & Manufacturing", "Tesla Electric Light & Manufacturing", "an electric lighting company in Tesla's name"], "answer_start": [176, 176, 130]}, "prediction_text": "Tesla Electric Light & Manufacturing."}
{"answers": {"text": ["installed electrical arc light based illumination systems designed by Tesla", "installed electrical arc light based illumination systems", "installed electrical arc light based illumination systems designed by Tesla and also had designs for dynamo electric machine commutators"], "answer_start": [226, 226, 226]}, "prediction_text": "Finished electrical arc light based illumination systems designed by Tesla and also had designs for dynamo electric machine commutators."}
{"answers": {"text": ["patents", "patents", "patents"], "answer_start": [374, 374, 374]}, "prediction_text": "Electrical arc light."}
{"answers": {"text": ["dynamo electric machine commutators", "electrical arc light based illumination systems", "electrical arc light based illumination systems"], "answer_start": [327, 236, 236]}, "prediction_text": "Electrical arc light based illumination systems."}
{"answers": {"text": ["Robert Lane and Benjamin Vail", "Robert Lane and Benjamin Vail", "Robert Lane and Benjamin Vail"], "answer_start": [77, 77, 77]}, "prediction_text": "Robert Lane and Benjamin Vail"}
{"answers": {"text": ["1886", "in 1886", "1886"], "answer_start": [71, 68, 71]}, "prediction_text": "1886"}
{"answers": {"text": ["Tesla Electric Light & Manufacturing", "Tesla Electric Light & Manufacturing", "Tesla Electric Light & Manufacturing"], "answer_start": [176, 176, 176]}, "prediction_text": "Tesla Electric Light & Manufacturing"}
{"answers": {"text": ["installed electrical arc light based illumination systems", "installed electrical arc light based illumination systems", "installed electrical arc light based illumination systems designed by Tesla and also had designs for dynamo electric machine commutators"], "answer_start": [226, 226, 226]}, "prediction_text": "Finished electrical arc light based illumination systems designed by Tesla and also had designs for dynamo electric machine commutators."}
{"answers": {"text": ["Tesla", "Tesla", "Tesla"], "answer_start": [31, 296, 176]}, "prediction_text": "Tesla Electric Light & Manufacturing."}
{"answers": {"text": ["forced Tesla out", "forced Tesla out", "forced Tesla out leaving him penniless"], "answer_start": [229, 229, 229]}, "prediction_text": "Force him out."}
{"answers": {"text": ["penniless", "penniless", "penniless"], "answer_start": [258, 258, 258]}, "prediction_text": "penniless"}
{"answers": {"text": ["ditch digger", "ditch digger", "ditch digger"], "answer_start": [445, 445, 445]}, "prediction_text": "Ditch digger."}
{"answers": {"text": ["1886/1887", "1886/1887", "1886/1887"], "answer_start": [505, 505, 505]}, "prediction_text": "1886/1887"}
{"answers": {"text": ["assigned them to the company in lieu of stock.", "assigned them to the company", "assigned them to the company in lieu of stock"], "answer_start": [335, 335, 335]}, "prediction_text": "He lost control of the patents he had generated since he had assigned them to the company in lieu of stock."}
{"answers": {"text": ["ditch digger", "ditch digger", "ditch digger"], "answer_start": [445, 445, 445]}, "prediction_text": "Ditch digger."}
{"answers": {"text": ["various electrical repair jobs", "various electrical repair jobs", "He had to work at various electrical repair jobs"], "answer_start": [400, 400, 382]}, "prediction_text": "Ditch digger."}
{"answers": {"text": ["a Western Union superintendent", "a Western Union superintendent", "a Western Union superintendent"], "answer_start": [40, 40, 40]}, "prediction_text": "Charles F. Peck"}
{"answers": {"text": ["April 1887", "in April 1887", "April 1887"], "answer_start": [334, 331, 334]}, "prediction_text": "April 1887"}
{"answers": {"text": ["\u2153 to Tesla, \u2153 to Peck and Brown, and \u2153 to fund development", "\u2153 to Tesla, \u2153 to Peck and Brown, and \u2153 to fund development", "\u2153 to Tesla, \u2153 to Peck and Brown, and \u2153 to fund development"], "answer_start": [447, 447, 447]}, "prediction_text": "Tesla."}
{"answers": {"text": ["Manhattan", "at 89 Liberty Street", "89 Liberty Street in Manhattan"], "answer_start": [566, 542, 545]}, "prediction_text": "Manhattan"}
{"answers": {"text": ["1886", "late 1886", "In late 1886"], "answer_start": [8, 3, 0]}, "prediction_text": "Late 1886"}
{"answers": {"text": ["Western Union superintendent", "Western Union superintendent", "Western Union superintendent"], "answer_start": [42, 42, 42]}, "prediction_text": "Western Union superintendent"}
{"answers": {"text": ["Charles F. Peck", "Charles F. Peck", "New York attorney Charles F. Peck"], "answer_start": [94, 94, 76]}, "prediction_text": "Alfred S. Brown"}
{"answers": {"text": ["89 Liberty Street in Manhattan", "Manhattan", "89 Liberty Street in Manhattan"], "answer_start": [545, 566, 545]}, "prediction_text": "Manhattan"}
{"answers": {"text": ["Tesla Electric Company", "the Tesla Electric Company", "Tesla Electric Company"], "answer_start": [361, 357, 361]}, "prediction_text": "Tesla Electric Company"}
{"answers": {"text": ["an induction motor", "an induction motor", "induction motor that ran on alternating current"], "answer_start": [65, 65, 68]}, "prediction_text": "Induction motor."}
{"answers": {"text": ["May 1888", "in May 1888", "May 1888"], "answer_start": [464, 461, 464]}, "prediction_text": "May 1888"}
{"answers": {"text": ["a commutator", "commutator", "a commutator"], "answer_start": [526, 528, 526]}, "prediction_text": "Commutator"}
{"answers": {"text": ["sparking", "sparking and the high maintenance", "sparking and the high maintenance of constantly servicing and replacing mechanical brushes"], "answer_start": [554, 554, 554]}, "prediction_text": "Sparking and high maintenance."}
{"answers": {"text": ["self-starting", "self-starting", "induction"], "answer_start": [487, 487, 68]}, "prediction_text": "self-starting design"}
{"answers": {"text": ["1887", "1887", "1887"], "answer_start": [56, 56, 56]}, "prediction_text": "1887"}
{"answers": {"text": ["because of its advantages in long-distance, high-voltage transmission", "advantages in long-distance, high-voltage transmission", "because of its advantages in long-distance, high-voltage transmission"], "answer_start": [201, 216, 201]}, "prediction_text": "Long-distance, high-voltage transmission."}
{"answers": {"text": ["mechanical brushes", "a commutator", "mechanical brushes"], "answer_start": [626, 526, 626]}, "prediction_text": "Mechanical brushes."}
{"answers": {"text": ["1888", "in May 1888", "1888"], "answer_start": [468, 461, 468]}, "prediction_text": "May 1888"}
{"answers": {"text": ["editor of Electrical World magazine", "the editor of Electrical World magazine", "editor of Electrical World magazine"], "answer_start": [13, 9, 13]}, "prediction_text": "Thomas Commerford Martin"}
{"answers": {"text": ["American Institute of Electrical Engineers", "the American Institute of Electrical Engineers", "American Institute of Electrical Engineers"], "answer_start": [205, 201, 205]}, "prediction_text": "American Institute of Electrical Engineers (now IEEE)"}
{"answers": {"text": ["1888", "1888", "1888"], "answer_start": [3, 3, 3]}, "prediction_text": "1888"}
{"answers": {"text": ["decided Tesla's patent would probably control the market", "decided Tesla's patent would probably control the market", "Tesla's patent would probably control the market"], "answer_start": [692, 692, 700]}, "prediction_text": "Tesla's patent."}
{"answers": {"text": ["Thomas Commerford Martin", "Thomas Commerford Martin", "Thomas Commerford Martin"], "answer_start": [50, 50, 50]}, "prediction_text": "Thomas Commerford Martin"}
{"answers": {"text": ["Thomas Commerford Martin", "Thomas Commerford Martin", "Thomas Commerford Martin"], "answer_start": [50, 50, 50]}, "prediction_text": "Thomas Commerford Martin"}
{"answers": {"text": ["George Westinghouse", "George Westinghouse", "George Westinghouse"], "answer_start": [344, 344, 344]}, "prediction_text": "Westinghouse Electric & Manufacturing Company."}
{"answers": {"text": ["Galileo Ferraris", "Galileo Ferraris", "the Italian physicist Galileo Ferraris"], "answer_start": [670, 670, 648]}, "prediction_text": "George Westinghouse"}
{"answers": {"text": ["physicist", "physicist", "Italian physicist"], "answer_start": [660, 660, 652]}, "prediction_text": "Galileo Ferraris"}
{"answers": {"text": ["Westinghouse Electric & Manufacturing Company", "Westinghouse Electric & Manufacturing Company", "Westinghouse Electric & Manufacturing Company"], "answer_start": [286, 286, 286]}, "prediction_text": "Westinghouse Electric & Manufacturing Company"}
{"answers": {"text": ["1888", "1888", "1888"], "answer_start": [8, 8, 8]}, "prediction_text": "1888"}
{"answers": {"text": ["$60,000 in cash and stock and a royalty of $2.50 per AC horsepower produced by each motor", "$60,000 in cash and stock and a royalty", "$60,000 in cash and stock and a royalty of $2.50 per AC horsepower produced by each motor"], "answer_start": [148, 148, 148]}, "prediction_text": "$60,000"}
{"answers": {"text": ["George Westinghouse", "Westinghouse", "George Westinghouse"], "answer_start": [62, 239, 62]}, "prediction_text": "George Westinghouse"}
{"answers": {"text": ["consultant", "consultant", "consultant"], "answer_start": [357, 357, 357]}, "prediction_text": "Consultant"}
{"answers": {"text": ["$60,000 in cash and stock and a royalty of $2.50 per AC horsepower produced by each motor", "$60,000 in cash and stock and a royalty", "$60,000 in cash and stock and a royalty of $2.50 per AC horsepower produced by each motor"], "answer_start": [148, 148, 148]}, "prediction_text": "$2,000 per month."}
{"answers": {"text": ["1888", "In July 1888,", "July 1888"], "answer_start": [8, 0, 3]}, "prediction_text": "July 1888"}
{"answers": {"text": ["$2,000", "$2,000", "$2,000"], "answer_start": [303, 303, 303]}, "prediction_text": "$2,000"}
{"answers": {"text": ["Pittsburgh", "Pittsburgh", "Pittsburgh"], "answer_start": [423, 423, 423]}, "prediction_text": "Pittsburgh"}
{"answers": {"text": ["Pittsburgh", "Pittsburgh", "Pittsburgh"], "answer_start": [34, 34, 34]}, "prediction_text": "Pittsburgh"}
{"answers": {"text": ["system to power the city's streetcars", "alternating current system", "an alternating current system to power the city's streetcars"], "answer_start": [87, 67, 64]}, "prediction_text": "AC power."}
{"answers": {"text": ["60-cycle", "60", "60"], "answer_start": [301, 301, 301]}, "prediction_text": "60-cycle AC current system."}
{"answers": {"text": ["DC traction motor", "a DC traction motor", "DC traction motor"], "answer_start": [548, 546, 548]}, "prediction_text": "DC traction motor"}
{"answers": {"text": ["to power the city's streetcars.", "the city's streetcars", "street cars"], "answer_start": [94, 103, 513]}, "prediction_text": "Streetcars."}
{"answers": {"text": ["a DC traction motor", "a DC traction motor", "DC traction motor"], "answer_start": [546, 546, 548]}, "prediction_text": "DC traction motor"}
{"answers": {"text": ["Thomas Edison and George Westinghouse", "Thomas Edison and George Westinghouse", "Thomas Edison and George Westinghouse"], "answer_start": [238, 238, 238]}, "prediction_text": "Thomas Edison and George Westinghouse."}
{"answers": {"text": ["lighting systems", "electrical distribution", "a competition between rival lighting systems"], "answer_start": [447, 187, 419]}, "prediction_text": "Edison and George Westinghouse."}
{"answers": {"text": ["AC development", "AC development", "AC development"], "answer_start": [1033, 1033, 1033]}, "prediction_text": "AC development."}
{"answers": {"text": ["General Electric", "General Electric", "General Electric"], "answer_start": [1172, 1172, 1172]}, "prediction_text": "General Electric"}
{"answers": {"text": ["AC", "AC", "AC"], "answer_start": [139, 1210, 139]}, "prediction_text": "AC"}
{"answers": {"text": ["Thomas Edison", "Thomas Edison", "Thomas Edison"], "answer_start": [238, 1068, 238]}, "prediction_text": "Thomas Edison"}
{"answers": {"text": ["1888", "1888", "1888"], "answer_start": [108, 393, 393]}, "prediction_text": "1888"}
{"answers": {"text": ["financial strain", "financial strain", "the financial strain of buying up patents and hiring the engineers needed to build"], "answer_start": [823, 823, 819]}, "prediction_text": "Financial strain."}
{"answers": {"text": ["General Electric", "General Electric", "General Electric"], "answer_start": [1172, 1172, 1172]}, "prediction_text": "General Electric"}
{"answers": {"text": ["George Westinghouse", "George Westinghouse", "George Westinghouse"], "answer_start": [9, 9, 9]}, "prediction_text": "George Westinghouse"}
{"answers": {"text": ["Chicago", "Chicago", "Chicago"], "answer_start": [91, 91, 91]}, "prediction_text": "Chicago"}
{"answers": {"text": ["General Electric", "General Electric", "General Electric"], "answer_start": [139, 139, 139]}, "prediction_text": "General Electric."}
{"answers": {"text": ["Tesla Polyphase System", "Tesla Polyphase System"], "answer_start": [494, 494]}, "prediction_text": "Tesla Polyphase System."}
{"answers": {"text": ["Tesla Polyphase System", "electrical effects"], "answer_start": [494, 550]}, "prediction_text": "Tesla Polyphase System."}
{"answers": {"text": ["George Westinghouse", "George Westinghouse", "George Westinghouse"], "answer_start": [9, 9, 9]}, "prediction_text": "George Westinghouse"}
{"answers": {"text": ["Chicago", "Chicago", "Chicago"], "answer_start": [91, 91, 91]}, "prediction_text": "Chicago"}
{"answers": {"text": ["1893", "1893", "1893"], "answer_start": [3, 54, 3]}, "prediction_text": "1893"}
{"answers": {"text": ["AC power", "alternating current", "AC power"], "answer_start": [282, 104, 282]}, "prediction_text": "AC power."}
{"answers": {"text": ["Richard Dean Adams", "Richard Dean Adams", "Richard Dean Adams"], "answer_start": [8, 8, 8]}, "prediction_text": "Richard Dean Adams"}
{"answers": {"text": ["Niagara Falls", "Niagara Falls", "Niagara Falls"], "answer_start": [46, 46, 46]}, "prediction_text": "Niagara Falls Cataract Construction Company"}
{"answers": {"text": ["Westinghouse Electric", "Westinghouse Electric", "Westinghouse Electric"], "answer_start": [955, 955, 955]}, "prediction_text": "Westinghouse Electric"}
{"answers": {"text": ["General Electric", "General Electric", "Westinghouse Electric"], "answer_start": [1048, 1048, 955]}, "prediction_text": "General Electric"}
{"answers": {"text": ["a two-phased system", "a two-phased system", "two-phased system"], "answer_start": [580, 580, 582]}, "prediction_text": "Two-phased system."}
{"answers": {"text": ["Richard Dean Adams", "Richard Dean Adams"], "answer_start": [8, 8]}, "prediction_text": "Richard Dean Adams"}
{"answers": {"text": ["1893", "In 1893", "1893"], "answer_start": [3, 0, 3]}, "prediction_text": "1893"}
{"answers": {"text": ["two-phased system", "a two-phased system", "two-phased system"], "answer_start": [582, 580, 582]}, "prediction_text": "Two-phased system."}
{"answers": {"text": ["most reliable", "most reliable", "would be the most reliable and that there was a Westinghouse system to light incandescent bulbs using two-phase alternating current"], "answer_start": [613, 613, 600]}, "prediction_text": "Reliability."}
{"answers": {"text": ["1896", "1896", "1896"], "answer_start": [240, 240, 240]}, "prediction_text": "1896"}
{"answers": {"text": ["$216,000", "$216,000", "a lump sum payment of $216,000"], "answer_start": [1032, 1032, 1010]}, "prediction_text": "$216,000"}
{"answers": {"text": ["$2.50 per AC horsepower royalty", "$2.50 per AC horsepower", "$2.50 per AC horsepower"], "answer_start": [1183, 1183, 1183]}, "prediction_text": "$2.50 per AC horsepower royalty."}
{"answers": {"text": ["$200,000", "estimated $200,000", "an estimated $200,000"], "answer_start": [502, 492, 489]}, "prediction_text": "$216,000"}
{"answers": {"text": ["J. P. Morgan", "J. P. Morgan", "financier J. P. Morgan"], "answer_start": [73, 73, 63]}, "prediction_text": "J. P. Morgan"}
{"answers": {"text": ["an estimated $200,000", "estimated $200,000", "an estimated $200,000 in licenses and royalties"], "answer_start": [489, 492, 489]}, "prediction_text": "$216,000"}
{"answers": {"text": ["$216,000", "$216,000", "a lump sum payment of $216,000"], "answer_start": [1032, 1032, 1010]}, "prediction_text": "$216,000"}
{"answers": {"text": ["35", "35", "35"], "answer_start": [31, 31, 31]}, "prediction_text": "35"}
{"answers": {"text": ["New York", "South Fifth Avenue", "South Fifth Avenue"], "answer_start": [189, 112, 112]}, "prediction_text": "New York"}
{"answers": {"text": ["electric lamps", "electric lamps wirelessly", "electric lamps"], "answer_start": [206, 206, 206]}, "prediction_text": "lamps"}
{"answers": {"text": ["Tesla coil", "the Tesla coil", "Tesla coil"], "answer_start": [345, 341, 345]}, "prediction_text": "Tesla coil"}
{"answers": {"text": ["1891", "On 30 July 1891", "30 July 1891"], "answer_start": [11, 0, 3]}, "prediction_text": "30 July 1891"}
{"answers": {"text": ["the Tesla coil.", "the Tesla coil", "Tesla coil"], "answer_start": [341, 341, 345]}, "prediction_text": "Tesla coil"}
{"answers": {"text": ["35", "35", "35"], "answer_start": [31, 31, 31]}, "prediction_text": "35"}
{"answers": {"text": ["wireless", "wireless", "wireless"], "answer_start": [221, 282, 282]}, "prediction_text": "Wireless power transmission."}
{"answers": {"text": ["American Institute of Electrical Engineers", "the American Institute of Electrical Engineers", "American Institute of Electrical Engineers"], "answer_start": [40, 36, 40]}, "prediction_text": "American Institute of Electrical Engineers"}
{"answers": {"text": ["American Institute of Electrical Engineers", "the American Institute of Electrical Engineers", "American Institute of Electrical Engineers"], "answer_start": [40, 36, 40]}, "prediction_text": "Institute of Radio Engineers"}
{"answers": {"text": ["1894", "1894", "1894"], "answer_start": [182, 182, 182]}, "prediction_text": "1894"}
{"answers": {"text": ["vice president", "vice president", "vice president"], "answer_start": [18, 18, 18]}, "prediction_text": "Vice President"}
{"answers": {"text": ["1892 to 1894", "from 1892 to 1894", "from 1892 to 1894"], "answer_start": [174, 169, 169]}, "prediction_text": "1892-1894"}
{"answers": {"text": ["the Institute of Radio Engineers", "the Institute of Radio Engineers", "Institute of Radio Engineers"], "answer_start": [111, 111, 115]}, "prediction_text": "Institute of Radio Engineers"}
{"answers": {"text": ["he had noticed damaged film in his laboratory in previous experiments", "he had noticed damaged film in his laboratory", "after he had noticed damaged film in his laboratory in previous experiments"], "answer_start": [109, 109, 103]}, "prediction_text": "He noticed damaged film in his laboratory in previous experiments."}
{"answers": {"text": ["5th Avenue laboratory fire of March 1895", "fire", "fire"], "answer_start": [477, 499, 499]}, "prediction_text": "The 5th Avenue laboratory fire."}
{"answers": {"text": ["December 1895", "December 1895", "December 1895"], "answer_start": [716, 716, 716]}, "prediction_text": "December 1895"}
{"answers": {"text": ["the metal locking screw on the camera lens", "the metal locking screw", "the metal locking screw on the camera lens"], "answer_start": [921, 921, 921]}, "prediction_text": "The metal locking screw on the camera lens."}
{"answers": {"text": ["1894", "1894", "1894"], "answer_start": [12, 12, 12]}, "prediction_text": "1894"}
{"answers": {"text": ["X-Rays", "x-rays", "\"Roentgen rays\" or \"X-Rays\""], "answer_start": [220, 763, 200]}, "prediction_text": "Roentgen rays"}
{"answers": {"text": ["lost in the 5th Avenue laboratory fire of March 1895", "was lost", "was lost in the 5th Avenue laboratory fire of March 1895"], "answer_start": [465, 461, 461]}, "prediction_text": "Lost."}
{"answers": {"text": ["X-ray image", "X-ray", "X-ray image"], "answer_start": [659, 659, 659]}, "prediction_text": "X-ray image."}
{"answers": {"text": ["Mark Twain", "Mark Twain", "Mark Twain"], "answer_start": [798, 798, 798]}, "prediction_text": "Mark Twain"}
{"answers": {"text": ["X-ray imaging", "X-ray imaging", "X-ray imaging"], "answer_start": [73, 147, 147]}, "prediction_text": "X-rays."}
{"answers": {"text": ["March 1896", "In March 1896", "March 1896"], "answer_start": [3, 0, 3]}, "prediction_text": "March 1896"}
{"answers": {"text": ["radiography", "braking radiation", "radiography"], "answer_start": [88, 393, 88]}, "prediction_text": "Bremsstrahlung"}
{"answers": {"text": ["X-rays", "X-rays", "X-rays"], "answer_start": [483, 483, 483]}, "prediction_text": "X-rays."}
{"answers": {"text": ["Tesla Coil", "the Tesla Coil", "Tesla Coil"], "answer_start": [301, 297, 301]}, "prediction_text": "Tesla Coil"}
{"answers": {"text": ["1896", "In March 1896", "March 1896"], "answer_start": [9, 0, 3]}, "prediction_text": "March 1896"}
{"answers": {"text": ["Tesla Coil", "Tesla Coil", "Tesla Coil"], "answer_start": [301, 301, 301]}, "prediction_text": "Tesla Coil"}
{"answers": {"text": ["Roentgen rays", "Roentgen rays", "Roentgen rays"], "answer_start": [575, 575, 575]}, "prediction_text": "Bremsstrahlung or braking radiation."}
{"answers": {"text": ["X-rays were longitudinal waves", "were longitudinal waves", "that X-rays were longitudinal waves, such as those produced in waves in plasmas."], "answer_start": [415, 422, 410]}, "prediction_text": "X-rays were longitudinal waves."}
{"answers": {"text": ["damage to the skin was not caused by the Roentgen rays, but by the ozone generated in contact with the skin", "ozone generated in contact with the skin", "the ozone generated in contact with the skin, and to a lesser extent, by nitrous acid"], "answer_start": [233, 300, 296]}, "prediction_text": "ozone generated in contact with the skin."}
{"answers": {"text": ["skin damage", "damage to the skin", "damage to the skin"], "answer_start": [176, 233, 233]}, "prediction_text": "skin damage"}
{"answers": {"text": ["his circuit and single-node X-ray-producing devices", "X-ray-producing devices", "circuit and single-node X-ray-producing devices"], "answer_start": [40, 68, 44]}, "prediction_text": "circuit and single-node X-ray-producing devices."}
{"answers": {"text": ["force-free magnetic fields", "force-free magnetic fields", "in force-free magnetic fields"], "answer_start": [523, 523, 520]}, "prediction_text": "force-free magnetic fields"}
{"answers": {"text": ["ozone generated in contact with the skin", "ozone generated in contact with the skin", "the ozone generated in contact with the skin, and to a lesser extent, by nitrous acid."], "answer_start": [300, 300, 296]}, "prediction_text": "ozone generated in contact with the skin."}
{"answers": {"text": ["longitudinal waves", "longitudinal waves", "longitudinal waves, such as those produced in waves in plasmas"], "answer_start": [427, 427, 427]}, "prediction_text": "longitudinal waves."}
{"answers": {"text": ["force-free magnetic fields", "waves in plasmas", "in force-free magnetic fields"], "answer_start": [523, 473, 520]}, "prediction_text": "Plasma waves."}
{"answers": {"text": ["In his many notes", "In his many notes"], "answer_start": [93, 93]}, "prediction_text": "In his notes."}
{"answers": {"text": ["Benjamin Lamme", "Benjamin Lamme", "Benjamin Lamme"], "answer_start": [47, 47, 47]}, "prediction_text": "Benjamin Lamme"}
{"answers": {"text": ["1893", "1893", "1893"], "answer_start": [20, 20, 20]}, "prediction_text": "1893"}
{"answers": {"text": ["Westinghouse Electric", "Westinghouse Electric", "Westinghouse Electric"], "answer_start": [149, 149, 149]}, "prediction_text": "Benjamin Lamme"}
{"answers": {"text": ["Egg of Columbus", "the Egg of Columbus", "Egg of Columbus"], "answer_start": [187, 183, 187]}, "prediction_text": "Egg of Columbus"}
{"answers": {"text": ["Tesla", "Tesla", "Tesla"], "answer_start": [0, 0, 0]}, "prediction_text": "Tesla"}
{"answers": {"text": ["1934", "On 11 July 1934", "11 July 1934"], "answer_start": [11, 0, 3]}, "prediction_text": "11 July 1934"}
{"answers": {"text": ["physically strike him", "physically strike him", "physically strike him"], "answer_start": [274, 274, 274]}, "prediction_text": "Struck him."}
{"answers": {"text": ["he could feel a sharp stinging pain where it entered his body", "he could feel a sharp stinging pain", "he could feel a sharp stinging pain"], "answer_start": [309, 309, 309]}, "prediction_text": "He felt a sharp stinging pain."}
{"answers": {"text": ["bits of metal", "bits of metal", "bits of metal projected by his \"electric gun,\""], "answer_start": [455, 455, 455]}, "prediction_text": "bits of metal."}
{"answers": {"text": ["National Electric Light Association", "possibility of the transmission", "National Electric Light Association"], "answer_start": [211, 24, 211]}, "prediction_text": "National Electric Light Association"}
{"answers": {"text": ["Tesla Coil", "the Tesla Coil", "Tesla Coil"], "answer_start": [368, 364, 368]}, "prediction_text": "Tesla Coil"}
{"answers": {"text": ["the Franklin Institute", "the Franklin Institute", "Franklin Institute"], "answer_start": [149, 149, 153]}, "prediction_text": "Franklin Institute"}
{"answers": {"text": ["1898", "In 1898,", "1898"], "answer_start": [3, 0, 3]}, "prediction_text": "1898"}
{"answers": {"text": ["teleautomaton", "teleautomaton", "teleautomaton"], "answer_start": [69, 69, 69]}, "prediction_text": "Teleautomaton"}
{"answers": {"text": ["Madison Square Garden", "Madison Square Garden", "Madison Square Garden"], "answer_start": [133, 133, 133]}, "prediction_text": "Madison Square Garden"}
{"answers": {"text": ["an electrical exhibition", "an electrical exhibition", "an electrical exhibition"], "answer_start": [105, 105, 105]}, "prediction_text": "Electrical exhibition at Madison Square Garden."}
{"answers": {"text": ["monkey", "monkey", "a trained monkey"], "answer_start": [311, 311, 301]}, "prediction_text": "A trained monkey."}
{"answers": {"text": ["1900", "1900", "1900"], "answer_start": [3, 3, 3]}, "prediction_text": "1901"}
{"answers": {"text": ["Marconi", "Guglielmo Marconi", "Guglielmo Marconi"], "answer_start": [132, 122, 122]}, "prediction_text": "Guglielmo Marconi"}
{"answers": {"text": ["1901", "1901", "1901"], "answer_start": [203, 203, 203]}, "prediction_text": "1901"}
{"answers": {"text": ["1943", "1943", "1943"], "answer_start": [476, 476, 476]}, "prediction_text": "1943"}
{"answers": {"text": ["Supreme Court of the United States", "Supreme Court of the United States", "Supreme Court of the United States"], "answer_start": [484, 484, 484]}, "prediction_text": "Supreme Court of the United States"}
{"answers": {"text": ["1899", "1899", "1899"], "answer_start": [10, 10, 10]}, "prediction_text": "1899"}
{"answers": {"text": ["Paris", "Paris", "Pikes Peak to Paris"], "answer_start": [523, 523, 509]}, "prediction_text": "Paris"}
{"answers": {"text": ["15 June 1899", "15 June 1899", "15 June 1899"], "answer_start": [644, 644, 644]}, "prediction_text": "15 June 1899"}
{"answers": {"text": ["five inches", "five inches", "five inches"], "answer_start": [765, 765, 765]}, "prediction_text": "Five inches."}
{"answers": {"text": ["atmospheric", "atmospheric", "atmospheric electricity"], "answer_start": [19, 19, 19]}, "prediction_text": "Atmospheric electricity"}
{"answers": {"text": ["stationary", "stationary", "stationary waves"], "answer_start": [118, 118, 118]}, "prediction_text": "stationary waves"}
{"answers": {"text": ["that the earth had a resonant frequency.", "the earth had a resonant frequency", "earth had a resonant frequency"], "answer_start": [259, 264, 268]}, "prediction_text": "Resonant frequency."}
{"answers": {"text": ["lightning", "lightning", "lightning"], "answer_start": [23, 23, 23]}, "prediction_text": "Artificial lightning."}
{"answers": {"text": ["135 feet", "135 feet long", "consisting of millions of volts and up to 135 feet long"], "answer_start": [92, 92, 50]}, "prediction_text": "135 feet long."}
{"answers": {"text": ["15 miles", "15 miles", "15 miles away"], "answer_start": [150, 150, 150]}, "prediction_text": "15 miles."}
{"answers": {"text": ["glowed even when turned off", "glowed", "glowed even when turned off"], "answer_start": [371, 371, 371]}, "prediction_text": "Sparks jumped from water line taps."}
{"answers": {"text": ["Butterflies were electrified", "electrified", "electrified, swirling in circles with blue halos of St. Elmo's fire around their wings"], "answer_start": [501, 518, 518]}, "prediction_text": "They were electrified."}
{"answers": {"text": ["power outage", "a power outage", "power outage"], "answer_start": [86, 84, 86]}, "prediction_text": "Power outage."}
{"answers": {"text": ["repeatedly burned out", "repeatedly burned out", "repeatedly burned out"], "answer_start": [353, 353, 353]}, "prediction_text": "Burned out."}
{"answers": {"text": ["powerful high frequency currents", "powerful high frequency currents", "powerful high frequency currents"], "answer_start": [387, 387, 387]}, "prediction_text": "High frequency currents."}
{"answers": {"text": ["destroy", "destroy", "jump through the windings and destroy the insulation"], "answer_start": [499, 499, 469]}, "prediction_text": "Destroyed it."}
{"answers": {"text": ["communications from another planet", "communications from another planet", "communications from another planet"], "answer_start": [103, 103, 103]}, "prediction_text": "Mars."}
{"answers": {"text": ["Mars", "Mars", "Mars"], "answer_start": [536, 536, 536]}, "prediction_text": "Mars"}
{"answers": {"text": ["Collier's Weekly", "Collier's Weekly", "Collier's Weekly"], "answer_start": [599, 599, 599]}, "prediction_text": "Collier's Weekly."}
{"answers": {"text": ["intercepted Marconi's European experiments", "he may have intercepted Marconi's European experiments", "he may have intercepted Marconi's European experiments in July 1899"], "answer_start": [870, 858, 858]}, "prediction_text": "Marconi's European experiments."}
{"answers": {"text": ["July 1899", "July 1899", "July 1899"], "answer_start": [916, 916, 916]}, "prediction_text": "July 1899"}
{"answers": {"text": ["$100,000", "$100,000", "$100,000"], "answer_start": [38, 38, 38]}, "prediction_text": "$100,000"}
{"answers": {"text": ["for Tesla to further develop and produce a new lighting system", "a new lighting system", "develop and produce a new lighting system"], "answer_start": [47, 88, 68]}, "prediction_text": "Experiments."}
{"answers": {"text": ["to fund his Colorado Springs experiments.", "his Colorado Springs experiments", "fund his Colorado Springs experiments"], "answer_start": [141, 149, 144]}, "prediction_text": "Colorado Springs experiments"}
{"answers": {"text": ["1899", "In 1899", "1899"], "answer_start": [3, 0, 3]}, "prediction_text": "1899"}
{"answers": {"text": ["1900", "On 7 January 1900", "7 January 1900"], "answer_start": [13, 0, 3]}, "prediction_text": "1904"}
{"answers": {"text": ["His lab was torn down", "torn down", "torn down in 1904"], "answer_start": [65, 77, 77]}, "prediction_text": "Torn down."}
{"answers": {"text": ["1904", "1904", "1904"], "answer_start": [90, 90, 90]}, "prediction_text": "1904"}
{"answers": {"text": ["sold", "sold", "sold two years later"], "answer_start": [118, 118, 118]}, "prediction_text": "Sold."}
{"answers": {"text": ["Wardenclyffe", "Wardenclyffe", "Wardenclyffe"], "answer_start": [134, 134, 134]}, "prediction_text": "Wardenclyffe"}
{"answers": {"text": ["trans-Atlantic wireless telecommunications facility", "trans-Atlantic wireless telecommunications", "trans-Atlantic wireless telecommunications"], "answer_start": [73, 73, 73]}, "prediction_text": "Trans-Atlantic wireless telecommunications facility"}
{"answers": {"text": ["near Shoreham, Long Island", "Long Island", "near Shoreham, Long Island"], "answer_start": [147, 162, 147]}, "prediction_text": "Shoreham, Long Island"}
{"answers": {"text": ["Morgan", "Morgan", "Morgan"], "answer_start": [23, 23, 23]}, "prediction_text": "Morgan"}
{"answers": {"text": ["Panic of 1901", "the Panic of 1901", "the Panic of 1901"], "answer_start": [185, 181, 181]}, "prediction_text": "Panic of 1901."}
{"answers": {"text": ["shocked", "shocked", "shocked by the reminder of his part in the stock market crash and by Tesla's breach of contract"], "answer_start": [241, 241, 241]}, "prediction_text": "Shocked."}
{"answers": {"text": ["over 50 letters", "over 50 letters", "50 letters"], "answer_start": [199, 199, 204]}, "prediction_text": "50 letters."}
{"answers": {"text": ["to complete the construction of Wardenclyffe.", "to complete the construction of Wardenclyffe", "to complete the construction of Wardenclyffe"], "answer_start": [272, 272, 272]}, "prediction_text": "To complete the construction of Wardenclyffe."}
{"answers": {"text": ["Marconi successfully transmitted the letter S from England to Newfoundland", "Marconi successfully transmitted the letter S from England to Newfoundland", "Marconi successfully transmitted the letter S from England to Newfoundland"], "answer_start": [18, 18, 18]}, "prediction_text": "Marconi successfully transmitted the letter S from England to Newfoundland."}
{"answers": {"text": ["187 feet", "187 feet", "187 feet (57 m)"], "answer_start": [405, 405, 405]}, "prediction_text": "187 feet."}
{"answers": {"text": ["200", "200", "200"], "answer_start": [53, 53, 53]}, "prediction_text": "150 kilowatts"}
{"answers": {"text": ["16,000 rpm", "16,000 rpm", "16,000 rpm"], "answer_start": [84, 84, 84]}, "prediction_text": "150 kilowatts."}
{"answers": {"text": ["1906", "1910\u20131911", "his 50th birthday in 1906"], "answer_start": [24, 121, 3]}, "prediction_text": "1906"}
{"answers": {"text": ["100\u20135,000 hp", "100\u20135,000", "100\u20135,000"], "answer_start": [231, 231, 231]}, "prediction_text": "150 kilowatts"}
{"answers": {"text": ["steam", "steam", "steam-powered"], "answer_start": [17, 17, 17]}, "prediction_text": "Steam-powered."}
{"answers": {"text": ["Houston Street lab", "his Houston Street lab", "his Houston Street lab"], "answer_start": [128, 124, 124]}, "prediction_text": "Houston Street."}
{"answers": {"text": ["the machine oscillated at the resonance frequency of his own building", "the danger", "the machine oscillated at the resonance frequency of his own building"], "answer_start": [243, 338, 243]}, "prediction_text": "The police arrived."}
{"answers": {"text": ["World Today", "World Today", "World Today"], "answer_start": [541, 541, 541]}, "prediction_text": "World Today"}
{"answers": {"text": ["eventually split the earth in two", "destroy civilization", "he could set the earth's crust into such a state of vibration that it would rise and fall hundreds of feet and practically destroy civilization"], "answer_start": [882, 813, 690]}, "prediction_text": "Destroy civilization."}
{"answers": {"text": ["application of electricity", "application of electricity to the brain", "the application of electricity"], "answer_start": [25, 25, 21]}, "prediction_text": "Electricity."}
{"answers": {"text": ["saturating them unconsciously with electricity", "saturating them unconsciously with electricity", "saturating them unconsciously with electricity"], "answer_start": [148, 148, 148]}, "prediction_text": "saturate them unconsciously with electricity."}
{"answers": {"text": ["William H. Maxwell", "William H. Maxwell", "William H. Maxwell"], "answer_start": [550, 550, 550]}, "prediction_text": "William H. Maxwell"}
{"answers": {"text": ["superintendent of New York City schools", "superintendent of New York City schools", "superintendent of New York City schools"], "answer_start": [509, 509, 509]}, "prediction_text": "Superintendent of New York City schools"}
{"answers": {"text": ["overseas", "overseas", "overseas"], "answer_start": [33, 33, 33]}, "prediction_text": "Europe"}
{"answers": {"text": ["lost", "lost", "lost the funding"], "answer_start": [82, 82, 82]}, "prediction_text": "Tesla lost his funding."}
{"answers": {"text": ["sold", "sold", "sold"], "answer_start": [171, 171, 171]}, "prediction_text": "Sold Wardenclyffe for $20,000."}
{"answers": {"text": ["$20,000", "$20,000", "$20,000"], "answer_start": [193, 193, 193]}, "prediction_text": "$20,000"}
{"answers": {"text": ["the Edison Medal.", "the Edison Medal", "the Edison Medal."], "answer_start": [396, 396, 396]}, "prediction_text": "Edison Medal"}
{"answers": {"text": ["Electrical Experimenter", "Electrical Experimenter", "Electrical Experimenter"], "answer_start": [43, 43, 43]}, "prediction_text": "Electrical Experimenter."}
{"answers": {"text": ["fluorescent screen", "fluorescent screen", "with the signal being viewed on a fluorescent screen"], "answer_start": [243, 243, 209]}, "prediction_text": "Viewing the fluorescent screen."}
{"answers": {"text": ["radar", "radar", "modern radar"], "answer_start": [491, 336, 329]}, "prediction_text": "Radar system."}
{"answers": {"text": ["\u00c9mile Girardeau", "\u00c9mile Girardeau", "\u00c9mile Girardeau,"], "answer_start": [440, 440, 440]}, "prediction_text": "\u00c9mile Girardeau"}
{"answers": {"text": ["Thomas Edison and Nikola Tesla", "Thomas Edison and Nikola Tesla", "Thomas Edison and Nikola Tesla"], "answer_start": [108, 108, 108]}, "prediction_text": "Thomas Edison and Nikola Tesla."}
{"answers": {"text": ["Sir William Henry Bragg and William Lawrence Bragg", "Sir William Henry Bragg and William Lawrence Bragg", "Sir William Henry Bragg and William Lawrence Bragg"], "answer_start": [244, 244, 244]}, "prediction_text": "Thomas Edison and Nikola Tesla."}
{"answers": {"text": ["Tesla and/or Edison had refused the prize", "Tesla and/or Edison had refused the prize", "Tesla and/or Edison had refused the prize"], "answer_start": [428, 428, 428]}, "prediction_text": "Refusal of the reward."}
{"answers": {"text": ["announced a winner", "he is announced a winner", "after he is announced a winner"], "answer_start": [694, 688, 682]}, "prediction_text": "The Nobel Foundation said, \"Any rumor that a person has not been given a Nobel Prize because he has made known his intention to refuse the reward is ridiculous.\":245"}
{"answers": {"text": ["animosity toward each other", "their animosity toward each other", "because of their animosity toward each other"], "answer_start": [160, 154, 143]}, "prediction_text": "They were animosity toward each other."}
{"answers": {"text": ["38", "38", "38"], "answer_start": [108, 108, 108]}, "prediction_text": "38"}
{"answers": {"text": ["Edison", "Edison", "Edison"], "answer_start": [51, 82, 82]}, "prediction_text": "Edison"}
{"answers": {"text": ["1937", "1937", "1937"], "answer_start": [182, 182, 182]}, "prediction_text": "1937"}
{"answers": {"text": ["U.S. Patent 1,655,114", "1,655,114", "1,655,114"], "answer_start": [41, 53, 53]}, "prediction_text": "1,655,114"}
{"answers": {"text": ["VTOL aircraft", "VTOL aircraft", "a biplane capable of taking off vertically"], "answer_start": [112, 112, 68]}, "prediction_text": "Biplane."}
{"answers": {"text": ["less than $1,000", "less than $1,000", "less than $1,000"], "answer_start": [299, 299, 299]}, "prediction_text": "$1,000."}
{"answers": {"text": ["turbine engines", "turbine engines", "turbine"], "answer_start": [501, 501, 501]}, "prediction_text": "Turbine engines."}
{"answers": {"text": ["$125 per month", "$125 per month", "$125 per month"], "answer_start": [87, 87, 87]}, "prediction_text": "$125 per month"}
{"answers": {"text": ["rent at the Hotel New Yorker", "paying his rent", "paying his rent at the Hotel New Yorker"], "answer_start": [124, 113, 113]}, "prediction_text": "Rent."}
{"answers": {"text": ["for the rest of Tesla's life", "the rest of Tesla's life.", "the rest of Tesla's life"], "answer_start": [185, 189, 189]}, "prediction_text": "$125 per month."}
{"answers": {"text": ["bad publicity", "bad publicity", "potential bad publicity surrounding the impoverished conditions their former star inventor was living under"], "answer_start": [314, 314, 304]}, "prediction_text": "Bad publicity."}
{"answers": {"text": ["mechanical energy", "mechanical energy", "mechanical energy"], "answer_start": [95, 95, 95]}, "prediction_text": "Mechanical energy."}
{"answers": {"text": ["over any terrestrial distance", "any terrestrial distance", "any terrestrial distance"], "answer_start": [131, 136, 136]}, "prediction_text": "Terrestrial distance."}
{"answers": {"text": ["minimal", "minimal", "minimal"], "answer_start": [118, 118, 118]}, "prediction_text": "minimal"}
{"answers": {"text": ["mineral deposits", "mineral deposits", "mineral deposits"], "answer_start": [267, 267, 267]}, "prediction_text": "Mineral deposits."}
{"answers": {"text": ["1935", "In 1935", "1935"], "answer_start": [3, 0, 3]}, "prediction_text": "1935"}
{"answers": {"text": ["feed the pigeons", "feed the pigeons", "feed the pigeons"], "answer_start": [143, 143, 143]}, "prediction_text": "Feed pigeons."}
{"answers": {"text": ["a doctor", "a doctor", "a doctor"], "answer_start": [465, 465, 465]}, "prediction_text": "A doctor."}
{"answers": {"text": ["broken", "broken", "broken"], "answer_start": [363, 363, 363]}, "prediction_text": "Three ribs were broken in the accident."}
{"answers": {"text": ["early 1938", "In early 1938", "early 1938"], "answer_start": [754, 751, 754]}, "prediction_text": "Early 1938"}
{"answers": {"text": ["the fall of 1937", "In the fall of 1937", "fall of 1937"], "answer_start": [3, 0, 7]}, "prediction_text": "1937"}
{"answers": {"text": ["\"teleforce\" weapon", "teleforce", "teleforce"], "answer_start": [46, 47, 47]}, "prediction_text": "Van de Graaff generator"}
{"answers": {"text": ["Van de Graaff generator", "the Van de Graaff generator", "the Van de Graaff generator"], "answer_start": [84, 80, 80]}, "prediction_text": "Van de Graaff generator"}
{"answers": {"text": ["infantry", "infantry", "infantry"], "answer_start": [247, 247, 247]}, "prediction_text": "Infantry"}
{"answers": {"text": ["anti-aircraft purposes", "anti-aircraft purposes", "anti-aircraft purposes"], "answer_start": [263, 263, 263]}, "prediction_text": "Anti-aircraft purposes."}
{"answers": {"text": ["death ray", "death ray", "death ray"], "answer_start": [163, 163, 163]}, "prediction_text": "Death ray"}
{"answers": {"text": ["1937", "In 1937", "1937"], "answer_start": [3, 0, 3]}, "prediction_text": "1937"}
{"answers": {"text": ["at a luncheon in his honor", "a luncheon in his honor", "a luncheon in his honor"], "answer_start": [9, 12, 12]}, "prediction_text": "Luncheon"}
{"answers": {"text": ["tungsten", "tungsten", "tungsten"], "answer_start": [288, 288, 288]}, "prediction_text": "tungsten"}
{"answers": {"text": ["high voltage", "high voltage", "high voltage"], "answer_start": [330, 330, 330]}, "prediction_text": "High voltage."}
{"answers": {"text": ["Only a little", "a little", "Only a little time"], "answer_start": [149, 154, 149]}, "prediction_text": "A little time."}
{"answers": {"text": ["charged particle beam weapons", "charged particle beam weapons", "charged particle beam"], "answer_start": [141, 141, 141]}, "prediction_text": "charged particle beam weapons"}
{"answers": {"text": ["Nikola Tesla Museum archive", "the Nikola Tesla Museum archive", "Nikola Tesla Museum"], "answer_start": [342, 338, 342]}, "prediction_text": "Nikola Tesla Museum Archive in Belgrade."}
{"answers": {"text": ["Belgrade", "Belgrade", "Belgrade"], "answer_start": [373, 373, 373]}, "prediction_text": "Belgrade"}
{"answers": {"text": ["millions", "millions", "millions"], "answer_start": [507, 507, 507]}, "prediction_text": "Millions of volts."}
{"answers": {"text": ["all war", "all war", "all war"], "answer_start": [298, 298, 298]}, "prediction_text": "War."}
{"answers": {"text": ["steal the invention", "steal", "steal"], "answer_start": [107, 107, 107]}, "prediction_text": "Steal it."}
{"answers": {"text": ["in his mind.", "in his mind", "all in his mind"], "answer_start": [410, 410, 406]}, "prediction_text": "In his mind."}
{"answers": {"text": ["his papers", "his papers", "his papers"], "answer_start": [158, 158, 158]}, "prediction_text": "His papers."}
{"answers": {"text": ["86", "86", "86"], "answer_start": [33, 33, 33]}, "prediction_text": "86"}
{"answers": {"text": ["7 January 1943", "7 January 1943", "7 January 1943"], "answer_start": [3, 3, 3]}, "prediction_text": "7 January 1943"}
{"answers": {"text": ["maid Alice Monaghan", "maid Alice Monaghan", "maid Alice Monaghan"], "answer_start": [120, 120, 120]}, "prediction_text": "Alice Monaghan"}
{"answers": {"text": ["\"do not disturb\" sign", "do not disturb\" sign", "\"do not disturb\" sign"], "answer_start": [189, 190, 189]}, "prediction_text": "The \"do not disturb\" sign."}
{"answers": {"text": ["coronary thrombosis", "coronary thrombosis", "coronary thrombosis"], "answer_start": [363, 363, 363]}, "prediction_text": "Coronary thrombosis."}
{"answers": {"text": ["FBI ordered the Alien Property Custodian to seize Tesla's belongings", "the FBI ordered the Alien Property Custodian to seize Tesla's belongings", "Alien Property Custodian"], "answer_start": [20, 16, 36]}, "prediction_text": "Seized."}
{"answers": {"text": ["John G. Trump", "John G. Trump", "John G. Trump"], "answer_start": [322, 322, 322]}, "prediction_text": "John G. Trump"}
{"answers": {"text": ["nothing", "nothing", "nothing"], "answer_start": [598, 598, 598]}, "prediction_text": "Tesla's belongings."}
{"answers": {"text": ["Manhattan Storage and Warehouse Company", "Manhattan Storage and Warehouse Company", "Manhattan Storage and Warehouse Company"], "answer_start": [235, 235, 235]}, "prediction_text": "Manhattan Storage and Warehouse Company."}
{"answers": {"text": ["New York City mayor Fiorello La Guardia", "Fiorello La Guardia", "New York City mayor Fiorello La Guardia"], "answer_start": [20, 40, 20]}, "prediction_text": "Fiorello La Guardia"}
{"answers": {"text": ["Louis Adamic", "Louis Adamic", "Slovene-American author Louis Adamic"], "answer_start": [109, 109, 85]}, "prediction_text": "Louis Adamic"}
{"answers": {"text": ["12 January", "12 January", "12 January"], "answer_start": [231, 231, 231]}, "prediction_text": "12 January"}
{"answers": {"text": ["two thousand", "two thousand", "two thousand people"], "answer_start": [243, 243, 243]}, "prediction_text": "Two thousand."}
{"answers": {"text": ["the Cathedral of Saint John the Divine", "the Cathedral of Saint John the Divine", "the Cathedral of Saint John the Divine"], "answer_start": [301, 301, 301]}, "prediction_text": "Cathedral of Saint John the Divine"}
{"answers": {"text": ["Belgrade", "Belgrade", "Belgrade"], "answer_start": [102, 102, 102]}, "prediction_text": "Belgrade"}
{"answers": {"text": ["Sava Kosanovi\u0107", "Sava Kosanovi\u0107", "Sava Kosanovi\u0107"], "answer_start": [49, 49, 49]}, "prediction_text": "Sava Kosanovi\u0107"}
{"answers": {"text": ["Charlotte Muzar", "Charlotte Muzar", "Charlotte Muzar"], "answer_start": [167, 167, 167]}, "prediction_text": "Charlotte Muzar"}
{"answers": {"text": ["Belgrade", "Belgrade", "Belgrade"], "answer_start": [102, 235, 235]}, "prediction_text": "Belgrade"}
{"answers": {"text": ["Nikola Tesla Museum", "the Nikola Tesla Museum", "the Nikola Tesla Museum"], "answer_start": [321, 317, 317]}, "prediction_text": "1957"}
{"answers": {"text": ["around 300", "around 300", "around 300"], "answer_start": [15, 15, 15]}, "prediction_text": "278"}
{"answers": {"text": ["26", "26", "26"], "answer_start": [248, 248, 248]}, "prediction_text": "26 countries."}
{"answers": {"text": ["Canada", "Canada", "Canada"], "answer_start": [355, 355, 355]}, "prediction_text": "Canada"}
{"answers": {"text": ["patent archives", "patent archives", "patent archives"], "answer_start": [177, 177, 177]}, "prediction_text": "Patent archives."}
{"answers": {"text": ["8:10 p.m", "8:10 p.m", "exactly 8:10 p.m.,"], "answer_start": [89, 89, 81]}, "prediction_text": "8:10 p.m."}
{"answers": {"text": ["9:00 a.m. until 6:00 p.m. or later", "9:00 a.m. until 6:00 p.m", "9:00 a.m. until 6:00 p.m. or later"], "answer_start": [28, 28, 28]}, "prediction_text": "9:00 a.m. to 6:00 p.m. or later."}
{"answers": {"text": ["3:00 a.m", "3:00 a.m", "3:00 a.m"], "answer_start": [484, 484, 484]}, "prediction_text": "3:00 a.m."}
{"answers": {"text": ["headwaiter", "the headwaiter", "the headwaiter"], "answer_start": [209, 205, 205]}, "prediction_text": "Headwaiter"}
{"answers": {"text": ["between 8 to 10 miles per day", "8 to 10 miles", "8 to 10 miles"], "answer_start": [27, 35, 35]}, "prediction_text": "8 to 10 miles per day."}
{"answers": {"text": ["exercise", "For exercise", "exercise"], "answer_start": [4, 0, 4]}, "prediction_text": "To stimulate his brain cells."}
{"answers": {"text": ["squished his toes", "squished his toes", "squished his toes one hundred times for each foot"], "answer_start": [61, 61, 61]}, "prediction_text": "Squished his toes."}
{"answers": {"text": ["brain cells", "brain cells", "brain cells"], "answer_start": [154, 154, 154]}, "prediction_text": "Brain cells."}
{"answers": {"text": ["telepathy", "telepathy", "telepathy"], "answer_start": [93, 93, 93]}, "prediction_text": "telepathy"}
{"answers": {"text": ["newspaper editor", "newspaper editor", "newspaper editor"], "answer_start": [21, 21, 21]}, "prediction_text": "Editor"}
{"answers": {"text": ["one", "one", "one"], "answer_start": [369, 369, 369]}, "prediction_text": "One."}
{"answers": {"text": ["pigeons", "pigeons", "pigeons"], "answer_start": [73, 73, 73]}, "prediction_text": "Pigeons"}
{"answers": {"text": ["over $2,000", "over $2,000,", "over $2,000"], "answer_start": [243, 243, 243]}, "prediction_text": "$2,000"}
{"answers": {"text": ["broken wing and leg", "broken wing and leg", "broken wing and leg"], "answer_start": [351, 351, 351]}, "prediction_text": "Broken wing and leg."}
{"answers": {"text": ["the park", "the park", "the park"], "answer_start": [42, 42, 42]}, "prediction_text": "Park"}
{"answers": {"text": ["hotel room", "his hotel room", "his hotel room"], "answer_start": [120, 116, 116]}, "prediction_text": "Hotel room"}
{"answers": {"text": ["142 pounds", "142 pounds", "142 pounds (64 kg)"], "answer_start": [52, 52, 52]}, "prediction_text": "142 pounds (64 kg)"}
{"answers": {"text": ["6 feet 2 inches", "6 feet 2 inches", "6 feet 2 inches (1.88 m)"], "answer_start": [10, 10, 10]}, "prediction_text": "6 feet 2 inches (1.88 m)"}
{"answers": {"text": ["1888 to about 1926", "from 1888 to about 1926", "1888 to about 1926"], "answer_start": [108, 103, 108]}, "prediction_text": "1888 to 1926."}
{"answers": {"text": ["New York City", "New York City", "New York City"], "answer_start": [169, 169, 169]}, "prediction_text": "New York City"}
{"answers": {"text": ["eight", "eight", "eight"], "answer_start": [129, 129, 129]}, "prediction_text": "8"}
{"answers": {"text": ["visions", "blinding flashes of light", "visions"], "answer_start": [504, 427, 504]}, "prediction_text": "Blinding flashes of light."}
{"answers": {"text": ["picture thinking", "picture thinking"], "answer_start": [945, 945]}, "prediction_text": "Picture thinking."}
{"answers": {"text": ["blinding flashes of light", "flashbacks", "blinding flashes of light"], "answer_start": [427, 1077, 427]}, "prediction_text": "Words or ideas."}
{"answers": {"text": ["photographic memory", "photographic", "photographic"], "answer_start": [77, 77, 77]}, "prediction_text": "photographic memory"}
{"answers": {"text": ["more than 48 hours", "48 hours", "48 hours"], "answer_start": [157, 167, 167]}, "prediction_text": "84 hours."}
{"answers": {"text": ["84 hours", "84 hours", "84 hours"], "answer_start": [280, 280, 280]}, "prediction_text": "84 hours."}
{"answers": {"text": ["Graz", "Graz", "Graz"], "answer_start": [35, 35, 35]}, "prediction_text": "Graz"}
{"answers": {"text": ["Kenneth Swezey", "Kenneth Swezey", "Kenneth Swezey,"], "answer_start": [316, 316, 316]}, "prediction_text": "Kenneth Swezey"}
{"answers": {"text": ["journalist", "journalist", "journalist"], "answer_start": [334, 334, 334]}, "prediction_text": "Journalist"}
{"answers": {"text": ["chastity", "his chastity", "chastity"], "answer_start": [33, 29, 33]}, "prediction_text": "Chastity"}
{"answers": {"text": ["women", "women"], "answer_start": [270, 270]}, "prediction_text": "Women."}
{"answers": {"text": ["toward the end of his life", "toward the end of his life", "toward the end of his life"], "answer_start": [100, 100, 100]}, "prediction_text": "Toward the end of his life."}
{"answers": {"text": ["Dorothy Skerrit", "Dorothy Skerrit", "Dorothy Skerrit"], "answer_start": [324, 324, 324]}, "prediction_text": "Dorothy Skerrit"}
{"answers": {"text": ["Robert Underwood Johnson", "Robert Underwood Johnson", "Robert Underwood Johnson"], "answer_start": [167, 167, 167]}, "prediction_text": "Robert Underwood Johnson"}
{"answers": {"text": ["seclude himself", "seclude himself", "seclude himself with his work"], "answer_start": [31, 31, 31]}, "prediction_text": "Seclude himself."}
{"answers": {"text": ["asocial", "asocial"], "answer_start": [10, 10]}, "prediction_text": "Modesty"}
{"answers": {"text": ["friend", "friend", "friend"], "answer_start": [482, 482, 482]}, "prediction_text": "Friend"}
{"answers": {"text": ["Mark Twain", "Mark Twain", "Mark Twain"], "answer_start": [194, 194, 194]}, "prediction_text": "Mark Twain"}
{"answers": {"text": ["lab", "in his lab", "in his lab and elsewhere"], "answer_start": [247, 240, 240]}, "prediction_text": "Mark Twain's lab."}
{"answers": {"text": ["late 1920s", "the late 1920s", "the late 1920s"], "answer_start": [382, 378, 378]}, "prediction_text": "Late 1920s"}
{"answers": {"text": ["overweight people", "overweight people", "overweight people"], "answer_start": [63, 63, 63]}, "prediction_text": "Overweight people."}
{"answers": {"text": ["secretary", "a secretary", "a secretary"], "answer_start": [106, 104, 104]}, "prediction_text": "Secretary"}
{"answers": {"text": ["her weight", "her weight", "her weight."], "answer_start": [127, 127, 127]}, "prediction_text": "Her weight."}
{"answers": {"text": ["go home and change", "go home and change", "to go home and change her dress"], "answer_start": [233, 233, 230]}, "prediction_text": "Go home and change her dress."}
{"answers": {"text": ["electron", "an electron"], "answer_start": [194, 191]}, "prediction_text": "Electron"}
{"answers": {"text": ["ether", "ether"], "answer_start": [592, 592]}, "prediction_text": "The ether."}
{"answers": {"text": ["transmitted electrical energy", "transmitted electrical energy", "transmitted electrical energy"], "answer_start": [604, 604, 604]}, "prediction_text": "transmitted electrical energy."}
{"answers": {"text": ["19th", "the 19th", "19th"], "answer_start": [550, 546, 550]}, "prediction_text": "19th century"}
{"answers": {"text": ["Einstein's", "Einstein's", "Einstein's"], "answer_start": [122, 122, 122]}, "prediction_text": "Einstein"}
{"answers": {"text": ["antagonistic", "antagonistic", "antagonistic"], "answer_start": [20, 20, 20]}, "prediction_text": "Antagonistic."}
{"answers": {"text": ["relativity", "relativity", "theory of relativity"], "answer_start": [143, 143, 133]}, "prediction_text": "Relativity"}
{"answers": {"text": ["gravity", "gravity", "of gravity"], "answer_start": [206, 206, 203]}, "prediction_text": "Dynamic theory of gravity."}
{"answers": {"text": ["1892", "1892, and in 1937", "1892"], "answer_start": [117, 117, 117]}, "prediction_text": "1892"}
{"answers": {"text": ["curved", "curved", "curved"], "answer_start": [295, 295, 295]}, "prediction_text": "Curved space."}
{"answers": {"text": ["81", "81", "81"], "answer_start": [143, 143, 143]}, "prediction_text": "81"}
{"answers": {"text": ["eugenics", "eugenics", "imposed selective breeding version of eugenics"], "answer_start": [92, 92, 54]}, "prediction_text": "Selective breeding."}
{"answers": {"text": ["ruthless", "ruthless", "ruthless workings"], "answer_start": [191, 191, 191]}, "prediction_text": "Pity."}
{"answers": {"text": ["pity", "pity", "pity"], "answer_start": [152, 152, 152]}, "prediction_text": "Pity"}
{"answers": {"text": ["1937", "1937", "1937"], "answer_start": [379, 379, 379]}, "prediction_text": "1937"}
{"answers": {"text": ["women", "women", "women"], "answer_start": [67, 207, 207]}, "prediction_text": "Queen Bees"}
{"answers": {"text": ["1926", "1926", "1926"], "answer_start": [3, 3, 3]}, "prediction_text": "1926"}
{"answers": {"text": ["Queen Bees", "Queen Bees", "Queen Bees"], "answer_start": [177, 177, 177]}, "prediction_text": "Queen Bees"}
{"answers": {"text": ["post-World War I", "post-World War I", "post-World War I"], "answer_start": [54, 54, 54]}, "prediction_text": "World War I"}
{"answers": {"text": ["Science and Discovery", "Science and Discovery", "Science and Discovery"], "answer_start": [106, 106, 106]}, "prediction_text": "Science and Discovery."}
{"answers": {"text": ["20 December 1914", "20 December 1914", "20 December 1914"], "answer_start": [198, 198, 198]}, "prediction_text": "20 December 1914"}
{"answers": {"text": ["League of Nations", "the League of Nations", "League of Nations"], "answer_start": [241, 237, 241]}, "prediction_text": "League of Nations"}
{"answers": {"text": ["Orthodox Christian", "Orthodox Christian", "Orthodox Christian"], "answer_start": [20, 20, 20]}, "prediction_text": "Orthodox Christian"}
{"answers": {"text": ["fanaticism", "fanaticism", "fanaticism"], "answer_start": [151, 151, 151]}, "prediction_text": "Fanaticism"}
{"answers": {"text": ["Buddhism and Christianity", "Buddhism and Christianity", "Buddhism and Christianity"], "answer_start": [212, 212, 212]}, "prediction_text": "Buddhism and Christianity."}
{"answers": {"text": ["\"A Machine to End War\"", "\"A Machine to End War\"", "A Machine to End War"], "answer_start": [113, 113, 114]}, "prediction_text": "\"A Machine to End War\""}
{"answers": {"text": ["uncertain", "uncertain", "uncertain"], "answer_start": [36, 36, 36]}, "prediction_text": "Uncertainty"}
{"answers": {"text": ["War", "War", "War"], "answer_start": [131, 131, 131]}, "prediction_text": "War"}
{"answers": {"text": ["books and articles", "a number of books and articles", "a number of books and articles"], "answer_start": [24, 12, 12]}, "prediction_text": "My Inventions: The Autobiography of Nikola Tesla, compiled and edited by Ben Johnston."}
{"answers": {"text": ["magazines and journals", "magazines and journals", "magazines and journals"], "answer_start": [47, 47, 47]}, "prediction_text": "Ben Johnston"}
{"answers": {"text": ["Ben Johnston", "Ben Johnston", "Ben Johnston"], "answer_start": [164, 164, 164]}, "prediction_text": "Ben Johnston"}
{"answers": {"text": ["the web", "on the web", "freely available on the web"], "answer_start": [49, 46, 29]}, "prediction_text": "The web."}
{"answers": {"text": ["1900", "1900", "1900"], "answer_start": [159, 159, 159]}, "prediction_text": "1900"}
{"answers": {"text": ["Inventions, Researches and Writings of Nikola Tesla.", "Researches and Writings of Nikola Tesla", "Researches and Writings of Nikola Tesla"], "answer_start": [279, 291, 291]}, "prediction_text": "Inventions, Researches, and Writings of Nikola Tesla."}
{"answers": {"text": ["science fiction", "science", "science fiction"], "answer_start": [204, 204, 204]}, "prediction_text": "Science fiction"}
{"answers": {"text": ["books, films, radio, TV, music, live theater, comics and video games", "books, films, radio, TV, music, live theater, comics and video games", "books, films, radio, TV, music, live theater, comics and video games"], "answer_start": [30, 30, 30]}, "prediction_text": "Books, films, radio, TV, music, live theater, comics, video games."}
{"answers": {"text": ["several", "several", "several"], "answer_start": [187, 187, 187]}, "prediction_text": "Several."}
{"answers": {"text": ["Time magazine", "Time", "Time"], "answer_start": [34, 34, 34]}, "prediction_text": "Time magazine"}
{"answers": {"text": ["75th birthday", "75th birthday", "75th birthday"], "answer_start": [11, 11, 11]}, "prediction_text": "Tesla's 75th birthday."}
{"answers": {"text": ["electrical power generation", "electrical power generation", "electrical power generation"], "answer_start": [148, 148, 148]}, "prediction_text": "Electrical power generation."}
{"answers": {"text": ["Einstein", "Albert Einstein", "Albert Einstein"], "answer_start": [284, 277, 277]}, "prediction_text": "Albert Einstein"}
{"answers": {"text": ["more than 70", "more than 70", "more than 70"], "answer_start": [217, 217, 217]}, "prediction_text": "70"}
{"answers": {"text": ["Computational complexity theory", "Computational complexity theory", "Computational complexity theory"], "answer_start": [0, 0, 0]}, "prediction_text": "Computational complexity theory"}
{"answers": {"text": ["inherent difficulty", "their inherent difficulty", "inherent difficulty"], "answer_start": [175, 169, 175]}, "prediction_text": "inherent difficulty"}
{"answers": {"text": ["computational problems", "A computational problem", "computational problem"], "answer_start": [133, 238, 240]}, "prediction_text": "computational problem"}
{"answers": {"text": ["if its solution requires significant resources", "its solution requires significant resources", "if its solution requires significant resources"], "answer_start": [46, 49, 46]}, "prediction_text": "Time and storage."}
{"answers": {"text": ["mathematical models of computation", "mathematical models of computation", "mathematical models of computation"], "answer_start": [176, 176, 176]}, "prediction_text": "Mathematical models of computation."}
{"answers": {"text": ["time and storage", "time and storage", "time and storage"], "answer_start": [305, 305, 305]}, "prediction_text": "time, storage."}
{"answers": {"text": ["number of gates in a circuit", "number of gates in a circuit", "number of gates"], "answer_start": [440, 440, 440]}, "prediction_text": "Gates."}
{"answers": {"text": ["determine the practical limits on what computers can and cannot do", "what computers can and cannot do", "determine the practical limits on what computers can and cannot do"], "answer_start": [615, 649, 615]}, "prediction_text": "Determines practical limits on what computers can and cannot do."}
{"answers": {"text": ["analysis of algorithms and computability theory", "analysis of algorithms and computability theory", "analysis of algorithms and computability theory"], "answer_start": [59, 59, 59]}, "prediction_text": "Analysis of algorithms and computability theory."}
{"answers": {"text": ["analysis of algorithms", "analysis of algorithms", "analysis of algorithms"], "answer_start": [59, 134, 134]}, "prediction_text": "Analysis of algorithms."}
{"answers": {"text": ["computational complexity theory", "computational complexity theory", "computational complexity theory"], "answer_start": [161, 161, 161]}, "prediction_text": "Computability theory."}
{"answers": {"text": ["computability theory", "computability theory", "computability theory"], "answer_start": [86, 663, 663]}, "prediction_text": "Analysis of algorithms."}
{"answers": {"text": ["problem instance", "a problem instance", "problem instance"], "answer_start": [187, 185, 187]}, "prediction_text": "problem instance"}
{"answers": {"text": ["the problem", "a problem", "problem"], "answer_start": [237, 293, 295]}, "prediction_text": "Problem instance."}
{"answers": {"text": ["concrete", "concrete", "abstract"], "answer_start": [402, 402, 317]}, "prediction_text": "Abstract."}
{"answers": {"text": ["instances", "the instance", "instance"], "answer_start": [67, 675, 679]}, "prediction_text": "Problem instance."}
{"answers": {"text": ["solution", "the solution", "solution"], "answer_start": [93, 730, 734]}, "prediction_text": "Output."}
{"answers": {"text": ["2000", "2000", "2000"], "answer_start": [190, 190, 190]}, "prediction_text": "2000 km"}
{"answers": {"text": ["round trip through all sites in Milan", "asking for a round trip through all sites in Milan whose total length is at most 10 km", "a round trip through all sites in Milan whose total length is at most 10 km"], "answer_start": [400, 387, 398]}, "prediction_text": "Milan."}
{"answers": {"text": ["computational problems", "computational problems", "computational problems"], "answer_start": [520, 520, 520]}, "prediction_text": "computational problems"}
{"answers": {"text": ["problem instance", "a problem instance", "problem instance"], "answer_start": [43, 41, 43]}, "prediction_text": "A string over an alphabet."}
{"answers": {"text": ["binary alphabet", "binary", "binary"], "answer_start": [131, 131, 131]}, "prediction_text": "Binary alphabet"}
{"answers": {"text": ["bitstrings", "bitstrings", "bitstrings"], "answer_start": [195, 195, 195]}, "prediction_text": "Bitstring"}
{"answers": {"text": ["binary notation", "binary notation", "binary notation"], "answer_start": [349, 349, 349]}, "prediction_text": "Binary notation."}
{"answers": {"text": ["adjacency matrices", "directly via their adjacency matrices"], "answer_start": [411, 392]}, "prediction_text": "Adjacency matrices."}
{"answers": {"text": ["Decision problems", "Decision problems", "Decision"], "answer_start": [0, 0, 0]}, "prediction_text": "Decision problems."}
{"answers": {"text": ["yes or no", "yes or no", "yes or no"], "answer_start": [179, 179, 179]}, "prediction_text": "Yes and no."}
{"answers": {"text": ["1 or 0", "1 or 0", "1 or 0"], "answer_start": [212, 212, 212]}, "prediction_text": "Yes and no."}
{"answers": {"text": ["yes", "yes", "yes"], "answer_start": [179, 339, 339]}, "prediction_text": "Yes."}
{"answers": {"text": ["yes", "yes", "yes"], "answer_start": [179, 605, 605]}, "prediction_text": "Yes."}
{"answers": {"text": ["arbitrary graph", "arbitrary", "arbitrary"], "answer_start": [67, 67, 67]}, "prediction_text": "Arbitrary graph."}
{"answers": {"text": ["formal language", "The formal language", "The formal language associated with this decision problem"], "answer_start": [167, 163, 163]}, "prediction_text": "Set of all connected graphs."}
{"answers": {"text": ["how graphs are encoded as binary strings", "how graphs are encoded as binary strings", "how graphs are encoded as binary strings"], "answer_start": [339, 339, 339]}, "prediction_text": "Deciding how graphs are encoded as binary strings."}
{"answers": {"text": ["a computational problem", "a computational problem", "a computational problem"], "answer_start": [22, 22, 22]}, "prediction_text": "computational problem"}
{"answers": {"text": ["a single output", "single", "single"], "answer_start": [52, 54, 54]}, "prediction_text": "1"}
{"answers": {"text": ["A function problem", "function", "function problem"], "answer_start": [0, 2, 2]}, "prediction_text": "Computational problem"}
{"answers": {"text": ["the integer factorization problem", "integer factorization", "integer factorization problem"], "answer_start": [277, 281, 281]}, "prediction_text": "Integer factorization problem."}
{"answers": {"text": ["complex", "complex", "complex"], "answer_start": [142, 142, 142]}, "prediction_text": "Complex"}
{"answers": {"text": ["decision problems", "as decision problems", "as decision problems"], "answer_start": [95, 190, 190]}, "prediction_text": "Deciding whether a given triple is a member of this set corresponds to solving the problem of multiplying two numbers."}
{"answers": {"text": ["set of triples", "triple", "the set of triples (a, b, c) such that the relation a \u00d7 b = c holds"], "answer_start": [284, 374, 280]}, "prediction_text": "triples"}
{"answers": {"text": ["how much time the best algorithm requires to solve the problem", "time", "time"], "answer_start": [82, 91, 91]}, "prediction_text": "Running time."}
{"answers": {"text": ["the instance", "the instance", "the size of the instance"], "answer_start": [199, 199, 405]}, "prediction_text": "size of the input in bits"}
{"answers": {"text": ["as a function of the size of the instance", "as a function of the size of the instance", "a function of the size of the instance"], "answer_start": [388, 388, 391]}, "prediction_text": "The running time."}
{"answers": {"text": ["bits", "bits", "bits"], "answer_start": [484, 484, 484]}, "prediction_text": "bits"}
{"answers": {"text": ["an increase in the input size", "input size", "input size"], "answer_start": [551, 570, 570]}, "prediction_text": "Input size."}
{"answers": {"text": ["Cobham's thesis", "Cobham's", "Cobham"], "answer_start": [347, 347, 347]}, "prediction_text": "Cobham's thesis."}
{"answers": {"text": ["the time taken", "the time taken", "the time taken"], "answer_start": [24, 24, 24]}, "prediction_text": "T(n)"}
{"answers": {"text": ["worst-case time complexity", "worst-case time complexity", "the worst-case time complexity"], "answer_start": [156, 156, 152]}, "prediction_text": "T(n)"}
{"answers": {"text": ["T(n)", "T(n)", "T(n)"], "answer_start": [183, 183, 183]}, "prediction_text": "T(n)"}
{"answers": {"text": ["polynomial time algorithm", "polynomial time", "polynomial time algorithm"], "answer_start": [320, 320, 320]}, "prediction_text": "polynomial time algorithm"}
{"answers": {"text": ["A Turing machine", "A Turing machine", "Turing machine"], "answer_start": [0, 0, 2]}, "prediction_text": "Turing machine"}
{"answers": {"text": ["an algorithm", "an algorithm", "an algorithm"], "answer_start": [427, 427, 427]}, "prediction_text": "A RAM machine."}
{"answers": {"text": ["the Turing machine", "the Turing machine", "Turing machine"], "answer_start": [924, 924, 928]}, "prediction_text": "Turing machine"}
{"answers": {"text": ["symbols", "symbols", "symbols"], "answer_start": [117, 117, 117]}, "prediction_text": "symbols"}
{"answers": {"text": ["A deterministic Turing machine", "deterministic", "deterministic Turing machine"], "answer_start": [0, 2, 2]}, "prediction_text": "fixed set of rules"}
{"answers": {"text": ["rules", "rules", "a fixed set of rules to determine its future actions"], "answer_start": [91, 91, 76]}, "prediction_text": "rules"}
{"answers": {"text": ["A probabilistic Turing machine", "probabilistic", "probabilistic Turing machine"], "answer_start": [130, 132, 132]}, "prediction_text": "probabilistic Turing machine"}
{"answers": {"text": ["A non-deterministic Turing machine", "non-deterministic", "non-deterministic Turing machine"], "answer_start": [398, 400, 400]}, "prediction_text": "Non-deterministic Turing machine."}
{"answers": {"text": ["randomized algorithms", "randomized algorithms", "randomized algorithms"], "answer_start": [375, 375, 375]}, "prediction_text": "randomized algorithms"}
{"answers": {"text": ["complexity classes", "complexity classes", "complexity classes"], "answer_start": [49, 49, 49]}, "prediction_text": "complexity classes"}
{"answers": {"text": ["time or space", "time or space", "time or space"], "answer_start": [330, 330, 330]}, "prediction_text": "Time and space."}
{"answers": {"text": ["probabilistic Turing machines, non-deterministic Turing machines", "probabilistic Turing machines, non-deterministic Turing machines"], "answer_start": [108, 108]}, "prediction_text": "Deterministic Turing machines, probabilistic Turing machines, non-deterministic Turing machines, quantum Turing machines, symmetric Turing machines, and alternating Turing machines."}
{"answers": {"text": ["random access machines", "random access machines", "random access machines"], "answer_start": [125, 125, 125]}, "prediction_text": "Random access machines."}
{"answers": {"text": ["computational power", "computational power", "computational power"], "answer_start": [248, 248, 248]}, "prediction_text": "Time and memory consumption."}
{"answers": {"text": ["time and memory", "time and memory consumption", "time and memory consumption"], "answer_start": [273, 273, 273]}, "prediction_text": "time and memory."}
{"answers": {"text": ["the machines operate deterministically", "deterministically", "the machines operate deterministically"], "answer_start": [382, 403, 382]}, "prediction_text": "Deterministic."}
{"answers": {"text": ["non-deterministic", "non-deterministic", "non-deterministic Turing machine"], "answer_start": [110, 110, 110]}, "prediction_text": "Non-deterministic Turing machine."}
{"answers": {"text": ["unusual resources", "more unusual resources", "more unusual resources"], "answer_start": [76, 71, 71]}, "prediction_text": "branching out to check many possibilities at once."}
{"answers": {"text": ["mathematical models", "mathematical models", "branching"], "answer_start": [402, 402, 363]}, "prediction_text": "branching out to check many different possibilities at once."}
{"answers": {"text": ["time", "non-deterministic time", "non-deterministic time"], "answer_start": [468, 450, 450]}, "prediction_text": "branching out to check many possibilities at once."}
{"answers": {"text": ["state transitions", "the total number of state transitions, or steps", "total number of state transitions, or steps, the machine makes before it halts and outputs the answer"], "answer_start": [261, 241, 245]}, "prediction_text": "f(n)"}
{"answers": {"text": ["difficulty", "difficulty", "difficulty"], "answer_start": [695, 695, 695]}, "prediction_text": "difficulty"}
{"answers": {"text": ["DTIME(f(n))", "DTIME(f(n)).", "DTIME(f(n))"], "answer_start": [873, 873, 873]}, "prediction_text": "DTIME(f(n))"}
{"answers": {"text": ["time", "time", "time"], "answer_start": [85, 583, 583]}, "prediction_text": "Time f(n)"}
{"answers": {"text": ["complexity resources", "complexity resources", "complexity"], "answer_start": [106, 106, 106]}, "prediction_text": "Computation resource."}
{"answers": {"text": ["computational resource", "computational", "computational"], "answer_start": [170, 170, 170]}, "prediction_text": "computational resource"}
{"answers": {"text": ["Blum complexity axioms", "the Blum complexity axioms", "the Blum complexity axioms"], "answer_start": [248, 244, 244]}, "prediction_text": "Blum complexity axioms"}
{"answers": {"text": ["Complexity measures", "complexity measures", "complexity"], "answer_start": [194, 278, 278]}, "prediction_text": "Communication complexity."}
{"answers": {"text": ["Complexity measures", "complexity measures", "complexity"], "answer_start": [194, 278, 396]}, "prediction_text": "Decision tree complexity."}
{"answers": {"text": ["best, worst and average", "best, worst and average case", "best, worst and average case complexity"], "answer_start": [4, 4, 4]}, "prediction_text": "Best, worst, average."}
{"answers": {"text": ["complexity measure", "complexity", "complexity"], "answer_start": [121, 121, 121]}, "prediction_text": "time complexity"}
{"answers": {"text": ["time", "time complexity", "time complexity"], "answer_start": [91, 91, 91]}, "prediction_text": "Time complexity."}
{"answers": {"text": ["inputs", "inputs", "inputs"], "answer_start": [154, 154, 154]}, "prediction_text": "time complexity (or any other complexity measure)"}
{"answers": {"text": ["deterministic sorting algorithm quicksort", "quicksort", "the deterministic sorting algorithm quicksort"], "answer_start": [26, 58, 22]}, "prediction_text": "quicksort"}
{"answers": {"text": ["worst-case", "worst", "worst-case"], "answer_start": [155, 155, 155]}, "prediction_text": "O(n log n)"}
{"answers": {"text": ["O(n2)", "O(n2)", "O(n2)"], "answer_start": [251, 251, 251]}, "prediction_text": "O(n2)"}
{"answers": {"text": ["the most efficient algorithm", "the most efficient algorithm", "the most efficient algorithm solving a given problem"], "answer_start": [178, 178, 178]}, "prediction_text": "Most efficient algorithm solving a given problem."}
{"answers": {"text": ["analysis of algorithms", "analysis of algorithms", "analysis of algorithms"], "answer_start": [399, 399, 399]}, "prediction_text": "Analysis of algorithms."}
{"answers": {"text": ["lower bounds", "lower", "lower bounds"], "answer_start": [123, 597, 597]}, "prediction_text": "Lower bound."}
{"answers": {"text": ["upper bound", "upper and lower bounds", "upper bound"], "answer_start": [434, 113, 434]}, "prediction_text": "T(n)"}
{"answers": {"text": ["all possible algorithms", "all possible algorithms", "all possible algorithms"], "answer_start": [676, 740, 740]}, "prediction_text": "\"all possible algorithms\""}
{"answers": {"text": ["big O notation", "big O notation", "big O notation"], "answer_start": [52, 52, 52]}, "prediction_text": "Big O notation."}
{"answers": {"text": ["constant factors and smaller terms", "constant factors and smaller terms", "constant factors and smaller terms"], "answer_start": [80, 80, 80]}, "prediction_text": "Constant factors and smaller terms."}
{"answers": {"text": ["T(n) = O(n2)", "T(n) = O(n2)", "T(n) = O(n2)"], "answer_start": [281, 281, 281]}, "prediction_text": "O(n2)"}
{"answers": {"text": ["the computational model", "specific details of the computational model used", "the specific details of the computational model used"], "answer_start": [177, 157, 153]}, "prediction_text": "specific details of the computational model used."}
{"answers": {"text": ["complexity classes", "complexity classes", "some complexity classes"], "answer_start": [16, 16, 11]}, "prediction_text": "Complicated definitions."}
{"answers": {"text": ["framework", "framework", "framework"], "answer_start": [90, 90, 90]}, "prediction_text": "Complicated definitions."}
{"answers": {"text": ["complicated definitions", "complicated definitions", "definitions"], "answer_start": [40, 40, 52]}, "prediction_text": "complexity classes"}
{"answers": {"text": ["chosen machine model", "the chosen machine model", "the chosen machine model"], "answer_start": [122, 118, 118]}, "prediction_text": "Machine model."}
{"answers": {"text": ["linear time", "linear", "linear"], "answer_start": [218, 218, 218]}, "prediction_text": "quadratic time."}
{"answers": {"text": ["single-tape Turing machines", "single-tape", "single-tape"], "answer_start": [318, 318, 318]}, "prediction_text": "Multi-tape Turing machine."}
{"answers": {"text": ["Cobham-Edmonds thesis", "Cobham-Edmonds", "Cobham-Edmonds thesis"], "answer_start": [398, 398, 398]}, "prediction_text": "Cobham-Edmonds thesis."}
{"answers": {"text": ["complexity class P", "P", "complexity class P"], "answer_start": [597, 614, 597]}, "prediction_text": "P"}
{"answers": {"text": ["time or space", "time or space", "time or space"], "answer_start": [65, 65, 65]}, "prediction_text": "Time and space."}
{"answers": {"text": ["bounding", "bounding", "bounding"], "answer_start": [52, 52, 52]}, "prediction_text": "Time or space."}
{"answers": {"text": ["complexity classes", "complexity classes", "complexity classes"], "answer_start": [15, 15, 15]}, "prediction_text": "complexity classes"}
{"answers": {"text": ["BPP, ZPP and RP", "BPP, ZPP and RP", "BPP, ZPP and RP"], "answer_start": [43, 43, 43]}, "prediction_text": "BPP, ZPP, RP."}
{"answers": {"text": ["Boolean", "Boolean", "Boolean circuits;"], "answer_start": [150, 150, 150]}, "prediction_text": "Boolean circuits."}
{"answers": {"text": ["quantum", "quantum", "quantum"], "answer_start": [209, 209, 209]}, "prediction_text": "Quantum Turing machines."}
{"answers": {"text": ["#P", "#P", "#P"], "answer_start": [234, 234, 234]}, "prediction_text": "ALL"}
{"answers": {"text": ["Interactive", "Interactive", "Interactive"], "answer_start": [357, 357, 357]}, "prediction_text": "Interactive proof systems."}
{"answers": {"text": ["computation time", "computation time", "computation time"], "answer_start": [113, 113, 113]}, "prediction_text": "Time and space hierarchy theorems."}
{"answers": {"text": ["DTIME(n2)", "DTIME(n2)", "DTIME(n2)"], "answer_start": [220, 220, 220]}, "prediction_text": "DTIME(n)"}
{"answers": {"text": ["time and space hierarchy theorems", "time and space hierarchy theorems", "time and space hierarchy theorems"], "answer_start": [369, 369, 369]}, "prediction_text": "Time and space hierarchy theorems."}
{"answers": {"text": ["a proper hierarchy on the classes defined", "a proper hierarchy on the classes", "a proper hierarchy"], "answer_start": [472, 472, 472]}, "prediction_text": "time and space hierarchy theorems."}
{"answers": {"text": ["quantitative statements", "quantitative", "quantitative"], "answer_start": [714, 714, 714]}, "prediction_text": "Quantitative statements."}
{"answers": {"text": ["time and space hierarchy theorems", "The time and space hierarchy theorems", "time and space hierarchy theorems"], "answer_start": [4, 0, 4]}, "prediction_text": "The time and space hierarchy theorems."}
{"answers": {"text": ["EXPTIME", "EXPTIME", "EXPTIME"], "answer_start": [186, 186, 186]}, "prediction_text": "EXPTIME"}
{"answers": {"text": ["PSPACE", "PSPACE", "PSPACE"], "answer_start": [268, 268, 268]}, "prediction_text": "P"}
{"answers": {"text": ["reduction", "a reduction", "reduction"], "answer_start": [59, 57, 59]}, "prediction_text": "Reduction"}
{"answers": {"text": ["another problem", "another problem", "another problem"], "answer_start": [122, 122, 122]}, "prediction_text": "another problem"}
{"answers": {"text": ["reduces", "reduces", "X reduces to Y"], "answer_start": [350, 350, 348]}, "prediction_text": "reduces to Y"}
{"answers": {"text": ["Karp reductions and Levin reductions", "Cook reductions, Karp reductions"], "answer_start": [469, 452]}, "prediction_text": "Cook reductions, Karp reductions, Levin reductions."}
{"answers": {"text": ["the bound on the complexity of reductions", "types of reductions", "the bound on the complexity of reductions"], "answer_start": [511, 389, 511]}, "prediction_text": "Cook reductions."}
{"answers": {"text": ["polynomial-time reduction", "polynomial-time", "polynomial-time reduction"], "answer_start": [38, 38, 38]}, "prediction_text": "Polynomial-time reduction."}
{"answers": {"text": ["multiplying two integers", "multiplying two integers", "multiplying two integers"], "answer_start": [207, 207, 207]}, "prediction_text": "Multiplying two integers."}
{"answers": {"text": ["polynomial time", "polynomial", "polynomial time"], "answer_start": [109, 109, 109]}, "prediction_text": "polynomial time reduction"}
{"answers": {"text": ["input", "input", "input"], "answer_start": [364, 364, 364]}, "prediction_text": "The same input."}
{"answers": {"text": ["multiplication", "multiplication", "multiplication"], "answer_start": [392, 520, 520]}, "prediction_text": "multiplication."}
{"answers": {"text": ["the type of reduction being used", "the type of reduction being used"], "answer_start": [315, 315]}, "prediction_text": "Type of reduction."}
{"answers": {"text": ["if every problem in C can be reduced to X", "problem in C is harder than X"], "answer_start": [121, 172]}, "prediction_text": "A problem X is hard for a class of problems C if every problem in C can be reduced to X."}
{"answers": {"text": ["solve any problem in C", "solve any problem in C", "solve any problem in C"], "answer_start": [241, 241, 241]}, "prediction_text": "solve any problem in C."}
{"answers": {"text": ["NP-hard", "NP-hard", "NP-hard problems"], "answer_start": [503, 503, 503]}, "prediction_text": "NP-hard problems."}
{"answers": {"text": ["NP-complete", "NP-complete", "NP-complete"], "answer_start": [244, 244, 244]}, "prediction_text": "NP-complete problems."}
{"answers": {"text": ["NP", "P", "P"], "answer_start": [244, 370, 370]}, "prediction_text": "P"}
{"answers": {"text": ["there is no known polynomial-time solution", "no known polynomial-time solution", "there is no known polynomial-time solution"], "answer_start": [513, 522, 513]}, "prediction_text": "There is no known polynomial-time solution for \u03a01."}
{"answers": {"text": ["NP", "NP", "NP"], "answer_start": [244, 810, 810]}, "prediction_text": "NP."}
{"answers": {"text": ["P", "P", "P"], "answer_start": [21, 21, 21]}, "prediction_text": "P"}
{"answers": {"text": ["Cobham\u2013Edmonds thesis", "Cobham\u2013Edmonds thesis", "Cobham\u2013Edmonds thesis"], "answer_start": [167, 167, 167]}, "prediction_text": "Cobham-Edmonds thesis"}
{"answers": {"text": ["NP", "NP", "NP"], "answer_start": [211, 211, 211]}, "prediction_text": "P"}
{"answers": {"text": ["Boolean satisfiability problem", "Boolean satisfiability problem"], "answer_start": [361, 361]}, "prediction_text": "The Hamiltonian path problem."}
{"answers": {"text": ["Turing machines", "deterministic Turing machines", "deterministic Turing machines"], "answer_start": [472, 458, 458]}, "prediction_text": "Turing machine."}
{"answers": {"text": ["more efficient solutions", "shown to have more efficient solutions", "many important problems can be shown to have more efficient solutions"], "answer_start": [227, 213, 182]}, "prediction_text": "Efficient."}
{"answers": {"text": ["protein structure prediction", "protein structure prediction", "protein structure prediction"], "answer_start": [365, 365, 365]}, "prediction_text": "Protein structure prediction."}
{"answers": {"text": ["$1,000,000", "US$1,000,000", "US$1,000,000"], "answer_start": [595, 593, 593]}, "prediction_text": "$1,000,000"}
{"answers": {"text": ["Ladner", "Ladner", "Ladner"], "answer_start": [16, 16, 16]}, "prediction_text": "Ladner"}
{"answers": {"text": ["NP-intermediate problems", "NP-intermediate problems", "NP-intermediate"], "answer_start": [134, 134, 134]}, "prediction_text": "NP-intermediate problem"}
{"answers": {"text": ["graph isomorphism problem", "the discrete logarithm problem", "graph isomorphism problem, the discrete logarithm problem and the integer factorization problem"], "answer_start": [164, 191, 164]}, "prediction_text": "The discrete logarithm problem."}
{"answers": {"text": ["The graph isomorphism problem", "graph isomorphism", "The graph isomorphism problem"], "answer_start": [0, 4, 0]}, "prediction_text": "The problem attributed to defining if two finite graphs are isomorphic is the computational problem of determining whether two finite graphs are isomorphic."}
{"answers": {"text": ["NP-complete", "NP-complete", "NP-complete"], "answer_start": [217, 327, 327]}, "prediction_text": "NP-complete"}
{"answers": {"text": ["polynomial time hierarchy", "polynomial time", "polynomial time hierarchy"], "answer_start": [381, 381, 381]}, "prediction_text": "The polynomial time hierarchy."}
{"answers": {"text": ["second level", "second", "second"], "answer_start": [424, 424, 424]}, "prediction_text": "Second level."}
{"answers": {"text": ["Laszlo Babai and Eugene Luks", "Babai and Eugene Luks", "Laszlo Babai and Eugene Luks"], "answer_start": [637, 644, 637]}, "prediction_text": "Laszlo Babai and Eugene Luks"}
{"answers": {"text": ["The integer factorization problem", "integer factorization", "integer factorization problem"], "answer_start": [0, 4, 4]}, "prediction_text": "Integer factorization."}
{"answers": {"text": ["k", "k", "k"], "answer_start": [224, 224, 224]}, "prediction_text": "k"}
{"answers": {"text": ["modern cryptographic systems", "modern cryptographic systems", "RSA algorithm"], "answer_start": [323, 323, 365]}, "prediction_text": "RSA algorithm"}
{"answers": {"text": ["the general number field sieve", "RSA", "general number field sieve"], "answer_start": [641, 365, 645]}, "prediction_text": "General number field sieve."}
{"answers": {"text": ["suspected to be unequal", "unequal", "Many known complexity classes are suspected to be unequal"], "answer_start": [34, 50, 0]}, "prediction_text": "That all complexity classes collapse to one class."}
{"answers": {"text": ["P \u2286 NP \u2286 PP \u2286 PSPACE", "P \u2286 NP \u2286 PP \u2286 PSPACE", "P \u2286 NP \u2286 PP \u2286 PSPACE"], "answer_start": [102, 102, 102]}, "prediction_text": "P \u2286 NP \u2286 PP \u2286 PSPACE."}
{"answers": {"text": ["between P and PSPACE", "between P and PSPACE", "between P and PSPACE"], "answer_start": [269, 269, 269]}, "prediction_text": "Between P and PSPACE."}
{"answers": {"text": ["Proving that any of these classes are unequal", "Proving that any of these classes are unequal", "Proving that any of these classes are unequal"], "answer_start": [403, 403, 403]}, "prediction_text": "Collapse to one class."}
{"answers": {"text": ["co-NP", "co-NP", "co-NP"], "answer_start": [22, 22, 22]}, "prediction_text": "Co-NP"}
{"answers": {"text": ["reversed", "reversed", "reversed"], "answer_start": [115, 115, 115]}, "prediction_text": "reversed"}
{"answers": {"text": ["not equal", "not equal", "not equal"], "answer_start": [167, 303, 167]}, "prediction_text": "P = NP"}
{"answers": {"text": ["P is not equal to NP", "not equal", "P is not equal to NP"], "answer_start": [298, 303, 298]}, "prediction_text": "P is not equal to NP."}
{"answers": {"text": ["L", "L", "L"], "answer_start": [30, 30, 30]}, "prediction_text": "L"}
{"answers": {"text": ["strictly contained in P or equal to P", "contained in P or equal to P.", "strictly contained in P or equal to P"], "answer_start": [101, 110, 101]}, "prediction_text": "Logarithmic space, complexity classes, and unkown."}
{"answers": {"text": ["complexity classes", "many complexity classes", "many complexity classes"], "answer_start": [162, 157, 157]}, "prediction_text": "NL and NC."}
{"answers": {"text": ["NL and NC", "NL and NC", "NL and NC"], "answer_start": [206, 206, 206]}, "prediction_text": "NL and NC."}
{"answers": {"text": ["if they are distinct or equal classes", "if they are distinct or equal classes", "if they are distinct or equal classes"], "answer_start": [237, 237, 237]}, "prediction_text": "The unknown."}
{"answers": {"text": ["intractable problems", "intractable problems", "intractable", "intractable"], "answer_start": [158, 158, 158, 158]}, "prediction_text": "intractable problems."}
{"answers": {"text": ["exponential-time algorithms", "exponential-time", "exponential-time algorithms", "exponential-time algorithms"], "answer_start": [673, 673, 673, 673]}, "prediction_text": "Exponential time algorithms."}
{"answers": {"text": ["NP-complete problems", "NP-complete", "NP-complete", "NP-complete"], "answer_start": [605, 605, 605, 605]}, "prediction_text": "EXPTIME-hard."}
{"answers": {"text": ["Presburger arithmetic", "Presburger", "Presburger arithmetic"], "answer_start": [219, 219, 219]}, "prediction_text": "Presburger arithmetic."}
{"answers": {"text": ["algorithms have been written", "algorithms have been written", "algorithms have been written that solve the problem in reasonable times in most cases"], "answer_start": [276, 276, 276]}, "prediction_text": "Algorithms."}
{"answers": {"text": ["NP-complete knapsack problem", "NP-complete knapsack", "the NP-complete knapsack problem"], "answer_start": [399, 399, 395]}, "prediction_text": "knapsack problem"}
{"answers": {"text": ["in less than quadratic time", "less than quadratic time", "less than quadratic time"], "answer_start": [455, 458, 458]}, "prediction_text": "Less than quadratic time."}
{"answers": {"text": ["NP-complete Boolean satisfiability problem", "NP-complete Boolean satisfiability", "the NP-complete Boolean satisfiability problem"], "answer_start": [539, 539, 535]}, "prediction_text": "knapsack problem"}
{"answers": {"text": ["foundations were laid out", "numerous foundations were laid out", "numerous foundations were laid out by various researchers"], "answer_start": [110, 101, 101]}, "prediction_text": "Defining Turing machines."}
{"answers": {"text": ["Alan Turing", "Alan Turing", "Alan Turing"], "answer_start": [230, 230, 230]}, "prediction_text": "Alan Turing"}
{"answers": {"text": ["Turing machines", "Turing machines", "Turing machines"], "answer_start": [211, 211, 211]}, "prediction_text": "Turing machines"}
{"answers": {"text": ["1936", "1936", "1936"], "answer_start": [245, 245, 245]}, "prediction_text": "1936"}
{"answers": {"text": ["a computer", "a computer", "a computer"], "answer_start": [319, 319, 319]}, "prediction_text": "A computer."}
{"answers": {"text": ["On the Computational Complexity of Algorithms", "On the Computational Complexity of Algorithms", "\"On the Computational Complexity of Algorithms\""], "answer_start": [137, 137, 136]}, "prediction_text": "Juris Hartmanis and Richard Stearns (1965)"}
{"answers": {"text": ["Juris Hartmanis and Richard Stearns", "Juris Hartmanis and Richard Stearns", "Juris Hartmanis and Richard Stearns"], "answer_start": [187, 187, 187]}, "prediction_text": "Juris Hartmanis and Richard Stearns."}
{"answers": {"text": ["1965", "1965", "1965"], "answer_start": [224, 224, 224]}, "prediction_text": "1965"}
{"answers": {"text": ["time and space", "definitions of time and space complexity", "time and space complexity"], "answer_start": [265, 250, 265]}, "prediction_text": "Time and space complexity."}
{"answers": {"text": ["1965", "1965", "1965"], "answer_start": [224, 335, 335]}, "prediction_text": "1965"}
{"answers": {"text": ["John Myhill", "John Myhill", "John Myhill"], "answer_start": [102, 102, 102]}, "prediction_text": "John Myhill"}
{"answers": {"text": ["1961", "1961", "1961"], "answer_start": [215, 215, 215]}, "prediction_text": "1961"}
{"answers": {"text": ["Hisao Yamada", "Hisao Yamada", "Hisao Yamada"], "answer_start": [233, 233, 233]}, "prediction_text": "Boris Trakhtenbrot"}
{"answers": {"text": ["input encoding", "input encoding", "input encoding"], "answer_start": [98, 98, 98]}, "prediction_text": "Concrete choice of input encoding."}
{"answers": {"text": ["encoding", "encoding", "encoding"], "answer_start": [104, 198, 198]}, "prediction_text": "The choice of encoding."}
{"answers": {"text": ["Manuel Blum", "Manuel Blum", "Manuel Blum"], "answer_start": [9, 9, 9]}, "prediction_text": "Manuel Blum"}
{"answers": {"text": ["speed-up theorem", "speed-up theorem", "speed-up theorem"], "answer_start": [129, 129, 129]}, "prediction_text": "Speed-up theorem."}
{"answers": {"text": ["\"Reducibility Among Combinatorial Problems\"", "Reducibility Among Combinatorial Problems", "\"Reducibility Among Combinatorial Problems\""], "answer_start": [433, 434, 433]}, "prediction_text": "Reducibility Among Combinatorial Problems."}
{"answers": {"text": ["21", "21", "21"], "answer_start": [502, 502, 502]}, "prediction_text": "21"}
{"answers": {"text": ["the curriculum.", "curriculum", "curriculum"], "answer_start": [619, 623, 623]}, "prediction_text": "curriculum"}
{"answers": {"text": ["pedagogy", "pedagogy", "formal education"], "answer_start": [328, 328, 91]}, "prediction_text": "Pedagogy"}
{"answers": {"text": ["university or college.", "university", "university or college"], "answer_start": [246, 246, 246]}, "prediction_text": "University or college."}
{"answers": {"text": ["lesson plan", "lesson plan", "lesson plan"], "answer_start": [531, 531, 531]}, "prediction_text": "Lesson plan."}
{"answers": {"text": ["school", "school", "school"], "answer_start": [66, 66, 66]}, "prediction_text": "School or other place of formal education."}
{"answers": {"text": ["cultures", "cultures", "cultures"], "answer_start": [32, 32, 32]}, "prediction_text": "Culture"}
{"answers": {"text": ["numeracy", "numeracy", "numeracy"], "answer_start": [91, 91, 91]}, "prediction_text": "Literacy."}
{"answers": {"text": ["craftsmanship", "craftsmanship", "craftsmanship"], "answer_start": [101, 101, 101]}, "prediction_text": "Arts, religion, civics, community roles."}
{"answers": {"text": ["life skills", "life skills", "life skills"], "answer_start": [187, 187, 187]}, "prediction_text": "Literacy and numeracy."}
{"answers": {"text": ["family member", "family member", "family member"], "answer_start": [171, 171, 171]}, "prediction_text": "Family member"}
{"answers": {"text": ["home schooling", "home schooling", "home schooling"], "answer_start": [59, 59, 59]}, "prediction_text": "Transient or ongoing role."}
{"answers": {"text": ["formal", "Informal", "formal education"], "answer_start": [19, 75, 19]}, "prediction_text": "Formal education."}
{"answers": {"text": ["transient", "transient", "Informal learning"], "answer_start": [134, 134, 75]}, "prediction_text": "Transient or temporary role."}
{"answers": {"text": ["knowledge or skills", "anyone with knowledge or skills", "knowledge or skills"], "answer_start": [204, 192, 204]}, "prediction_text": "Knowledge or skills in the wider community setting."}
{"answers": {"text": ["spiritual", "spiritual teachers", "gurus, mullahs, rabbis, pastors/youth pastors and lamas"], "answer_start": [14, 14, 42]}, "prediction_text": "Guru"}
{"answers": {"text": ["religious", "religious", "religious"], "answer_start": [109, 109, 109]}, "prediction_text": "Religious text."}
{"answers": {"text": ["the Quran, Torah or Bible", "Quran", "Torah"], "answer_start": [133, 137, 144]}, "prediction_text": "Quran"}
{"answers": {"text": ["Religious and spiritual teachers", "pastors", "Religious and spiritual teachers"], "answer_start": [0, 80, 0]}, "prediction_text": "Religious and spiritual teachers."}
{"answers": {"text": ["homeschooling", "homeschooling", "homeschooling"], "answer_start": [75, 75, 75]}, "prediction_text": "Homeschooling"}
{"answers": {"text": ["paid professionals.", "professionals", "paid professionals"], "answer_start": [155, 160, 155]}, "prediction_text": "Professionals"}
{"answers": {"text": ["Chartered", "Chartered", "Chartered"], "answer_start": [290, 290, 290]}, "prediction_text": "Chartered accountant"}
{"answers": {"text": ["the wider community", "wider community", "in the wider community"], "answer_start": [96, 100, 93]}, "prediction_text": "In the wider community."}
{"answers": {"text": ["paid professionals.", "professionals", "Formal teaching"], "answer_start": [155, 180, 117]}, "prediction_text": "Professionals"}
{"answers": {"text": ["school functions", "school functions", "school functions"], "answer_start": [193, 193, 193]}, "prediction_text": "Study halls."}
{"answers": {"text": ["extracurricular", "extracurricular", "extracurricular"], "answer_start": [240, 240, 240]}, "prediction_text": "Student discipline."}
{"answers": {"text": ["study halls", "study halls"], "answer_start": [150, 150]}, "prediction_text": "Study halls."}
{"answers": {"text": ["teachers", "teachers", "teachers"], "answer_start": [92, 295, 295]}, "prediction_text": "Teachers"}
{"answers": {"text": ["teacher's colleges", "colleges", "teacher's colleges"], "answer_start": [166, 176, 166]}, "prediction_text": "Teacher's colleges"}
{"answers": {"text": ["to serve and protect the public interest", "serve and protect the public", "to serve and protect the public interest"], "answer_start": [218, 221, 218]}, "prediction_text": "To serve and protect the public interest."}
{"answers": {"text": ["the public", "public", "public interest"], "answer_start": [239, 243, 243]}, "prediction_text": "Public interest."}
{"answers": {"text": ["teachers", "teachers", "the teaching profession"], "answer_start": [114, 114, 333]}, "prediction_text": "Teachers"}
{"answers": {"text": ["standards of practice", "standards of practice", "standards of practice for the teaching profession"], "answer_start": [307, 307, 307]}, "prediction_text": "Standards of practice."}
{"answers": {"text": ["members", "members", "members"], "answer_start": [177, 177, 177]}, "prediction_text": "Teachers."}
{"answers": {"text": ["allegations of professional misconduct", "professional misconduct", "professional misconduct"], "answer_start": [211, 226, 226]}, "prediction_text": "Professional misconduct."}
{"answers": {"text": ["teacher's colleges", "college", "the college"], "answer_start": [21, 434, 430]}, "prediction_text": "The State Board of Education."}
{"answers": {"text": ["teacher's colleges", "teacher's colleges", "teacher's colleges"], "answer_start": [21, 21, 21]}, "prediction_text": "The State Board of Education."}
{"answers": {"text": ["teacher's colleges", "State Board of Education", "teacher's colleges"], "answer_start": [21, 563, 21]}, "prediction_text": "State Board of Education"}
{"answers": {"text": ["outdoors", "outdoors", "outdoors"], "answer_start": [123, 123, 123]}, "prediction_text": "Outdoors."}
{"answers": {"text": ["tutor", "tutor", "tutor"], "answer_start": [200, 200, 200]}, "prediction_text": "Tutor"}
{"answers": {"text": ["academy", "academy", "academy"], "answer_start": [73, 73, 73]}, "prediction_text": "Outdoors."}
{"answers": {"text": ["facilitate student learning", "facilitate student learning", "facilitate student learning"], "answer_start": [23, 23, 23]}, "prediction_text": "Facilitate student learning."}
{"answers": {"text": ["informal", "informal", "informal"], "answer_start": [58, 58, 58]}, "prediction_text": "informal"}
{"answers": {"text": ["pedagogy", "pedagogy", "pedagogy"], "answer_start": [247, 247, 247]}, "prediction_text": "Pedagogy"}
{"answers": {"text": ["field trips", "field trips", "field trips"], "answer_start": [555, 555, 555]}, "prediction_text": "Pedagogy"}
{"answers": {"text": ["increasing use of technology", "use of technology,", "increasing use of technology"], "answer_start": [572, 583, 572]}, "prediction_text": "Technology."}
{"answers": {"text": ["the internet", "the internet", "technology"], "answer_start": [627, 627, 590]}, "prediction_text": "Technology"}
{"answers": {"text": ["skill", "skill", "skill"], "answer_start": [74, 74, 74]}, "prediction_text": "Skill"}
{"answers": {"text": ["the relevant authority", "teacher", "teacher"], "answer_start": [142, 83, 83]}, "prediction_text": "The relevant authority."}
{"answers": {"text": ["learning", "learning", "learning"], "answer_start": [300, 300, 300]}, "prediction_text": "Learning disabilities."}
{"answers": {"text": ["infants", "infants", "infants"], "answer_start": [229, 229, 229]}, "prediction_text": "Infants"}
{"answers": {"text": ["standardized", "standardized", "standardized"], "answer_start": [102, 102, 102]}, "prediction_text": "Standardized curricula."}
{"answers": {"text": ["particular skills", "educational", "particular skills"], "answer_start": [89, 51, 89]}, "prediction_text": "skills"}
{"answers": {"text": ["self-study and problem solving", "classroom", "in self-study and problem solving"], "answer_start": [758, 156, 755]}, "prediction_text": "Self-study and problem solving."}
{"answers": {"text": ["encourage", "encourage", "encourage"], "answer_start": [921, 921, 921]}, "prediction_text": "Deflate the cocky."}
{"answers": {"text": ["deflate", "deflate", "deflate"], "answer_start": [902, 902, 902]}, "prediction_text": "Deflate the cocky."}
{"answers": {"text": ["a coach", "coach", "coach"], "answer_start": [1046, 1048, 1048]}, "prediction_text": "Coach using the whole gamut of psychology to get each new class of rookies off the bench and into the game."}
{"answers": {"text": ["the relationship between teachers and children", "the relationship between teachers and children.", "the relationship between teachers and children.", "the relationship between teachers and children"], "answer_start": [96, 96, 96, 96]}, "prediction_text": "Teachers and children."}
{"answers": {"text": ["the whole curriculum", "whole curriculum", "whole curriculum", "the whole curriculum"], "answer_start": [249, 253, 253, 249]}, "prediction_text": "The whole curriculum."}
{"answers": {"text": ["different subject specialists", "subject specialists", "subject specialists", "teachers"], "answer_start": [315, 325, 325, 121]}, "prediction_text": "Different subject specialists."}
{"answers": {"text": ["primary school", "primary", "primary", "primary school"], "answer_start": [48, 498, 498, 498]}, "prediction_text": "Primary school"}
{"answers": {"text": ["surrogate", "surrogate", "surrogate", "surrogate"], "answer_start": [566, 566, 566, 566]}, "prediction_text": "Form tutor"}
{"answers": {"text": ["alternative", "alternative", "platoon\" system"], "answer_start": [68, 68, 165]}, "prediction_text": "Platoon system."}
{"answers": {"text": ["platoon", "platoon", "platoon\" system"], "answer_start": [165, 165, 165]}, "prediction_text": "Platoon system"}
{"answers": {"text": ["staying with the same group of peers for all classes", "staying with the same group of peers for all classes", "by staying with the same group of peers for all classes"], "answer_start": [537, 537, 534]}, "prediction_text": "The same group of peers."}
{"answers": {"text": ["knowledgeable", "knowledgeable", "knowledgeable"], "answer_start": [412, 412, 412]}, "prediction_text": "knowledgeable"}
{"answers": {"text": ["United States", "United States", "United States"], "answer_start": [36, 36, 36]}, "prediction_text": "United States"}
{"answers": {"text": ["Co-teaching", "Co-teaching", "Co-teaching"], "answer_start": [0, 0, 0]}, "prediction_text": "Co-teaching"}
{"answers": {"text": ["two or more", "two or more", "two or more"], "answer_start": [100, 100, 100]}, "prediction_text": "2 or more teachers."}
{"answers": {"text": ["learning", "learning", "learning"], "answer_start": [233, 233, 233]}, "prediction_text": "Learning."}
{"answers": {"text": ["harmoniously", "in sync", "harmoniously"], "answer_start": [129, 358, 129]}, "prediction_text": "Co-teachers work in sync with one another to create a climate of learning."}
{"answers": {"text": ["social networking support", "social networking", "social networking"], "answer_start": [257, 257, 257]}, "prediction_text": "Social networking support."}
{"answers": {"text": ["corporal punishment", "corporal punishment", "corporal"], "answer_start": [82, 82, 82]}, "prediction_text": "Corporal punishment"}
{"answers": {"text": ["substitute parent", "substitute parent", "substitute parent"], "answer_start": [167, 167, 167]}, "prediction_text": "Substitute parent."}
{"answers": {"text": ["all the normal forms of parental discipline", "parental", "normal forms of parental discipline"], "answer_start": [191, 215, 199]}, "prediction_text": "Normal forms of parental discipline."}
{"answers": {"text": ["the most common", "most common", "most common"], "answer_start": [36, 40, 40]}, "prediction_text": "Throughout the history of education."}
{"answers": {"text": ["While a child was in school", "While a child was in school", "While a child was in school"], "answer_start": [103, 103, 103]}, "prediction_text": "When the child was in school."}
{"answers": {"text": ["one of the most common", "one of the most common", "most common", "most common"], "answer_start": [141, 141, 152, 152]}, "prediction_text": "Common."}
{"answers": {"text": ["Most Western countries", "Most Western countries", "Most Western countries", "Most Western countries"], "answer_start": [221, 221, 221, 221]}, "prediction_text": "United States"}
{"answers": {"text": ["United States", "United States", "United States", "United States"], "answer_start": [311, 311, 311, 311]}, "prediction_text": "United States"}
{"answers": {"text": ["Supreme Court", "US Supreme Court", "US Supreme Court", "US Supreme Court"], "answer_start": [340, 337, 337, 337]}, "prediction_text": "US Supreme Court"}
{"answers": {"text": ["physical pain", "physical pain", "physical pain", "physical pain"], "answer_start": [122, 122, 122, 122]}, "prediction_text": "Physical pain."}
{"answers": {"text": ["30", "30", "30"], "answer_start": [0, 0, 0]}, "prediction_text": "30"}
{"answers": {"text": ["the South", "the South", "the South"], "answer_start": [68, 68, 68]}, "prediction_text": "Alabama, Arkansas, Georgia, Louisiana, Mississippi, Oklahoma, Tennessee, and Texas."}
{"answers": {"text": ["declining", "declining", "declining"], "answer_start": [131, 131, 131]}, "prediction_text": "Declining."}
{"answers": {"text": ["a specially made wooden paddle", "wooden paddle", "wooden paddle"], "answer_start": [430, 447, 447]}, "prediction_text": "paddle"}
{"answers": {"text": ["privately in the principal's office", "principal's office.", "principal's office"], "answer_start": [566, 583, 583]}, "prediction_text": "Private schools."}
{"answers": {"text": ["caning", "caning", "caning"], "answer_start": [39, 39, 39]}, "prediction_text": "Caning."}
{"answers": {"text": ["some Asian, African and Caribbean countries", "Asian, African and Caribbean", "Asian, African and Caribbean"], "answer_start": [81, 86, 86]}, "prediction_text": "Asian, African, and Caribbean countries."}
{"answers": {"text": ["see School corporal punishment.", "School corporal punishment", "School corporal punishment"], "answer_start": [162, 166, 166]}, "prediction_text": "School corporal punishment."}
{"answers": {"text": ["detention", "detention", "detention"], "answer_start": [10, 10, 10]}, "prediction_text": "detention"}
{"answers": {"text": ["detention", "detention", "detention"], "answer_start": [10, 10, 10]}, "prediction_text": "detention"}
{"answers": {"text": ["in schools", "school", "school"], "answer_start": [58, 174, 174]}, "prediction_text": "In a classroom."}
{"answers": {"text": ["quietly", "quietly", "quietly"], "answer_start": [468, 468, 468]}, "prediction_text": "In a classroom."}
{"answers": {"text": ["lines or a punishment essay", "punishment essay", "lines or a punishment essay"], "answer_start": [432, 443, 432]}, "prediction_text": "Lines."}
{"answers": {"text": ["assertive", "assertive", "assertive"], "answer_start": [101, 101, 101]}, "prediction_text": "Assertive teacher"}
{"answers": {"text": ["immediate and fair punishment for misbehavior", "immediate and fair punishment for misbehavior and firm, clear boundaries", "immediate and fair punishment for misbehavior"], "answer_start": [210, 210, 210]}, "prediction_text": "immediate and fair punishment for misbehavior and firm, clear boundaries."}
{"answers": {"text": ["firm, clear boundaries", "firm, clear", "clear"], "answer_start": [260, 260, 266]}, "prediction_text": "firm, clear boundaries"}
{"answers": {"text": ["sarcasm and attempts to humiliate pupils", "sarcasm and attempts to humiliate pupils", "sarcasm and attempts to humiliate pupils"], "answer_start": [387, 387, 387]}, "prediction_text": "Sarcasm and attempts to humiliate pupils."}
{"answers": {"text": ["respect", "respect", "respect"], "answer_start": [363, 363, 363]}, "prediction_text": "Positive reinforcement."}
{"answers": {"text": ["some teachers and parents", "some teachers and parents", "some teachers and parents advocate"], "answer_start": [74, 74, 74]}, "prediction_text": "Teachers and parents."}
{"answers": {"text": ["East Asia", "East Asia", "countries\u2014in East Asia"], "answer_start": [470, 470, 457]}, "prediction_text": "East Asia."}
{"answers": {"text": ["weakness in school discipline", "weakness in school discipline", "the weakness in school discipline"], "answer_start": [262, 262, 258]}, "prediction_text": "Weakness in school discipline."}
{"answers": {"text": ["a more assertive and confrontational style", "strict discipline", "more assertive and confrontational"], "answer_start": [109, 506, 111]}, "prediction_text": "Firm control."}
{"answers": {"text": ["Japan", "Japan", "Japan"], "answer_start": [202, 202, 202]}, "prediction_text": "Japan"}
{"answers": {"text": ["Japan", "Japan", "Japan"], "answer_start": [202, 202, 202]}, "prediction_text": "Japan"}
{"answers": {"text": ["Japan", "Japan", "Japan"], "answer_start": [202, 202, 202]}, "prediction_text": "Japan"}
{"answers": {"text": ["Japan", "Japan", "Japan"], "answer_start": [202, 202, 202]}, "prediction_text": "Japan"}
{"answers": {"text": ["40 to 50 students", "40 to 50 students,", "40 to 50 students"], "answer_start": [39, 39, 39]}, "prediction_text": "40 to 50 students."}
{"answers": {"text": ["instruction", "instruction", "instruction"], "answer_start": [121, 121, 121]}, "prediction_text": "instruction"}
{"answers": {"text": ["motivated students", "motivated students", "motivated students"], "answer_start": [271, 271, 271]}, "prediction_text": "Motivated students."}
{"answers": {"text": ["attention-seeking and disruptive students", "attention-seeking and disruptive students", "attention-seeking and disruptive students"], "answer_start": [300, 300, 300]}, "prediction_text": "Students."}
{"answers": {"text": ["motivated students", "motivated students", "motivated students"], "answer_start": [271, 370, 370]}, "prediction_text": "Motivated students."}
{"answers": {"text": ["popularly based authority", "popularly based", "popularly based"], "answer_start": [44, 44, 44]}, "prediction_text": "dictatorial authority"}
{"answers": {"text": ["governments", "governments", "governments"], "answer_start": [137, 137, 137]}, "prediction_text": "Government and schools alike."}
{"answers": {"text": ["persuasion and negotiation", "persuasion and negotiation", "persuasion and negotiation"], "answer_start": [405, 405, 405]}, "prediction_text": "persuasion and negotiation."}
{"answers": {"text": ["easier and more efficient", "easier and more efficient", "easier and more efficient"], "answer_start": [242, 242, 242]}, "prediction_text": "Sudbury model democratic schools' claim."}
{"answers": {"text": ["good, clear laws", "laws", "laws"], "answer_start": [568, 580, 580]}, "prediction_text": "Laws and regulations."}
{"answers": {"text": ["enthusiasm", "enthusiasm", "enthusiasm"], "answer_start": [113, 113, 113]}, "prediction_text": "enthusiasm"}
{"answers": {"text": ["passion", "passion", "passion"], "answer_start": [378, 378, 378]}, "prediction_text": "Passion."}
{"answers": {"text": ["teach by rote", "teach by rote", "teach by rote"], "answer_start": [431, 431, 431]}, "prediction_text": "Teach by rote."}
{"answers": {"text": ["higher", "higher", "higher than teachers who didn't show much enthusiasm"], "answer_start": [771, 771, 771]}, "prediction_text": "Higher."}
{"answers": {"text": ["teacher enthusiasm", "enthusiasm", "teacher enthusiasm"], "answer_start": [207, 22, 207]}, "prediction_text": "Emotional facial expressions."}
{"answers": {"text": ["read lecture material", "read lecture material", "read lecture material"], "answer_start": [699, 699, 699]}, "prediction_text": "Read lecture material outside the classroom."}
{"answers": {"text": ["nonverbal expressions of enthusiasm", "nonverbal expressions of enthusiasm", "nonverbal expressions of enthusiasm"], "answer_start": [400, 400, 400]}, "prediction_text": "Demonstrative gesturing."}
{"answers": {"text": ["Controlled, experimental studies", "Controlled, experimental", "Controlled, experimental"], "answer_start": [301, 301, 301]}, "prediction_text": "Controlled, experimental studies."}
{"answers": {"text": ["higher", "higher levels", "higher levels"], "answer_start": [578, 578, 578]}, "prediction_text": "Higher."}
{"answers": {"text": ["self-determined", "self-determined", "self-determined"], "answer_start": [338, 338, 338]}, "prediction_text": "self-determined"}
{"answers": {"text": ["enthusiasm", "enthusiasm", "enthusiasm"], "answer_start": [46, 560, 560]}, "prediction_text": "Enthusiastic teachers."}
{"answers": {"text": ["emotional contagion", "emotional contagion", "emotional contagion"], "answer_start": [768, 768, 768]}, "prediction_text": "Emotional contagion."}
{"answers": {"text": ["Teacher enthusiasm", "excitement", "Teacher enthusiasm"], "answer_start": [111, 233, 111]}, "prediction_text": "Enthusiastic teachers."}
{"answers": {"text": ["student-teacher relationships", "student-teacher relationships", "student-teacher relationships"], "answer_start": [90, 90, 90]}, "prediction_text": "Student-teacher relationships."}
{"answers": {"text": ["beneficial", "beneficial", "beneficial"], "answer_start": [177, 177, 177]}, "prediction_text": "beneficial relations"}
{"answers": {"text": ["the goals he receives from his superior.", "goals he receives from his superior.", "personal goals"], "answer_start": [589, 593, 679]}, "prediction_text": "Personal goals."}
{"answers": {"text": ["aligning his personal goals with his academic goals.", "aligning his personal goals with his academic goals.", "aligning his personal goals with his academic goals"], "answer_start": [666, 666, 666]}, "prediction_text": "Aligning personal goals with academic goals."}
{"answers": {"text": ["student motivation and attitudes towards school", "student motivation and attitudes towards school", "student motivation and attitudes towards school"], "answer_start": [20, 20, 20]}, "prediction_text": "Student motivation and attitudes towards school."}
{"answers": {"text": ["friendly and supportive", "friendly and supportive", "friendly and supportive"], "answer_start": [70, 70, 70]}, "prediction_text": "Friendly and supportive."}
{"answers": {"text": ["friendly and supportive", "friendly and supportive", "friendly and supportive"], "answer_start": [70, 70, 70]}, "prediction_text": "Friendly and supportive."}
{"answers": {"text": ["interacting and working directly with students", "more time interacting and working directly with students", "interacting and working directly with students"], "answer_start": [189, 179, 189]}, "prediction_text": "Interacting and working directly with students."}
{"answers": {"text": ["effective", "Effective", "Effective"], "answer_start": [268, 288, 288]}, "prediction_text": "Effective teachers."}
{"answers": {"text": ["enthusiasm about the students", "students", "enthusiasm about the students"], "answer_start": [200, 221, 200]}, "prediction_text": "The second aspect of teacher enthusiasm is enthusiasm about the students."}
{"answers": {"text": ["enthusiastic", "enthusiastic", "enthusiastic"], "answer_start": [568, 568, 824]}, "prediction_text": "Enthusiasm."}
{"answers": {"text": ["in the student", "t in the student", "student"], "answer_start": [797, 795, 804]}, "prediction_text": "The student."}
{"answers": {"text": ["very influential", "influential", "influential"], "answer_start": [867, 872, 872]}, "prediction_text": "Influential."}
{"answers": {"text": ["teaching", "teaching", "being around their students"], "answer_start": [47, 293, 401]}, "prediction_text": "Teaching."}
{"answers": {"text": ["sexual misconduct", "sexual", "sexual misconduct"], "answer_start": [35, 35, 35]}, "prediction_text": "Sexual misconduct."}
{"answers": {"text": ["9.6%", "9.6%", "9.6%"], "answer_start": [191, 191, 191]}, "prediction_text": "9.6%"}
{"answers": {"text": ["United States", "United States", "United States"], "answer_start": [215, 215, 215]}, "prediction_text": "United States"}
{"answers": {"text": ["sometime during their educational career.", "educational career"], "answer_start": [391, 413]}, "prediction_text": "During their educational career."}
{"answers": {"text": ["American Association of University Women", "American Association of University Women", "American Association of University Women"], "answer_start": [136, 136, 136]}, "prediction_text": "American Association of University Women"}
{"answers": {"text": ["England", "England", "England"], "answer_start": [11, 11, 11]}, "prediction_text": "England"}
{"answers": {"text": ["priests, religious leaders, and case workers as well as teachers", "priests, religious leaders, and case workers as well as teachers", "priests, religious leaders, and case workers as well as teachers"], "answer_start": [103, 103, 103]}, "prediction_text": "Priests, religious leaders, case workers."}
{"answers": {"text": ["2,869", "2,869", "2,869"], "answer_start": [324, 324, 324]}, "prediction_text": "2,869"}
{"answers": {"text": ["The AAUW study", "AAUW", "AAUW study"], "answer_start": [684, 688, 688]}, "prediction_text": "AAUW study"}
{"answers": {"text": ["United States", "United States", "United States"], "answer_start": [7, 7, 7]}, "prediction_text": "United States"}
{"answers": {"text": ["increased scrutiny on teacher misconduct", "increased scrutiny", "increased scrutiny on teacher misconduct"], "answer_start": [133, 133, 133]}, "prediction_text": "Increased scrutiny."}
{"answers": {"text": ["Fears of being labelled a pedophile or hebephile", "Fears of being labelled a pedophile or hebephile", "Fears of being labelled a pedophile"], "answer_start": [395, 395, 395]}, "prediction_text": "Fears of being labelled a pedophile or hebephile."}
{"answers": {"text": ["Chris Keates", "Chris Keates", "Chris Keates"], "answer_start": [0, 0, 0]}, "prediction_text": "Chris Keates"}
{"answers": {"text": ["child protection and parental rights groups", "child protection and parental rights groups", "child protection and parental rights groups"], "answer_start": [350, 350, 350]}, "prediction_text": "National Association of Schoolmasters Union of Women Teachers."}
{"answers": {"text": ["a shortage of male teachers", "a shortage of male teachers", "shortage of male teachers"], "answer_start": [560, 560, 562]}, "prediction_text": "shortage of male teachers."}
{"answers": {"text": ["the sex offenders register", "sex offenders", "sex offenders register"], "answer_start": [194, 198, 198]}, "prediction_text": "Sex offenders register."}
{"answers": {"text": ["occupational stress", "occupational stress", "occupational stress"], "answer_start": [76, 76, 76]}, "prediction_text": "Occupational stress."}
{"answers": {"text": ["long hours", "organizational change, relationships with students, fellow teachers, and administrative personnel, working environment, expectations to substitute", "organizational change, relationships with students, fellow teachers, and administrative personnel, working environment, expectations to substitute, long hours"], "answer_start": [376, 228, 228]}, "prediction_text": "long hours with a heavy workload"}
{"answers": {"text": ["occupational burnout", "occupational burnout", "occupational burnout"], "answer_start": [462, 462, 462]}, "prediction_text": "Occupational burnout."}
{"answers": {"text": ["stress", "Stress", "occupational stress"], "answer_start": [89, 204, 76]}, "prediction_text": "Organizational change."}
{"answers": {"text": ["occupational stress", "occupational stress", "Stress"], "answer_start": [76, 76, 204]}, "prediction_text": "long hours with a heavy workload"}
{"answers": {"text": ["42%", "42%", "42%"], "answer_start": [24, 24, 24]}, "prediction_text": "42%"}
{"answers": {"text": ["UK", "UK", "UK"], "answer_start": [31, 31, 31]}, "prediction_text": "UK"}
{"answers": {"text": ["twice the figure for the average profession", "twice", "twice"], "answer_start": [76, 76, 76]}, "prediction_text": "twice"}
{"answers": {"text": ["2012", "2012", "2012 study"], "answer_start": [123, 123, 123]}, "prediction_text": "2012 study."}
{"answers": {"text": ["average workers", "average workers", "average workers"], "answer_start": [222, 222, 222]}, "prediction_text": "Average workers."}
{"answers": {"text": ["several", "several", "several"], "answer_start": [10, 10, 10]}, "prediction_text": "Several."}
{"answers": {"text": ["Organizational interventions", "effective", "Organizational interventions"], "answer_start": [73, 255, 73]}, "prediction_text": "Mentoring"}
{"answers": {"text": ["Individual-level interventions", "Individual-level interventions", "Individual-level interventions"], "answer_start": [322, 322, 322]}, "prediction_text": "Stress-management training."}
{"answers": {"text": ["occupational stress among teachers", "occupational stress", "occupational stress"], "answer_start": [286, 286, 432]}, "prediction_text": "occupational stress"}
{"answers": {"text": ["Organizational interventions", "Organizational", "Organizational interventions"], "answer_start": [73, 73, 73]}, "prediction_text": "Individual-level intervention."}
{"answers": {"text": ["a university or college", "university or college", "university or college"], "answer_start": [126, 128, 128]}, "prediction_text": "Universities or colleges."}
{"answers": {"text": ["certification by a recognized body", "certification", "certification by a recognized body"], "answer_start": [175, 175, 175]}, "prediction_text": "A background check and psychiatric evaluation."}
{"answers": {"text": ["elementary school education certificate", "elementary school education certificate", "elementary school education certificate"], "answer_start": [264, 264, 264]}, "prediction_text": "Elementary school education certificate."}
{"answers": {"text": ["a background check and psychiatric evaluation", "background check and psychiatric evaluation", "background check and psychiatric evaluation"], "answer_start": [649, 651, 651]}, "prediction_text": "Background check and psychiatric evaluation."}
{"answers": {"text": ["US", "US", "US"], "answer_start": [606, 606, 606]}, "prediction_text": "US"}
{"answers": {"text": ["the individual states and territories", "individual states", "individual states and territories"], "answer_start": [58, 62, 62]}, "prediction_text": "The individual states and territories."}
{"answers": {"text": ["three", "three", "three"], "answer_start": [143, 143, 143]}, "prediction_text": "3"}
{"answers": {"text": ["tertiary education", "tertiary education", "tertiary education"], "answer_start": [281, 281, 281]}, "prediction_text": "Secondary schools/high schools."}
{"answers": {"text": ["universities and/or TAFE colleges", "universities and/or TAFE colleges", "universities and/or TAFE colleges"], "answer_start": [301, 301, 301]}, "prediction_text": "Universities and/or TAFE colleges."}
{"answers": {"text": ["primary", "primary education", "primary education"], "answer_start": [175, 175, 175]}, "prediction_text": "Primary schools"}
{"answers": {"text": ["a post-secondary degree Bachelor's Degree", "a post-secondary degree Bachelor's Degree", "post-secondary degree Bachelor's Degree"], "answer_start": [28, 28, 30]}, "prediction_text": "$40,000/year"}
{"answers": {"text": ["a second Bachelor's Degree such as a Bachelor of Education", "a second Bachelor's Degree", "a second Bachelor's Degree"], "answer_start": [89, 89, 89]}, "prediction_text": "A second Bachelor's Degree."}
{"answers": {"text": ["the private sector, businesses and sponsors", "private sector", "private sector, businesses and sponsors"], "answer_start": [388, 392, 392]}, "prediction_text": "Private sector."}
{"answers": {"text": ["civil servants", "civil servants", "civil servants"], "answer_start": [32, 32, 32]}, "prediction_text": "Civil servants recruited in special university classes."}
{"answers": {"text": ["Lehramtstudien (Teaching Education Studies)", "Lehramtstudien", "special university classes"], "answer_start": [95, 95, 60]}, "prediction_text": "Special university classes."}
{"answers": {"text": ["Grundschule", "Grundschule", "Grundschule"], "answer_start": [212, 212, 212]}, "prediction_text": "Grundschule"}
{"answers": {"text": ["civil servants' salary index scale (Bundesbesoldungsordnung)", "Bundesbesoldungsordnung", "civil servants' salary index scale"], "answer_start": [393, 429, 393]}, "prediction_text": "Bundesbesoldungsordnung"}
{"answers": {"text": ["Gymnasium", "Gymnasium", "Gymnasium"], "answer_start": [345, 345, 345]}, "prediction_text": "Gymnasium"}
{"answers": {"text": ["Extra pay", "Extra pay", "Extra pay"], "answer_start": [182, 182, 182]}, "prediction_text": "Extra pay."}
{"answers": {"text": ["27,814", "\u20ac27,814", "\u20ac27,814"], "answer_start": [325, 324, 324]}, "prediction_text": "\u20ac27,814 p.a."}
{"answers": {"text": ["53,423", "\u20ac53,423", "\u20ac53,423"], "answer_start": [363, 362, 362]}, "prediction_text": "\u20ac53,423"}
{"answers": {"text": ["90,000", "\u20ac90,000", "\u20ac90,000"], "answer_start": [529, 528, 528]}, "prediction_text": "\u20ac90,000"}
{"answers": {"text": ["the Teaching Council", "Teaching Council", "Teaching Council"], "answer_start": [44, 48, 48]}, "prediction_text": "The Teaching Council."}
{"answers": {"text": ["Section 30", "Section 30", "Section 30"], "answer_start": [72, 72, 72]}, "prediction_text": "Section 30."}
{"answers": {"text": ["2001", "2001", "2001"], "answer_start": [111, 111, 111]}, "prediction_text": "2001"}
{"answers": {"text": ["Oireachtas funds", "Oireachtas", "Oireachtas funds"], "answer_start": [254, 254, 254]}, "prediction_text": "Oireachtas funds."}
{"answers": {"text": ["2006", "2006", "2006"], "answer_start": [5, 5, 5]}, "prediction_text": "2006"}
{"answers": {"text": ["new entrants to the teaching profession", "new entrants", "new entrants"], "answer_start": [48, 48, 48]}, "prediction_text": "New entrants to the teaching profession."}
{"answers": {"text": ["on a phased basis", "phased basis", "on a phased basis"], "answer_start": [308, 313, 308]}, "prediction_text": "On a phased basis."}
{"answers": {"text": ["those who refuse vetting", "those who refuse vetting", "those who refuse vetting"], "answer_start": [159, 159, 159]}, "prediction_text": "Non-teaching posts."}
{"answers": {"text": ["41,004", "\u00a341,004", "41,004"], "answer_start": [84, 83, 84]}, "prediction_text": "\u00a341,004"}
{"answers": {"text": ["experience and extra responsibilities", "experience and extra responsibilities", "experience and extra responsibilities"], "answer_start": [165, 165, 165]}, "prediction_text": "Extra responsibilities."}
{"answers": {"text": ["20,980", "\u00a320,980", "20,980"], "answer_start": [233, 232, 233]}, "prediction_text": "\u00a320,980"}
{"answers": {"text": ["a bachelor's degree", "bachelor's degree", "bachelor's"], "answer_start": [312, 314, 314]}, "prediction_text": "Bachelor's degree."}
{"answers": {"text": ["September 2007", "September 2007", "September 2007"], "answer_start": [94, 94, 94]}, "prediction_text": "September 2007"}
{"answers": {"text": ["alternative licensing programs", "alternative licensing programs", "alternative licensing programs"], "answer_start": [20, 20, 20]}, "prediction_text": "Alternative licensing programs."}
{"answers": {"text": ["hard-to-fill positions", "hard-to-fill", "hard-to-fill"], "answer_start": [99, 99, 99]}, "prediction_text": "Secondary school teachers."}
{"answers": {"text": ["vary", "vary", "vary"], "answer_start": [279, 279, 279]}, "prediction_text": "vary"}
{"answers": {"text": ["Excellent job opportunities", "Excellent", "Excellent"], "answer_start": [123, 123, 123]}, "prediction_text": "Excellent job opportunities."}
{"answers": {"text": ["secondary school teachers", "secondary school teachers", "secondary school teachers"], "answer_start": [197, 197, 197]}, "prediction_text": "Secondary school teachers."}
{"answers": {"text": ["the General Teaching Council for Scotland (GTCS)", "General Teaching Council for Scotland", "General Teaching Council for Scotland"], "answer_start": [61, 65, 65]}, "prediction_text": "GTCS"}
{"answers": {"text": ["Teaching", "Teaching", "Teaching"], "answer_start": [73, 111, 111]}, "prediction_text": "Initial Teacher Education (ITE)"}
{"answers": {"text": ["seven", "seven", "seven"], "answer_start": [290, 290, 290]}, "prediction_text": "Seven."}
{"answers": {"text": ["Provisional Registration", "Provisional Registration", "Provisional Registration"], "answer_start": [373, 373, 373]}, "prediction_text": "Provisional Registration."}
{"answers": {"text": ["after a year", "a year", "a year"], "answer_start": [466, 472, 472]}, "prediction_text": "One year."}
{"answers": {"text": ["April 2008", "April 2008", "April 2008"], "answer_start": [30, 30, 30]}, "prediction_text": "April 2008"}
{"answers": {"text": ["20,427", "\u00a320,427", "20,427"], "answer_start": [87, 86, 87]}, "prediction_text": "\u00a320,427"}
{"answers": {"text": ["32,583", "\u00a332,583", "32,583"], "answer_start": [120, 119, 120]}, "prediction_text": "\u00a332,583"}
{"answers": {"text": ["earn Chartered Teacher Status", "complete the modules to earn Chartered Teacher Status", "complete the modules to earn Chartered Teacher Status"], "answer_start": [226, 202, 202]}, "prediction_text": "Complete modules."}
{"answers": {"text": ["trade unions", "trade unions", "Educational Institute of Scotland"], "answer_start": [518, 518, 560]}, "prediction_text": "Educational Institute of Scotland and Scottish Secondary Teachers' Association."}
{"answers": {"text": ["Wales", "Wales", "Wales"], "answer_start": [13, 13, 13]}, "prediction_text": "Wales"}
{"answers": {"text": ["Welsh", "Welsh", "Welsh"], "answer_start": [216, 216, 216]}, "prediction_text": "Welsh"}
{"answers": {"text": ["until the age of 16", "age of 16", "until the age of 16"], "answer_start": [535, 545, 535]}, "prediction_text": "Until the age of 16."}
{"answers": {"text": ["22", "22 per cent", "22"], "answer_start": [235, 235, 235]}, "prediction_text": "22%"}
{"answers": {"text": ["all age groups", "all age groups", "all age groups"], "answer_start": [381, 381, 381]}, "prediction_text": "All age groups."}
{"answers": {"text": ["trade unions", "ATL, NUT or NASUWT", "trade unions"], "answer_start": [47, 68, 47]}, "prediction_text": "ATL, NUT, NASUWT."}
{"answers": {"text": ["falling", "falling", "falling"], "answer_start": [168, 168, 168]}, "prediction_text": "Falling."}
{"answers": {"text": ["between 2005 and 2010", "2005 and 2010", "between 2005 and 2010"], "answer_start": [332, 340, 332]}, "prediction_text": "2005-2010"}
{"answers": {"text": ["trade unions", "trade unions", "trade unions"], "answer_start": [47, 47, 47]}, "prediction_text": "National Association of School Teachers"}
{"answers": {"text": ["concern", "A growing cause of concern", "concern"], "answer_start": [247, 228, 247]}, "prediction_text": "Attacks on teachers in Welsh schools."}
{"answers": {"text": ["each state", "each state", "each state"], "answer_start": [22, 22, 22]}, "prediction_text": "Each state."}
{"answers": {"text": ["ten years", "ten years", "ten years"], "answer_start": [222, 222, 222]}, "prediction_text": "Ten years."}
{"answers": {"text": ["a bachelor's degree", "bachelor's degree", "bachelor's degree"], "answer_start": [277, 279, 279]}, "prediction_text": "A bachelor's degree."}
{"answers": {"text": ["charter schools", "charter schools", "charter schools"], "answer_start": [371, 371, 371]}, "prediction_text": "Charter schools."}
{"answers": {"text": ["No Child Left Behind", "No Child Left Behind", "No Child Left Behind"], "answer_start": [502, 502, 502]}, "prediction_text": "A bachelor's degree."}
{"answers": {"text": ["relatively low salaries", "relatively low salaries", "relatively low salaries"], "answer_start": [37, 37, 37]}, "prediction_text": "Low salaries."}
{"answers": {"text": ["average teacher salaries", "average teacher salaries", "teacher salaries"], "answer_start": [71, 71, 79]}, "prediction_text": "Average teacher salaries."}
{"answers": {"text": ["more experience and higher education", "more experience and higher education", "more experience and higher education"], "answer_start": [238, 238, 238]}, "prediction_text": "Experience and education."}
{"answers": {"text": ["elementary school teachers", "elementary school teachers", "preschool teachers"], "answer_start": [1348, 1348, 761]}, "prediction_text": "Elementary school teachers."}
{"answers": {"text": ["TeachersPayTeachers.com", "TeachersPayTeachers.com", "TeachersPayTeachers.com"], "answer_start": [2142, 2142, 2142]}, "prediction_text": "TeachersPayTeachers.com"}
{"answers": {"text": ["many", "three", "many"], "answer_start": [10, 84, 10]}, "prediction_text": "3"}
{"answers": {"text": ["Protestant", "Protestant", "Protestant"], "answer_start": [160, 1932, 1932]}, "prediction_text": "Protestant"}
{"answers": {"text": ["not always", "not always"], "answer_start": [1139, 1139]}, "prediction_text": "No."}
{"answers": {"text": ["(Roman) Catholic, (Eastern) Orthodox Catholic, and Protestant/Non-Denominational", "(Roman) Catholic, (Eastern) Orthodox Catholic, and Protestant/Non-Denominational", "(Roman) Catholic, (Eastern) Orthodox Catholic, and Protestant/Non-Denominational"], "answer_start": [109, 109, 109]}, "prediction_text": "Roman Catholic, Eastern Orthodox Catholic, Protestant/Non-Denominational."}
{"answers": {"text": ["LDS Church", "LDS Church", "LDS Church"], "answer_start": [52, 52, 52]}, "prediction_text": "LDS Church"}
{"answers": {"text": ["many individuals", "many individuals", "many individuals"], "answer_start": [285, 285, 285]}, "prediction_text": "A trusted friend."}
{"answers": {"text": ["spiritual", "spiritual", "spiritual"], "answer_start": [206, 429, 429]}, "prediction_text": "Spiritual mentorship."}
{"answers": {"text": ["the husband and father", "husband and father", "husband"], "answer_start": [576, 580, 580]}, "prediction_text": "The father of the house."}
{"answers": {"text": ["the father of the house", "father of the house", "father of the house"], "answer_start": [929, 933, 933]}, "prediction_text": "Father of the house."}
{"answers": {"text": ["guru", "guru", "guru"], "answer_start": [48, 48, 48]}, "prediction_text": "Guru"}
{"answers": {"text": ["extremely high", "high", "extremely high"], "answer_start": [170, 180, 170]}, "prediction_text": "High"}
{"answers": {"text": ["their disciples", "lives of their disciples", "disciples"], "answer_start": [256, 247, 262]}, "prediction_text": "Disciples"}
{"answers": {"text": ["the West", "the West", "the West"], "answer_start": [119, 119, 119]}, "prediction_text": "Western traditions."}
{"answers": {"text": ["a Lama", "Lama", "Lama"], "answer_start": [77, 79, 79]}, "prediction_text": "Lama"}
{"answers": {"text": ["be reborn", "consciously determined to be reborn", "to be reborn"], "answer_start": [151, 125, 148]}, "prediction_text": "Continue their Bodhisattva vow."}
{"answers": {"text": ["Tulku", "Tulku", "Tulku"], "answer_start": [235, 235, 235]}, "prediction_text": "Tulku"}
{"answers": {"text": ["often many times", "many times", "many times"], "answer_start": [162, 168, 168]}, "prediction_text": "Many times."}
{"answers": {"text": ["through phowa and siddhi", "phowa and siddhi", "phowa and siddhi"], "answer_start": [100, 108, 108]}, "prediction_text": "Phowa and siddhi."}
{"answers": {"text": ["ulemas", "ulemas", "ulemas"], "answer_start": [98, 98, 98]}, "prediction_text": "mullahs"}
{"answers": {"text": ["ulemas", "ulemas", "ulemas"], "answer_start": [98, 98, 98]}, "prediction_text": "The teacher."}
{"answers": {"text": ["Sufism", "Sufism", "Sufism"], "answer_start": [401, 401, 401]}, "prediction_text": "Sufism"}
{"answers": {"text": ["actions-oriented", "actions-oriented", "actions-oriented"], "answer_start": [487, 487, 487]}, "prediction_text": "Actions-oriented"}
{"answers": {"text": ["Qutb", "Qutb", "Qutb"], "answer_start": [684, 684, 684]}, "prediction_text": "Qutb"}
{"answers": {"text": ["German", "German", "German"], "answer_start": [39, 119, 119]}, "prediction_text": "German"}
{"answers": {"text": ["18 February 1546", "1546", "1546"], "answer_start": [95, 107, 107]}, "prediction_text": "18 February 1546"}
{"answers": {"text": ["Catholic Church.", "Late Medieval Catholic Church", "Late Medieval Catholic Church"], "answer_start": [306, 292, 292]}, "prediction_text": "Late Medieval Catholic Church"}
{"answers": {"text": ["God's punishment", "God's punishment for sin", "God's punishment"], "answer_start": [372, 372, 372]}, "prediction_text": "Freedom from God's punishment for sin."}
{"answers": {"text": ["excommunication", "excommunication", "excommunication"], "answer_start": [702, 702, 702]}, "prediction_text": "Excommunicated him."}
{"answers": {"text": ["gift of God's grace", "free gift", "through faith"], "answer_start": [118, 113, 138]}, "prediction_text": "Faith in Jesus Christ."}
{"answers": {"text": ["faith in Jesus Christ", "faith", "faith"], "answer_start": [146, 146, 146]}, "prediction_text": "Salvation."}
{"answers": {"text": ["the Pope", "Pope", "Pope"], "answer_start": [242, 246, 246]}, "prediction_text": "The Pope."}
{"answers": {"text": ["Bible", "Bible", "Bible"], "answer_start": [272, 272, 272]}, "prediction_text": "The Bible."}
{"answers": {"text": ["holy priesthood", "holy priesthood", "holy priesthood"], "answer_start": [410, 410, 410]}, "prediction_text": "A holy priesthood."}
{"answers": {"text": ["Bible", "Bible"], "answer_start": [23, 23]}, "prediction_text": "The Tyndale Bible."}
{"answers": {"text": ["standard version", "standard version", "writing of an English translation"], "answer_start": [190, 190, 302]}, "prediction_text": "The development of a standard version of the German language."}
{"answers": {"text": ["Tyndale Bible", "Tyndale Bible.", "Tyndale"], "answer_start": [341, 341, 341]}, "prediction_text": "Tyndale Bible"}
{"answers": {"text": ["singing in churches", "singing", "singing"], "answer_start": [396, 396, 396]}, "prediction_text": "singing in churches."}
{"answers": {"text": ["Protestant clergy to marry.", "Protestant clergy to marry", "clerical marriage"], "answer_start": [512, 512, 484]}, "prediction_text": "Protestant clergy to marry."}
{"answers": {"text": ["10 November 1483", "10 November 1483", "10 November 1483"], "answer_start": [106, 106, 106]}, "prediction_text": "1483"}
{"answers": {"text": ["Eisleben, Saxony", "Eisleben, Saxony", "Eisleben, Saxony"], "answer_start": [126, 126, 126]}, "prediction_text": "Eisleben, Saxony, then part of the Holy Roman Empire."}
{"answers": {"text": ["Holy Roman Empire", "Holy Roman Empire", "Holy Roman Empire."], "answer_start": [161, 161, 161]}, "prediction_text": "Holy Roman Empire"}
{"answers": {"text": ["Catholic", "Catholic", "Catholic"], "answer_start": [201, 201, 201]}, "prediction_text": "Catholic"}
{"answers": {"text": ["lawyer", "lawyer", "lawyer"], "answer_start": [870, 870, 870]}, "prediction_text": "Lawyer"}
{"answers": {"text": ["University of Erfurt", "University of Erfurt", "University of Erfurt"], "answer_start": [42, 42, 42]}, "prediction_text": "University of Erfurt"}
{"answers": {"text": ["beerhouse and whorehouse", "beerhouse and whorehouse", "beerhouse and whorehouse"], "answer_start": [94, 94, 94]}, "prediction_text": "Beerhouse and whorehouse."}
{"answers": {"text": ["at four", "four", "four"], "answer_start": [140, 143, 143]}, "prediction_text": "Four every morning."}
{"answers": {"text": ["rote learning", "rote", "\"a day of rote learning and often wearying spiritual exercises.\""], "answer_start": [203, 203, 193]}, "prediction_text": "rote learning and often wearying spiritual exercises."}
{"answers": {"text": ["1505", "1505", "1505"], "answer_start": [293, 293, 293]}, "prediction_text": "1505"}
{"answers": {"text": ["law", "law", "law"], "answer_start": [59, 59, 59]}, "prediction_text": "Law school."}
{"answers": {"text": ["uncertainty", "uncertainty", "uncertainty"], "answer_start": [170, 170, 170]}, "prediction_text": "Uncertainty."}
{"answers": {"text": ["theology and philosophy", "theology and philosophy", "theology and philosophy"], "answer_start": [236, 236, 236]}, "prediction_text": "Aristotle, William of Ockham, and Gabriel Biel."}
{"answers": {"text": ["by experience", "by experience", "experience"], "answer_start": [534, 534, 537]}, "prediction_text": "By experience."}
{"answers": {"text": ["God", "God", "God"], "answer_start": [917, 917, 917]}, "prediction_text": "God."}
{"answers": {"text": ["death and divine judgment,", "death and divine judgment", "death"], "answer_start": [227, 227, 227]}, "prediction_text": "Death and divine judgment."}
{"answers": {"text": ["2 July 1505", "2 July 1505", "1505"], "answer_start": [49, 49, 56]}, "prediction_text": "2 July 1505."}
{"answers": {"text": ["Augustinian cloister in Erfurt", "closed Augustinian cloister", "Augustinian cloister in Erfurt"], "answer_start": [431, 424, 431]}, "prediction_text": "Erfurt"}
{"answers": {"text": ["deaths of two friends", "deaths of two friends", "deaths of two friends"], "answer_start": [539, 539, 539]}, "prediction_text": "The friend blamed the decision on Luther's sadness over the deaths of two friends."}
{"answers": {"text": ["Luther's education", "Luther's education", "education"], "answer_start": [801, 801, 810]}, "prediction_text": "Luther's education."}
{"answers": {"text": ["Augustinian order", "fasting, long hours in prayer, pilgrimage, and frequent confession", "Augustinian order"], "answer_start": [32, 71, 32]}, "prediction_text": "Fasting, long hours in prayer, pilgrimage, and frequent confession."}
{"answers": {"text": ["deep spiritual despair", "deep spiritual despair", "deep spiritual despair"], "answer_start": [190, 190, 190]}, "prediction_text": "He described his time in the order as one of deep spiritual despair."}
{"answers": {"text": ["jailer and hangman", "jailer", "jailer and hangman of my poor soul."], "answer_start": [295, 295, 295]}, "prediction_text": "Jailer and hangman."}
{"answers": {"text": ["Johann von Staupitz", "Johann von Staupitz", "Johann von Staupitz,"], "answer_start": [332, 332, 332]}, "prediction_text": "Johann von Staupitz"}
{"answers": {"text": ["a change of heart", "change of heart", "true repentance does not involve self-inflicted penances and punishments but rather a change of heart."], "answer_start": [562, 564, 478]}, "prediction_text": "Johann von Staupitz taught that true repentance does not involve self-inflicted penances and punishments."}
{"answers": {"text": ["1507", "1507", "1507"], "answer_start": [3, 3, 3]}, "prediction_text": "1507"}
{"answers": {"text": ["von Staupitz", "von Staupitz", "von Staupitz"], "answer_start": [57, 57, 57]}, "prediction_text": "von Staupitz"}
{"answers": {"text": ["1508", "1508", "1508"], "answer_start": [51, 51, 51]}, "prediction_text": "1508"}
{"answers": {"text": ["9 March 1508", "9 March 1508", "9 March 1508,"], "answer_start": [220, 220, 220]}, "prediction_text": "1508"}
{"answers": {"text": ["Sentences by Peter Lombard", "Sentences", "Sentences by Peter Lombard"], "answer_start": [271, 271, 271]}, "prediction_text": "Sentences by Peter Lombard."}
{"answers": {"text": ["19 October 1512", "19 October 1512", "1512"], "answer_start": [3, 3, 14]}, "prediction_text": "1512"}
{"answers": {"text": ["21 October 1512", "21 October 1512", "October 1512,"], "answer_start": [66, 66, 69]}, "prediction_text": "21 October 1512"}
{"answers": {"text": ["Doctor in Bible", "Doctor in Bible", "Doctor in Bible."], "answer_start": [210, 210, 210]}, "prediction_text": "Doctor of Theology"}
{"answers": {"text": ["University of Wittenberg", "University of Wittenberg", "University of Wittenberg."], "answer_start": [283, 283, 283]}, "prediction_text": "University of Wittenberg"}
{"answers": {"text": ["Doctor of Theology", "Doctor of Theology", "Doctor of Theology"], "answer_start": [39, 39, 39]}, "prediction_text": "Doctor of Theology"}
{"answers": {"text": ["1516", "1516", "1516"], "answer_start": [3, 3, 3]}, "prediction_text": "1516"}
{"answers": {"text": ["rebuild St. Peter's Basilica", "rebuild St. Peter's Basilica", "rebuild St. Peter's Basilica"], "answer_start": [169, 169, 169]}, "prediction_text": "To rebuild St. Peter's Basilica."}
{"answers": {"text": ["Roman Catholic", "Roman Catholic", "Roman Catholic"], "answer_start": [207, 207, 207]}, "prediction_text": "Roman Catholic theology."}
{"answers": {"text": ["charity and good works", "charity and good works", "in charity and good works"], "answer_start": [371, 371, 368]}, "prediction_text": "Charity and good works."}
{"answers": {"text": ["charity and good works", "benefits of good works could be obtained by donating money to the church", "justification rather depends only on such faith as is active in charity and good works"], "answer_start": [371, 424, 307]}, "prediction_text": "Good works are obtained by donating money to the church."}
{"answers": {"text": ["31 October 1517", "31 October 1517", "1517"], "answer_start": [3, 3, 14]}, "prediction_text": "31 October 1517"}
{"answers": {"text": ["Albert of Mainz", "Albert of Mainz", ", Albert of Mainz"], "answer_start": [48, 48, 46]}, "prediction_text": "Albert of Mainz"}
{"answers": {"text": ["The Ninety-Five Theses", "The Ninety-Five Theses", "The Ninety-Five Theses."], "answer_start": [240, 240, 240]}, "prediction_text": "The Ninety-Five Theses."}
{"answers": {"text": ["Hans Hillerbrand", "Hans Hillerbrand", "Hillerbrand"], "answer_start": [264, 264, 269]}, "prediction_text": "Hans Hillerbrand"}
{"answers": {"text": ["Thesis 86", "Thesis 86,", "86"], "answer_start": [612, 612, 619]}, "prediction_text": "The Ninety-Five Theses."}
{"answers": {"text": ["Johann Tetzel", "Johann Tetzel", "Tetzel"], "answer_start": [42, 42, 49]}, "prediction_text": "Johann Tetzel"}
{"answers": {"text": ["coin in the coffer", "coin", "coin in the coffer rings, the soul from purgatory"], "answer_start": [77, 77, 77]}, "prediction_text": "\"As soon as the coin in the coffer rings.\""}
{"answers": {"text": ["Luther", "Luther", "Luther"], "answer_start": [0, 0, 0]}, "prediction_text": "Luther"}
{"answers": {"text": ["Johann Tetzel", "Johann Tetzel", "Tetzel"], "answer_start": [42, 42, 49]}, "prediction_text": "Johann Tetzel"}
{"answers": {"text": ["God", "God", "God"], "answer_start": [40, 40, 40]}, "prediction_text": "God"}
{"answers": {"text": ["salvation", "salvation", "salvation"], "answer_start": [151, 151, 151]}, "prediction_text": "Salvation."}
{"answers": {"text": ["punishments", "punishments", "all punishments"], "answer_start": [122, 122, 118]}, "prediction_text": "Punishments."}
{"answers": {"text": ["false assurances", "false assurances", "false assurances."], "answer_start": [253, 253, 253]}, "prediction_text": "indulgences"}
{"answers": {"text": ["Christ", "Christ", "Christ"], "answer_start": [227, 227, 227]}, "prediction_text": "Christ"}
{"answers": {"text": ["Tetzel", "Tetzel", "Tetzel"], "answer_start": [35, 35, 35]}, "prediction_text": "Tetzel"}
{"answers": {"text": ["capacity to exaggerate", "capacity to exaggerate", "capacity to exaggerate"], "answer_start": [154, 154, 154]}, "prediction_text": "Overstated."}
{"answers": {"text": ["indulgences for the dead,", "in regard to indulgences for the dead", "teaching"], "answer_start": [227, 214, 257]}, "prediction_text": "In regard to indulgences for the dead."}
{"answers": {"text": ["indulgences for the living", "on indulgences for the living", "in line"], "answer_start": [269, 266, 300]}, "prediction_text": "The living."}
{"answers": {"text": ["the posting on the door", "the posting on the door", "posting on the door"], "answer_start": [101, 101, 105]}, "prediction_text": "The story of the posting on the door."}
{"answers": {"text": ["posting on the door", "story of the posting on the door", "posting on the door"], "answer_start": [105, 92, 105]}, "prediction_text": "The story of the posting on the door."}
{"answers": {"text": ["Philipp Melanchthon", "Philipp Melanchthon", "Philipp Melanchthon"], "answer_start": [258, 258, 258]}, "prediction_text": "Philipp Melanchthon"}
{"answers": {"text": ["not in Wittenberg", "not in Wittenberg", "not in Wittenberg"], "answer_start": [312, 312, 312]}, "prediction_text": "Wittenberg"}
{"answers": {"text": ["little foundation in truth", "has little foundation in truth", "settled as one of the pillars of history"], "answer_start": [191, 187, 145]}, "prediction_text": "Walter Kr\u00e4mer, Gerhard Trenkler, Gerhard Ritter, Gerhard Prause."}
{"answers": {"text": ["January 1518", "January 1518", "1518"], "answer_start": [17, 17, 25]}, "prediction_text": "January 1518"}
{"answers": {"text": ["printing press", "printing press", "printing press."], "answer_start": [207, 207, 207]}, "prediction_text": "printing press"}
{"answers": {"text": ["friends of Luther", "friends of Luther", "friends of Luther"], "answer_start": [35, 35, 35]}, "prediction_text": "Friends of Luther"}
{"answers": {"text": ["two weeks", "two weeks", "two weeks"], "answer_start": [230, 230, 230]}, "prediction_text": "Two weeks."}
{"answers": {"text": ["two months", "two months", "two months"], "answer_start": [300, 300, 300]}, "prediction_text": "Two months."}
{"answers": {"text": ["1519", "1519", "1519"], "answer_start": [85, 85, 85]}, "prediction_text": "1519"}
{"answers": {"text": ["Students", "Students", "Students"], "answer_start": [91, 91, 91]}, "prediction_text": "Students"}
{"answers": {"text": ["early part", "early", "early"], "answer_start": [223, 223, 223]}, "prediction_text": "Early part of his career."}
{"answers": {"text": ["1520", "1520", "To the Christian Nobility of the German Nation, On the Babylonian Captivity of the Church, and On the Freedom of a Christian."], "answer_start": [346, 346, 352]}, "prediction_text": "1520"}
{"answers": {"text": ["On the Freedom of a Christian", "On the Freedom of a Christian", "On the Freedom of a Christian."], "answer_start": [447, 447, 447]}, "prediction_text": "On the Freedom of a Christian."}
{"answers": {"text": ["lectured", "lectured on the Psalms", "lectured"], "answer_start": [26, 26, 26]}, "prediction_text": "Lectured on the Psalms, the books of Hebrews, Romans, and Galatians."}
{"answers": {"text": ["penance and righteousness", "penance and righteousness", "penance and righteousness"], "answer_start": [179, 179, 179]}, "prediction_text": "Penance and righteousness."}
{"answers": {"text": ["corrupt in its ways", "corrupt", "corrupt"], "answer_start": [281, 281, 281]}, "prediction_text": "He decided that the Catholic Church was corrupt in its ways and had lost sight of what he saw as several of the central truths of Christianity."}
{"answers": {"text": ["central truths of Christianity", "central truths of Christianity", "several of the central truths of Christianity"], "answer_start": [353, 353, 338]}, "prediction_text": "The Church's lost sight of the central truths of Christianity."}
{"answers": {"text": ["doctrine of justification", "doctrine of justification", "salvation or redemption is a gift of God's grace, attainable only through faith in Jesus as the Messiah"], "answer_start": [423, 702, 554]}, "prediction_text": "The doctrine of justification."}
{"answers": {"text": ["God", "God", "God"], "answer_start": [64, 64, 64]}, "prediction_text": "God."}
{"answers": {"text": ["1525", "1525", "1525"], "answer_start": [122, 122, 122]}, "prediction_text": "1525"}
{"answers": {"text": ["gift from God", "that which brings the Holy Spirit through the merits of Christ", "Faith is that which brings the Holy Spirit through the merits of Christ"], "answer_start": [869, 779, 770]}, "prediction_text": "The righteousness of God."}
{"answers": {"text": ["Smalcald Articles", "the Smalcald Articles", "Smalcald Articles"], "answer_start": [1198, 1194, 1198]}, "prediction_text": "In the Smalcald Articles."}
{"answers": {"text": ["lives by faith", "faith", "faith"], "answer_start": [1131, 1140, 1140]}, "prediction_text": "Faith."}
{"answers": {"text": ["Christ and His salvation", "rediscovery of \"Christ and His salvation\"", "rediscovery of \"Christ and His salvation\"", "Christ"], "answer_start": [25, 9, 9, 25]}, "prediction_text": "Luther's rediscovery of \"Christ and His salvation\""}
{"answers": {"text": ["Christ and His salvation", "Christ and His salvation", "Christ and His salvation", "Christ and His salvation"], "answer_start": [25, 25, 25, 25]}, "prediction_text": "Luther's rediscovery of \"Christ and His salvation\""}
{"answers": {"text": ["sale of indulgences", "sale of indulgences", "sale of indulgences", "sale of indulgences"], "answer_start": [151, 151, 151, 151]}, "prediction_text": "The sale of indulgences."}
{"answers": {"text": ["two points", "two", "two", "two"], "answer_start": [68, 68, 68, 68]}, "prediction_text": "2"}
{"answers": {"text": ["Archbishop Albrecht", "Archbishop Albrecht", "Archbishop Albrecht of Mainz and Magdeburg"], "answer_start": [0, 0, 0]}, "prediction_text": "Archbishop Albrecht of Mainz and Magdeburg"}
{"answers": {"text": ["Rome", "Rome", "Rome"], "answer_start": [178, 178, 178]}, "prediction_text": "Rome"}
{"answers": {"text": ["papal dispensation", "papal dispensation", "pay off a papal dispensation for his tenure"], "answer_start": [240, 240, 230]}, "prediction_text": "The revenue from the indulgences."}
{"answers": {"text": ["one half", "one half", "half"], "answer_start": [376, 376, 380]}, "prediction_text": "Half."}
{"answers": {"text": ["December 1517", "December 1517", "1517"], "answer_start": [146, 146, 155]}, "prediction_text": "December 1517"}
{"answers": {"text": ["Pope Leo X", "Pope Leo X", "Leo X"], "answer_start": [0, 0, 5]}, "prediction_text": "Pope Leo X"}
{"answers": {"text": ["papal theologians and envoys", "papal theologians and envoys", "papal theologians and envoys"], "answer_start": [154, 154, 154]}, "prediction_text": "The 95 Theses."}
{"answers": {"text": ["October 1518", "October 1518", "1518"], "answer_start": [507, 507, 515]}, "prediction_text": "October 1518."}
{"answers": {"text": ["papacy was the Antichrist", "papacy was the Antichrist", "papacy was the Antichrist"], "answer_start": [724, 724, 724]}, "prediction_text": "He told the legate that the papacy was the Antichrist."}
{"answers": {"text": ["arrest Luther", "to arrest Luther", "arrest"], "answer_start": [1033, 1030, 1033]}, "prediction_text": "To arrest Luther if he failed to recant."}
{"answers": {"text": ["January 1519", "January 1519", "1519"], "answer_start": [3, 3, 11]}, "prediction_text": "January 1519"}
{"answers": {"text": ["remain silent", "remain silent", "remain silent if his opponents did"], "answer_start": [210, 210, 210]}, "prediction_text": "Remain silent."}
{"answers": {"text": ["Johann Eck", "Johann Eck", "Eck"], "answer_start": [261, 261, 268]}, "prediction_text": "Johann Eck"}
{"answers": {"text": ["Matthew 16:18", "Matthew 16:18", "Matthew 16:18"], "answer_start": [523, 523, 523]}, "prediction_text": "Matthew 16:18."}
{"answers": {"text": ["new Jan Hus", "Jan Hus", "new Jan Hus"], "answer_start": [707, 711, 707]}, "prediction_text": "Jan Hus"}
{"answers": {"text": ["15 June 1520", "15 June 1520", "1520"], "answer_start": [3, 3, 11]}, "prediction_text": "15 June 1520"}
{"answers": {"text": ["recanted 41 sentences", "recanted 41 sentences", "recanted 41 sentences"], "answer_start": [124, 124, 124]}, "prediction_text": "Recant 41 sentences drawn from his writings."}
{"answers": {"text": ["60 days", "60 days", "60 days"], "answer_start": [203, 203, 203]}, "prediction_text": "60 days."}
{"answers": {"text": ["Karl von Miltitz", "Karl von Miltitz", ". Karl von Miltitz"], "answer_start": [284, 284, 282]}, "prediction_text": "Johann Eck"}
{"answers": {"text": ["3 January 1521", "3 January 1521", "3 January 1521"], "answer_start": [682, 682, 682]}, "prediction_text": "3 January 1521."}
{"answers": {"text": ["secular authorities", "secular", "secular authorities."], "answer_start": [56, 56, 56]}, "prediction_text": "The secular authorities."}
{"answers": {"text": ["18 April 1521", "18 April 1521", "1521"], "answer_start": [80, 80, 89]}, "prediction_text": "18 April 1521"}
{"answers": {"text": ["estates of the Holy Roman Empire", "estates of the Holy Roman Empire", "general assembly of the estates of the Holy Roman Empire"], "answer_start": [183, 183, 159]}, "prediction_text": "The 95 Theses."}
{"answers": {"text": ["Emperor Charles V", "Emperor Charles V", "Emperor Charles V"], "answer_start": [317, 317, 317]}, "prediction_text": "Emperor Charles V"}
{"answers": {"text": ["Prince Frederick III", "Prince Frederick III", "Elector of Saxony"], "answer_start": [346, 346, 368]}, "prediction_text": "Prince Frederick III"}
{"answers": {"text": ["Johann Eck", "Johann Eck", "Eck"], "answer_start": [0, 0, 7]}, "prediction_text": "Johann Eck"}
{"answers": {"text": ["Archbishop of Trier", "Archbishop of Trier", "Archbishop of Trier"], "answer_start": [65, 65, 65]}, "prediction_text": "Archbishop of Trier"}
{"answers": {"text": ["stood by their contents", "whether he stood by their contents", "whether he stood by their contents."], "answer_start": [203, 192, 192]}, "prediction_text": "Time to think."}
{"answers": {"text": ["next day", "next day", "the next day:"], "answer_start": [391, 391, 387]}, "prediction_text": "Next day."}
{"answers": {"text": ["confirmed", "confirmed", "confirmed"], "answer_start": [235, 235, 235]}, "prediction_text": "He prayed, consulted friends, and gave his response the next day."}
{"answers": {"text": ["raised his arm", "raised his arm", "raised his arm"], "answer_start": [34, 34, 34]}, "prediction_text": "Raised his arm in the traditional salute of a knight winning a bout."}
{"answers": {"text": ["knight winning a bout", "a knight winning a bout", "traditional salute of a knight winning a bout"], "answer_start": [81, 79, 57]}, "prediction_text": "Traditional salute of a knight winning a bout."}
{"answers": {"text": ["Michael Mullett", "Michael Mullett", "Michael Mullett"], "answer_start": [105, 105, 105]}, "prediction_text": "Michael Mullett"}
{"answers": {"text": ["epoch-making oratory", "world classic of epoch-making oratory", "world classic of epoch-making oratory"], "answer_start": [166, 149, 149]}, "prediction_text": "World classic of epoch-making oratory."}
{"answers": {"text": ["recant his writings", "recant his writings", "recant"], "answer_start": [18, 18, 18]}, "prediction_text": "Recant his writings."}
{"answers": {"text": ["Luther", "Luther", "Luther"], "answer_start": [0, 0, 0]}, "prediction_text": "Luther"}
{"answers": {"text": ["not recorded", "not recorded in witness accounts", "not recorded in witness accounts"], "answer_start": [276, 276, 276]}, "prediction_text": "Inserted before \"May God help me\" in later versions of the speech."}
{"answers": {"text": ["more dramatic form", "more dramatic form", "more dramatic form"], "answer_start": [439, 439, 439]}, "prediction_text": "More dramatic form of words."}
{"answers": {"text": ["private conferences", "private conferences", "conferences"], "answer_start": [25, 25, 33]}, "prediction_text": "Over the next five days, private conferences were held to determine Luther's fate."}
{"answers": {"text": ["25 May 1521", "25 May 1521", "25 May 1521"], "answer_start": [146, 146, 146]}, "prediction_text": "25 May 1521"}
{"answers": {"text": ["Emperor", "Emperor", "Emperor"], "answer_start": [87, 87, 87]}, "prediction_text": "Emperor"}
{"answers": {"text": ["his arrest", "requiring his arrest", "arrest"], "answer_start": [225, 215, 229]}, "prediction_text": "Luther was required to be arrested."}
{"answers": {"text": ["kill Luther", "kill", "kill"], "answer_start": [407, 407, 407]}, "prediction_text": "Kill him."}
{"answers": {"text": ["Luther's disappearance", "disappearance", "disappearance"], "answer_start": [0, 9, 9]}, "prediction_text": "Luther's disappearance during his return trip back to Wittenberg."}
{"answers": {"text": ["Wartburg Castle", "Wartburg Castle", "Wartburg Castle"], "answer_start": [266, 266, 266]}, "prediction_text": "Eisenach"}
{"answers": {"text": ["my Patmos", "my Patmos", "my Patmos"], "answer_start": [349, 349, 349]}, "prediction_text": "Patmos"}
{"answers": {"text": ["New Testament", "New Testament", "New Testament"], "answer_start": [383, 383, 383]}, "prediction_text": "New Testament"}
{"answers": {"text": ["shamed", "shamed", "shamed"], "answer_start": [542, 542, 542]}, "prediction_text": "Luther translated the New Testament from Greek into German."}
{"answers": {"text": ["a sin", "sin", "sin", "sin"], "answer_start": [126, 128, 128, 128]}, "prediction_text": "Sin."}
{"answers": {"text": ["cannot be earned", "cannot be earned", "cannot be earned", "All humans are sinners by nature, he explained, and God's grace (which cannot be earned) alone can make them just."], "answer_start": [204, 204, 204, 133]}, "prediction_text": "God's grace cannot be earned."}
{"answers": {"text": ["1 August 1521", "1 August 1521", "1 August 1521", "1521"], "answer_start": [251, 251, 251, 260]}, "prediction_text": "1 August 1521"}
{"answers": {"text": ["trust in Christ", "trust in Christ", "trust in Christ", "trust in Christ be stronger,"], "answer_start": [369, 369, 369, 369]}, "prediction_text": "Trust in Christ."}
{"answers": {"text": ["justice", "justice", "justice", "justice"], "answer_start": [544, 544, 544, 544]}, "prediction_text": "Justice."}
{"answers": {"text": ["summer of 1521", "summer of 1521", "1521"], "answer_start": [7, 7, 17]}, "prediction_text": "1521"}
{"answers": {"text": ["condemned as idolatry", "idolatry", "gift"], "answer_start": [198, 211, 290]}, "prediction_text": "He described the mass as a gift."}
{"answers": {"text": ["a gift", "gift", "gift"], "answer_start": [288, 290, 290]}, "prediction_text": "Gift."}
{"answers": {"text": ["private confession and absolution", "private confession and absolution", "private confession"], "answer_start": [472, 472, 472]}, "prediction_text": "Private confession and absolution."}
{"answers": {"text": ["break their vows", "illegitimate", "break their vows without sin,"], "answer_start": [664, 715, 664]}, "prediction_text": "He assured them that they could break their vows without sin."}
{"answers": {"text": ["prophetic faith", "placing them on prophetic faith", "placing them on prophetic faith."], "answer_start": [117, 101, 101]}, "prediction_text": "He broadened the Reformation in terms of prophecy by placing it on prophetic faith."}
{"answers": {"text": ["1521", "1521", "1521"], "answer_start": [3, 3, 3]}, "prediction_text": "1521"}
{"answers": {"text": ["Daniel 8:9\u201312, 23\u201325", "prophecy", "prophecy of the Little Horn"], "answer_start": [203, 34, 172]}, "prediction_text": "Prophecy"}
{"answers": {"text": ["the Little Horn", "Little Horn", "Little Horn"], "answer_start": [184, 188, 188]}, "prediction_text": "Little Horn of Daniel 8:9\u201312, 23\u201325."}
{"answers": {"text": ["antichrist", "antichrist of 2 Thessalonians 2", "The antichrist of 2 Thessalonians 2"], "answer_start": [229, 229, 225]}, "prediction_text": "The Little Horn of Daniel 7."}
{"answers": {"text": ["Gabriel Zwilling", "Gabriel Zwilling"], "answer_start": [185, 185]}, "prediction_text": "Gabriel Zwilling"}
{"answers": {"text": ["June 1521", "June 1521", "1521"], "answer_start": [254, 254, 259]}, "prediction_text": "June 1521."}
{"answers": {"text": ["disturbances", "disturbances", "disturbances"], "answer_start": [326, 326, 326]}, "prediction_text": "disturbances"}
{"answers": {"text": ["Zwickau prophets", "Zwickau prophets", "Augustinian friars"], "answer_start": [765, 765, 366]}, "prediction_text": "Augustinian friars"}
{"answers": {"text": ["town council", "the town council", "town council"], "answer_start": [909, 905, 909]}, "prediction_text": "The town council."}
{"answers": {"text": ["6 March 1522", "6 March 1522", "6 March 1522"], "answer_start": [42, 42, 42]}, "prediction_text": "6 March 1522"}
{"answers": {"text": ["personal presence", "personal presence and living word", "trust God's word"], "answer_start": [204, 204, 529]}, "prediction_text": "The violence."}
{"answers": {"text": ["preached eight sermons", "preached eight sermons", "preached"], "answer_start": [311, 311, 311]}, "prediction_text": "Preached eight sermons."}
{"answers": {"text": ["Invocavit Sermons", "Invocavit Sermons", "Invocavit Sermons"], "answer_start": [362, 362, 362]}, "prediction_text": "Invocavit Sermons"}
{"answers": {"text": ["trust God's word", "trust God's word", "love, patience, charity, and freedom"], "answer_start": [529, 529, 462]}, "prediction_text": "Trust God's word."}
{"answers": {"text": ["immediate", "immediate", "immediate"], "answer_start": [40, 40, 40]}, "prediction_text": "Immediate."}
{"answers": {"text": ["Jerome Schurf", "Jerome Schurf", "Schurf"], "answer_start": [97, 97, 104]}, "prediction_text": "Jerome Schurf"}
{"answers": {"text": ["After the sixth sermon", "sixth", "After the sixth sermon"], "answer_start": [51, 61, 51]}, "prediction_text": "Sixth sermon."}
{"answers": {"text": ["joy", "joy", "joy"], "answer_start": [143, 143, 143]}, "prediction_text": "Misguided people."}
{"answers": {"text": ["misguided", "misguided", "misguided"], "answer_start": [249, 249, 249]}, "prediction_text": "Misguided people."}
{"answers": {"text": ["public order", "public order", "public order,"], "answer_start": [119, 119, 119]}, "prediction_text": "Public order."}
{"answers": {"text": ["conservative", "conservative", "reinvention as a conservative"], "answer_start": [167, 167, 150]}, "prediction_text": "Conservative force."}
{"answers": {"text": ["Zwickau prophets", "Zwickau prophets", "Zwickau prophets"], "answer_start": [230, 230, 230]}, "prediction_text": "Zwickau prophets"}
{"answers": {"text": ["unrest and violence.", "social unrest and violence", "social unrest"], "answer_start": [390, 383, 383]}, "prediction_text": "Social unrest and violence."}
{"answers": {"text": ["established Church", "the established Church", "established Church"], "answer_start": [291, 287, 291]}, "prediction_text": "The authorities."}
{"answers": {"text": ["Zwickau prophet", "Preachers", "Zwickau prophet"], "answer_start": [108, 90, 108]}, "prediction_text": "Prophet Nicholas Storch"}
{"answers": {"text": ["German Peasants' War", "German Peasants' War", "German Peasants' War"], "answer_start": [180, 180, 180]}, "prediction_text": "German Peasants' War of 1524-25."}
{"answers": {"text": ["1524\u201325", "1524\u201325", "1524\u201325,"], "answer_start": [204, 204, 204]}, "prediction_text": "1524-25"}
{"answers": {"text": ["support an attack", "support an attack", "support an attack on the upper classes"], "answer_start": [505, 505, 505]}, "prediction_text": "Support an attack on the upper classes in general."}
{"answers": {"text": ["upper classes", "upper classes", "upper classes"], "answer_start": [530, 530, 530]}, "prediction_text": "The upper classes."}
{"answers": {"text": ["temporal authorities", "temporal authorities", "temporal authorities"], "answer_start": [165, 165, 165]}, "prediction_text": "The temporal authorities."}
{"answers": {"text": ["tour of Thuringia", "Thuringia", "Thuringia"], "answer_start": [196, 204, 204]}, "prediction_text": "Thuringia"}
{"answers": {"text": ["mad dogs", "Murderous, Thieving Hordes", "mad dogs"], "answer_start": [569, 333, 569]}, "prediction_text": "The Twelve Articles."}
{"answers": {"text": ["the devil's work", "devil's work", "devil's work,"], "answer_start": [497, 501, 501]}, "prediction_text": "Burning convents, monasteries, bishops' palaces, libraries."}
{"answers": {"text": ["the nobles", "nobles", "nobles"], "answer_start": [530, 534, 534]}, "prediction_text": "The nobles."}
{"answers": {"text": ["on three grounds", "three", "three"], "answer_start": [46, 49, 49]}, "prediction_text": "3"}
{"answers": {"text": ["ignoring Christ's counsel", "ignoring", "ignoring"], "answer_start": [152, 152, 152]}, "prediction_text": "ignoring."}
{"answers": {"text": ["God", "God", "God"], "answer_start": [327, 327, 327]}, "prediction_text": "God."}
{"answers": {"text": ["Divine Right of Kings", "Divine Right of Kings", "Divine Right of Kings"], "answer_start": [450, 450, 450]}, "prediction_text": "Divine Right of Kings."}
{"answers": {"text": ["in body and soul", "death in body and soul", "death in body and soul,"], "answer_start": [677, 671, 671]}, "prediction_text": "Death in body and soul."}
{"answers": {"text": ["backing for the uprising", "backing", "backing"], "answer_start": [17, 17, 17]}, "prediction_text": "Luther denied the rebels the backing for the uprising."}
{"answers": {"text": ["Swabian League", "Swabian League", "Swabian League"], "answer_start": [122, 122, 122]}, "prediction_text": "Swabian League"}
{"answers": {"text": ["15 May 1525", "15 May 1525", "1525"], "answer_start": [171, 171, 178]}, "prediction_text": "15 May 1525"}
{"answers": {"text": ["M\u00fcntzer's execution", "M\u00fcntzer's execution", "M\u00fcntzer's execution,"], "answer_start": [196, 196, 196]}, "prediction_text": "Execution of M\u00fcntzer."}
{"answers": {"text": ["the secular powers", "secular powers", "wing of the secular powers"], "answer_start": [432, 436, 424]}, "prediction_text": "Secular powers."}
{"answers": {"text": ["Katharina von Bora", "Katharina von Bora", "Katharina von Bora,"], "answer_start": [22, 22, 22]}, "prediction_text": "Katharina von Bora"}
{"answers": {"text": ["in herring barrels", "herring barrels", "herring barrels."], "answer_start": [176, 179, 179]}, "prediction_text": "In herring barrels."}
{"answers": {"text": ["26 years old", "26", "26"], "answer_start": [377, 377, 377]}, "prediction_text": "26 years old."}
{"answers": {"text": ["41 years old", "41", "41"], "answer_start": [405, 405, 405]}, "prediction_text": "41 years old."}
{"answers": {"text": ["April 1523", "April 1523", "1523"], "answer_start": [119, 119, 125]}, "prediction_text": "April 1523"}
{"answers": {"text": ["13 June 1525", "13 June 1525", "13 June 1525,"], "answer_start": [3, 3, 3]}, "prediction_text": "13 June 1525"}
{"answers": {"text": ["evening", "evening", "evening"], "answer_start": [177, 177, 177]}, "prediction_text": "27 June."}
{"answers": {"text": ["wedding banquet", "wedding banquet", "banquet"], "answer_start": [282, 282, 290]}, "prediction_text": "The ceremonial walk to the church and the wedding banquet."}
{"answers": {"text": ["27 June", "27 June", "two weeks later"], "answer_start": [349, 349, 330]}, "prediction_text": "27 June."}
{"answers": {"text": ["Johannes Bugenhagen", "Johannes Bugenhagen", "Bugenhagen"], "answer_start": [45, 45, 228]}, "prediction_text": "Johannes Bugenhagen"}
{"answers": {"text": ["seal of approval", "seal of approval on clerical marriage", "seal of approval on clerical marriage", "seal of approval on clerical marriage"], "answer_start": [130, 130, 130, 130]}, "prediction_text": "Reckless."}
{"answers": {"text": ["clerical marriage", "clerical", "clerical marriage.", "clerical marriage."], "answer_start": [150, 150, 150, 150]}, "prediction_text": "Luther's marriage."}
{"answers": {"text": ["on Biblical grounds", "Biblical grounds", "Biblical grounds,", "Biblical grounds,"], "answer_start": [208, 211, 211, 211]}, "prediction_text": "Because he felt that vows of celibacy were reckless."}
{"answers": {"text": ["death of a heretic", "the death of a heretic", "expect the death of a heretic", "expect the death of a heretic"], "answer_start": [566, 562, 555, 555]}, "prediction_text": "Death of a heretic."}
{"answers": {"text": ["reckless", "reckless", "reckless", "reckless"], "answer_start": [308, 308, 308, 308]}, "prediction_text": "reckless"}
{"answers": {"text": ["The Black Cloister", "The Black Cloister", ", \"The Black Cloister,\""], "answer_start": [52, 52, 49]}, "prediction_text": "1525-32"}
{"answers": {"text": ["former monastery", "a former monastery", "former monastery,"], "answer_start": [33, 31, 33]}, "prediction_text": "A wedding present from John the Steadfast."}
{"answers": {"text": ["six children", "six", "six"], "answer_start": [265, 265, 265]}, "prediction_text": "Six children."}
{"answers": {"text": ["riches of Croesus", "poverty", "my poverty for the riches of Croesus."], "answer_start": [710, 694, 691]}, "prediction_text": "poverty"}
{"answers": {"text": ["farming the land", "farming the land", "farming"], "answer_start": [512, 512, 512]}, "prediction_text": "Farming the land."}
{"answers": {"text": ["choosing their own ministers", "choosing their own ministers", "confessional church based on personal faith and experience and a territorial church including all in a given locality"], "answer_start": [117, 117, 235]}, "prediction_text": "Luther found himself increasingly occupied in organising a new church."}
{"answers": {"text": ["supervisory church body", "supervisory church body", "supervisory church"], "answer_start": [507, 507, 507]}, "prediction_text": "A supervisory church body."}
{"answers": {"text": ["new form", "a new form", "two catechisms."], "answer_start": [544, 542, 631]}, "prediction_text": "Catechisms."}
{"answers": {"text": ["two catechisms", "two catechisms", "two catechisms."], "answer_start": [631, 631, 631]}, "prediction_text": "Two catechisms."}
{"answers": {"text": ["revolutionary", "theology of the cross,", "revolutionary"], "answer_start": [667, 708, 667]}, "prediction_text": "Luther's thought is revolutionary to the extent that it is a theology of the cross, the negation of every affirmation: as long as the cross is at the center, the system building tendency of reason is held in check, and system building does not degenerate into System."}
{"answers": {"text": ["extreme change", "extreme change", "extreme change."], "answer_start": [59, 59, 59]}, "prediction_text": "extreme change"}
{"answers": {"text": ["Electorate of Saxony", "Electorate of Saxony", "Electorate"], "answer_start": [181, 181, 181]}, "prediction_text": "Electorate of Saxony."}
{"answers": {"text": ["adviser", "adviser", "adviser"], "answer_start": [221, 221, 221]}, "prediction_text": "Advice."}
{"answers": {"text": ["John the Steadfast", "John the Steadfast", "John the Steadfast,"], "answer_start": [341, 341, 341]}, "prediction_text": "John the Steadfast"}
{"answers": {"text": ["under the temporal sovereign", "church government under the temporal sovereign", "church government under the temporal sovereign"], "answer_start": [659, 641, 641]}, "prediction_text": "The relationship with the elector had unintended consequences on church government."}
{"answers": {"text": ["early 1526", "1526", "1526"], "answer_start": [95, 101, 101]}, "prediction_text": "1526"}
{"answers": {"text": ["1523 adaptation of the Latin Mass", "Latin Mass", "adaptation of the Latin Mass"], "answer_start": [153, 176, 158]}, "prediction_text": "The new mass did not replace the old mass."}
{"answers": {"text": ["simple people", "simple people", "\"simple people"], "answer_start": [218, 218, 217]}, "prediction_text": "The simple people."}
{"answers": {"text": ["sacrifice", "everything that smacks of sacrifice", "everything that smacks of sacrifice"], "answer_start": [388, 362, 362]}, "prediction_text": "Sacrifice, sacrifice, sacrifice, wine, bread, host, chalice, altar, candles."}
{"answers": {"text": ["freedom of ceremony", "freedom", "wine as well as the bread"], "answer_start": [632, 632, 462]}, "prediction_text": "The new mass allowed freedom of ceremony."}
{"answers": {"text": ["1527", "1527", "1527"], "answer_start": [130, 130, 130]}, "prediction_text": "1527"}
{"answers": {"text": ["visitation of the Electorate", "Electorate of Saxony", "visitation of the Electorate of Saxony,"], "answer_start": [75, 93, 75]}, "prediction_text": "The visitation of the Electorate of Saxony."}
{"answers": {"text": ["Christian education", "Christian", "standard of pastoral care and Christian education"], "answer_start": [189, 189, 159]}, "prediction_text": "Christian education."}
{"answers": {"text": ["Christian doctrine", "Christian doctrine", "Christian doctrine"], "answer_start": [327, 327, 327]}, "prediction_text": "Christian doctrine."}
{"answers": {"text": ["incapable of teaching", "teaching", "teaching"], "answer_start": [409, 422, 422]}, "prediction_text": "teach"}
{"answers": {"text": ["catechism", "catechism", "catechism"], "answer_start": [19, 19, 19]}, "prediction_text": "The Large Catechism."}
{"answers": {"text": ["1529", "1529", "1529"], "answer_start": [106, 106, 106]}, "prediction_text": "1529"}
{"answers": {"text": ["pastors and teachers", "pastors and teachers", "pastors"], "answer_start": [155, 155, 155]}, "prediction_text": "Pastors and teachers."}
{"answers": {"text": ["the people", "the people", "people"], "answer_start": [240, 240, 244]}, "prediction_text": "The people themselves."}
{"answers": {"text": ["questions and answers", "questions and answers", "questions and answers in the catechism so that the basics of Christian faith"], "answer_start": [461, 461, 461]}, "prediction_text": "Questions and answers."}
{"answers": {"text": ["The catechism", "catechism", "catechism"], "answer_start": [0, 4, 4]}, "prediction_text": "The Small Catechism"}
{"answers": {"text": ["writings in volumes", "plan to collect my writings in volumes", "Saturnian hunger,"], "answer_start": [88, 69, 188]}, "prediction_text": "Collecting his writings in volumes."}
{"answers": {"text": ["the Catechism", "Catechism", "Catechism"], "answer_start": [347, 351, 351]}, "prediction_text": "Catechism"}
{"answers": {"text": ["Small Catechism", "Small Catechism", "The Small Catechism"], "answer_start": [367, 367, 363]}, "prediction_text": "The Small Catechism."}
{"answers": {"text": ["the Bible", "Bible", "Bible"], "answer_start": [521, 525, 525]}, "prediction_text": "Bible"}
{"answers": {"text": ["Small Catechism", "Small Catechism", "Small Catechism"], "answer_start": [9, 9, 9]}, "prediction_text": "Small Catechism"}
{"answers": {"text": ["Larger Catechism", "Larger Catechism", "Larger Catechism"], "answer_start": [107, 107, 107]}, "prediction_text": "Larger Catechism."}
{"answers": {"text": ["German vernacular", "German", "German"], "answer_start": [161, 161, 161]}, "prediction_text": "Small Catechism, Larger Catechism."}
{"answers": {"text": ["as persons", "persons to be known.", "Father, the Son, or the Holy Spirit."], "answer_start": [622, 625, 333]}, "prediction_text": "He rewrote each article of the Creed to express the character of the Father, the Son, or the Holy Spirit."}
{"answers": {"text": ["with the Father", "with the Father", "Father"], "answer_start": [775, 775, 784]}, "prediction_text": "The Father."}
{"answers": {"text": ["1522", "1522", "1522"], "answer_start": [68, 68, 68]}, "prediction_text": "1522"}
{"answers": {"text": ["1534", "1534", "1534"], "answer_start": [153, 153, 153]}, "prediction_text": "1534"}
{"answers": {"text": ["the translation", "translation", "translation"], "answer_start": [228, 232, 232]}, "prediction_text": "The translation of the Old Testament."}
{"answers": {"text": ["alone", "alone", "alone"], "answer_start": [420, 420, 420]}, "prediction_text": "\"alone\""}
{"answers": {"text": ["Faith alone", "faith", "faith in Christ without any works of the Law"], "answer_start": [905, 671, 671]}, "prediction_text": "Faith."}
{"answers": {"text": ["Saxon chancellery", "Saxon chancellery", "variant of German spoken at the Saxon chancellery,"], "answer_start": [62, 62, 30]}, "prediction_text": "Saxon chancellery"}
{"answers": {"text": ["northern and southern", "northern and southern", "both northern and southern Germans"], "answer_start": [102, 102, 97]}, "prediction_text": "Northern and southern Germans."}
{"answers": {"text": ["everyday Germans", "everyday Germans", "everyday Germans"], "answer_start": [207, 207, 207]}, "prediction_text": "Germans"}
{"answers": {"text": ["read it without hindrance", "may read it without hindrance", "removing impediments and difficulties so that other people may read it without hindrance"], "answer_start": [300, 296, 237]}, "prediction_text": "To make the Bible accessible to everyday Germans."}
{"answers": {"text": ["impediments and difficulties", "impediments and difficulties", "impediments"], "answer_start": [246, 246, 246]}, "prediction_text": "impediments and difficulties."}
{"answers": {"text": ["German-language publications", "German-language publications", "German-language publications,"], "answer_start": [41, 41, 41]}, "prediction_text": "German-language publications."}
{"answers": {"text": ["Bible translation", "Bible", "Bible"], "answer_start": [129, 129, 129]}, "prediction_text": "Luther Bible"}
{"answers": {"text": ["evolution of the German language", "evolution of the German language and literature", "evolution of the German language and literature"], "answer_start": [199, 199, 199]}, "prediction_text": "Evolution of the German language and literature."}
{"answers": {"text": ["Lucas Cranach", "Lucas Cranach", "Lucas Cranach"], "answer_start": [314, 314, 314]}, "prediction_text": "Lucas Cranach"}
{"answers": {"text": ["William Tyndale", "William Tyndale's", "Tyndale"], "answer_start": [508, 508, 516]}, "prediction_text": "William Tyndale"}
{"answers": {"text": ["authoring hymns", "hymn-writer", "hymn-writer"], "answer_start": [35, 22, 22]}, "prediction_text": "Hymn-writer"}
{"answers": {"text": ["high art and folk music", "high art and folk music", "singing of German hymns in connection with worship"], "answer_start": [262, 262, 395]}, "prediction_text": "singing, lute, waldzither."}
{"answers": {"text": ["singing of German hymns", "singing of German hymns", "singing"], "answer_start": [395, 395, 395]}, "prediction_text": "German hymns."}
{"answers": {"text": ["lute", "a lute", "singing"], "answer_start": [526, 524, 395]}, "prediction_text": "Lute"}
{"answers": {"text": ["waldzither", "waldzither", "waldzither"], "answer_start": [555, 555, 555]}, "prediction_text": "Waldzither"}
{"answers": {"text": ["events in his life", "particular events in his life", "events in his life"], "answer_start": [52, 41, 52]}, "prediction_text": "Johann Esch and Heinrich Voes."}
{"answers": {"text": ["for Lutheran views", "Lutheran views", "Lutheran views,"], "answer_start": [259, 263, 263]}, "prediction_text": "For Lutheran views."}
{"answers": {"text": ["Ein neues Lied wir heben an", "Ein neues Lied wir heben an", "Ein neues Lied wir heben an"], "answer_start": [315, 315, 315]}, "prediction_text": "\"Ein neues Lied wir heben an\" (\"A new song we raise\")"}
{"answers": {"text": ["John C. Messenger", "John C. Messenger", "Messenger"], "answer_start": [408, 408, 416]}, "prediction_text": "John C. Messenger"}
{"answers": {"text": ["Flung to the Heedless Winds", "Flung to the Heedless Winds", "A new song we raise"], "answer_start": [469, 469, 346]}, "prediction_text": "John C. Messenger's translation."}
{"answers": {"text": ["1524", "1524", "1524"], "answer_start": [9, 9, 9]}, "prediction_text": "1524"}
{"answers": {"text": ["Apostles' Creed", "explanation of the Apostles' Creed", "three-part explanation of the Apostles' Creed"], "answer_start": [188, 169, 158]}, "prediction_text": "Luther's 1529 three-part explanation of the Apostles' Creed."}
{"answers": {"text": ["Small Catechism", "Small Catechism", "Small Catechism"], "answer_start": [211, 211, 211]}, "prediction_text": "Small Catechism."}
{"answers": {"text": ["German creedal hymn", "creedal", "earlier German creedal hymn,"], "answer_start": [280, 287, 272]}, "prediction_text": "German creedal hymn."}
{"answers": {"text": ["difficulty of its tune", "perceived difficulty of its tune", "difficulty of its tune"], "answer_start": [639, 629, 639]}, "prediction_text": "Because of the perceived difficulty of its tune."}
{"answers": {"text": ["1538", "1538", "1538"], "answer_start": [9, 9, 9]}, "prediction_text": "1538"}
{"answers": {"text": ["Small Catechism", "Small Catechism", "Small Catechism,"], "answer_start": [146, 146, 146]}, "prediction_text": "Small Catechism."}
{"answers": {"text": ["specific catechism questions", "specific catechism questions", "specific catechism questions"], "answer_start": [365, 365, 365]}, "prediction_text": "Catechism questions."}
{"answers": {"text": ["multiple revisions", "multiple revisions", "multiple revisions"], "answer_start": [423, 423, 423]}, "prediction_text": "Multiple revisions."}
{"answers": {"text": ["Luther's tune", "Luther's tune", "adopted Luther's tune"], "answer_start": [635, 635, 627]}, "prediction_text": "Luther's tune."}
{"answers": {"text": ["1523", "1523", "1523"], "answer_start": [87, 87, 87]}, "prediction_text": "1523"}
{"answers": {"text": ["Psalm 130", "Psalm 130", "Psalm 130"], "answer_start": [115, 115, 115]}, "prediction_text": "Psalm 130."}
{"answers": {"text": ["write psalm-hymns", "write psalm-hymns", "to write psalm-hymns"], "answer_start": [188, 188, 185]}, "prediction_text": "Write psalm-hymns for use in German worship."}
{"answers": {"text": ["Achtliederbuch", "Achtliederbuch", "Achtliederbuch"], "answer_start": [321, 321, 321]}, "prediction_text": "Achtliederbuch"}
{"answers": {"text": ["Reformation doctrine", "essential Reformation doctrine", "essential Reformation doctrine"], "answer_start": [552, 542, 542]}, "prediction_text": "Grace alone."}
{"answers": {"text": ["Nun komm, der Heiden Heiland", "Nun komm, der Heiden Heiland", "Nun komm, der Heiden Heiland"], "answer_start": [84, 84, 84]}, "prediction_text": "\"A solus ortus cardine\""}
{"answers": {"text": ["Veni redemptor gentium", "Veni redemptor gentium", "Veni redemptor gentium"], "answer_start": [159, 159, 159]}, "prediction_text": "Veni redemptor gentium."}
{"answers": {"text": ["main hymn", "hymn", "main hymn"], "answer_start": [194, 199, 194]}, "prediction_text": "Hauptlied."}
{"answers": {"text": ["two hymns", "two", "two"], "answer_start": [443, 443, 443]}, "prediction_text": "2"}
{"answers": {"text": ["German Te Deum", "the German Te Deum", "German Te Deum"], "answer_start": [1072, 1068, 1072]}, "prediction_text": "German Te Deum."}
{"answers": {"text": ["baptism", "baptism", "baptism"], "answer_start": [170, 170, 170]}, "prediction_text": "The Lutheran Reformation."}
{"answers": {"text": ["Johann Walter", "Johann Walter", "Walter"], "answer_start": [231, 231, 238]}, "prediction_text": "Johann Walter"}
{"answers": {"text": ["prayer for grace", "grace", "prayer for grace"], "answer_start": [297, 308, 297]}, "prediction_text": "Prayer for grace."}
{"answers": {"text": ["J. S. Bach", "J. S. Bach", "Bach"], "answer_start": [479, 479, 485]}, "prediction_text": "J. S. Bach"}
{"answers": {"text": ["Halle", "Halle", "Halle"], "answer_start": [409, 409, 409]}, "prediction_text": "Halle"}
{"answers": {"text": ["early Lutheran hymnals", "hymnals", "early Lutheran hymnals"], "answer_start": [32, 47, 32]}, "prediction_text": "Early Lutheran hymnals."}
{"answers": {"text": ["four", "four", "four"], "answer_start": [108, 108, 108]}, "prediction_text": "Four."}
{"answers": {"text": ["18", "18", "18"], "answer_start": [173, 173, 173]}, "prediction_text": "24 songs."}
{"answers": {"text": ["24", "24", "24"], "answer_start": [219, 219, 219]}, "prediction_text": "24 songs."}
{"answers": {"text": ["Eyn geystlich Gesangk Buchleyn", "Eyn geystlich Gesangk Buchleyn", "Eyn geystlich Gesangk Buchleyn"], "answer_start": [297, 297, 297]}, "prediction_text": "Erfurt Enchiridion"}
{"answers": {"text": ["Johann Sebastian Bach", "Johann Sebastian Bach", "Bach"], "answer_start": [50, 50, 67]}, "prediction_text": "Johann Sebastian Bach"}
{"answers": {"text": ["chorale cantatas", "chorale cantatas", "chorale cantatas"], "answer_start": [134, 134, 134]}, "prediction_text": "Christ lag in Todes Banden."}
{"answers": {"text": ["1707", "1707", "1707"], "answer_start": [232, 232, 232]}, "prediction_text": "1707"}
{"answers": {"text": ["1724 to 1725", "1724 to 1725", "1724 to 1725"], "answer_start": [266, 266, 266]}, "prediction_text": "1724 to 1725."}
{"answers": {"text": ["1735", "1735", "W\u00e4r Gott nicht mit uns diese Zeit"], "answer_start": [537, 537, 542]}, "prediction_text": "1735"}
{"answers": {"text": ["sleeps", "sleeps", "sleeps"], "answer_start": [169, 169, 169]}, "prediction_text": "sleeps in peace."}
{"answers": {"text": ["idea of torments", "torments", "Bible"], "answer_start": [388, 396, 288]}, "prediction_text": "Purgatory, torments for the saints, and the existence of Purgatory."}
{"answers": {"text": ["sleep in peace", "enter a prepared bedchamber in which they sleep in peace.", "sleeps"], "answer_start": [591, 549, 169]}, "prediction_text": "In his Smalcald Articles, Luther described the saints as currently residing \"in their graves and in heaven.\""}
{"answers": {"text": ["rejected the existence", "rejected the existence of", "rejected"], "answer_start": [616, 616, 616]}, "prediction_text": "He thought that Purgatory involved Christian souls undergoing penitential suffering after death."}
{"answers": {"text": ["Smalcald Articles", "in their graves and in heaven", "Smalcald Articles"], "answer_start": [805, 871, 805]}, "prediction_text": "In his Smalcald Articles."}
{"answers": {"text": ["Franz Pieper", "Franz Pieper", "Pieper"], "answer_start": [24, 24, 30]}, "prediction_text": "Franz Pieper"}
{"answers": {"text": ["Johann Gerhard", "Johann Gerhard", "Gerhard"], "answer_start": [174, 174, 181]}, "prediction_text": "Johann Gerhard"}
{"answers": {"text": ["Gerhard. Lessing", "Lessing", "Lessing"], "answer_start": [181, 190, 190]}, "prediction_text": "Johann Gerhard"}
{"answers": {"text": ["1755", "1755", "1755"], "answer_start": [199, 199, 199]}, "prediction_text": "1755"}
{"answers": {"text": ["Commentary on Genesis", "Commentary on Genesis", "Commentary on Genesis"], "answer_start": [9, 9, 9]}, "prediction_text": "Genesis."}
{"answers": {"text": ["Francis Blackburne", "Francis Blackburne", "Blackburne"], "answer_start": [170, 170, 178]}, "prediction_text": "Gottfried Fritschel"}
{"answers": {"text": ["1765", "1765", "1765"], "answer_start": [192, 192, 192]}, "prediction_text": "1867"}
{"answers": {"text": ["Gottfried Fritschel", "Gottfried Fritschel", "Fritschel"], "answer_start": [272, 272, 282]}, "prediction_text": "Gottfried Fritschel"}
{"answers": {"text": ["dreams", "dreams", "dreams"], "answer_start": [557, 557, 557]}, "prediction_text": "Dreams."}
{"answers": {"text": ["October 1529", "October 1529", "1529"], "answer_start": [3, 3, 11]}, "prediction_text": "October 1529"}
{"answers": {"text": ["Landgrave of Hesse", "Landgrave of Hesse", "Landgrave of Hesse"], "answer_start": [27, 27, 27]}, "prediction_text": "Philip I"}
{"answers": {"text": ["doctrinal unity", "doctrinal unity", "doctrinal unity"], "answer_start": [138, 138, 138]}, "prediction_text": "Doctrinal unity."}
{"answers": {"text": ["fourteen points", "fourteen", "fourteen"], "answer_start": [215, 215, 215]}, "prediction_text": "14"}
{"answers": {"text": ["nature of the Eucharist", "nature of the Eucharist", "sacrament of the Lord's Supper"], "answer_start": [271, 271, 301]}, "prediction_text": "The nature of the Eucharist."}
{"answers": {"text": ["words spoken by Jesus", "significance of the words spoken by Jesus", "words spoken"], "answer_start": [127, 107, 127]}, "prediction_text": "The theologians differed on the significance of the words spoken by Jesus at the Last Supper."}
{"answers": {"text": ["body and blood of Christ", "Real Presence", "Real Presence of the body and blood of Christ in the consecrated bread and wine"], "answer_start": [321, 300, 300]}, "prediction_text": "The Real Presence of the body and blood of Christ."}
{"answers": {"text": ["sacramental union", "sacramental union", "sacramental union"], "answer_start": [401, 401, 401]}, "prediction_text": "The sacramental union."}
{"answers": {"text": ["symbolically present", "spiritually or symbolically present", "symbolically present."], "answer_start": [479, 464, 479]}, "prediction_text": "Spiritually or symbolically present."}
{"answers": {"text": ["confrontational", "confrontational", "confrontational"], "answer_start": [696, 696, 696]}, "prediction_text": "Confrontational."}
{"answers": {"text": ["1530", "1530", "1530"], "answer_start": [98, 98, 98]}, "prediction_text": "1530"}
{"answers": {"text": ["Marburg Colloquy", "Marburg Colloquy", "Marburg Colloquy"], "answer_start": [48, 48, 48]}, "prediction_text": "The Marburg Colloquy."}
{"answers": {"text": ["Schmalkaldic League", "Schmalkaldic League", "Schmalkaldic League"], "answer_start": [160, 160, 160]}, "prediction_text": "Schmalkaldic League"}
{"answers": {"text": ["The Swiss cities", "Swiss cities", "The Swiss cities"], "answer_start": [314, 318, 314]}, "prediction_text": "Swiss cities"}
{"answers": {"text": ["George, Margrave of Brandenburg-Ansbach", "George, Margrave of Brandenburg-Ansbach", "George, Margrave of Brandenburg-Ansbach"], "answer_start": [273, 273, 273]}, "prediction_text": "George, Margrave of Brandenburg-Ansbach"}
{"answers": {"text": ["antithetical", "antithetical", "antithetical"], "answer_start": [74, 74, 74]}, "prediction_text": "That faith and reason were antithetical."}
{"answers": {"text": ["reason", "reason", "human reason"], "answer_start": [152, 152, 555]}, "prediction_text": "reason"}
{"answers": {"text": ["no way contributes", "in no way", "in no way"], "answer_start": [342, 339, 339]}, "prediction_text": "He wrote, \"For reason in no way contributes to faith. [...] For reason is the greatest enemy that faith has; it never comes to the aid of spiritual things.\""}
{"answers": {"text": ["reason", "reason", "Reason"], "answer_start": [381, 381, 332]}, "prediction_text": "Reason"}
{"answers": {"text": ["different epistemological spheres.", "separate spheres of knowledge that each applies to", "separate spheres of knowledge"], "answer_start": [1796, 855, 855]}, "prediction_text": "The separate spheres of knowledge."}
{"answers": {"text": ["Jesus Christ was born a Jew", "that Jesus Christ was born a Jew", "conversion"], "answer_start": [27, 22, 118]}, "prediction_text": "The Old Testament."}
{"answers": {"text": ["Jewish conversion to Christianity", "large-scale Jewish conversion to Christianity", "large-scale Jewish conversion"], "answer_start": [284, 272, 272]}, "prediction_text": "Jewish conversion to Christianity."}
{"answers": {"text": ["Jews", "Jews", "Jews"], "answer_start": [375, 375, 375]}, "prediction_text": "Jews."}
{"answers": {"text": ["Anabaptists", "Anabaptists, Zwinglianism, and the papacy", "Anabaptists"], "answer_start": [457, 457, 457]}, "prediction_text": "Anabaptists"}
{"answers": {"text": ["1543", "1543", "1543"], "answer_start": [504, 504, 504]}, "prediction_text": "1543"}
{"answers": {"text": ["as a scourge", "scourge", "argued against resisting"], "answer_start": [259, 264, 120]}, "prediction_text": "As a scourge sent to punish Christians."}
{"answers": {"text": ["to punish Christians", "punish Christians by God", "scourge sent to punish Christians"], "answer_start": [277, 280, 264]}, "prediction_text": "To punish Christians."}
{"answers": {"text": ["destroy the antichrist", "destroy the antichrist", "punish"], "answer_start": [354, 354, 280]}, "prediction_text": "To punish Christians."}
{"answers": {"text": ["the papacy", "papacy", "papacy"], "answer_start": [405, 409, 409]}, "prediction_text": "Pope."}
{"answers": {"text": ["secular war", "non-religious", "non-religious war"], "answer_start": [994, 732, 732]}, "prediction_text": "Non-religious war."}
{"answers": {"text": ["Qur'an", "Qur'an", "Qur'an"], "answer_start": [48, 48, 48]}, "prediction_text": "The Qur'an."}
{"answers": {"text": ["critical pamphlets on Islam", "critical", "critical pamphlets on Islam"], "answer_start": [86, 86, 86]}, "prediction_text": "Critical pamphlets."}
{"answers": {"text": ["Islam", "Islam", "pamphlets"], "answer_start": [108, 108, 95]}, "prediction_text": "Islam and the Turk."}
{"answers": {"text": ["tool of the devil", "a tool of the devil", "tool of the devil,"], "answer_start": [202, 200, 202]}, "prediction_text": "indifferent"}
{"answers": {"text": ["exposed to scrutiny.", "wanting it exposed to scrutiny.", "exposed to scrutiny"], "answer_start": [423, 412, 423]}, "prediction_text": "To expose it to scrutiny."}
{"answers": {"text": ["God's wrath to Christians", "God's wrath", "God's wrath to Christians."], "answer_start": [222, 222, 222]}, "prediction_text": "God's wrath to Christians."}
{"answers": {"text": ["Johannes Agricola", "Agricola", "Agricola"], "answer_start": [15, 316, 316]}, "prediction_text": "Johannes Agricola"}
{"answers": {"text": ["city hall", "city hall", "city hall."], "answer_start": [495, 495, 495]}, "prediction_text": "City hall."}
{"answers": {"text": ["theses against Agricola", "six series of theses", "six series of theses against Agricola"], "answer_start": [558, 544, 544]}, "prediction_text": "Responded with six series of theses against Agricola and the antinomians."}
{"answers": {"text": ["On the Councils and the Church", "On the Councils and the Church", "On the Councils"], "answer_start": [811, 811, 811]}, "prediction_text": "On the Councils and the Church."}
{"answers": {"text": ["second use of the law", "second use of the law", "second use of the law"], "answer_start": [129, 129, 129]}, "prediction_text": "The second use of the law."}
{"answers": {"text": ["work sorrow over sin", "work sorrow over sin in man's heart", "work sorrow over sin in man's heart"], "answer_start": [199, 199, 199]}, "prediction_text": "To work sorrow over sin in man's heart."}
{"answers": {"text": ["everything", "everything", "everything that is used to work sorrow over sin is called the law,"], "answer_start": [333, 333, 333]}, "prediction_text": "The law."}
{"answers": {"text": ["eliminate the accusing law", "eliminate the accusing law.", "eliminate the accusing law"], "answer_start": [643, 643, 643]}, "prediction_text": "Remove the three letters l-a-w from the church."}
{"answers": {"text": ["essentially holy people", "holy people", "only of essentially holy people"], "answer_start": [876, 888, 868]}, "prediction_text": "essentially holy people."}
{"answers": {"text": ["ought to live", "live", "how the Christian ought to live"], "answer_start": [231, 240, 213]}, "prediction_text": "How to live."}
{"answers": {"text": ["Ten Commandments", "the Ten Commandments", "Ten Commandments"], "answer_start": [51, 47, 51]}, "prediction_text": "Natural law."}
{"answers": {"text": ["third use of the law", "third use of the law", "third use of the law"], "answer_start": [286, 286, 286]}, "prediction_text": "The third use of the law."}
{"answers": {"text": ["illustration of the Ten Commandments", "an illustration of the Ten Commandments", "illustration of the Ten Commandments,"], "answer_start": [396, 393, 396]}, "prediction_text": "An example of the Ten Commandments."}
{"answers": {"text": ["Ten Commandments", "Ten Commandments", "his or her vocations on a daily basis"], "answer_start": [416, 416, 469]}, "prediction_text": "The Ten Commandments."}
{"answers": {"text": ["baptism", "baptism", "baptism"], "answer_start": [112, 112, 112]}, "prediction_text": "Baptism"}
{"answers": {"text": ["Ten Commandments", "Ten Commandments", "Ten Commandments,"], "answer_start": [4, 4, 4]}, "prediction_text": "The Ten Commandments."}
{"answers": {"text": ["service to the neighbor", "service to the neighbor in the common", "service to the neighbor in the common, daily vocations of this perishing world"], "answer_start": [413, 413, 413]}, "prediction_text": "To serve the neighbor."}
{"answers": {"text": ["wanted to marry", "marry one of his wife's ladies-in-waiting", "marry one of his wife's ladies-in-waiting."], "answer_start": [96, 106, 106]}, "prediction_text": "Marry one of his wife's ladies-in-waiting."}
{"answers": {"text": ["bigamy", "bigamy", "bigamy"], "answer_start": [52, 52, 52]}, "prediction_text": "bigamy"}
{"answers": {"text": ["one of his wife's ladies-in-waiting", "Margarethe von der Saale", "one of his wife's ladies-in-waiting"], "answer_start": [112, 516, 112]}, "prediction_text": "Margarethe von der Saale"}
{"answers": {"text": ["holds Luther accountable", "Luther", "one of the worst mistakes Luther made,"], "answer_start": [1064, 1070, 936]}, "prediction_text": "Martin Brecht"}
{"answers": {"text": ["lasting damage", "lasting damage", "caused lasting damage"], "answer_start": [1245, 1245, 1238]}, "prediction_text": "lasting damage to Luther's reputation."}
{"answers": {"text": ["expelled Jews", "expelled", "expelled"], "answer_start": [336, 336, 336]}, "prediction_text": "Expelled them."}
{"answers": {"text": ["Jews", "Jews", "Jews"], "answer_start": [134, 231, 134]}, "prediction_text": "Jews"}
{"answers": {"text": ["murder of Christ", "murder of Christ", "murder of Christ,"], "answer_start": [271, 271, 271]}, "prediction_text": "murder of Christ"}
{"answers": {"text": ["divinity of Jesus", "Jesus", "divinity of Jesus"], "answer_start": [448, 460, 448]}, "prediction_text": "divinity of Jesus"}
{"answers": {"text": ["convert them to Christianity.", "convert", "convert"], "answer_start": [946, 946, 946]}, "prediction_text": "Convert them to Christianity."}
{"answers": {"text": ["Von den Juden und Ihren L\u00fcgen", "treatise Von den Juden und Ihren L\u00fcgen", "Von den Juden und Ihren L\u00fcgen"], "answer_start": [69, 60, 69]}, "prediction_text": "Von den Juden und Ihren L\u00fcgen."}
{"answers": {"text": ["1543", "1543", "1543"], "answer_start": [244, 244, 244]}, "prediction_text": "1543"}
{"answers": {"text": ["three years before", "three years before", "three years before his death"], "answer_start": [250, 250, 250]}, "prediction_text": "Three years."}
{"answers": {"text": ["the devil's people", "the devil's people", "the devil's people"], "answer_start": [346, 346, 346]}, "prediction_text": "The Jews were no longer the chosen people."}
{"answers": {"text": ["sanction for murder", "a sanction for murder.", "a sanction for murder."], "answer_start": [1059, 1057, 1057]}, "prediction_text": "\"God's anger with them is so intense.\""}
{"answers": {"text": ["the Jews", "Jews", "Jews"], "answer_start": [25, 29, 29]}, "prediction_text": "Jews."}
{"answers": {"text": ["Martin Luther", "Martin Luther", "that priest whose name was Martin Luther"], "answer_start": [213, 213, 186]}, "prediction_text": "Martin Luther"}
{"answers": {"text": ["doomed to perdition", "doomed to perdition", "doomed to perdition."], "answer_start": [366, 366, 366]}, "prediction_text": "\"That whoever would help the Jews was doomed to perdition.\""}
{"answers": {"text": ["Luther's anti-Jewish works", "Luther's anti-Jewish works", "Luther's anti-Jewish works"], "answer_start": [445, 445, 445]}, "prediction_text": "Luther's anti-Jewish works."}
{"answers": {"text": ["Throughout the 1580s", "the 1580s", "1580s"], "answer_start": [646, 657, 661]}, "prediction_text": "1580s"}
{"answers": {"text": ["Luther", "Luther", "Luther"], "answer_start": [0, 0, 0]}, "prediction_text": "Luther"}
{"answers": {"text": ["anti-Jewish rhetoric", "anti-Jewish rhetoric", "anti-Jewish rhetoric"], "answer_start": [169, 169, 169]}, "prediction_text": "Anti-Jewish rhetoric."}
{"answers": {"text": ["attacks on Jews", "ideal underpinning", "antisemitism"], "answer_start": [338, 303, 238]}, "prediction_text": "The Nazis' actions in the 1930s and 1940s were traced back to Luther's rhetoric."}
{"answers": {"text": ["Luther", "Luther", "Luther"], "answer_start": [648, 0, 648]}, "prediction_text": "Luther"}
{"answers": {"text": ["radically anti-Semitic", "most radically anti-Semitic tract ever published", "most radically anti-Semitic tract ever published"], "answer_start": [944, 939, 939]}, "prediction_text": "Der St\u00fcrmer described Luther's On the Jews and their Lies as \"the most radically anti-Semitic tract ever published.\""}
{"answers": {"text": ["17 December 1941", "17 December 1941", "1941"], "answer_start": [3, 3, 15]}, "prediction_text": "17 December 1941"}
{"answers": {"text": ["Luther", "Luther", "Luther"], "answer_start": [190, 190, 190]}, "prediction_text": "Luther"}
{"answers": {"text": ["Diarmaid MacCulloch", "Diarmaid MacCulloch", "MacCulloch"], "answer_start": [467, 467, 476]}, "prediction_text": "Daniel Goldhagen"}
{"answers": {"text": ["Bishop Martin Sasse", "Martin Sasse", "Sasse"], "answer_start": [331, 338, 603]}, "prediction_text": "Martin Sasse"}
{"answers": {"text": ["greatest antisemite", "greatest antisemite of his time", "the greatest antisemite of his time,"], "answer_start": [857, 857, 853]}, "prediction_text": "A blueprint."}
{"answers": {"text": ["opportunistic", "opportunistic", "limited"], "answer_start": [245, 245, 202]}, "prediction_text": "Opportunistic."}
{"answers": {"text": ["misguided agitation", "misguided agitation", "misguided agitation"], "answer_start": [406, 406, 406]}, "prediction_text": "\"Luther fatefully became one of the 'church fathers' of anti-Semitism and thus provided material for the modern hatred of the Jews, cloaking it with the authority of the Reformer.\""}
{"answers": {"text": ["modern hatred of the Jews", "hatred of the Jews", "hatred of the Jews"], "answer_start": [555, 562, 562]}, "prediction_text": "Modern hatred of the Jews."}
{"answers": {"text": ["18th and 19th centuries", "18th and 19th centuries", "18th and 19th centuries"], "answer_start": [724, 724, 724]}, "prediction_text": "18th and 19th centuries."}
{"answers": {"text": ["religious and in no respect racial", "entirely religious", "entirely religious and in no respect racial"], "answer_start": [1327, 1318, 1318]}, "prediction_text": "Roland Bainton said that Luther's position on Jews was entirely religious and in no respect racial."}
{"answers": {"text": ["violence", "violence", "violence"], "answer_start": [161, 161, 161]}, "prediction_text": "Violence."}
{"answers": {"text": ["Ronald Berger", "Ronald Berger", "Berger"], "answer_start": [237, 237, 244]}, "prediction_text": "Ronald Berger"}
{"answers": {"text": ["hysterical and demonizing mentality", "a \"hysterical and demonizing mentality\" about Jews", "hysterical and demonizing mentality"], "answer_start": [459, 456, 459]}, "prediction_text": "Anti-Semitism."}
{"answers": {"text": ["Lutheran clergy and theologians", "German Lutheran clergy and theologians", "Luther"], "answer_start": [745, 738, 810]}, "prediction_text": "Paul Rose"}
{"answers": {"text": ["Luther's hostile publications", "Luther's hostile publications", "Luther's hostile publications towards the Jews"], "answer_start": [810, 810, 810]}, "prediction_text": "Luther's hostile publications."}
{"answers": {"text": ["declining state of mind", "a declining state of mind", "health deteriorated"], "answer_start": [281, 279, 202]}, "prediction_text": "Declining state of mind."}
{"answers": {"text": ["his health", "state of mind", "declining state of mind"], "answer_start": [198, 291, 281]}, "prediction_text": "health deterioration"}
{"answers": {"text": ["vulgarity and violence", "vulgarity and violence", "\"vulgarity and violence"], "answer_start": [365, 365, 364]}, "prediction_text": "Vulgarity and violence."}
{"answers": {"text": ["Muslims) and Catholics", "Turks\" (Muslims) and Catholics", "Turks"], "answer_start": [476, 468, 468]}, "prediction_text": "Muslims and Catholics."}
{"answers": {"text": ["Luther's Last Battles: Politics and Polemics 1531\u201346", "Luther's Last Battles: Politics and Polemics 1531\u201346", "Luther's Last Battles: Politics and Polemics 1531\u201346"], "answer_start": [51, 51, 51]}, "prediction_text": "Luther's Last Battles: Politics and Polemics 1531-46 (1983)"}
{"answers": {"text": ["Since the 1980s", "1980s", "Since the 1980s"], "answer_start": [0, 10, 0]}, "prediction_text": "1980s"}
{"answers": {"text": ["least prejudiced", "least prejudiced", "least prejudiced toward Jews"], "answer_start": [342, 342, 342]}, "prediction_text": "Less prejudiced."}
{"answers": {"text": ["Richard (Dick) Geary", "Richard (Dick) Geary", "Geary"], "answer_start": [396, 396, 411]}, "prediction_text": "Richard Geary"}
{"answers": {"text": ["1928-1933", "1928-1933"], "answer_start": [645, 645]}, "prediction_text": "1928-1933"}
{"answers": {"text": ["his health deteriorated", "deteriorated", "deteriorated"], "answer_start": [157, 168, 168]}, "prediction_text": "Suffering."}
{"answers": {"text": ["bigamy of the Philip of Hesse", "the bigamy of the Philip of Hesse incident", "bigamy of the Philip of Hesse"], "answer_start": [314, 310, 314]}, "prediction_text": "Philip of Hesse."}
{"answers": {"text": ["kidney and bladder stones", "kidney and bladder stones", "kidney and bladder stones, and arthritis,"], "answer_start": [456, 456, 456]}, "prediction_text": "Kidney and bladder stones."}
{"answers": {"text": ["arthritis, and an ear infection", "arthritis, and an ear infection ruptured an ear drum", "arthritis, and an ear infection ruptured an ear drum. In December 1544, he began to feel the effects of angina."], "answer_start": [487, 487, 487]}, "prediction_text": "M\u00e9ni\u00e8re's disease, vertigo, fainting, tinnitus, cataract, kidney stones, arthritis, ear infection ruptured ear drum."}
{"answers": {"text": ["angina", "angina", "angina"], "answer_start": [591, 591, 591]}, "prediction_text": "kidney and bladder stones, arthritis, ear infection ruptured an ear drum."}
{"answers": {"text": ["poor physical health", "poor physical health", "poor physical health"], "answer_start": [4, 4, 4]}, "prediction_text": "His poor physical health."}
{"answers": {"text": ["writings and comments", "his writings and comments", "writings and comments."], "answer_start": [73, 69, 73]}, "prediction_text": "His wife Katharina."}
{"answers": {"text": ["harsher", "harsher", "harsher"], "answer_start": [58, 58, 58]}, "prediction_text": "Short-tempered and harsher."}
{"answers": {"text": ["His wife Katharina", "wife Katharina", "Katharina"], "answer_start": [96, 100, 105]}, "prediction_text": "Katharina"}
{"answers": {"text": ["three times", "three", "three"], "answer_start": [257, 257, 257]}, "prediction_text": "3 times."}
{"answers": {"text": ["Eisleben", "Eisleben", "Eisleben"], "answer_start": [33, 33, 33]}, "prediction_text": "Eisleben"}
{"answers": {"text": ["15 February 1546", "15 February 1546", "15 February 1546"], "answer_start": [66, 66, 66]}, "prediction_text": "15 February 1546"}
{"answers": {"text": ["Jews", "Jews", "entirely devoted to the obdurate Jews, whom it was a matter of great urgency to expel from all German territory,"], "answer_start": [154, 154, 121]}, "prediction_text": "Jews."}
{"answers": {"text": ["all German territory", "all German territory", "1546"], "answer_start": [212, 212, 78]}, "prediction_text": "Germany"}
{"answers": {"text": ["that they convert", "became Christians", "convert"], "answer_start": [528, 438, 538]}, "prediction_text": "Christian love."}
{"answers": {"text": ["Mansfeld", "Mansfeld", "Mansfeld"], "answer_start": [27, 27, 27]}, "prediction_text": "Mansfeld"}
{"answers": {"text": ["negotiations", "negotiations for a settlement", "siblings' families continuing in their father Hans Luther's copper mining"], "answer_start": [443, 443, 78]}, "prediction_text": "The minds in Mansfeld."}
{"answers": {"text": ["late 1545", "1545", "1545"], "answer_start": [411, 416, 416]}, "prediction_text": "Late 1545."}
{"answers": {"text": ["early 1546", "1546", "1546"], "answer_start": [506, 512, 512]}, "prediction_text": "Early 1546"}
{"answers": {"text": ["his siblings' families", "his siblings", "siblings' families"], "answer_start": [74, 74, 78]}, "prediction_text": "Hans Luther's copper mining trade."}
{"answers": {"text": ["17 February 1546", "17 February 1546", "17 February 1546"], "answer_start": [48, 48, 48]}, "prediction_text": "17 February 1546"}
{"answers": {"text": ["chest pains", "chest pains", "chest pains"], "answer_start": [95, 95, 95]}, "prediction_text": "Chest pains."}
{"answers": {"text": ["Ps. 31:5", "Ps. 31:5", "Ps. 31:5"], "answer_start": [225, 225, 225]}, "prediction_text": "Ps. 31:5"}
{"answers": {"text": ["prayer of the dying", "the common prayer of the dying", "common prayer of the dying."], "answer_start": [247, 236, 240]}, "prediction_text": "Into your hand I commit my spirit."}
{"answers": {"text": ["1 a.m", "1 a.m.", "1 a.m"], "answer_start": [271, 271, 271]}, "prediction_text": "1 a.m."}
{"answers": {"text": ["apoplectic stroke", "apoplectic stroke", "apoplectic stroke"], "answer_start": [3, 3, 3]}, "prediction_text": "Apoplectic stroke"}
{"answers": {"text": ["2:45 a.m", "2:45 a.m.", "2:45 a.m"], "answer_start": [83, 83, 83]}, "prediction_text": "2:45 a.m."}
{"answers": {"text": ["18 February 1546", "18 February 1546", "18 February 1546"], "answer_start": [96, 96, 96]}, "prediction_text": "18 February 1546"}
{"answers": {"text": ["in the Castle Church", "Castle Church in Wittenberg", "Castle Church in Wittenberg,"], "answer_start": [173, 180, 180]}, "prediction_text": "Castle Church"}
{"answers": {"text": ["Johannes Bugenhagen and Philipp Melanchthon", "Johannes Bugenhagen and Philipp Melanchthon", "Johannes Bugenhagen and Philipp Melanchthon"], "answer_start": [265, 265, 265]}, "prediction_text": "Philipp Melanchthon"}
{"answers": {"text": ["his last statement", "his last statement", "last statement"], "answer_start": [61, 61, 65]}, "prediction_text": "A piece of paper."}
{"answers": {"text": ["Latin", "Latin", "Latin"], "answer_start": [102, 102, 102]}, "prediction_text": "Latin"}
{"answers": {"text": ["\"We are beggars,\"", "We are beggars", "We are beggars,"], "answer_start": [120, 121, 121]}, "prediction_text": "\"We are beggars\""}
{"answers": {"text": ["monumental", "printed images of Luther that emphasized his monumental size", "printed"], "answer_start": [69, 24, 24]}, "prediction_text": "Printed images of Luther."}
{"answers": {"text": ["frail Catholic saints", "frail", "frail"], "answer_start": [155, 155, 155]}, "prediction_text": "Frail."}
{"answers": {"text": ["physically imposing", "physically imposing", "stout man"], "answer_start": [322, 322, 204]}, "prediction_text": "Large body, strong mouth, piercing deep-set eyes, fleshy face, and squat neck."}
{"answers": {"text": ["religious orders", "medieval religious orders", "ascetic life of the medieval religious orders"], "answer_start": [611, 602, 582]}, "prediction_text": "Frail Catholic saints."}
{"answers": {"text": ["1530s and 1540s", "1530s and 1540s", "1530s and 1540s"], "answer_start": [7, 7, 7]}, "prediction_text": "1530s and 1540s."}
{"answers": {"text": ["18 February", "18 February", "18 February"], "answer_start": [22, 22, 22]}, "prediction_text": "18 February."}
{"answers": {"text": ["Episcopal (United States) Calendar of Saints.", "Episcopal", "Episcopal"], "answer_start": [101, 101, 101]}, "prediction_text": "Episcopal (United States) Calendar of Saints"}
{"answers": {"text": ["31 October", "31 October", "31 October"], "answer_start": [215, 215, 215]}, "prediction_text": "31 October."}
{"answers": {"text": ["Church of England's Calendar of Saints", "Calendar of Saints", "Church of England's Calendar of Saints"], "answer_start": [154, 174, 154]}, "prediction_text": "Episcopal (United States) Calendar of Saints"}
{"answers": {"text": ["Luther is honoured", "honoured", "honoured"], "answer_start": [0, 10, 10]}, "prediction_text": "Luther is honoured on 18 February with a commemoration in the Lutheran Calendar of Saints."}
{"answers": {"text": ["SoCal", "SoCal", "SoCal"], "answer_start": [39, 39, 39]}, "prediction_text": "SoCal"}
{"answers": {"text": ["10 counties", "10", "10"], "answer_start": [133, 133, 133]}, "prediction_text": "10 counties."}
{"answers": {"text": ["economic center", "major economic center", "economic center"], "answer_start": [514, 508, 514]}, "prediction_text": "Economic."}
{"answers": {"text": ["demographics and economic ties", "economic", "demographics and economic"], "answer_start": [214, 231, 214]}, "prediction_text": "Demographics and economic ties."}
{"answers": {"text": ["historical political divisions", "historical political divisions", "historical political divisions"], "answer_start": [451, 451, 451]}, "prediction_text": "demographics and economic ties."}
{"answers": {"text": ["Southern California Megaregion", "the greater Southern California Megaregion", "Southern California Megaregion"], "answer_start": [62, 50, 62]}, "prediction_text": "Greater Southern California Megaregion"}
{"answers": {"text": ["11", "11", "11"], "answer_start": [105, 105, 105]}, "prediction_text": "11"}
{"answers": {"text": ["Nevada", "Nevada", "Nevada"], "answer_start": [214, 214, 214]}, "prediction_text": "Las Vegas"}
{"answers": {"text": ["Mexican", "Mexican", "Mexican"], "answer_start": [243, 243, 243]}, "prediction_text": "Mexico"}
{"answers": {"text": ["Tijuana", "Tijuana", "Tijuana"], "answer_start": [263, 263, 263]}, "prediction_text": "Las Vegas"}
{"answers": {"text": ["Pacific", "Pacific", "Pacific"], "answer_start": [82, 82, 82]}, "prediction_text": "Pacific coastline"}
{"answers": {"text": ["seven", "seven", "seven"], "answer_start": [246, 246, 246]}, "prediction_text": "Seven."}
{"answers": {"text": ["12 million", "over 12 million inhabitants", "12 million"], "answer_start": [691, 686, 691]}, "prediction_text": "12 million."}
{"answers": {"text": ["San Diego", "the San Diego area", "San Diego"], "answer_start": [793, 789, 793]}, "prediction_text": "San Luis Obispo"}
{"answers": {"text": ["17.5 million", "over 17.5 million", "17.5 million"], "answer_start": [1020, 1015, 1020]}, "prediction_text": "17.5 million."}
{"answers": {"text": ["Colorado River", "the Colorado River", "Colorado River"], "answer_start": [43, 39, 43]}, "prediction_text": "Colorado Desert"}
{"answers": {"text": ["Colorado Desert", "the Colorado Desert", "Colorado Desert"], "answer_start": [19, 15, 19]}, "prediction_text": "Colorado Desert"}
{"answers": {"text": ["Mojave Desert", "the Mojave Desert", "Mojave Desert"], "answer_start": [94, 90, 94]}, "prediction_text": "Mojave Desert"}
{"answers": {"text": ["Mexico\u2013United States border", "the Mexico\u2013United States border", "Mexico\u2013United States border"], "answer_start": [168, 164, 168]}, "prediction_text": "Mexico\u2013United States border"}
{"answers": {"text": ["California", "California", "California"], "answer_start": [16, 16, 16]}, "prediction_text": "California"}
{"answers": {"text": ["3,792,621", "3,792,621", "3,792,621"], "answer_start": [159, 159, 159]}, "prediction_text": "3,792,621"}
{"answers": {"text": ["Los Angeles", "Los Angeles", "Los Angeles"], "answer_start": [170, 170, 49]}, "prediction_text": "Los Angeles"}
{"answers": {"text": ["San Diego", "San Diego", "San Diego"], "answer_start": [324, 324, 324]}, "prediction_text": "San Diego"}
{"answers": {"text": ["south", "south"], "answer_start": [280, 280]}, "prediction_text": "South."}
{"answers": {"text": ["Los Angeles", "Los Angeles", "Riverside"], "answer_start": [16, 16, 68]}, "prediction_text": "San Bernardino"}
{"answers": {"text": ["United States", "the United States", "Los Angeles"], "answer_start": [174, 170, 16]}, "prediction_text": "United States"}
{"answers": {"text": ["counties", "counties", "five most populous in the state"], "answer_start": [4, 4, 86]}, "prediction_text": "Counties"}
{"answers": {"text": ["15", "15", "Riverside"], "answer_start": [141, 141, 68]}, "prediction_text": "Los Angeles"}
{"answers": {"text": ["counties", "Riverside"], "answer_start": [4, 68]}, "prediction_text": "Los Angeles"}
{"answers": {"text": ["Hollywood", "Hollywood", "Hollywood"], "answer_start": [106, 106, 106]}, "prediction_text": "Hollywood"}
{"answers": {"text": ["Los Angeles", "Los Angeles", "Los Angeles"], "answer_start": [135, 70, 135]}, "prediction_text": "Los Angeles"}
{"answers": {"text": ["The Walt Disney Company", "The Walt Disney Company", "The Walt Disney Company"], "answer_start": [249, 249, 249]}, "prediction_text": "Walt Disney Company"}
{"answers": {"text": ["music", "major record companies"], "answer_start": [36, 432]}, "prediction_text": "Music"}
{"answers": {"text": ["Sony", "Sony", "Sony"], "answer_start": [418, 418, 418]}, "prediction_text": "MGM"}
{"answers": {"text": ["skateboard", "skateboard", "skateboard"], "answer_start": [64, 64, 64]}, "prediction_text": "Skateboard culture."}
{"answers": {"text": ["Tony Hawk", "Tony Hawk", "Tony Hawk"], "answer_start": [206, 206, 206]}, "prediction_text": "Tony Hawk"}
{"answers": {"text": ["Shaun White", "Shaun White", "Shaun White"], "answer_start": [354, 354, 354]}, "prediction_text": "Shaun White"}
{"answers": {"text": ["Oahu", "Oahu", "Oahu"], "answer_start": [580, 580, 580]}, "prediction_text": "Oahu"}
{"answers": {"text": ["Transpac", "Transpac", "Transpac"], "answer_start": [873, 873, 873]}, "prediction_text": "Transpacific Yacht Race"}
{"answers": {"text": ["Palm Springs", "Palm Springs", "Palm Springs"], "answer_start": [112, 112, 112]}, "prediction_text": "Palm Springs"}
{"answers": {"text": ["beaches", "for its popular beaches", "beaches"], "answer_start": [80, 64, 80]}, "prediction_text": "Popular beaches."}
{"answers": {"text": ["southern", "the desert", "southern"], "answer_start": [38, 93, 38]}, "prediction_text": "Southern California"}
{"answers": {"text": ["open spaces", "nearby open spaces", "nearby open spaces"], "answer_start": [167, 160, 160]}, "prediction_text": "Open spaces."}
{"answers": {"text": ["37\u00b0 9' 58.23\"", "37\u00b0 9' 58.23\"", "37\u00b0 9' 58.23\""], "answer_start": [194, 194, 194]}, "prediction_text": "37\u00b0 9' 58.23\""}
{"answers": {"text": ["11", "11", "11"], "answer_start": [225, 225, 225]}, "prediction_text": "11 miles (18 km) south of San Jose."}
{"answers": {"text": ["ten", "ten", "ten"], "answer_start": [453, 453, 453]}, "prediction_text": "Ten."}
{"answers": {"text": ["Tehachapi Mountains", "Tehachapi Mountains"], "answer_start": [740, 740]}, "prediction_text": "Tehachapi Mountains"}
{"answers": {"text": ["northern", "the Tehachapi Mountains", "northern"], "answer_start": [767, 736, 767]}, "prediction_text": "Tehachapi Mountains"}
{"answers": {"text": ["Mexico", "Mexico", "Mexico"], "answer_start": [136, 136, 136]}, "prediction_text": "Mexico"}
{"answers": {"text": ["Alta California", "Alta California", "Alta California"], "answer_start": [281, 281, 281]}, "prediction_text": "Alta California"}
{"answers": {"text": ["Monterey", "Monterey", "Monterey"], "answer_start": [217, 217, 217]}, "prediction_text": "Monterey"}
{"answers": {"text": ["the Missouri Compromise", "the Compromise of 1850", "Compromise of 1850"], "answer_start": [523, 572, 576]}, "prediction_text": "Compromise of 1850"}
{"answers": {"text": ["free", "a free state", "free"], "answer_start": [647, 645, 647]}, "prediction_text": "Free state"}
{"answers": {"text": ["inequitable taxes", "inequitable taxes", "inequitable taxes"], "answer_start": [45, 45, 45]}, "prediction_text": "taxes"}
{"answers": {"text": ["Cow Counties", "Cow Counties", "Cow Counties"], "answer_start": [132, 132, 132]}, "prediction_text": "Cow Counties"}
{"answers": {"text": ["three", "three", "three"], "answer_start": [179, 179, 179]}, "prediction_text": "3 times."}
{"answers": {"text": ["75", "75%", "75"], "answer_start": [470, 470, 470]}, "prediction_text": "75%"}
{"answers": {"text": ["Milton Latham", "Milton Latham", "Milton Latham"], "answer_start": [790, 790, 790]}, "prediction_text": "Milton Latham"}
{"answers": {"text": ["Los Angeles Times", "the Los Angeles Times", "Los Angeles Times"], "answer_start": [13, 9, 13]}, "prediction_text": "Los Angeles Times"}
{"answers": {"text": ["1900", "1900", "1900"], "answer_start": [3, 3, 3]}, "prediction_text": "1900"}
{"answers": {"text": ["1999", "1999", "1999"], "answer_start": [185, 185, 185]}, "prediction_text": "1900"}
{"answers": {"text": ["Imperial", "Imperial", "1999"], "answer_start": [222, 222, 185]}, "prediction_text": "Imperial"}
{"answers": {"text": ["seven", "seven", "seven"], "answer_start": [77, 77, 77]}, "prediction_text": "7"}
{"answers": {"text": ["regional tourism groups", "regional tourism groups", "AAA Auto Clubs"], "answer_start": [55, 55, 156]}, "prediction_text": "AAA Auto Clubs, California State Automobile Association, Automobile Club of Southern California."}
{"answers": {"text": ["California State Automobile Association", "the California State Automobile Association", "California State Automobile Association"], "answer_start": [189, 185, 189]}, "prediction_text": "California State Automobile Association"}
{"answers": {"text": ["three-region", "the three-region point of view", "three-region"], "answer_start": [452, 448, 452]}, "prediction_text": "Three-region point of view."}
{"answers": {"text": ["Tehachapis", "the Tehachapis", "Tehachapis"], "answer_start": [538, 534, 538]}, "prediction_text": "Tehachapis"}
{"answers": {"text": ["southern", "southern California", "southern California"], "answer_start": [773, 773, 773]}, "prediction_text": "Southern California"}
{"answers": {"text": ["third", "third", "third"], "answer_start": [190, 190, 190]}, "prediction_text": "Los Angeles"}
{"answers": {"text": ["vast areas", "vast areas", "vast areas"], "answer_start": [136, 136, 136]}, "prediction_text": "vast areas"}
{"answers": {"text": ["suburban", "suburban", "suburban communities and use of automobiles and highways"], "answer_start": [378, 378, 378]}, "prediction_text": "Automobiles and highways."}
{"answers": {"text": ["highways", "highways"], "answer_start": [426, 426]}, "prediction_text": "highways"}
{"answers": {"text": ["international metropolitan", "an international metropolitan region", "international metropolitan"], "answer_start": [680, 677, 680]}, "prediction_text": "Metropolitan region"}
{"answers": {"text": ["Camp Pendleton", "Camp Pendleton", "Camp Pendleton"], "answer_start": [75, 75, 75]}, "prediction_text": "Interstate 5."}
{"answers": {"text": ["Inland Empire", "Temecula and Murrieta"], "answer_start": [286, 183]}, "prediction_text": "Temecula and Murrieta."}
{"answers": {"text": ["United States Census Bureau", "the United States Census Bureau", "United States Census Bureau"], "answer_start": [318, 314, 318]}, "prediction_text": "United States Census Bureau"}
{"answers": {"text": ["Orange", "Orange Counties", "Orange"], "answer_start": [521, 521, 521]}, "prediction_text": "Orange County"}
{"answers": {"text": ["1990s", "1990s", "1990s"], "answer_start": [688, 688, 688]}, "prediction_text": "1990s"}
{"answers": {"text": ["Mediterranean", "a Mediterranean climate", "Mediterranean"], "answer_start": [31, 29, 31]}, "prediction_text": "Mediterranean climate"}
{"answers": {"text": ["infrequent rain", "infrequent rain", "infrequent rain"], "answer_start": [59, 59, 59]}, "prediction_text": "Mediterranean climate."}
{"answers": {"text": ["60's", "60's", "60's"], "answer_start": [243, 243, 243]}, "prediction_text": "90-60's"}
{"answers": {"text": ["very rare", "very rare", "very rare"], "answer_start": [353, 353, 353]}, "prediction_text": "Rare."}
{"answers": {"text": ["70", "70", "70"], "answer_start": [269, 269, 269]}, "prediction_text": "70-50's"}
{"answers": {"text": ["Pacific Ocean", "Pacific Ocean", "Pacific Ocean"], "answer_start": [222, 222, 222]}, "prediction_text": "Pacific Ocean islands"}
{"answers": {"text": ["varied", "varied", "natural ecosystem"], "answer_start": [48, 48, 97]}, "prediction_text": "Geologic"}
{"answers": {"text": ["topographic", "topographic", "topographic"], "answer_start": [80, 80, 80]}, "prediction_text": "Topographic and natural ecosystem landscapes."}
{"answers": {"text": ["Peninsular", "Peninsular Ranges", "Peninsular Ranges"], "answer_start": [313, 313, 313]}, "prediction_text": "Peninsular Ranges"}
{"answers": {"text": ["valleys", "valleys", "interior valleys"], "answer_start": [383, 383, 374]}, "prediction_text": "Deserts"}
{"answers": {"text": ["10,000", "10,000", "10,000"], "answer_start": [50, 50, 50]}, "prediction_text": "10,000"}
{"answers": {"text": ["small", "small", "small"], "answer_start": [96, 96, 96]}, "prediction_text": "10,000"}
{"answers": {"text": ["6.7", "6.7", "6.7"], "answer_start": [246, 246, 246]}, "prediction_text": "6.7"}
{"answers": {"text": ["property damage", "property damage"], "answer_start": [402, 402]}, "prediction_text": "Property damage."}
{"answers": {"text": ["$20 billion", "over $20 billion", "over $20 billion"], "answer_start": [471, 466, 466]}, "prediction_text": "$20 billion"}
{"answers": {"text": ["San Andreas", "the San Andreas Fault", "San Andreas Fault"], "answer_start": [73, 69, 73]}, "prediction_text": "San Andreas Fault"}
{"answers": {"text": ["6.7", "6.7+", "6.7+"], "answer_start": [44, 44, 44]}, "prediction_text": "6.7+"}
{"answers": {"text": ["Puente Hills", "the Puente Hills Fault", "Puente Hills Fault"], "answer_start": [181, 177, 181]}, "prediction_text": "San Jacinto Fault"}
{"answers": {"text": ["USGS", "The USGS", "USGS"], "answer_start": [234, 230, 234]}, "prediction_text": "USGS"}
{"answers": {"text": ["occurrence", "occurrence", "occurrence"], "answer_start": [309, 309, 309]}, "prediction_text": "magnitude 6.7+ event, magnitude 8.0 event, San Andreas Fault, San Jacinto Fault, Puente Hills Fault, Elsinore Fault Zone."}
{"answers": {"text": ["economically", "economically", "economically"], "answer_start": [60, 60, 60]}, "prediction_text": "Economically."}
{"answers": {"text": ["global", "global", "global"], "answer_start": [207, 207, 207]}, "prediction_text": "Global recognition."}
{"answers": {"text": ["economic", "economic", "economic activity"], "answer_start": [254, 254, 254]}, "prediction_text": "Tourist destinations"}
{"answers": {"text": ["2010", "2010", "2010"], "answer_start": [10, 10, 10]}, "prediction_text": "2010"}
{"answers": {"text": ["high growth rates", "high growth rates", "high growth rates"], "answer_start": [114, 114, 114]}, "prediction_text": "High growth rates."}
{"answers": {"text": ["10.0%", "10.0%", "10.0%"], "answer_start": [196, 196, 196]}, "prediction_text": "10.0%"}
{"answers": {"text": ["tech-oriented", "tech-oriented"], "answer_start": [311, 311]}, "prediction_text": "Tech-oriented economy"}
{"answers": {"text": ["Greater Sacramento", "Greater Sacramento", "Greater Sacramento"], "answer_start": [365, 365, 365]}, "prediction_text": "Greater Sacramento region"}
{"answers": {"text": ["Metropolitan Statistical Areas", "Metropolitan Statistical Areas", "Metropolitan Statistical Areas"], "answer_start": [69, 69, 69]}, "prediction_text": "Metropolitan Statistical Areas"}
{"answers": {"text": ["two", "two", "two"], "answer_start": [197, 197, 197]}, "prediction_text": "2"}
{"answers": {"text": ["five million", "five million", "five million"], "answer_start": [241, 241, 241]}, "prediction_text": "Five million."}
{"answers": {"text": ["Southern Border Region", "the Southern Border Region", "Southern Border Region"], "answer_start": [672, 668, 672]}, "prediction_text": "Southern Border Region."}
{"answers": {"text": ["17,786,419", "17,786,419", "17,786,419"], "answer_start": [311, 311, 311]}, "prediction_text": "17,786,419"}
{"answers": {"text": ["Los Angeles", "Los Angeles", "Los Angeles"], "answer_start": [0, 0, 0]}, "prediction_text": "Los Angeles"}
{"answers": {"text": ["1.3 million", "1.3 million", "San Diego"], "answer_start": [54, 54, 40]}, "prediction_text": "San Diego"}
{"answers": {"text": ["twelve", "twelve", "twelve"], "answer_start": [250, 250, 250]}, "prediction_text": "12"}
{"answers": {"text": ["100,000", "100,000", "100,000"], "answer_start": [316, 316, 316]}, "prediction_text": "100,000"}
{"answers": {"text": ["Riverside", "Riverside", "Riverside"], "answer_start": [478, 478, 478]}, "prediction_text": "Riverside"}
{"answers": {"text": ["petroleum", "petroleum", "petroleum"], "answer_start": [142, 142, 142]}, "prediction_text": "Petroleum"}
{"answers": {"text": ["Hollywood", "Hollywood", "Hollywood"], "answer_start": [319, 319, 319]}, "prediction_text": "Hollywood"}
{"answers": {"text": ["the housing bubble", "the housing bubble"], "answer_start": [495, 495]}, "prediction_text": "Housing bubble"}
{"answers": {"text": ["diverse", "diverse", "diverse"], "answer_start": [33, 33, 33]}, "prediction_text": "Abundance of petroleum."}
{"answers": {"text": ["heavily impacted", "heavily impacted"], "answer_start": [538, 538]}, "prediction_text": "Impacted."}
{"answers": {"text": ["1920s", "1920s", "1920s"], "answer_start": [10, 10, 10]}, "prediction_text": "1920s"}
{"answers": {"text": ["richest", "rich", "one of the richest"], "answer_start": [113, 113, 102]}, "prediction_text": "Rich."}
{"answers": {"text": ["citrus", "citrus", "citrus"], "answer_start": [166, 166, 166]}, "prediction_text": "Citrus."}
{"answers": {"text": ["cattle", "cattle", "cattle"], "answer_start": [155, 155, 155]}, "prediction_text": "Cattle"}
{"answers": {"text": ["aerospace", "aerospace", "aerospace"], "answer_start": [293, 293, 293]}, "prediction_text": "Aerospace"}
{"answers": {"text": ["business", "major business", "major business"], "answer_start": [42, 36, 36]}, "prediction_text": "CBD"}
{"answers": {"text": ["Central business districts", "Central business districts", "Central business districts"], "answer_start": [62, 62, 62]}, "prediction_text": "Central Business District"}
{"answers": {"text": ["South Coast Metro", "South Coast Metro", "South Coast Metro"], "answer_start": [192, 192, 192]}, "prediction_text": "Downtown Los Angeles"}
{"answers": {"text": ["business", "major business districts", "major business"], "answer_start": [42, 36, 36]}, "prediction_text": "Business district"}
{"answers": {"text": ["Los Angeles Area", "the Los Angeles Area", "major business"], "answer_start": [11, 7, 36]}, "prediction_text": "Downtown Los Angeles"}
{"answers": {"text": ["San Fernando Valley", "the San Fernando Valley", "San Fernando Valley"], "answer_start": [374, 370, 374]}, "prediction_text": "San Fernando Valley"}
{"answers": {"text": ["Los Angeles", "Los Angeles", "Los Angeles"], "answer_start": [148, 148, 218]}, "prediction_text": "Los Angeles"}
{"answers": {"text": ["business", "business districts", "business"], "answer_start": [48, 48, 48]}, "prediction_text": "Business districts"}
{"answers": {"text": ["Riverside", "Riverside", "Downtown Riverside"], "answer_start": [19, 19, 182]}, "prediction_text": "Riverside"}
{"answers": {"text": ["Hospitality Business/Financial Centre", "Downtown Riverside", "Hospitality Business/Financial Centre"], "answer_start": [95, 182, 95]}, "prediction_text": "Hospitality Business/Financial Centre"}
{"answers": {"text": ["Orange", "Orange County", "Orange County"], "answer_start": [0, 0, 0]}, "prediction_text": "Orange County"}
{"answers": {"text": ["University of California, Irvine", "the University of California, Irvine", "University of California"], "answer_start": [268, 264, 268]}, "prediction_text": "University of California, Irvine"}
{"answers": {"text": ["West Irvine", "West Irvine", "West Irvine"], "answer_start": [302, 302, 302]}, "prediction_text": "Irvine Business Centers"}
{"answers": {"text": ["South Coast Metro", "the South Coast Metro", "South Coast Metro"], "answer_start": [92, 88, 92]}, "prediction_text": "South Coast Metro"}
{"answers": {"text": ["rapidly", "rapidly", "rapidly"], "answer_start": [19, 19, 19]}, "prediction_text": "Rapidly developing."}
{"answers": {"text": ["Downtown San Diego", "Downtown San Diego", "Downtown"], "answer_start": [0, 0, 0]}, "prediction_text": "Downtown San Diego"}
{"answers": {"text": ["Northern San Diego", "Northern San Diego", "Northern San Diego"], "answer_start": [271, 271, 271]}, "prediction_text": "Northern San Diego and North County regions."}
{"answers": {"text": ["North County", "North County", "North County regions"], "answer_start": [306, 306, 306]}, "prediction_text": "North County regions."}
{"answers": {"text": ["San Diego", "San Diego", "San Diego"], "answer_start": [55, 55, 9]}, "prediction_text": "University City"}
{"answers": {"text": ["Los Angeles International Airport", "Los Angeles International Airport", "Los Angeles International Airport"], "answer_start": [31, 31, 31]}, "prediction_text": "Los Angeles International Airport"}
{"answers": {"text": ["passenger volume", "passenger volume", "passenger volume"], "answer_start": [117, 117, 117]}, "prediction_text": "Passenger traffic."}
{"answers": {"text": ["third", "third", "third"], "answer_start": [194, 194, 194]}, "prediction_text": "Third."}
{"answers": {"text": ["San Diego International Airport", "San Diego International Airport", "San Diego International Airport"], "answer_start": [314, 314, 314]}, "prediction_text": "San Diego International Airport"}
{"answers": {"text": ["Van Nuys Airport", "Van Nuys Airport", "Van Nuys Airport"], "answer_start": [394, 394, 394]}, "prediction_text": "Van Nuys Airport"}
{"answers": {"text": ["Metrolink", "Metrolink", "Metrolink"], "answer_start": [52, 52, 52]}, "prediction_text": "Metrolink"}
{"answers": {"text": ["seven", "seven", "seven"], "answer_start": [11, 11, 11]}, "prediction_text": "Seven lines."}
{"answers": {"text": ["Six", "Six", "Six"], "answer_start": [0, 0, 0]}, "prediction_text": "Seven."}
{"answers": {"text": ["Orange", "Orange", "Orange"], "answer_start": [249, 249, 249]}, "prediction_text": "Orange"}
{"answers": {"text": ["Port of Los Angeles", "the Port of Los Angeles", "Port of Los Angeles"], "answer_start": [40, 36, 40]}, "prediction_text": "Los Angeles"}
{"answers": {"text": ["Port of San Diego", "Port of Long Beach", "Port of Long Beach"], "answer_start": [196, 118, 118]}, "prediction_text": "Long Beach"}
{"answers": {"text": ["Southern", "Southern California", "Southern"], "answer_start": [0, 0, 0]}, "prediction_text": "Los Angeles"}
{"answers": {"text": ["The Tech Coast", "The Tech Coast", "Tech Coast"], "answer_start": [0, 0, 4]}, "prediction_text": "Tech Coast"}
{"answers": {"text": ["research", "research", "research"], "answer_start": [183, 183, 183]}, "prediction_text": "Research universities."}
{"answers": {"text": ["private", "private", "private"], "answer_start": [579, 579, 579]}, "prediction_text": "University of California"}
{"answers": {"text": ["5", "5", "5"], "answer_start": [270, 270, 270]}, "prediction_text": "5"}
{"answers": {"text": ["12", "12", "12"], "answer_start": [370, 370, 370]}, "prediction_text": "12"}
{"answers": {"text": ["NFL", "Professional", "NFL"], "answer_start": [72, 0, 72]}, "prediction_text": "NFL"}
{"answers": {"text": ["NBA", "NBA", "NBA"], "answer_start": [116, 116, 116]}, "prediction_text": "NBA"}
{"answers": {"text": ["MLB", "MLB", "MLB"], "answer_start": [164, 164, 164]}, "prediction_text": "Soccer"}
{"answers": {"text": ["Los Angeles Kings", "Los Angeles Kings", "Los Angeles Kings"], "answer_start": [245, 245, 245]}, "prediction_text": "Los Angeles Kings"}
{"answers": {"text": ["LA Galaxy", "LA Galaxy", "LA Galaxy"], "answer_start": [289, 289, 289]}, "prediction_text": "LA Galaxy"}
{"answers": {"text": ["Chivas USA", "Chivas", "Chivas"], "answer_start": [95, 179, 179]}, "prediction_text": "Chivas USA"}
{"answers": {"text": ["two", "two", "two"], "answer_start": [30, 30, 30]}, "prediction_text": "2"}
{"answers": {"text": ["2014", "2014", "2014"], "answer_start": [13, 215, 215]}, "prediction_text": "2014"}
{"answers": {"text": ["StubHub Center", "the StubHub Center", "StubHub Center"], "answer_start": [132, 128, 132]}, "prediction_text": "StubHub Center"}
{"answers": {"text": ["2018", "in 2018", "2018"], "answer_start": [278, 275, 278]}, "prediction_text": "2018"}
{"answers": {"text": ["College", "College", "College"], "answer_start": [0, 0, 0]}, "prediction_text": "College sports"}
{"answers": {"text": ["UCLA", "UCLA", "UCLA"], "answer_start": [60, 60, 60]}, "prediction_text": "UCLA"}
{"answers": {"text": ["Trojans", "Trojans", "Trojans"], "answer_start": [84, 84, 84]}, "prediction_text": "USC Trojans"}
{"answers": {"text": ["Pac-12", "the Pac-12", "Pac-12"], "answer_start": [135, 131, 135]}, "prediction_text": "Pac-12"}
{"answers": {"text": ["Division I", "Division I", "Division I"], "answer_start": [117, 117, 117]}, "prediction_text": "Pac-12 Conference"}
{"answers": {"text": ["Rugby", "Rugby", "Rugby"], "answer_start": [0, 0, 0]}, "prediction_text": "Rugby"}
{"answers": {"text": ["high school", "high school", "high school"], "answer_start": [74, 74, 74]}, "prediction_text": "High school"}
{"answers": {"text": ["an official school sport", "an official school", "official school sport"], "answer_start": [144, 144, 147]}, "prediction_text": "Official school sport."}
{"answers": {"text": ["BSkyB", "BSkyB", "BSkyB"], "answer_start": [98, 98, 98]}, "prediction_text": "BSkyB"}
{"answers": {"text": ["BSkyB", "BSkyB", "BSkyB"], "answer_start": [98, 98, 98]}, "prediction_text": "BSkyB"}
{"answers": {"text": ["2014", "2014", "2014"], "answer_start": [187, 187, 187]}, "prediction_text": "2014"}
{"answers": {"text": ["Sky plc", "British Sky Broadcasting Group plc", "British Sky Broadcasting Group plc"], "answer_start": [361, 306, 306]}, "prediction_text": "British Sky Broadcasting Group plc"}
{"answers": {"text": ["Sky UK Limited", "Sky UK Limited", "Sky UK Limited"], "answer_start": [471, 471, 471]}, "prediction_text": "Sky UK Limited"}
{"answers": {"text": ["2006", "2006", "2006"], "answer_start": [231, 231, 231]}, "prediction_text": "2006"}
{"answers": {"text": ["two", "two", "two"], "answer_start": [286, 286, 286]}, "prediction_text": "2"}
{"answers": {"text": ["Sky", "Sky", "Sky"], "answer_start": [370, 370, 370]}, "prediction_text": "Sky"}
{"answers": {"text": ["\u00a31.3bn", "\u00a31.3bn", "\u00a34.2bn"], "answer_start": [407, 407, 441]}, "prediction_text": "\u00a34.2 billion"}
{"answers": {"text": ["ONdigital", "ONdigital", "ONdigital"], "answer_start": [55, 55, 55]}, "prediction_text": "ONdigital"}
{"answers": {"text": ["Freeview", "ITV Digital", "ITV Digital"], "answer_start": [184, 145, 145]}, "prediction_text": "ITV Digital"}
{"answers": {"text": ["three", "three", "three"], "answer_start": [307, 307, 307]}, "prediction_text": "Three."}
{"answers": {"text": ["Sky Three", "Sky Three", "Sky Three"], "answer_start": [485, 485, 485]}, "prediction_text": "Sky Three"}
{"answers": {"text": ["Pick TV", "Pick TV", "Pick TV"], "answer_start": [553, 553, 553]}, "prediction_text": "Pick TV"}
{"answers": {"text": ["Sky+ PVR", "Sky+ PVR", "Sky+ PVR"], "answer_start": [65, 65, 65]}, "prediction_text": "Sky+ PVR with Sky+ subscription package."}
{"answers": {"text": ["September 2007", "September 2007", "March 2008"], "answer_start": [801, 801, 894]}, "prediction_text": "September 2007"}
{"answers": {"text": ["monthly fee", "a monthly fee", "SkyHD box"], "answer_start": [403, 401, 760]}, "prediction_text": "Sky+ HD Box."}
{"answers": {"text": ["January 2010", "In January 2010", "February 2011"], "answer_start": [444, 441, 652]}, "prediction_text": "January 2010"}
{"answers": {"text": ["Sky+HD Box", "Sky+HD Box", "Sky+HD Box"], "answer_start": [570, 570, 570]}, "prediction_text": "Sky+ HD Box"}
{"answers": {"text": ["VideoGuard", "VideoGuard pay-TV", "VideoGuard"], "answer_start": [19, 19, 19]}, "prediction_text": "VideoGuard"}
{"answers": {"text": ["NDS", "NDS", "NDS"], "answer_start": [64, 64, 64]}, "prediction_text": "NDS"}
{"answers": {"text": ["Cisco Systems", "Cisco Systems", "Cisco Systems company"], "answer_start": [71, 71, 71]}, "prediction_text": "Cisco Systems"}
{"answers": {"text": ["BSkyB", "BSkyB", "BSkyB"], "answer_start": [229, 229, 229]}, "prediction_text": "BSkyB"}
{"answers": {"text": ["Sky+", "Sky+", "PVR"], "answer_start": [539, 539, 516]}, "prediction_text": "Sky+"}
{"answers": {"text": ["basic channels", "the basic channels", "the basic channels"], "answer_start": [233, 229, 229]}, "prediction_text": "Basic channels."}
{"answers": {"text": ["2007", "2007", "2007"], "answer_start": [3, 3, 3]}, "prediction_text": "2007"}
{"answers": {"text": ["substantially increased the asking price", "that BSkyB had substantially increased the asking price for the channels", "increased the asking price"], "answer_start": [318, 303, 332]}, "prediction_text": "Substantially more value."}
{"answers": {"text": ["Video On Demand", "Video On Demand content", "HD channels and Video On Demand"], "answer_start": [502, 502, 486]}, "prediction_text": "Video On Demand content."}
{"answers": {"text": ["HD channels", "HD channels", "HD channels"], "answer_start": [486, 486, 486]}, "prediction_text": "HD channels."}
{"answers": {"text": ["July 2013", "In July 2013", "July 2013"], "answer_start": [3, 0, 3]}, "prediction_text": "July 2013"}
{"answers": {"text": ["2013", "2013", "31 July 2013"], "answer_start": [158, 158, 150]}, "prediction_text": "2013"}
{"answers": {"text": ["OneDrive", "OneDrive", "OneDrive"], "answer_start": [555, 555, 555]}, "prediction_text": "OneDrive."}
{"answers": {"text": ["OneDrive for Business", "OneDrive for Business", "OneDrive for Business"], "answer_start": [593, 593, 593]}, "prediction_text": "OneDrive for Business"}
{"answers": {"text": ["cloud storage", "cloud storage", "cloud storage service"], "answer_start": [288, 288, 288]}, "prediction_text": "Cloud storage service."}
{"answers": {"text": ["Sam Chisholm", "Sam Chisholm", "Sam Chisholm and Rupert Murdoch"], "answer_start": [97, 97, 97]}, "prediction_text": "Sam Chisholm"}
{"answers": {"text": ["Astra", "Astra's", "Astra's satellites"], "answer_start": [295, 295, 295]}, "prediction_text": "Astra's satellites."}
{"answers": {"text": ["27 September 2001", "27 September 2001", "September 2001"], "answer_start": [423, 423, 426]}, "prediction_text": "27 September 2001."}
{"answers": {"text": ["Sky Digital", "Sky Digital", "Sky Digital platform"], "answer_start": [481, 481, 481]}, "prediction_text": "Sky Digital platform."}
{"answers": {"text": ["3.5 million", "3.5 million", "3.5 million"], "answer_start": [876, 876, 876]}, "prediction_text": "3.5 million"}
{"answers": {"text": ["BSkyB", "British Sky Broadcasting", "British Sky Broadcasting"], "answer_start": [53, 25, 25]}, "prediction_text": "BSkyB"}
{"answers": {"text": ["telecommunications", "telecommunications", "British telecommunications company"], "answer_start": [73, 73, 65]}, "prediction_text": "British Sky Broadcasting"}
{"answers": {"text": ["11 million", "11 million", "11 million customers"], "answer_start": [321, 321, 321]}, "prediction_text": "11 million"}
{"answers": {"text": ["Freeview", "Freeview", "Freeview"], "answer_start": [428, 428, 428]}, "prediction_text": "Freeview"}
{"answers": {"text": ["Sky Q Hub", "Sky Q Hub", "Sky Q Hub"], "answer_start": [206, 206, 206]}, "prediction_text": "Sky Q Hub"}
{"answers": {"text": ["Sky Q Silver set top boxes", "the Sky Q Silver set top boxes", "Sky Q Silver"], "answer_start": [451, 447, 451]}, "prediction_text": "Sky Q Silver set top boxes."}
{"answers": {"text": ["share recordings", "to share recordings and other media", "share recordings"], "answer_start": [611, 608, 611]}, "prediction_text": "Share recordings and other media."}
{"answers": {"text": ["2016", "later in 2016", "2016"], "answer_start": [763, 754, 763]}, "prediction_text": "2016"}
{"answers": {"text": ["2016", "in 2016", "2016"], "answer_start": [94, 91, 94]}, "prediction_text": "2016"}
{"answers": {"text": ["DVB-compliant MPEG-2", "DVB-compliant MPEG-2", "MPEG-2"], "answer_start": [46, 46, 60]}, "prediction_text": "MPEG-2"}
{"answers": {"text": ["Dolby Digital", "Dolby Digital", "Dolby Digital"], "answer_start": [135, 135, 135]}, "prediction_text": "Dolby Digital."}
{"answers": {"text": ["MPEG-4", "MPEG-4", "MPEG-4"], "answer_start": [267, 267, 267]}, "prediction_text": "MPEG-4"}
{"answers": {"text": ["OpenTV", "OpenTV", "OpenTV"], "answer_start": [383, 383, 383]}, "prediction_text": "OpenTV"}
{"answers": {"text": ["DVB-S2", "DVB-S2", "DVB-compliant MPEG-2"], "answer_start": [311, 311, 46]}, "prediction_text": "DVB-S2."}
{"answers": {"text": ["1998", "1998", "1998"], "answer_start": [33, 33, 33]}, "prediction_text": "1998"}
{"answers": {"text": ["Astra 2A", "the Astra 2A", "Astra 2A"], "answer_start": [63, 59, 63]}, "prediction_text": "Astra 2A"}
{"answers": {"text": ["Eutelsat's Eurobird 1", "Eutelsat's Eurobird 1", "Eutelsat's Eurobird 1"], "answer_start": [260, 260, 260]}, "prediction_text": "Astra 2A"}
{"answers": {"text": ["hundreds", "hundreds", "hundreds"], "answer_start": [403, 403, 403]}, "prediction_text": "Hundreds."}
{"answers": {"text": ["28.5\u00b0E", "28.5\u00b0E", "28.5\u00b0E"], "answer_start": [551, 551, 551]}, "prediction_text": "28.5\u00b0E"}
{"answers": {"text": ["22 May 2006", "on 22 May 2006", "22 May 2006"], "answer_start": [45, 42, 45]}, "prediction_text": "22 May 2006"}
{"answers": {"text": ["40,000", "40,000", "40,000"], "answer_start": [98, 98, 98]}, "prediction_text": "40,000"}
{"answers": {"text": ["Thomson", "Thomson", "STB"], "answer_start": [293, 293, 270]}, "prediction_text": "Thomson"}
{"answers": {"text": ["17,000", "17,000", "17,000"], "answer_start": [495, 495, 495]}, "prediction_text": "17,000"}
{"answers": {"text": ["4,222,000", "4,222,000", "4,222,000"], "answer_start": [643, 643, 643]}, "prediction_text": "4,222,000"}
{"answers": {"text": ["8 February 2007", "On 8 February 2007", "8 February 2007"], "answer_start": [3, 0, 3]}, "prediction_text": "8 February 2007"}
{"answers": {"text": ["March", "in March", "March"], "answer_start": [412, 409, 412]}, "prediction_text": "March."}
{"answers": {"text": ["digital terrestrial", "digital terrestrial", "digital terrestrial"], "answer_start": [451, 451, 451]}, "prediction_text": "Digital terrestrial platform"}
{"answers": {"text": ["Virgin Media", "Virgin Media", "Virgin Media"], "answer_start": [535, 535, 535]}, "prediction_text": "Virgin Media"}
{"answers": {"text": ["English Premier League Football", "English Premier League Football", "sport (including English Premier League Football), films, entertainment and news"], "answer_start": [264, 264, 247]}, "prediction_text": "English Premier League Football."}
{"answers": {"text": ["free-to-view", "free-to-view", "free-to-air"], "answer_start": [327, 327, 227]}, "prediction_text": "Free-to-air unencrypted broadcasts."}
{"answers": {"text": ["monthly subscription", "a monthly subscription", "monthly subscription"], "answer_start": [375, 373, 375]}, "prediction_text": "VideoGuard UK equipped receiver."}
{"answers": {"text": ["VideoGuard UK", "VideoGuard UK", "VideoGuard UK"], "answer_start": [465, 465, 465]}, "prediction_text": "VideoGuard UK equipped receiver."}
{"answers": {"text": ["Ku band", "9.75/10.600 GHz", "universal Ku band"], "answer_start": [24, 37, 14]}, "prediction_text": "Ku band LNB (9.75/10.600 GHz)"}
{"answers": {"text": ["Sky", "Sky", "Sky service"], "answer_start": [532, 532, 532]}, "prediction_text": "Sky service"}
{"answers": {"text": ["1991", "autumn of 1991", "1991"], "answer_start": [17, 7, 17]}, "prediction_text": "1991"}
{"answers": {"text": ["ITV", "ITV", "ITV"], "answer_start": [129, 129, 129]}, "prediction_text": "ITV"}
{"answers": {"text": ["\u00a334m", "\u00a334m", "\u00a334m per year"], "answer_start": [249, 249, 249]}, "prediction_text": "\u00a334m."}
{"answers": {"text": ["BBC", "The BBC", "BBC"], "answer_start": [354, 350, 354]}, "prediction_text": "BSkyB."}
{"answers": {"text": ["\u00a3304m", "\u00a3304m", "\u00a3304m"], "answer_start": [426, 426, 426]}, "prediction_text": "\u00a334m per year."}
{"answers": {"text": ["Ofcom", "Ofcom", "Ofcom"], "answer_start": [134, 134, 134]}, "prediction_text": "Ofcom"}
{"answers": {"text": ["\u00a315\u2013100,000", "\u00a315\u2013100,000", "\u00a315\u2013100,000"], "answer_start": [283, 283, 283]}, "prediction_text": "\u00a315-100,000"}
{"answers": {"text": ["no", "no", "Third-party channels"], "answer_start": [10, 10, 296]}, "prediction_text": "No."}
{"answers": {"text": ["not", "not", "BSkyB does not carry any control"], "answer_start": [529, 529, 518]}, "prediction_text": "No."}
{"answers": {"text": ["not", "not", "BSkyB does not carry any control"], "answer_start": [529, 529, 518]}, "prediction_text": "No."}
{"answers": {"text": ["1 October 1998", "1 October 1998", "1 October 1998"], "answer_start": [51, 51, 51]}, "prediction_text": "1998"}
{"answers": {"text": ["Sky Digital", "Sky Digital", "Sky Digital"], "answer_start": [81, 81, 81]}, "prediction_text": "Sky Digital"}
{"answers": {"text": ["Sky Active", "Open", "Sky Active"], "answer_start": [434, 414, 434]}, "prediction_text": "Sky Active"}
{"answers": {"text": ["ONdigital", "ONdigital", "ONdigital"], "answer_start": [470, 470, 470]}, "prediction_text": "ONdigital (later ITV Digital) terrestrial offering and cable services."}
{"answers": {"text": ["100,000", "over 100,000", "100,000"], "answer_start": [562, 557, 562]}, "prediction_text": "100,000"}
{"answers": {"text": ["2007", "2007", "2007"], "answer_start": [28, 28, 28]}, "prediction_text": "2007"}
{"answers": {"text": ["Virgin Media", "Virgin Media", "Virgin Media"], "answer_start": [0, 0, 0]}, "prediction_text": "Virgin Media"}
{"answers": {"text": ["Video On Demand", "Video On Demand service", "(HDTV)"], "answer_start": [552, 552, 98]}, "prediction_text": "Video On Demand."}
{"answers": {"text": ["BBC HD", "BBC HD", "BBC HD"], "answer_start": [215, 215, 215]}, "prediction_text": "BBC HD"}
{"answers": {"text": ["Channel 4 HD", "Channel 4 HD", "Channel 4 HD"], "answer_start": [431, 431, 431]}, "prediction_text": "BBC HD"}
{"answers": {"text": ["10 million", "10 million", "10 million"], "answer_start": [61, 61, 61]}, "prediction_text": "10 million."}
{"answers": {"text": ["25m", "25m people", "36% of households"], "answer_start": [287, 287, 222]}, "prediction_text": "25m."}
{"answers": {"text": ["August 2004", "August 2004", "August 2004"], "answer_start": [333, 333, 333]}, "prediction_text": "August 2004"}
{"answers": {"text": ["36%", "36% of households", "2.4m customers"], "answer_start": [222, 222, 371]}, "prediction_text": "36%"}
{"answers": {"text": ["flattened", "flattened", "flattened"], "answer_start": [559, 559, 559]}, "prediction_text": "Flat."}
{"answers": {"text": ["Welfare Cash Card", "Welfare Cash Card", "Welfare Cash Card"], "answer_start": [558, 558, 558]}, "prediction_text": "Welfare Cash Card"}
{"answers": {"text": ["essentials", "only \"essentials\"", "essentials"], "answer_start": [673, 667, 673]}, "prediction_text": "Essentials."}
{"answers": {"text": ["often damaging", "often damaging", "often damaging"], "answer_start": [406, 406, 406]}, "prediction_text": "Alcohol, tobacco, and gambling."}
{"answers": {"text": ["Sky TV bills", "claimants' \"Sky TV bills", "claimants"], "answer_start": [108, 96, 96]}, "prediction_text": "Sky TV bills."}
{"answers": {"text": ["a man's presence", "mother is wrongly claiming to be living alone", "betray a man's presence in the household"], "answer_start": [290, 180, 283]}, "prediction_text": "Man's presence in the household."}
{"answers": {"text": ["\u00a330m", "\u00a330m", "\u00a330m"], "answer_start": [53, 53, 53]}, "prediction_text": "\u00a330m"}
{"answers": {"text": ["no", "no indication", "no indication"], "answer_start": [224, 224, 224]}, "prediction_text": "Yes."}
{"answers": {"text": ["Virgin Media", "Virgin Media", "Virgin Media"], "answer_start": [419, 419, 419]}, "prediction_text": "Virgin Media"}
{"answers": {"text": ["BSkyB", "BSkyB", "BSkyB"], "answer_start": [409, 409, 409]}, "prediction_text": "BSkyB"}
{"answers": {"text": ["basic channels", "the carriage of their respective basic channels", "carriage of their respective basic channels"], "answer_start": [543, 510, 514]}, "prediction_text": "The carriage of their respective basic channels."}
{"answers": {"text": ["diversified", "highly diversified", "highly diversified"], "answer_start": [34, 27, 27]}, "prediction_text": "Diversified"}
{"answers": {"text": ["second", "second", "second"], "answer_start": [266, 266, 266]}, "prediction_text": "Second."}
{"answers": {"text": ["fourth", "fourth", "fourth"], "answer_start": [315, 315, 315]}, "prediction_text": "Fourth."}
{"answers": {"text": ["Melbourne", "Melbourne", "Melbourne"], "answer_start": [536, 401, 401]}, "prediction_text": "Melbourne"}
{"answers": {"text": ["Melbourne Cricket Ground", "The Melbourne Cricket Ground", "Melbourne Cricket Ground"], "answer_start": [536, 532, 536]}, "prediction_text": "Melbourne Cricket Ground"}
{"answers": {"text": ["Bendigo", "Bendigo", "Bendigo"], "answer_start": [181, 181, 181]}, "prediction_text": "Bendigo"}
{"answers": {"text": ["New South Wales", "New South Wales", "New South Wales"], "answer_start": [348, 348, 348]}, "prediction_text": "New South Wales"}
{"answers": {"text": ["Buckland Valley", "Buckland Valley near Bright", "Buckland Valley"], "answer_start": [394, 394, 394]}, "prediction_text": "Bright"}
{"answers": {"text": ["over 1,000", "1,000", "1,000"], "answer_start": [547, 552, 552]}, "prediction_text": "1,000"}
{"answers": {"text": ["cramped and unsanitary", "cramped and unsanitary", "cramped and unsanitary"], "answer_start": [466, 466, 466]}, "prediction_text": "cramped and unsanitary"}
{"answers": {"text": ["multi-member proportional", "multi-member proportional", "multi-member proportional representation system"], "answer_start": [84, 84, 84]}, "prediction_text": "Multi-member proportional representation."}
{"answers": {"text": ["eight", "eight", "eight"], "answer_start": [172, 172, 172]}, "prediction_text": "22 two-member electorates."}
{"answers": {"text": ["five", "five", "five"], "answer_start": [226, 226, 226]}, "prediction_text": "Five."}
{"answers": {"text": ["four years", "four years", "four years"], "answer_start": [418, 418, 418]}, "prediction_text": "Four years."}
{"answers": {"text": ["every four years", "every four years", "four years"], "answer_start": [505, 505, 511]}, "prediction_text": "Every four years."}
{"answers": {"text": ["Australian Labor Party", "Australian Labor Party", "Labor"], "answer_start": [16, 16, 233]}, "prediction_text": "ALP"}
{"answers": {"text": ["Liberal Party", "Liberal Party of Australia", "Liberals"], "answer_start": [63, 63, 373]}, "prediction_text": "Labor"}
{"answers": {"text": ["National Party", "National Party of Australia", "Nationals"], "answer_start": [107, 107, 498]}, "prediction_text": "Liberal Party of Australia"}
{"answers": {"text": ["The Greens", "Australian Greens", "Greens"], "answer_start": [584, 161, 588]}, "prediction_text": "Nationals"}
{"answers": {"text": ["Labor", "Australian Labor Party", "Labor"], "answer_start": [233, 16, 233]}, "prediction_text": "Liberal Party of Australia"}
{"answers": {"text": ["61.1%", "61.1%", "61.1%"], "answer_start": [6, 6, 6]}, "prediction_text": "61.1%"}
{"answers": {"text": ["26.7%", "26.7%", "26.7%"], "answer_start": [134, 134, 134]}, "prediction_text": "26.7%"}
{"answers": {"text": ["Buddhism", "Buddhism", "Buddhism"], "answer_start": [226, 226, 226]}, "prediction_text": "Buddhism"}
{"answers": {"text": ["168,637", "168,637", "168,637"], "answer_start": [287, 287, 287]}, "prediction_text": "168,637"}
{"answers": {"text": ["20%", "20%", "20%"], "answer_start": [440, 440, 440]}, "prediction_text": "20%"}
{"answers": {"text": ["south-east", "south-east", "the south-east of Australia"], "answer_start": [48, 48, 44]}, "prediction_text": "South-east."}
{"answers": {"text": ["most densely populated", "most", "most densely populated state"], "answer_start": [97, 97, 97]}, "prediction_text": "Second most populous state."}
{"answers": {"text": ["second", "second-most", "second-most populous"], "answer_start": [134, 134, 134]}, "prediction_text": "Second-most populous."}
{"answers": {"text": ["Melbourne", "Melbourne", "Melbourne"], "answer_start": [321, 321, 321]}, "prediction_text": "Melbourne"}
{"answers": {"text": ["second-largest", "second-largest", "second-largest city"], "answer_start": [353, 353, 353]}, "prediction_text": "Second-largest city."}
{"answers": {"text": ["Koori", "Koori", "Koori"], "answer_start": [146, 146, 146]}, "prediction_text": "Koori"}
{"answers": {"text": ["1788", "1788", "1788"], "answer_start": [254, 254, 254]}, "prediction_text": "1788"}
{"answers": {"text": ["New South Wales", "New South Wales", "New South Wales"], "answer_start": [305, 305, 305]}, "prediction_text": "Sullivan Bay."}
{"answers": {"text": ["Sullivan Bay", "Sullivan Bay", "Sullivan Bay"], "answer_start": [375, 375, 375]}, "prediction_text": "Sullivan Bay"}
{"answers": {"text": ["1803", "1803", "1803"], "answer_start": [367, 367, 367]}, "prediction_text": "1803"}
{"answers": {"text": ["26,000 square kilometres", "26,000 square kilometres", "26,000 square kilometres"], "answer_start": [10, 10, 10]}, "prediction_text": "10,000 sq mi"}
{"answers": {"text": ["50%", "50%", "50%"], "answer_start": [130, 130, 130]}, "prediction_text": "50%"}
{"answers": {"text": ["6,000 square kilometres", "6,000 square kilometres", "6,000 square kilometres"], "answer_start": [208, 208, 208]}, "prediction_text": "6,000 square kilometers."}
{"answers": {"text": ["90%", "90%", "90%"], "answer_start": [401, 401, 401]}, "prediction_text": "121,200 tonnes."}
{"answers": {"text": ["270,000", "270,000", "121,200"], "answer_start": [618, 618, 590]}, "prediction_text": "270,000 tonnes."}
{"answers": {"text": ["1975", "1975", "1975"], "answer_start": [47, 47, 47]}, "prediction_text": "1975"}
{"answers": {"text": ["1855 colonial constitution", "the 1855 colonial constitution", "1855 colonial constitution"], "answer_start": [70, 66, 70]}, "prediction_text": "1855 colonial constitution."}
{"answers": {"text": ["Parliament of Victoria", "the Parliament of Victoria", "Parliament of Victoria"], "answer_start": [337, 333, 337]}, "prediction_text": "Parliament of Victoria"}
{"answers": {"text": ["\"entrenched\" provisions", "certain \"entrenched\" provisions", "\"entrenched\" provisions"], "answer_start": [380, 372, 380]}, "prediction_text": "Entrenched provisions."}
{"answers": {"text": ["Victoria Constitution Act 1855", "the Victoria Constitution Act 1855", "Victoria Constitution Act 185"], "answer_start": [145, 141, 145]}, "prediction_text": "Victoria Constitution Act 1855"}
{"answers": {"text": ["warmest regions", "semi-deserts", "semi-deserts"], "answer_start": [44, 95, 95]}, "prediction_text": "Hot winds."}
{"answers": {"text": ["32 \u00b0C", "32 \u00b0C", "32 \u00b0C (90 \u00b0F)"], "answer_start": [137, 137, 137]}, "prediction_text": "32 \u00b0C (90 \u00b0F)"}
{"answers": {"text": ["15 \u00b0C", "15 \u00b0C", "15 \u00b0C (59 \u00b0F)"], "answer_start": [169, 169, 169]}, "prediction_text": "2-7 \u00b0C (4-13 \u00b0F)"}
{"answers": {"text": ["48.8 \u00b0C", "48.8 \u00b0C", "48.8 \u00b0C (119.8 \u00b0F)"], "answer_start": [387, 387, 387]}, "prediction_text": "48.8 \u00b0C (119.8 \u00b0F)"}
{"answers": {"text": ["2009", "the 2009 southeastern Australia heat wave", "7 February 2009"], "answer_start": [462, 458, 434]}, "prediction_text": "7 February 2009"}
{"answers": {"text": ["state or government", "state or government schools", "state or government schools"], "answer_start": [89, 89, 89]}, "prediction_text": "Public schools."}
{"answers": {"text": ["Victoria Department of Education", "the Victoria Department of Education", "Victoria Department of Education"], "answer_start": [153, 149, 153]}, "prediction_text": "Victoria Department of Education"}
{"answers": {"text": ["some extra costs", "some extra costs are levied", "extra costs"], "answer_start": [226, 226, 231]}, "prediction_text": "extra costs."}
{"answers": {"text": ["Roman Catholic Church", "the Roman Catholic Church", "Roman Catholic"], "answer_start": [316, 312, 316]}, "prediction_text": "Roman Catholic Church"}
{"answers": {"text": ["curriculum", "curriculum standards", "curriculum"], "answer_start": [644, 644, 644]}, "prediction_text": "Government-set curriculum standards."}
{"answers": {"text": ["major car brands", "major car brands", "car"], "answer_start": [77, 77, 258]}, "prediction_text": "Ford"}
{"answers": {"text": ["2017", "2017", "2017"], "answer_start": [341, 341, 341]}, "prediction_text": "February 2014"}
{"answers": {"text": ["May 2013", "in May 2013", "2013"], "answer_start": [381, 378, 385]}, "prediction_text": "May 2013"}
{"answers": {"text": ["October 2016", "in October 2016", "2013"], "answer_start": [515, 512, 385]}, "prediction_text": "October 2016"}
{"answers": {"text": ["Ford", "Ford"], "answer_start": [449, 449]}, "prediction_text": "Ford"}
{"answers": {"text": ["2,000 m", "almost 2,000 m", "2,000 m"], "answer_start": [219, 212, 219]}, "prediction_text": "2,000 m (6,600 ft)"}
{"answers": {"text": ["Mount Bogong", "Mount Bogong", "Mount Bogong"], "answer_start": [244, 244, 244]}, "prediction_text": "Mount Bogong"}
{"answers": {"text": ["1,986 m", "1,986 m", "1,986 m"], "answer_start": [277, 277, 277]}, "prediction_text": "1,986 m (6,516 ft)"}
{"answers": {"text": ["river systems", "river systems", "river systems"], "answer_start": [393, 393, 393]}, "prediction_text": "Topologically diverse areas."}
{"answers": {"text": ["helmeted honeyeater", "the helmeted honeyeater", "helmeted honeyeater"], "answer_start": [845, 841, 845]}, "prediction_text": "Helmeted honeyeater"}
{"answers": {"text": ["Victorian Alps", "The Victorian Alps", "Victorian Alps"], "answer_start": [4, 0, 4]}, "prediction_text": "The Alps."}
{"answers": {"text": ["Great Dividing Range", "the Great Dividing Range", "Great Dividing Range"], "answer_start": [95, 91, 95]}, "prediction_text": "Great Dividing Range"}
{"answers": {"text": ["east-west", "east-west", "east-west"], "answer_start": [142, 142, 142]}, "prediction_text": "East-west."}
{"answers": {"text": ["below 0 \u00b0C", "below 0 \u00b0C", "below 0 \u00b0C (32 \u00b0F)"], "answer_start": [246, 246, 246]}, "prediction_text": "0 \u00b0C (32 \u00b0F)"}
{"answers": {"text": ["\u221211.7 \u00b0C", "\u221211.7 \u00b0C", "\u221211.7 \u00b0C (10.9 \u00b0F)"], "answer_start": [343, 343, 343]}, "prediction_text": "\u221211.7 \u00b0C (10.9 \u00b0F)"}
{"answers": {"text": ["government-owned", "government", "several private and public railway operators"], "answer_start": [104, 104, 42]}, "prediction_text": "The Victorian Government."}
{"answers": {"text": ["Metro Trains Melbourne", "Metro Trains Melbourne", "Metro Trains Melbourne"], "answer_start": [153, 153, 153]}, "prediction_text": "Metro Trains Melbourne"}
{"answers": {"text": ["Victorian Government", "the Victorian Government", "Victorian Government"], "answer_start": [298, 294, 298]}, "prediction_text": "Victorian Government"}
{"answers": {"text": ["freight services", "freight", "freight"], "answer_start": [476, 476, 476]}, "prediction_text": "Freight train"}
{"answers": {"text": ["passenger", "extensive, electrified, passenger system", "passenger"], "answer_start": [214, 190, 214]}, "prediction_text": "Electrified."}
{"answers": {"text": ["37", "37", "37"], "answer_start": [26, 26, 26]}, "prediction_text": "37 seats."}
{"answers": {"text": ["12", "12", "12"], "answer_start": [82, 82, 82]}, "prediction_text": "12 seats."}
{"answers": {"text": ["Legislative Assembly", "the Legislative Assembly", "Legislative Assembly"], "answer_start": [176, 172, 176]}, "prediction_text": "Legislative Assembly"}
{"answers": {"text": ["Legislative Council", "the Legislative Council", "Legislative Council"], "answer_start": [223, 219, 223]}, "prediction_text": "Legislative Council"}
{"answers": {"text": ["Linda Dessau", "Linda Dessau", "Linda Dessau"], "answer_start": [460, 460, 460]}, "prediction_text": "Linda Dessau"}
{"answers": {"text": ["1 July 1851", "1 July 1851", "1 July 1851"], "answer_start": [3, 3, 3]}, "prediction_text": "1 July 1851."}
{"answers": {"text": ["1851", "in 1851", "1851"], "answer_start": [233, 230, 233]}, "prediction_text": "1851"}
{"answers": {"text": ["gold rush", "gold rush", "gold rushes"], "answer_start": [394, 394, 394]}, "prediction_text": "The finding of gold in Victoria caused the rapid growth of the colony."}
{"answers": {"text": ["sevenfold", "sevenfold", "76,000 to 540,000"], "answer_start": [544, 544, 559]}, "prediction_text": "Sevenfold."}
{"answers": {"text": ["20 million ounces", "20 million ounces", "20 million ounces"], "answer_start": [753, 753, 753]}, "prediction_text": "20 million ounces."}
{"answers": {"text": ["1,548", "1,548", "1,548"], "answer_start": [32, 32, 32]}, "prediction_text": "1,548"}
{"answers": {"text": ["489", "489", "489"], "answer_start": [54, 54, 54]}, "prediction_text": "489"}
{"answers": {"text": ["540,800", "540,800", "540,800"], "answer_start": [115, 115, 115]}, "prediction_text": "489,800"}
{"answers": {"text": ["63,519", "63,519", "63,519"], "answer_start": [541, 541, 541]}, "prediction_text": "63,519"}
{"answers": {"text": ["61", "61", "61"], "answer_start": [212, 212, 212]}, "prediction_text": "90%"}
{"answers": {"text": ["Victoria", "Victoria", "Victoria"], "answer_start": [0, 0, 0]}, "prediction_text": "Victoria"}
{"answers": {"text": ["3 million", "3 million", "3 million"], "answer_start": [87, 87, 87]}, "prediction_text": "60%"}
{"answers": {"text": ["60%", "60%", "60%"], "answer_start": [68, 68, 68]}, "prediction_text": "60%"}
{"answers": {"text": ["two-thirds", "nearly two-thirds", "two-thirds"], "answer_start": [130, 123, 130]}, "prediction_text": "Nearly two-thirds."}
{"answers": {"text": ["Asia", "Asia", "Asia"], "answer_start": [617, 617, 617]}, "prediction_text": "Asia"}
{"answers": {"text": ["1,600 mm", "1,600 mm (5 ft 3 in) broad gauge", "1,600 mm (5 ft 3 in) broad gauge"], "answer_start": [178, 178, 178]}, "prediction_text": "1,600 mm (5 ft 3 in)"}
{"answers": {"text": ["1,435 mm", "1,435 mm (4 ft 8 1\u20442 in) standard gauge", "1,435 mm (4 ft 8 1\u20442 in) standard gauge"], "answer_start": [334, 334, 334]}, "prediction_text": "1,435 mm (4 ft 8 1\u20442 in) standard gauge."}
{"answers": {"text": ["760 mm", "760 mm (2 ft 6 in) narrow gauge lines", "760 mm (2 ft 6 in) narrow gauge lines"], "answer_start": [409, 409, 409]}, "prediction_text": "1,435 mm (4 ft 8 1\u20442 in)"}
{"answers": {"text": ["mountainous areas", "mountainous areas", "mountainous areas"], "answer_start": [531, 531, 531]}, "prediction_text": "Mountainous areas."}
{"answers": {"text": ["five", "five", "five"], "answer_start": [474, 474, 474]}, "prediction_text": "5"}
{"answers": {"text": ["1788", "1788", "1788"], "answer_start": [55, 55, 55]}, "prediction_text": "1788"}
{"answers": {"text": ["New South Wales", "New South Wales", "New South Wales"], "answer_start": [110, 110, 110]}, "prediction_text": "New South Wales"}
{"answers": {"text": ["New Holland", "New Holland", "New Holland"], "answer_start": [151, 151, 151]}, "prediction_text": "New Holland"}
{"answers": {"text": ["Sydney", "Sydney", "Sydney"], "answer_start": [219, 219, 219]}, "prediction_text": "Sydney"}
{"answers": {"text": ["1854", "1854", "1854"], "answer_start": [3, 3, 3]}, "prediction_text": "1854"}
{"answers": {"text": ["British troops", "British troops", "British troops"], "answer_start": [171, 171, 171]}, "prediction_text": "British troops"}
{"answers": {"text": ["Eureka Stockade", "Eureka Stockade", "Eureka Stockade"], "answer_start": [132, 132, 132]}, "prediction_text": "Eureka Stockade"}
{"answers": {"text": ["mining licence fees", "mining licence fees", "mining licence fees"], "answer_start": [299, 299, 299]}, "prediction_text": "Mining licence fees."}
{"answers": {"text": ["Colony of Victoria Act", "the Colony of Victoria Act", "Colony of Victoria Act 1855"], "answer_start": [455, 451, 455]}, "prediction_text": "Colony of Victoria Act 1855."}
{"answers": {"text": ["most seats", "the most seats in the Legislative Assembly", "most seats in the Legislative Assembly"], "answer_start": [83, 79, 83]}, "prediction_text": "The Premier needs to lead in the Legislative Assembly by setting the legislative and political agenda."}
{"answers": {"text": ["Premier", "The Premier is the public face of government and, with cabinet", "Premier of Victoria"], "answer_start": [127, 123, 4]}, "prediction_text": "Daniel Andrews"}
{"answers": {"text": ["representatives", "representatives elected to either house of parliament", "representatives elected to either house of parliament"], "answer_start": [250, 250, 250]}, "prediction_text": "Daniel Andrews"}
{"answers": {"text": ["Daniel Andrews", "Daniel Andrews", "Daniel Andrews"], "answer_start": [515, 515, 515]}, "prediction_text": "Daniel Andrews"}
{"answers": {"text": ["elected", "elected", "elected"], "answer_start": [266, 266, 266]}, "prediction_text": "Elected to either house of parliament."}
{"answers": {"text": ["$8.7 billion", "24%", "$8.7 billion"], "answer_start": [89, 120, 89]}, "prediction_text": "17%"}
{"answers": {"text": ["17%", "17%", "17%"], "answer_start": [82, 82, 82]}, "prediction_text": "24%"}
{"answers": {"text": ["32,463", "32,463", "32,463"], "answer_start": [204, 204, 204]}, "prediction_text": "32,463"}
{"answers": {"text": ["136,000 square kilometres", "136,000 square kilometres", "136,000 square kilometres"], "answer_start": [233, 233, 233]}, "prediction_text": "52,500 sq mi"}
{"answers": {"text": ["60%", "60%", "60%"], "answer_start": [318, 318, 318]}, "prediction_text": "60%"}
{"answers": {"text": ["tourism", "tourism", "big"], "answer_start": [37, 37, 25]}, "prediction_text": "Cultural tourism and sports tourism."}
{"answers": {"text": ["sports", "sports tourism", "sports"], "answer_start": [92, 92, 92]}, "prediction_text": "Sports tourism."}
{"answers": {"text": ["Melbourne", "Melbourne", "Melbourne"], "answer_start": [144, 144, 144]}, "prediction_text": "Melbourne"}
{"answers": {"text": ["regional cities", "in regional cities", "Phillip Island"], "answer_start": [175, 172, 257]}, "prediction_text": "Regional cities."}
{"answers": {"text": ["SurfClassic", "SurfClassic", "SurfClassic"], "answer_start": [485, 485, 485]}, "prediction_text": "Bright Autumn Festival"}
{"answers": {"text": ["the southern and central parts of France", "southern and central parts of France,", "about one-eighth"], "answer_start": [86, 90, 128]}, "prediction_text": "Southern and central parts of France."}
{"answers": {"text": ["about one-eighth the number", "about one-eighth", "about one-eighth"], "answer_start": [128, 128, 128]}, "prediction_text": "One-eighth."}
{"answers": {"text": ["from 1562 to 1598", "1562 to 1598", "562 to 1598"], "answer_start": [469, 474, 475]}, "prediction_text": "1562-1598"}
{"answers": {"text": ["the Edict of Nantes", "Edict of Nantes", "the Edict of Nantes"], "answer_start": [532, 536, 532]}, "prediction_text": "Edict of Nantes"}
{"answers": {"text": ["granted the Huguenots substantial religious, political and military autonomy", "granted the Huguenots substantial religious, political and military autonomy", "granted the Huguenots substantial religious, political and military autonomy"], "answer_start": [559, 559, 559]}, "prediction_text": "Gave the Huguenots substantial religious, political, and military autonomy."}
{"answers": {"text": ["derision", "derision", "derision"], "answer_start": [26, 26, 26]}, "prediction_text": "political power in France."}
{"answers": {"text": ["Geneva", "Geneva", "Geneva"], "answer_start": [547, 547, 547]}, "prediction_text": "Geneva"}
{"answers": {"text": ["Besan\u00e7on Hugues", "Besan\u00e7on Hugues", "Besan\u00e7on Hugues"], "answer_start": [177, 177, 177]}, "prediction_text": "Besan\u00e7on Hugues"}
{"answers": {"text": ["Amboise plot", "Amboise", "the Amboise plot"], "answer_start": [1007, 1007, 1003]}, "prediction_text": "Amboise"}
{"answers": {"text": ["1560", "1560", "1560"], "answer_start": [1023, 1023, 1023]}, "prediction_text": "1560"}
{"answers": {"text": ["availability of the Bible in vernacular languages", "the Bible in vernacular languages", "The availability of the Bible in vernacular languages"], "answer_start": [4, 20, 0]}, "prediction_text": "The availability of the Bible in vernacular languages."}
{"answers": {"text": ["Around 1294", "1294", "Around 1294"], "answer_start": [275, 282, 275]}, "prediction_text": "1294"}
{"answers": {"text": ["Guyard de Moulin", "Guyard de Moulin", "Guyard de Moulin"], "answer_start": [366, 366, 366]}, "prediction_text": "Jean de R\u00e9ly"}
{"answers": {"text": ["1487", "1487", "1487"], "answer_start": [500, 500, 500]}, "prediction_text": "Jean de R\u00e9ly."}
{"answers": {"text": ["Paris", "Paris", "Paris"], "answer_start": [491, 491, 491]}, "prediction_text": "Paris"}
{"answers": {"text": ["villes de s\u00fbret\u00e9", "\"villes de s\u00fbret\u00e9\"", "villes de s\u00fbret\u00e9"], "answer_start": [52, 51, 52]}, "prediction_text": "Villes de s\u00fbret\u00e9."}
{"answers": {"text": ["Montpellier", "Montpellier", "Montpellier"], "answer_start": [0, 0, 0]}, "prediction_text": "Montpellier"}
{"answers": {"text": ["Edict of Al\u00e8s", "Edict of Al\u00e8s", "Edict of Al\u00e8s"], "answer_start": [455, 455, 455]}, "prediction_text": "Edict of Al\u00e8s (1629)"}
{"answers": {"text": ["1622", "1622", "1622"], "answer_start": [266, 266, 266]}, "prediction_text": "1622"}
{"answers": {"text": ["1629", "1629", "1629"], "answer_start": [470, 470, 470]}, "prediction_text": "1629"}
{"answers": {"text": ["at the Cape of Good Hope", "Cape of Good Hope", "the Cape of Good Hope"], "answer_start": [29, 36, 32]}, "prediction_text": "Cape Town"}
{"answers": {"text": ["Cape Town", "Cape Town", "Cape Town"], "answer_start": [365, 365, 365]}, "prediction_text": "Cape Town"}
{"answers": {"text": ["Maria de la Queillerie", "Maria de la Queillerie", "Maria de la Queillerie"], "answer_start": [190, 190, 190]}, "prediction_text": "Fran\u00e7ois Villion (Viljoen)"}
{"answers": {"text": ["Dutch East India Company", "Dutch East India Company", "Dutch East India Company"], "answer_start": [522, 522, 522]}, "prediction_text": "Dutch East India Company"}
{"answers": {"text": ["1700", "1700", "1700"], "answer_start": [753, 753, 753]}, "prediction_text": "1700"}
{"answers": {"text": ["1624", "1624", "1624"], "answer_start": [115, 115, 115]}, "prediction_text": "1624"}
{"answers": {"text": ["Jess\u00e9 de Forest", "Jess\u00e9 de Forest", "Jess\u00e9 de Forest"], "answer_start": [71, 71, 71]}, "prediction_text": "Jess\u00e9 de Forest"}
{"answers": {"text": ["L'\u00c9glise fran\u00e7aise \u00e0 la Nouvelle-Amsterdam", "L'\u00c9glise fran\u00e7aise \u00e0 la Nouvelle-Amsterdam", "L'\u00c9glise fran\u00e7aise \u00e0 la Nouvelle-Amsterdam (the French church in New Amsterdam)"], "answer_start": [482, 482, 482]}, "prediction_text": "L'\u00c9glise fran\u00e7aise \u00e0 la Nouvelle-Amsterdam (French church in New Amsterdam)"}
{"answers": {"text": ["L'Eglise du Saint-Esprit", "L'Eglise du Saint-Esprit", "L'Eglise du Saint-Esprit"], "answer_start": [594, 594, 594]}, "prediction_text": "L'\u00c9glise fran\u00e7aise \u00e0 la Nouvelle-Amsterdam (French church in New Amsterdam)"}
{"answers": {"text": ["Brooklyn", "Brooklyn", "Brooklyn"], "answer_start": [954, 954, 954]}, "prediction_text": "Bushwick"}
{"answers": {"text": ["the Charleston Orange district", "Charleston Orange district", "Charleston Orange district"], "answer_start": [661, 665, 665]}, "prediction_text": "Charleston."}
{"answers": {"text": ["the British Landgrave Edmund Bellinger", "Edmund Bellinger", "Edmund Bellinger"], "answer_start": [955, 977, 977]}, "prediction_text": "British Landgrave Edmund Bellinger"}
{"answers": {"text": ["Pons", "Pons in France", "Pons"], "answer_start": [148, 148, 148]}, "prediction_text": "Pons"}
{"answers": {"text": ["1697", "1697", "1697"], "answer_start": [796, 796, 796]}, "prediction_text": "1697"}
{"answers": {"text": ["Charleston, South Carolina", "Charleston", "Charleston, South Carolina"], "answer_start": [75, 75, 75]}, "prediction_text": "Charleston"}
{"answers": {"text": ["William III of Orange", "Stadtholder William III of Orange", "Stadtholder William III of Orange"], "answer_start": [12, 0, 0]}, "prediction_text": "William III"}
{"answers": {"text": ["King of England", "King of England", "King of England"], "answer_start": [52, 52, 52]}, "prediction_text": "King of England"}
{"answers": {"text": ["League of Augsburg", "League of Augsburg", "League of Augsburg"], "answer_start": [194, 194, 194]}, "prediction_text": "League of Augsburg"}
{"answers": {"text": ["Dutch Republic", "Dutch Republic", "Dutch Republic"], "answer_start": [332, 332, 332]}, "prediction_text": "France"}
{"answers": {"text": ["1672", "1672", "1672"], "answer_start": [169, 169, 169]}, "prediction_text": "1672"}
{"answers": {"text": ["Edict of Fontainebleau", "Edict of Fontainebleau", "the Edict of Fontainebleau"], "answer_start": [300, 300, 296]}, "prediction_text": "Edict of Fontainebleau (1685)"}
{"answers": {"text": ["1685", "1685", "1685"], "answer_start": [324, 324, 324]}, "prediction_text": "1685"}
{"answers": {"text": ["Louis XIV", "Louis XIV", "Louis XIV"], "answer_start": [221, 221, 221]}, "prediction_text": "Edict of Fontainebleau (1685)"}
{"answers": {"text": ["500,000", "500,000", "roughly 500,000"], "answer_start": [508, 508, 500]}, "prediction_text": "500,000"}
{"answers": {"text": ["Catholic Church in France", "Catholic Church in France", "The Catholic Church in France"], "answer_start": [4, 4, 0]}, "prediction_text": "Catholic Church"}
{"answers": {"text": ["St. Bartholomew's Day massacre", "St. Bartholomew's Day massacre", "St. Bartholomew's Day massacre"], "answer_start": [209, 209, 209]}, "prediction_text": "St. Bartholomew's Day massacre."}
{"answers": {"text": ["5,000 to 30,000", "5,000 to 30,000", "5,000 to 30,000"], "answer_start": [245, 245, 245]}, "prediction_text": "5,000 to 30,000."}
{"answers": {"text": ["their own militia", "the Huguenots had their own militia"], "answer_start": [509, 491]}, "prediction_text": "They had their own militia."}
{"answers": {"text": ["some of the Huguenots were nobles trying to establish separate centers of power in southern France", "political reasons", "some of the Huguenots were nobles trying to establish separate centers of power in southern France"], "answer_start": [349, 310, 349]}, "prediction_text": "Political reasons."}
{"answers": {"text": ["between 1621 and 1629", "between 1621 and 1629", "between 1621 and 1629"], "answer_start": [205, 205, 205]}, "prediction_text": "1621-1629"}
{"answers": {"text": ["southwestern France", "southwestern France", "mainly in southwestern France"], "answer_start": [184, 184, 174]}, "prediction_text": "Southwestern France."}
{"answers": {"text": ["Henry IV", "Henry IV", "Henry IV"], "answer_start": [316, 316, 316]}, "prediction_text": "Louis XIII"}
{"answers": {"text": ["Louis XIII", "Louis XIII", "Louis XIII"], "answer_start": [444, 444, 444]}, "prediction_text": "Louis XIII"}
{"answers": {"text": ["Huguenot rebellions", "Huguenot rebellions", "the Huguenot rebellions"], "answer_start": [143, 143, 139]}, "prediction_text": "Huguenot rebellions."}
{"answers": {"text": ["one million", "Approximately one million", "Approximately one million"], "answer_start": [14, 0, 0]}, "prediction_text": "Approximately one million."}
{"answers": {"text": ["2%", "2%", "2%"], "answer_start": [70, 70, 70]}, "prediction_text": "2%"}
{"answers": {"text": ["Alsace", "Alsace", "Alsace"], "answer_start": [117, 117, 117]}, "prediction_text": "Alsace"}
{"answers": {"text": ["C\u00e9vennes", "C\u00e9vennes", "C\u00e9vennes mountain region"], "answer_start": [152, 152, 152]}, "prediction_text": "C\u00e9vennes mountain region."}
{"answers": {"text": ["Australia", "Australia", "Australia"], "answer_start": [444, 283, 283]}, "prediction_text": "France"}
{"answers": {"text": ["New Rochelle", "New Rochelle", "New Rochelle"], "answer_start": [226, 226, 226]}, "prediction_text": "New Rochelle"}
{"answers": {"text": ["New Paltz", "New Paltz", "New Paltz"], "answer_start": [271, 271, 271]}, "prediction_text": "New Paltz"}
{"answers": {"text": ["\"Huguenot Street Historic District\" in New Paltz", "Huguenot Street Historic District", "The \"Huguenot Street Historic District\" in New Paltz"], "answer_start": [286, 287, 282]}, "prediction_text": "Huguenot Street Historic District"}
{"answers": {"text": ["the oldest street in the United States of America", "the oldest street in the United States of America", "the oldest street in the United States of America"], "answer_start": [402, 402, 402]}, "prediction_text": "Huguenot Street Historic District"}
{"answers": {"text": ["Staten Island", "Staten Island", "Staten Island"], "answer_start": [515, 515, 515]}, "prediction_text": "New Rochelle"}
{"answers": {"text": ["the Dutch Republic", "Dutch Republic", "Dutch Republic"], "answer_start": [45, 49, 49]}, "prediction_text": "Netherlands"}
{"answers": {"text": ["an estimated total of 75,000 to 100,000 people", "75,000 to 100,000", "75,000 to 100,000"], "answer_start": [113, 135, 135]}, "prediction_text": "75,000 to 100,000."}
{"answers": {"text": ["ca. 2 million", "2 million", "2 million"], "answer_start": [367, 371, 371]}, "prediction_text": "2 million."}
{"answers": {"text": ["Amsterdam and the area of West Frisia", "Amsterdam and the area of West Frisia", "Amsterdam and the area of West Frisia"], "answer_start": [508, 508, 508]}, "prediction_text": "Amsterdam and West Frisia."}
{"answers": {"text": ["the revocation of the Edict of Nantes", "Edict of Nantes", "the revocation of the Edict of Nantes"], "answer_start": [6, 28, 6]}, "prediction_text": "Edict of Nantes"}
{"answers": {"text": ["Tours", "Tours", "Tours"], "answer_start": [367, 367, 367]}, "prediction_text": "Tours"}
{"answers": {"text": ["Huguon", "Huguon", "Huguon"], "answer_start": [123, 123, 123]}, "prediction_text": "Huguon"}
{"answers": {"text": ["the ghost of le roi Huguet", "ghost of le roi Huguet", "the ghost of le roi Huguet"], "answer_start": [169, 173, 169]}, "prediction_text": "Huguon"}
{"answers": {"text": ["pr\u00e9tendus r\u00e9form\u00e9s", "pr\u00e9tendus r\u00e9form\u00e9s", "pr\u00e9tendus r\u00e9form\u00e9s"], "answer_start": [382, 382, 382]}, "prediction_text": "Huguon"}
{"answers": {"text": ["night", "night", "at night"], "answer_start": [456, 456, 453]}, "prediction_text": "Night."}
{"answers": {"text": ["Canterbury", "Canterbury", "Canterbury"], "answer_start": [48, 48, 48]}, "prediction_text": "Kent"}
{"answers": {"text": ["The Weavers", "The Weavers", "The Weavers"], "answer_start": [191, 191, 191]}, "prediction_text": "Weavers' house."}
{"answers": {"text": ["economic separation", "economic separation", "worked as weavers"], "answer_start": [649, 649, 172]}, "prediction_text": "Economic separation."}
{"answers": {"text": ["Kent, particularly Sandwich, Faversham and Maidstone", "Sandwich, Faversham and Maidstone", "Sandwich, Faversham and Maidstone"], "answer_start": [767, 786, 786]}, "prediction_text": "Sandwich, Faversham, Maidstone."}
{"answers": {"text": ["a restaurant", "restaurant", "a restaurant"], "answer_start": [339, 341, 339]}, "prediction_text": "Weavers' windows."}
{"answers": {"text": ["Cork City", "Cork City", "Cork City"], "answer_start": [342, 342, 342]}, "prediction_text": "Cork City"}
{"answers": {"text": ["Dublin, Cork, Youghal and Waterford", "Dublin, Cork, Youghal and Waterford", "Dublin, Cork, Youghal and Waterford"], "answer_start": [42, 42, 42]}, "prediction_text": "Dublin, Cork, Youghal, Waterford."}
{"answers": {"text": ["Dublin", "Dublin", "Dublin"], "answer_start": [375, 375, 375]}, "prediction_text": "Dublin"}
{"answers": {"text": ["a High Sheriff and one of the founders of the Bank of Ireland", "High Sheriff", "a High Sheriff and one of the founders of the Bank of Ireland"], "answer_start": [395, 397, 395]}, "prediction_text": "High Sheriff."}
{"answers": {"text": ["1696", "1696", "1696"], "answer_start": [505, 505, 505]}, "prediction_text": "1696"}
{"answers": {"text": ["brain drain", "brain drain", "brain drain"], "answer_start": [46, 46, 46]}, "prediction_text": "Brain drain"}
{"answers": {"text": ["New France", "New France", "New France"], "answer_start": [227, 227, 227]}, "prediction_text": "New France"}
{"answers": {"text": ["non-Catholics", "non-Catholics", "non-Catholics"], "answer_start": [200, 200, 200]}, "prediction_text": "Non-Catholics."}
{"answers": {"text": ["Seven Years' War", "Seven Years' War", "Seven Years' War"], "answer_start": [481, 481, 481]}, "prediction_text": "Seven Years' War"}
{"answers": {"text": ["1759-60", "1759-60", "1759-60"], "answer_start": [634, 634, 634]}, "prediction_text": "1759-60"}
{"answers": {"text": ["Henry of Navarre", "Henry of Navarre", "Henry of Navarre"], "answer_start": [157, 157, 157]}, "prediction_text": "Henry IV"}
{"answers": {"text": ["1598", "1598", "1598"], "answer_start": [146, 146, 146]}, "prediction_text": "1598"}
{"answers": {"text": ["granted the Protestants equality with Catholics", "granted the Protestants equality", "granted the Protestants equality with Catholics under the throne and a degree of religious and political freedom within their domains"], "answer_start": [390, 390, 390]}, "prediction_text": "Protected Catholic interests."}
{"answers": {"text": ["the founding of new Protestant churches", "founding of new Protestant churches in Catholic-controlled regions", "the founding of new Protestant churches in Catholic-controlled regions"], "answer_start": [595, 599, 595]}, "prediction_text": "the founding of new Protestant churches."}
{"answers": {"text": ["Protestantism", "Protestantism", "Protestantism"], "answer_start": [246, 246, 246]}, "prediction_text": "Protestantism"}
{"answers": {"text": ["education of children as Catholics", "education of children as Catholics", "required education of children as Catholics"], "answer_start": [53, 53, 44]}, "prediction_text": "Education of children."}
{"answers": {"text": ["prohibited emigration", "prohibited emigration", "prohibited emigration"], "answer_start": [93, 93, 93]}, "prediction_text": "prohibited Protestant services, required education of children, prohibited emigration."}
{"answers": {"text": ["Four thousand", "Four thousand", "Four thousand"], "answer_start": [442, 442, 442]}, "prediction_text": "Four thousand."}
{"answers": {"text": ["\"new converts\"", "\"new converts\"", "new converts"], "answer_start": [750, 750, 751]}, "prediction_text": "New converts"}
{"answers": {"text": ["Holland, Prussia, and South Africa", "Holland, Prussia, and South Africa", "Britain as well as Holland, Prussia, and South Africa"], "answer_start": [406, 406, 387]}, "prediction_text": "Holland, Prussia, South Africa."}
{"answers": {"text": ["Switzerland and the Netherlands", "Switzerland and the Netherlands.", "Switzerland and the Netherlands"], "answer_start": [71, 71, 71]}, "prediction_text": "Switzerland and the Netherlands."}
{"answers": {"text": ["1555", "1555", "1555"], "answer_start": [201, 201, 201]}, "prediction_text": "1555"}
{"answers": {"text": ["France Antarctique", "France Antarctique", "France Antarctique"], "answer_start": [215, 215, 215]}, "prediction_text": "Fort Coligny"}
{"answers": {"text": ["1560", "1560", "1560"], "answer_start": [581, 581, 581]}, "prediction_text": "1560"}
{"answers": {"text": ["the Guanabara Confession of Faith", "Guanabara Confession of Faith", "the Guanabara Confession of Faith"], "answer_start": [905, 909, 905]}, "prediction_text": "Guanabara Confession of Faith."}
{"answers": {"text": ["Afrikaans", "Afrikaans", "Afrikaans"], "answer_start": [116, 116, 116]}, "prediction_text": "Afrikaans."}
{"answers": {"text": ["wine industry", "wine", "The wine industry"], "answer_start": [748, 748, 744]}, "prediction_text": "Wine industry."}
{"answers": {"text": ["Western Cape province", "Western Cape province"], "answer_start": [25, 25]}, "prediction_text": "Western Cape."}
{"answers": {"text": ["surnames", "names", "surnames"], "answer_start": [141, 81, 141]}, "prediction_text": "French Huguenot ancestry."}
{"answers": {"text": ["Paul Revere", "Paul Revere", "Paul Revere"], "answer_start": [0, 0, 0]}, "prediction_text": "Henry Laurens"}
{"answers": {"text": ["Henry Laurens", "Henry Laurens", "Henry Laurens"], "answer_start": [57, 57, 57]}, "prediction_text": "Henry Laurens"}
{"answers": {"text": ["Charleston, South Carolina", "Charleston", "Charleston, South Carolina"], "answer_start": [467, 467, 467]}, "prediction_text": "Charleston"}
{"answers": {"text": ["Manakin Episcopal Church", "Manakin Episcopal Church", "Manakin Episcopal Church"], "answer_start": [569, 569, 569]}, "prediction_text": "Manakin Episcopal Church"}
{"answers": {"text": ["Texas", "Texas", "Texas"], "answer_start": [715, 715, 715]}, "prediction_text": "Texas"}
{"answers": {"text": ["lace", "lace", "British lace"], "answer_start": [79, 79, 71]}, "prediction_text": "Lace industry"}
{"answers": {"text": ["'Bucks Point'", "Bucks Point", "Bucks Point"], "answer_start": [523, 524, 524]}, "prediction_text": "Bucks Point"}
{"answers": {"text": ["twenty-five widows who settled in Dover", "twenty-five widows who settled in Dover", "twenty-five widows who settled in Dover"], "answer_start": [331, 331, 331]}, "prediction_text": "19th century sources."}
{"answers": {"text": ["first half of the eighteenth century", "first half of the eighteenth century", "first half of the eighteenth century"], "answer_start": [702, 702, 702]}, "prediction_text": "First half of the eighteenth century."}
{"answers": {"text": ["Dorotheenstadt and Friedrichstadt", "Dorotheenstadt and Friedrichstadt", "Dorotheenstadt and Friedrichstadt"], "answer_start": [57, 57, 57]}, "prediction_text": "Dorotheenstadt and Friedrichstadt."}
{"answers": {"text": ["one-fifth", "one-fifth", "one-fifth"], "answer_start": [101, 101, 101]}, "prediction_text": "One-fifth."}
{"answers": {"text": ["in protest against the occupation of Prussia by Napoleon", "in protest", "in protest against the occupation of Prussia by Napoleon"], "answer_start": [299, 299, 299]}, "prediction_text": "To protest against Napoleon's occupation of Prussia."}
{"answers": {"text": ["1806-07", "1806-07.", "1806-07"], "answer_start": [359, 359, 359]}, "prediction_text": "1806-07"}
{"answers": {"text": ["Fredericia (Denmark), Berlin, Stockholm, Hamburg, Frankfurt, Helsinki, and Emden", "Fredericia (Denmark), Berlin, Stockholm, Hamburg, Frankfurt, Helsinki, and Emden", "Fredericia (Denmark), Berlin, Stockholm, Hamburg, Frankfurt, Helsinki, and Emden"], "answer_start": [480, 480, 480]}, "prediction_text": "Copenhagen, Stockholm, Hamburg, Frankfurt, Helsinki, Emden."}
{"answers": {"text": ["Prussia", "Great Elector Frederick William", "Prussia"], "answer_start": [175, 201, 175]}, "prediction_text": "Prussia"}
{"answers": {"text": ["C\u00e9vennes", "C\u00e9vennes", "C\u00e9vennes region in the south"], "answer_start": [407, 407, 407]}, "prediction_text": "Switzerland."}
{"answers": {"text": ["Camisards", "Camisards", "the Camisards"], "answer_start": [494, 494, 490]}, "prediction_text": "Camisards"}
{"answers": {"text": ["the Catholic Church in the region", "Catholic Church", "the Catholic Church in the region"], "answer_start": [538, 542, 538]}, "prediction_text": "Catholic Church"}
{"answers": {"text": ["1702 and 1709", "1702 and 1709", "1702 and 1709"], "answer_start": [699, 699, 699]}, "prediction_text": "1702-1709"}
{"answers": {"text": ["Jacksonville", "Jacksonville", "Jacksonville"], "answer_start": [172, 172, 172]}, "prediction_text": "Jacksonville"}
{"answers": {"text": ["Jean Ribault", "Jean Ribault", "Jean Ribault"], "answer_start": [60, 60, 60]}, "prediction_text": "Jean Ribault"}
{"answers": {"text": ["Fort Caroline", "Fort Caroline", "Fort Caroline"], "answer_start": [105, 105, 105]}, "prediction_text": "Fort Caroline"}
{"answers": {"text": ["Spanish", "Spanish", "Spanish"], "answer_start": [486, 609, 609]}, "prediction_text": "French"}
{"answers": {"text": ["1565", "1565", "1565"], "answer_start": [347, 347, 347]}, "prediction_text": "September 1565."}
{"answers": {"text": ["Charlesfort", "Charlesfort", "Charlesfort"], "answer_start": [216, 216, 216]}, "prediction_text": "Charlesfort"}
{"answers": {"text": ["Parris Island", "Southeastern U.S.", "Parris Island"], "answer_start": [231, 170, 231]}, "prediction_text": "Jacksonville, Florida"}
{"answers": {"text": ["Pedro Men\u00e9ndez de Avil\u00e9s", "Pedro Men\u00e9ndez de Avil\u00e9s", "Pedro Men\u00e9ndez de Avil\u00e9s"], "answer_start": [667, 667, 667]}, "prediction_text": "Pedro Men\u00e9ndez de Avil\u00e9s"}
{"answers": {"text": ["1562", "1562", "1564"], "answer_start": [77, 77, 344]}, "prediction_text": "1564"}
{"answers": {"text": ["The Wars of Religion", "The Wars of Religion", "The Wars of Religion"], "answer_start": [262, 262, 262]}, "prediction_text": "Wars of Religion"}
{"answers": {"text": ["Virginia", "Virginia", "Virginia"], "answer_start": [80, 80, 80]}, "prediction_text": "Virginia"}
{"answers": {"text": ["Lower Norfolk County", "Lower Norfolk County", "Lower Norfolk County"], "answer_start": [147, 147, 147]}, "prediction_text": "Lower Norfolk County."}
{"answers": {"text": ["Manakin Town", "Manakin Town", "Manakin Town"], "answer_start": [322, 322, 322]}, "prediction_text": "Manakin Town"}
{"answers": {"text": ["390", "390", "390"], "answer_start": [556, 556, 556]}, "prediction_text": "390"}
{"answers": {"text": ["12 May 1705", "1705", "12 May 1705"], "answer_start": [420, 427, 420]}, "prediction_text": "1705"}
{"answers": {"text": ["1568\u20131609", "1568\u20131609", "1568\u20131609"], "answer_start": [121, 121, 121]}, "prediction_text": "1568-1609"}
{"answers": {"text": ["Spain", "Spain", "Spain"], "answer_start": [71, 71, 71]}, "prediction_text": "Spain"}
{"answers": {"text": ["\"Apologie\"", "Apologie\" of William the Silent", "Apologie"], "answer_start": [241, 242, 242]}, "prediction_text": "Apology of William the Silent."}
{"answers": {"text": ["William the Silent", "Pierre L'Oyseleur", "William the Silent"], "answer_start": [490, 365, 255]}, "prediction_text": "Pierre L'Oyseleur"}
{"answers": {"text": ["Calvinist", "Calvinist", "Walloon"], "answer_start": [531, 531, 775]}, "prediction_text": "Huguenot Pierre L'Oyseleur"}
{"answers": {"text": ["Foreign Protestants Naturalization Act", "Foreign Protestants Naturalization Act,", "Foreign Protestants Naturalization Act"], "answer_start": [46, 46, 46]}, "prediction_text": "Foreign Protestants Naturalization Act."}
{"answers": {"text": ["1708", "1708", "1708"], "answer_start": [26, 26, 26]}, "prediction_text": "1708"}
{"answers": {"text": ["50,000", "50,000", "50,000"], "answer_start": [99, 99, 99]}, "prediction_text": "50,000"}
{"answers": {"text": ["Andrew Lortie", "Andrew Lortie", "Andrew Lortie"], "answer_start": [317, 317, 317]}, "prediction_text": "Andrew Lortie"}
{"answers": {"text": ["the doctrine of transubstantiation", "doctrine of transubstantiation during Mass", "the Pope and the doctrine of transubstantiation"], "answer_start": [495, 499, 482]}, "prediction_text": "Pope and doctrine of transubstantiation."}
{"answers": {"text": ["Williamite war", "Williamite", "the Williamite war"], "answer_start": [269, 269, 265]}, "prediction_text": "Williamite war"}
{"answers": {"text": ["William of Orange", "William of Orange", "William of Orange"], "answer_start": [244, 244, 244]}, "prediction_text": "William of Orange"}
{"answers": {"text": ["Dublin, Cork, Portarlington, Lisburn, Waterford and Youghal", "Dublin, Cork, Portarlington, Lisburn, Waterford and Youghal", "Dublin, Cork, Portarlington, Lisburn, Waterford and Youghal"], "answer_start": [420, 420, 420]}, "prediction_text": "Dublin, Cork, Portarlington, Lisburn, Waterford, Youghal."}
{"answers": {"text": ["flax cultivation", "flax", "flax cultivation"], "answer_start": [579, 579, 579]}, "prediction_text": "Flax cultivation."}
{"answers": {"text": ["Irish linen industry", "linen", "Irish linen"], "answer_start": [618, 624, 618]}, "prediction_text": "Flax cultivation."}
{"answers": {"text": ["Prince Louis de Cond\u00e9", "Louis de Cond\u00e9", "Prince Louis de Cond\u00e9"], "answer_start": [0, 7, 0]}, "prediction_text": "Count Ludwig von Nassau-Saarbr\u00fccken"}
{"answers": {"text": ["Count Ludwig von Nassau-Saarbr\u00fccken", "Ludwig von Nassau-Saarbr\u00fccken", "Count Ludwig von Nassau-Saarbr\u00fccken"], "answer_start": [92, 98, 92]}, "prediction_text": "Ludwig von Nassau-Saarbr\u00fccken"}
{"answers": {"text": ["glass-making", "glass-making", "glass-making"], "answer_start": [352, 352, 352]}, "prediction_text": "Glass-making works"}
{"answers": {"text": ["1890s", "1890s", "1890s"], "answer_start": [754, 754, 754]}, "prediction_text": "1890s"}
{"answers": {"text": ["1604", "1604", "1604"], "answer_start": [189, 189, 189]}, "prediction_text": "1604"}
{"answers": {"text": ["Electorate of Brandenburg and Electorate of the Palatinate", "the Electorate of Brandenburg and Electorate of the Palatinate", "the Electorate of Brandenburg and Electorate of the Palatinate"], "answer_start": [158, 154, 154]}, "prediction_text": "England and Wales."}
{"answers": {"text": ["Protestant", "Protestant", "Protestant"], "answer_start": [42, 42, 42]}, "prediction_text": "Protestant European nations."}
{"answers": {"text": ["Quebec", "Quebec", "Quebec"], "answer_start": [468, 468, 468]}, "prediction_text": "Quebec"}
{"answers": {"text": ["Dutch Cape Colony", "Dutch Cape Colony", "Dutch Cape Colony"], "answer_start": [337, 337, 337]}, "prediction_text": "Dutch Cape Colony"}
{"answers": {"text": ["they were accepted and allowed to worship freely", "allowed to worship freely", "they were accepted and allowed to worship freely"], "answer_start": [482, 505, 482]}, "prediction_text": "Freedom."}
{"answers": {"text": ["Hugues Capet", "Hugues Capet", "Hugues Capet"], "answer_start": [265, 265, 265]}, "prediction_text": "Hugues Capet"}
{"answers": {"text": ["The \"Hugues hypothesis\"", "\"Hugues hypothesis\"", "Hugues hypothesis"], "answer_start": [188, 192, 193]}, "prediction_text": "Hugues hypothesis"}
{"answers": {"text": ["Janet Gray", "Janet Gray", "Janet Gray"], "answer_start": [442, 442, 442]}, "prediction_text": "Janet Gray"}
{"answers": {"text": ["little Hugos, or those who want Hugo", "little Hugos", "little Hugos, or those who want Hugo."], "answer_start": [555, 555, 555]}, "prediction_text": "Hugues Capet."}
{"answers": {"text": ["double or triple non-French linguistic origins", "non-French linguistic origins"], "answer_start": [24, 41]}, "prediction_text": "Hugues hypothesis."}
{"answers": {"text": ["Jacques Lefevre", "Jacques Lefevre", "Jacques Lefevre"], "answer_start": [104, 104, 104]}, "prediction_text": "Jacques Lefevre"}
{"answers": {"text": ["University of Paris", "University of Paris", "University of Paris"], "answer_start": [375, 375, 375]}, "prediction_text": "University of Paris"}
{"answers": {"text": ["1530", "1523", "1530"], "answer_start": [513, 453, 513]}, "prediction_text": "1530"}
{"answers": {"text": ["William Farel", "William Farel", "William Farel"], "answer_start": [519, 519, 519]}, "prediction_text": "Jean Cauvin"}
{"answers": {"text": ["Jean Cauvin (John Calvin)", "Jean Cauvin", "Jean Cauvin"], "answer_start": [663, 663, 663]}, "prediction_text": "Jean Cauvin"}
{"answers": {"text": ["24 August \u2013 3 October 1572", "24 August \u2013 3 October 1572", "24 August \u2013 3 October 1572"], "answer_start": [62, 62, 62]}, "prediction_text": "24 August 1572"}
{"answers": {"text": ["Catholics", "Catholics", "Catholics"], "answer_start": [90, 90, 90]}, "prediction_text": "Catholics"}
{"answers": {"text": ["Nearly 3,000", "Nearly 3,000", "Nearly 3,000"], "answer_start": [352, 352, 352]}, "prediction_text": "3,000"}
{"answers": {"text": ["1573", "1573", "1573"], "answer_start": [773, 773, 773]}, "prediction_text": "1573"}
{"answers": {"text": ["almost 25,000", "almost 25,000", "almost 25,000"], "answer_start": [636, 636, 636]}, "prediction_text": "25,000"}
{"answers": {"text": ["Louis XIV", "Louis XIV", "Louis XIV"], "answer_start": [0, 0, 0]}, "prediction_text": "Louis XIV"}
{"answers": {"text": ["acted increasingly aggressively to force the Huguenots to convert", "aggressively", "increasingly aggressively"], "answer_start": [40, 59, 46]}, "prediction_text": "He issued Edict of Fontainebleau."}
{"answers": {"text": ["he sent missionaries, backed by a fund to financially reward converts", "missionaries", "At first he sent missionaries, backed by a fund to financially reward converts to Catholicism"], "answer_start": [116, 124, 107]}, "prediction_text": "Missionaries."}
{"answers": {"text": ["closed Huguenot schools", "closed Huguenot schools", "closed Huguenot schools and excluded them from favored professions"], "answer_start": [229, 229, 229]}, "prediction_text": "Closed Huguenot schools."}
{"answers": {"text": ["dragonnades", "dragonnades", "dragonnades"], "answer_start": [323, 323, 323]}, "prediction_text": "Dragonnades."}
{"answers": {"text": ["Westchester", "Westchester", "Westchester"], "answer_start": [39, 39, 39]}, "prediction_text": "Westchester"}
{"answers": {"text": ["\"Bauffet's Point\"", "Bauffet's Point", "Bauffet's Point"], "answer_start": [235, 236, 236]}, "prediction_text": "Long Island Sound."}
{"answers": {"text": ["John Pell, Lord of Pelham Manor", "John Pell", "John Pell"], "answer_start": [435, 435, 435]}, "prediction_text": "John Pell, Lord of Pelham Manor."}
{"answers": {"text": ["La Rochelle", "La Rochelle", "La Rochelle"], "answer_start": [593, 593, 593]}, "prediction_text": "La Rochelle"}
{"answers": {"text": ["Trinity-St. Paul's Episcopal Church", "Trinity-St. Paul's Episcopal Church", "Trinity-St. Paul's Episcopal Church"], "answer_start": [986, 986, 986]}, "prediction_text": "Trinity-St. Paul's Episcopal Church"}
{"answers": {"text": ["affiliated with other Protestant denominations", "affiliated with other Protestant denominations", "affiliated with other Protestant denominations with more numerous members"], "answer_start": [80, 80, 80]}, "prediction_text": "Adapted quickly and often married outside their immediate French communities."}
{"answers": {"text": ["married outside their immediate French communities", "married outside their immediate French communities", "adapted quickly and often married outside their immediate French communities"], "answer_start": [195, 195, 169]}, "prediction_text": "Married outside their French communities."}
{"answers": {"text": ["E.I. du Pont", "E.I. du Pont", "E.I. du Pont"], "answer_start": [599, 599, 599]}, "prediction_text": "E.I. du Pont"}
{"answers": {"text": ["into the nineteenth century", "well into the nineteenth century", "well into the nineteenth century"], "answer_start": [388, 383, 383]}, "prediction_text": "Well into the nineteenth century."}
{"answers": {"text": ["Eleutherian gunpowder mills", "Eleutherian gunpowder mills.", "Eleutherian"], "answer_start": [660, 660, 660]}, "prediction_text": "Eleutherian"}
{"answers": {"text": ["Pierre Bayle", "Pierre Bayle", "Pierre Bayle"], "answer_start": [67, 67, 67]}, "prediction_text": "Pierre Bayle"}
{"answers": {"text": ["Rotterdam", "Rotterdam", "Rotterdam"], "answer_start": [104, 104, 104]}, "prediction_text": "Rotterdam"}
{"answers": {"text": ["Historical and Critical Dictionary", "Historical and Critical Dictionary", "Historical and Critical Dictionary"], "answer_start": [186, 186, 186]}, "prediction_text": "Historical and Critical Dictionary."}
{"answers": {"text": ["US Library of Congress", "US Library of Congress", "US Library of Congress"], "answer_start": [273, 273, 273]}, "prediction_text": "US Library of Congress"}
{"answers": {"text": ["Saint Nicolas", "Saint Nicolas", "Saint Nicolas"], "answer_start": [696, 696, 696]}, "prediction_text": "Saint Nicolas"}
{"answers": {"text": ["The French Protestant Church of London", "The French Protestant Church of London", "The French Protestant Church of London"], "answer_start": [0, 0, 0]}, "prediction_text": "French Protestant Church of London"}
{"answers": {"text": ["1550", "1550", "1550"], "answer_start": [75, 75, 75]}, "prediction_text": "1550"}
{"answers": {"text": ["Soho Square", "Soho Square", "Soho Square"], "answer_start": [102, 102, 102]}, "prediction_text": "Soho Square"}
{"answers": {"text": ["Shoreditch", "Shoreditch", "Shoreditch"], "answer_start": [144, 144, 144]}, "prediction_text": "Shoreditch"}
{"answers": {"text": ["1724", "1724", "1724"], "answer_start": [447, 447, 447]}, "prediction_text": "1724"}
{"answers": {"text": ["Lutheran and Reformed", "Lutheran and Reformed", "Lutheran and Reformed"], "answer_start": [57, 57, 57]}, "prediction_text": "Lutheran and Reformed."}
{"answers": {"text": ["Germany and Scandinavia", "Germany and Scandinavia", "Germany and Scandinavia"], "answer_start": [89, 89, 89]}, "prediction_text": "Germany, Scandinavia, and the Baltic."}
{"answers": {"text": ["Edict of Potsdam", "Edict of Potsdam", "Edict of Potsdam"], "answer_start": [270, 270, 270]}, "prediction_text": "Edict of Potsdam."}
{"answers": {"text": ["Elector of Brandenburg and Duke of Prussia", "Elector of Brandenburg and Duke of Prussia", "Elector of Brandenburg and Duke of Prussia"], "answer_start": [402, 402, 402]}, "prediction_text": "Elector of Brandenburg and Duke of Prussia."}
{"answers": {"text": ["Huguenots furnished two new regiments", "two new regiments", "furnished two new regiments of his army"], "answer_start": [450, 470, 460]}, "prediction_text": "Two new regiments."}
{"answers": {"text": ["Frederick William", "Frederick William", "Frederick William"], "answer_start": [0, 0, 0]}, "prediction_text": "Frederick William"}
{"answers": {"text": ["Theodor Fontane", "Theodor Fontane", "Theodor Fontane"], "answer_start": [268, 268, 268]}, "prediction_text": "Theodor Fontane"}
{"answers": {"text": ["Adolf Galland", "Adolf Galland", "Adolf Galland"], "answer_start": [403, 403, 403]}, "prediction_text": "Adolf Galland"}
{"answers": {"text": ["Lothar de Maizi\u00e8re", "Lothar de Maizi\u00e8re", "Lothar de Maizi\u00e8re"], "answer_start": [588, 588, 588]}, "prediction_text": "Lothar de Maizi\u00e8re"}
{"answers": {"text": ["Federal Minister of the Interior", "Minister of the Interior", "Federal Minister of the Interior"], "answer_start": [668, 676, 668]}, "prediction_text": "Minister of the Interior"}
{"answers": {"text": ["solar", "solar power", "solar power, nuclear power or geothermal energy", "solar"], "answer_start": [149, 149, 149, 149]}, "prediction_text": "Solar power."}
{"answers": {"text": ["Rankine", "Rankine cycle", "Rankine cycle", "Rankine cycle"], "answer_start": [283, 283, 283, 283]}, "prediction_text": "Rankine cycle"}
{"answers": {"text": ["steam", "steam", "steam", "steam"], "answer_start": [348, 348, 348, 348]}, "prediction_text": "Steam"}
{"answers": {"text": ["high", "high", "high pressure", "high"], "answer_start": [385, 385, 385, 385]}, "prediction_text": "High pressure."}
{"answers": {"text": ["external combustion", "external combustion", "external combustion engines", "external combustion"], "answer_start": [18, 18, 18, 18]}, "prediction_text": "External combustion engines, non-combustion heat sources, solar power, nuclear power, geothermal energy."}
{"answers": {"text": ["atmospheric engine", "atmospheric engine", "atmospheric engine", "the atmospheric engine"], "answer_start": [117, 117, 117, 113]}, "prediction_text": "Atmospheric engine"}
{"answers": {"text": ["Thomas Newcomen", "Thomas Newcomen", "Thomas Newcomen", "Thomas Newcomen"], "answer_start": [149, 149, 149, 149]}, "prediction_text": "Thomas Newcomen"}
{"answers": {"text": ["1712", "around 1712", "1712", "1712"], "answer_start": [172, 165, 172, 172]}, "prediction_text": "1712"}
{"answers": {"text": ["steam pump", "steam pump", "steam pump", "steam pump"], "answer_start": [214, 214, 214, 214]}, "prediction_text": "Steam pump"}
{"answers": {"text": ["Papin", "Papin", "Papin", "Papin"], "answer_start": [256, 256, 256, 256]}, "prediction_text": "Thomas Newcomen"}
{"answers": {"text": ["United Kingdom", "United Kingdom", "United Kingdom"], "answer_start": [93, 93, 93]}, "prediction_text": "United Kingdom"}
{"answers": {"text": ["21 February 1804", "21 February 1804", "21 February 1804"], "answer_start": [116, 116, 116]}, "prediction_text": "21 February 1804"}
{"answers": {"text": ["Abercynon", "Abercynon in south Wales", "Abercynon"], "answer_start": [309, 309, 309]}, "prediction_text": "Abercynon"}
{"answers": {"text": ["Wales", "south Wales", "Wales"], "answer_start": [328, 322, 328]}, "prediction_text": "Wales"}
{"answers": {"text": ["south", "south", "south"], "answer_start": [322, 322, 322]}, "prediction_text": "South Wales"}
{"answers": {"text": ["water pump", "water pump", "water pump"], "answer_start": [58, 58, 58]}, "prediction_text": "Water pump"}
{"answers": {"text": ["multi-stage centrifugal", "multi-stage centrifugal", "multi-stage centrifugal pumps"], "answer_start": [190, 190, 190]}, "prediction_text": "Multi-stage centrifugal pumps."}
{"answers": {"text": ["1850s", "1850s", "1850s"], "answer_start": [417, 417, 417]}, "prediction_text": "1850s"}
{"answers": {"text": ["steam locomotives", "steam locomotives", "steam locomotives"], "answer_start": [485, 485, 485]}, "prediction_text": "Steam locomotives."}
{"answers": {"text": ["lower-pressure boiler feed water", "water", "lower-pressure boiler feed water"], "answer_start": [279, 306, 279]}, "prediction_text": "Steam locomotives."}
{"answers": {"text": ["three", "three or four", "three"], "answer_start": [204, 204, 204]}, "prediction_text": "3"}
{"answers": {"text": ["quadruple expansion engines", "quadruple", "quadruple expansion engines"], "answer_start": [263, 263, 263]}, "prediction_text": "Triple expansion engines."}
{"answers": {"text": ["19th", "19th", "19th"], "answer_start": [729, 729, 729]}, "prediction_text": "19th century"}
{"answers": {"text": ["marine triple expansion", "marine triple expansion", "marine triple expansion engines"], "answer_start": [805, 805, 805]}, "prediction_text": "Y-S-T engines."}
{"answers": {"text": ["Olympic", "Olympic", "Olympic class"], "answer_start": [1178, 1178, 1178]}, "prediction_text": "Olympic class"}
{"answers": {"text": ["Corliss", "patent valve", "Corliss"], "answer_start": [849, 87, 849]}, "prediction_text": "Stephenson, Joy, Walschaerts."}
{"answers": {"text": ["Joy", "Joy", "Joy"], "answer_start": [820, 820, 820]}, "prediction_text": "Corliss."}
{"answers": {"text": ["lengthening rubbing surfaces of the valve", "lengthening rubbing surfaces", "lengthening rubbing surfaces of the valve"], "answer_start": [461, 461, 461]}, "prediction_text": "Lengthening rubbing surfaces."}
{"answers": {"text": ["Lead fusible plugs", "Lead fusible plugs", "Lead fusible plugs"], "answer_start": [0, 0, 0]}, "prediction_text": "Lead fusible plugs."}
{"answers": {"text": ["melts", "melts", "the lead melts"], "answer_start": [179, 179, 170]}, "prediction_text": "Melts."}
{"answers": {"text": ["steam escapes", "steam escapes,", "the steam escapes"], "answer_start": [193, 193, 189]}, "prediction_text": "The steam escapes."}
{"answers": {"text": ["manually suppress the fire", "manually suppress the fire", "manually suppress the fire"], "answer_start": [244, 244, 244]}, "prediction_text": "Manually suppress the fire."}
{"answers": {"text": ["dampening the fire", "dampening the fire", "dampening the fire"], "answer_start": [344, 344, 344]}, "prediction_text": "Depressurize the boiler."}
{"answers": {"text": ["James Watt", "James Watt", "James Watt", "James Watt"], "answer_start": [8, 8, 8, 8]}, "prediction_text": "James Watt"}
{"answers": {"text": ["rotary", "rotary", "rotary motion", "continuous rotary motion"], "answer_start": [68, 68, 68, 57]}, "prediction_text": "Rotary motion."}
{"answers": {"text": ["ten", "ten-horsepower", "ten-horsepower", "ten"], "answer_start": [90, 90, 90, 90]}, "prediction_text": "Ten horsepower."}
{"answers": {"text": ["1883", "1883", "1883", "1883"], "answer_start": [267, 267, 267, 267]}, "prediction_text": "1883"}
{"answers": {"text": ["Industrial Revolution", "Industrial Revolution", "Industrial Revolution", "the Industrial Revolution"], "answer_start": [386, 386, 386, 382]}, "prediction_text": "Industrial Revolution"}
{"answers": {"text": ["first", "first century AD", "first century AD", "first century AD"], "answer_start": [61, 61, 61, 61]}, "prediction_text": "AD"}
{"answers": {"text": ["Hero of Alexandria", "Hero of Alexandria", "Hero of Alexandria", "Hero of Alexandria"], "answer_start": [176, 176, 176, 176]}, "prediction_text": "Hero of Alexandria"}
{"answers": {"text": ["Greek", "Greek", "Greek", "Greek"], "answer_start": [156, 156, 156, 156]}, "prediction_text": "Greek"}
{"answers": {"text": ["Giovanni Branca", "Giovanni Branca", "Giovanni Branca", "Giovanni Branca"], "answer_start": [458, 458, 458, 458]}, "prediction_text": "Giovanni Branca"}
{"answers": {"text": ["1606", "1606", "1606", "1606"], "answer_start": [532, 532, 532, 532]}, "prediction_text": "1606"}
{"answers": {"text": ["compound", "compound", "compound engines"], "answer_start": [33, 33, 33]}, "prediction_text": "Compound engines"}
{"answers": {"text": ["expansions", "expansions", "expansions"], "answer_start": [254, 254, 254]}, "prediction_text": "Expansion, double, triple expansion."}
{"answers": {"text": ["shipping", "shipping", "shipping"], "answer_start": [335, 335, 335]}, "prediction_text": "Shipping"}
{"answers": {"text": ["internal combustion engines", "internal combustion engines", "internal combustion engines"], "answer_start": [546, 546, 546]}, "prediction_text": "Internal combustion engines."}
{"answers": {"text": ["coal", "coal"], "answer_start": [399, 399]}, "prediction_text": "Steam turbines."}
{"answers": {"text": ["steam turbines", "steam turbines", "steam turbines"], "answer_start": [68, 68, 68]}, "prediction_text": "Steam turbines"}
{"answers": {"text": ["late", "late part", "late"], "answer_start": [99, 99, 99]}, "prediction_text": "Late part of the 19th century."}
{"answers": {"text": ["several hundred", "several hundred horsepower", "several hundred"], "answer_start": [238, 238, 238]}, "prediction_text": "several hundred horsepower"}
{"answers": {"text": ["90", "90%", "90%"], "answer_start": [691, 691, 691]}, "prediction_text": "90%"}
{"answers": {"text": ["electric", "electric", "electric"], "answer_start": [624, 702, 624]}, "prediction_text": "Electric power."}
{"answers": {"text": ["burning combustible materials", "burning combustible materials", "burning combustible materials"], "answer_start": [120, 120, 120]}, "prediction_text": "Burning combustible materials."}
{"answers": {"text": ["combustion chamber", "combustion chamber", "combustion chamber"], "answer_start": [220, 220, 220]}, "prediction_text": "combustion chamber"}
{"answers": {"text": ["solar", "solar", "solar"], "answer_start": [321, 321, 321]}, "prediction_text": "Solar energy."}
{"answers": {"text": ["electric", "electric heating element", "electric"], "answer_start": [475, 475, 475]}, "prediction_text": "Electric heating element."}
{"answers": {"text": ["steam engine indicator", "steam engine indicator", "steam engine indicator"], "answer_start": [81, 81, 81]}, "prediction_text": "Steam engine indicator"}
{"answers": {"text": ["1851", "1851", "1851"], "answer_start": [135, 135, 135]}, "prediction_text": "1851"}
{"answers": {"text": ["Charles Porter", "Charles Porter", "Charles Porter"], "answer_start": [241, 241, 241]}, "prediction_text": "Charles Porter"}
{"answers": {"text": ["Charles Richard", "Charles Richard", "Charles Richard"], "answer_start": [259, 259, 259]}, "prediction_text": "Charles Richard"}
{"answers": {"text": ["London Exhibition", "London Exhibition", "London Exhibition"], "answer_start": [292, 292, 292]}, "prediction_text": "London Exhibition"}
{"answers": {"text": ["90", "90\u00b0", "90\u00b0"], "answer_start": [123, 123, 123]}, "prediction_text": "90\u00b0 out of phase."}
{"answers": {"text": ["180", "180\u00b0", "180\u00b0"], "answer_start": [313, 313, 313]}, "prediction_text": "180\u00b0."}
{"answers": {"text": ["90", "90\u00b0 to each other", "90\u00b0"], "answer_start": [343, 343, 343]}, "prediction_text": "90\u00b0."}
{"answers": {"text": ["counterflow", "counterflow", "counterflow"], "answer_start": [95, 95, 95]}, "prediction_text": "counterflow"}
{"answers": {"text": ["two", "two", "two"], "answer_start": [234, 234, 234]}, "prediction_text": "2 strokes."}
{"answers": {"text": ["one", "one", "one"], "answer_start": [204, 204, 204]}, "prediction_text": "2"}
{"answers": {"text": ["four", "four", "four"], "answer_start": [279, 279, 279]}, "prediction_text": "4"}
{"answers": {"text": ["expansion", "expansion", "expansion"], "answer_start": [304, 304, 304]}, "prediction_text": "Expansion."}
{"answers": {"text": ["Quasiturbine", "Quasiturbine", "Quasiturbine"], "answer_start": [905, 905, 905]}, "prediction_text": "Quasiturbine"}
{"answers": {"text": ["counterflow", "counterflow", "counterflow"], "answer_start": [74, 74, 74]}, "prediction_text": "Counterflow cycle."}
{"answers": {"text": ["port", "additional port", "an additional port"], "answer_start": [401, 390, 387]}, "prediction_text": "Port uncovered by the piston."}
{"answers": {"text": ["oscillating cylinder", "oscillating cylinder", "oscillating"], "answer_start": [3, 3, 3]}, "prediction_text": "Expansion steam engine."}
{"answers": {"text": ["trunnion", "trunnion", "trunnion"], "answer_start": [334, 334, 334]}, "prediction_text": "Trunnion"}
{"answers": {"text": ["models", "models", "models"], "answer_start": [387, 387, 387]}, "prediction_text": "Models."}
{"answers": {"text": ["ships", "ships", "ships"], "answer_start": [488, 488, 488]}, "prediction_text": "Ships."}
{"answers": {"text": ["recycled continuously", "recycled continuously", "recycled continuously"], "answer_start": [101, 101, 101]}, "prediction_text": "Recycled continuously."}
{"answers": {"text": ["open loop", "open loop", "open loop"], "answer_start": [138, 138, 138]}, "prediction_text": "Open loop system."}
{"answers": {"text": ["Mercury", "Mercury", "Mercury"], "answer_start": [455, 455, 455]}, "prediction_text": "Mercury"}
{"answers": {"text": ["water", "water", "water"], "answer_start": [293, 293, 293]}, "prediction_text": "Water"}
{"answers": {"text": ["working fluid", "working fluid", "the working fluid"], "answer_start": [60, 60, 56]}, "prediction_text": "Working fluid."}
{"answers": {"text": ["565", "565 \u00b0C", "565 \u00b0C"], "answer_start": [274, 274, 274]}, "prediction_text": "565 \u00b0C"}
{"answers": {"text": ["stainless steel", "stainless steel", "stainless steel"], "answer_start": [301, 301, 301]}, "prediction_text": "Stainless steel."}
{"answers": {"text": ["63%", "63%", "63%"], "answer_start": [415, 415, 415]}, "prediction_text": "63%"}
{"answers": {"text": ["30 \u00b0C", "30 \u00b0C", "30 \u00b0C"], "answer_start": [356, 356, 356]}, "prediction_text": "30 \u00b0C."}
{"answers": {"text": ["Steam engines", "Steam engines", "Steam engines", "Steam engines"], "answer_start": [0, 0, 0, 0]}, "prediction_text": "Steam engines."}
{"answers": {"text": ["steamboats", "steamboats", "steamboats and road vehicles", "steamboats"], "answer_start": [271, 271, 271, 271]}, "prediction_text": "Railway locomotives, ships, steamboats."}
{"answers": {"text": ["Stanley Steamer", "Stanley Steamer", "Stanley Steamer", "Stanley Steamer"], "answer_start": [515, 515, 515, 515]}, "prediction_text": "Stanley Steamer"}
{"answers": {"text": ["factories", "factories", "factories", "factories"], "answer_start": [144, 144, 144, 144]}, "prediction_text": "Factories, mills, mines."}
{"answers": {"text": ["increase in the land available for cultivation", "increase in the land available for cultivation", "farm tractors", "an increase in the land available for cultivation"], "answer_start": [336, 336, 437, 333]}, "prediction_text": "Increased land available for cultivation."}
{"answers": {"text": ["Catch Me Who Can", "Catch Me Who Can", "Catch Me Who Can"], "answer_start": [90, 90, 90]}, "prediction_text": "Catch Me Who Can"}
{"answers": {"text": ["Matthew Murray", "Matthew Murray", "Matthew Murray"], "answer_start": [192, 192, 192]}, "prediction_text": "Matthew Murray"}
{"answers": {"text": ["twin-cylinder", "twin-cylinder", "twin-cylinder"], "answer_start": [154, 154, 154]}, "prediction_text": "Twin-cylinder locomotive"}
{"answers": {"text": ["Middleton Railway", "Middleton Railway", "Middleton Railway"], "answer_start": [251, 251, 251]}, "prediction_text": "Middleton Railway"}
{"answers": {"text": ["Stockton and Darlington", "Stockton and Darlington Railway", "Stockton and Darlington Railway"], "answer_start": [325, 325, 325]}, "prediction_text": "The Rocket"}
{"answers": {"text": ["Arthur Woolf", "Arthur Woolf", "Arthur Woolf"], "answer_start": [102, 102, 102]}, "prediction_text": "Arthur Woolf"}
{"answers": {"text": ["British", "British", "British"], "answer_start": [85, 85, 85]}, "prediction_text": "British"}
{"answers": {"text": ["torque variability", "torque variability", "torque variability"], "answer_start": [661, 661, 661]}, "prediction_text": "Heat loss."}
{"answers": {"text": ["cylinder volume", "cylinder", "cylinder volume"], "answer_start": [761, 761, 761]}, "prediction_text": "cylinder volume"}
{"answers": {"text": ["90", "90%", "90%"], "answer_start": [81, 81, 81]}, "prediction_text": "90%"}
{"answers": {"text": ["reciprocating steam engines", "reciprocating", "reciprocating steam engines"], "answer_start": [826, 826, 826]}, "prediction_text": "Diesel engines."}
{"answers": {"text": ["gas turbines", "gas turbines", "gas turbines"], "answer_start": [908, 908, 908]}, "prediction_text": "Gas turbines."}
{"answers": {"text": ["steam turbines", "steam turbines with reduction gearing", "steam turbines"], "answer_start": [575, 575, 575]}, "prediction_text": "Diesel engines, gas turbines."}
{"answers": {"text": ["reduction", "reduction", "reduction"], "answer_start": [595, 595, 595]}, "prediction_text": "Reduction gearing."}
{"answers": {"text": ["Rankine cycle", "The Rankine cycle", "The Rankine cycle"], "answer_start": [4, 0, 0]}, "prediction_text": "Rankine cycle"}
{"answers": {"text": ["removed in a condenser", "removed in a condenser", "removed in a condenser"], "answer_start": [478, 478, 478]}, "prediction_text": "Removed in a condenser."}
{"answers": {"text": ["1990s", "1990s", "1990s"], "answer_start": [589, 589, 589]}, "prediction_text": "1990s"}
{"answers": {"text": ["biomass", "biomass", "biomass"], "answer_start": [717, 717, 717]}, "prediction_text": "Biomass plants."}
{"answers": {"text": ["Scottish", "Scottish", "Scottish"], "answer_start": [808, 808, 808]}, "prediction_text": "Scottish"}
{"answers": {"text": ["duty", "duty"], "answer_start": [70, 70]}, "prediction_text": "Duty."}
{"answers": {"text": ["17", "17.", "17"], "answer_start": [529, 529, 529]}, "prediction_text": "17 million."}
{"answers": {"text": ["7 million", "7 million", "7 million"], "answer_start": [378, 378, 378]}, "prediction_text": "7 million."}
{"answers": {"text": ["94", "94 pounds", "94 pounds"], "answer_start": [300, 300, 300]}, "prediction_text": "94 pounds."}
{"answers": {"text": ["Watt", "Watt", "Watt"], "answer_start": [121, 121, 121]}, "prediction_text": "Watt"}
{"answers": {"text": ["steam turbines", "internal combustion engines", "Reciprocating piston", "steam turbines"], "answer_start": [308, 159, 0, 308]}, "prediction_text": "Steam turbines."}
{"answers": {"text": ["Reciprocating piston", "steam turbines", "electric motors and internal combustion", "Reciprocating piston type"], "answer_start": [0, 308, 139, 0]}, "prediction_text": "Electric motors."}
{"answers": {"text": ["turbine", "turbine type", "electric motors", "turbine type"], "answer_start": [428, 428, 139, 428]}, "prediction_text": "Turbine type steam engine."}
{"answers": {"text": ["internal combustion", "internal combustion", "internal combustion"], "answer_start": [159, 159, 159]}, "prediction_text": "Steam turbines"}
{"answers": {"text": ["Thomas Savery", "Thomas Savery.", "Thomas Savery", "Thomas Savery"], "answer_start": [81, 81, 81, 81]}, "prediction_text": "Thomas Savery"}
{"answers": {"text": ["water pump", "water pump", "vacuum", "water pump"], "answer_start": [48, 48, 133, 48]}, "prediction_text": "Water pump"}
{"answers": {"text": ["1698", "1698", "1698", "1698"], "answer_start": [73, 73, 73, 73]}, "prediction_text": "1698"}
{"answers": {"text": ["Bento de Moura Portugal", "Bento de Moura Portugal", "Bento de Moura Portugal", "Bento de Moura Portugal"], "answer_start": [555, 555, 555, 555]}, "prediction_text": "Bento de Moura Portugal"}
{"answers": {"text": ["John Smeaton", "John Smeaton", "John Smeaton", "John Smeaton"], "answer_start": [698, 698, 698, 698]}, "prediction_text": "John Smeaton"}
{"answers": {"text": ["Richard Trevithick", "Richard Trevithick", "Richard Trevithick", "Richard Trevithick"], "answer_start": [12, 12, 12, 12]}, "prediction_text": "Richard Trevithick"}
{"answers": {"text": ["Oliver Evans", "Oliver Evans", "Oliver Evans", "Oliver Evans"], "answer_start": [48, 48, 48, 48]}, "prediction_text": "Richard Trevithick"}
{"answers": {"text": ["1802", "1802", "1802", "1802"], "answer_start": [170, 170, 170, 170]}, "prediction_text": "1802"}
{"answers": {"text": ["transport", "transport", "transport applications", "transport"], "answer_start": [289, 289, 289, 289]}, "prediction_text": "Transport applications."}
{"answers": {"text": ["power", "power", "power source"], "answer_start": [460, 460, 460]}, "prediction_text": "Power source"}
{"answers": {"text": ["Energiprojekt AB", "Energiprojekt AB", "Energiprojekt AB"], "answer_start": [219, 219, 219]}, "prediction_text": "Energiprojekt AB"}
{"answers": {"text": ["Sweden", "Sweden", "Sweden"], "answer_start": [239, 239, 239]}, "prediction_text": "Sweden"}
{"answers": {"text": ["5", "5-cylinder", "5"], "answer_start": [439, 439, 439]}, "prediction_text": "5 cylinders."}
{"answers": {"text": ["8.8", "8.8", "8.8"], "answer_start": [521, 521, 521]}, "prediction_text": "4 kg (8.8 lb)"}
{"answers": {"text": ["27-30", "27-30%", "27-30%"], "answer_start": [385, 385, 385]}, "prediction_text": "27-30%"}
{"answers": {"text": ["surface condensers", "surface condensers", "surface condensers"], "answer_start": [60, 60, 60]}, "prediction_text": "Surface condensers."}
{"answers": {"text": ["automobile radiator", "automobile radiator", "an automobile radiator"], "answer_start": [395, 395, 392]}, "prediction_text": "Automobile radiator."}
{"answers": {"text": ["where water is costly", "where water is costly", "locations where water is costly"], "answer_start": [440, 440, 430]}, "prediction_text": "Ocean, rivers, lakes."}
{"answers": {"text": ["wet", "wet", "wet"], "answer_start": [476, 476, 476]}, "prediction_text": "Wet cooling tower."}
{"answers": {"text": ["3600", "3600", "3600"], "answer_start": [921, 921, 921]}, "prediction_text": "3600 cubic meters."}
{"answers": {"text": ["centrifugal governor", "centrifugal governor", "centrifugal governor"], "answer_start": [4, 4, 4]}, "prediction_text": "centrifugal governor"}
{"answers": {"text": ["Boulton", "Boulton", "Boulton"], "answer_start": [106, 106, 106]}, "prediction_text": "Boulton & Watt"}
{"answers": {"text": ["flour mill", "flour mill", "a flour mill"], "answer_start": [127, 127, 125]}, "prediction_text": "Flour mill Boulton & Watt."}
{"answers": {"text": ["cotton spinning", "operations requiring constant speed", "cotton spinning"], "answer_start": [608, 563, 608]}, "prediction_text": "Cotton spinning."}
{"answers": {"text": ["hold a set speed", "hold a set speed", "hold a set speed"], "answer_start": [200, 200, 200]}, "prediction_text": "Holding a set speed."}
{"answers": {"text": ["1880", "1880", "1880"], "answer_start": [124, 124, 124]}, "prediction_text": "1880"}
{"answers": {"text": ["railway locomotives", "railway locomotives", "railway locomotives"], "answer_start": [164, 164, 164]}, "prediction_text": "Railway locomotives."}
{"answers": {"text": ["complicated", "complicated", "complicated"], "answer_start": [216, 216, 216]}, "prediction_text": "Road engines."}
{"answers": {"text": ["1930", "1930", "1930"], "answer_start": [423, 423, 423]}, "prediction_text": "1930"}
{"answers": {"text": ["road engines", "road engines", "road engines"], "answer_start": [65, 65, 65]}, "prediction_text": "Railroads"}
{"answers": {"text": ["shortening the cutoff", "\"shortening the cutoff\"", "shortening the cutoff"], "answer_start": [274, 273, 274]}, "prediction_text": "Shortcut cutoff."}
{"answers": {"text": ["kick back", "kick back", "kick back"], "answer_start": [751, 751, 751]}, "prediction_text": "Kick back."}
{"answers": {"text": ["evacuate the cylinder", "evacuate the cylinder", "evacuate the cylinder"], "answer_start": [682, 682, 682]}, "prediction_text": "Choke the cylinder."}
{"answers": {"text": ["fixed", "fixed", "fixed"], "answer_start": [40, 40, 40]}, "prediction_text": "Fixed length."}
{"answers": {"text": ["Jer\u00f3nimo de Ayanz y Beaumont", "Jer\u00f3nimo de Ayanz y Beaumont", "Jer\u00f3nimo de Ayanz y Beaumont", "Jer\u00f3nimo de Ayanz y Beaumont"], "answer_start": [135, 135, 135, 135]}, "prediction_text": "Jer\u00f3nimo de Ayanz y Beaumont"}
{"answers": {"text": ["Spanish", "Spanish", "Spanish", "Spanish"], "answer_start": [118, 118, 118, 118]}, "prediction_text": "Spanish"}
{"answers": {"text": ["1606", "1606", "1606", "1606"], "answer_start": [212, 212, 212, 212]}, "prediction_text": "1606"}
{"answers": {"text": ["1698", "1698", "1698", "1698"], "answer_start": [221, 221, 221, 221]}, "prediction_text": "1698"}
{"answers": {"text": ["1712", "1712", "1712", "1712"], "answer_start": [591, 591, 591, 591]}, "prediction_text": "1712"}
{"answers": {"text": ["rotating discs", "rotating discs", "rotating discs"], "answer_start": [48, 48, 48]}, "prediction_text": "Discs"}
{"answers": {"text": ["drive shaft", "a drive shaft", "drive shaft"], "answer_start": [77, 75, 77]}, "prediction_text": "drive shaft"}
{"answers": {"text": ["static discs", "static discs)", "static discs"], "answer_start": [128, 128, 128]}, "prediction_text": "Static discs."}
{"answers": {"text": ["turbine casing", "turbine casing", "the turbine casing"], "answer_start": [155, 155, 151]}, "prediction_text": "The stators are attached to the turbine casing."}
{"answers": {"text": ["3600 revolutions per minute", "3600 revolutions per minute", "3600 revolutions per minute"], "answer_start": [1061, 1061, 1061]}, "prediction_text": "3600 RPM."}
{"answers": {"text": ["lower", "lower", "lower", "lower"], "answer_start": [96, 96, 96, 96]}, "prediction_text": "Lower."}
{"answers": {"text": ["electric motors", "electric motors", "internal combustion engines or electric motors", "electric motors"], "answer_start": [232, 232, 201, 232]}, "prediction_text": "Electric motors."}
{"answers": {"text": ["steam turbine", "steam turbine", "steam turbine plant", "steam turbine plant"], "answer_start": [297, 297, 297, 297]}, "prediction_text": "Steam turbine plants."}
{"answers": {"text": ["Advanced Steam", "Advanced Steam movement", "cogeneration processes", "Advanced Steam movement"], "answer_start": [581, 581, 506, 581]}, "prediction_text": "Advanced Steam movement"}
{"answers": {"text": ["pollution", "pollution", "Advanced Steam movement", "pollution"], "answer_start": [432, 432, 581, 432]}, "prediction_text": "Pollution."}
{"answers": {"text": ["Wankel", "Wankel", "the Wankel engine"], "answer_start": [82, 82, 78]}, "prediction_text": "Wankel engine."}
{"answers": {"text": ["cylinders and valve gear", "cylinders and valve gear", "cylinders and valve gear"], "answer_start": [112, 112, 112]}, "prediction_text": "Valve gear, cylinders, piston."}
{"answers": {"text": ["thermal expansion", "thermal expansion", "thermal expansion"], "answer_start": [509, 509, 509]}, "prediction_text": "thermal expansion"}
{"answers": {"text": ["1775", "1763\u20131775", "1775", "1775"], "answer_start": [61, 56, 61, 61]}, "prediction_text": "1775"}
{"answers": {"text": ["condenser", "a separate condenser", "condenser", "a separate condenser"], "answer_start": [125, 114, 125, 114]}, "prediction_text": "Condenser."}
{"answers": {"text": ["half", "half as much", "half as much coal", "half"], "answer_start": [174, 174, 174, 174]}, "prediction_text": "Half."}
{"answers": {"text": ["Newcomen's", "Newcomen's", "Newcomen's and Watt's", "Newcomen"], "answer_start": [230, 242, 242, 242]}, "prediction_text": "John Smeaton"}
{"answers": {"text": ["piston", "a piston", "piston", "a piston"], "answer_start": [342, 340, 342, 340]}, "prediction_text": "piston"}
{"answers": {"text": ["two", "two", "two"], "answer_start": [33, 33, 33]}, "prediction_text": "2"}
{"answers": {"text": ["plug valve", "plug valve", "a plug valve"], "answer_start": [286, 286, 284]}, "prediction_text": "Plug valve."}
{"answers": {"text": ["adjustable spring-loaded", "adjustable spring-loaded", "adjustable spring-loaded valve"], "answer_start": [642, 642, 642]}, "prediction_text": "Adjustable spring-loaded valve."}
{"answers": {"text": ["seal", "seal", "a seal"], "answer_start": [754, 754, 752]}, "prediction_text": "Seal."}
{"answers": {"text": ["more power", "more power", "greater steam pressure and more power"], "answer_start": [569, 569, 542]}, "prediction_text": "power from the engine."}
{"answers": {"text": ["Corliss steam engine", "Corliss", "the Corliss steam engine"], "answer_start": [42, 42, 38]}, "prediction_text": "Corliss steam engine"}
{"answers": {"text": ["1849", "1849", "1849"], "answer_start": [76, 76, 76]}, "prediction_text": "1849"}
{"answers": {"text": ["30%", "30% less steam", "30%"], "answer_start": [386, 386, 386]}, "prediction_text": "30% less steam."}
{"answers": {"text": ["four", "four", "four"], "answer_start": [94, 94, 94]}, "prediction_text": "Four."}
{"answers": {"text": ["Rumford medal", "Rumford medal", "the Rumford medal"], "answer_start": [238, 238, 234]}, "prediction_text": "Rumford medal"}
{"answers": {"text": ["thermodynamic", "thermodynamic theory", "thermodynamic theory"], "answer_start": [56, 56, 56]}, "prediction_text": "Thermodynamic theory"}
{"answers": {"text": ["Watt", "Watt", "Watt"], "answer_start": [333, 333, 333]}, "prediction_text": "Joseph Black"}
{"answers": {"text": ["condenser", "separate condenser", "the separate condenser"], "answer_start": [401, 392, 388]}, "prediction_text": "The separate condenser."}
{"answers": {"text": ["Joseph Black", "Joseph Black", "Joseph Black"], "answer_start": [502, 502, 502]}, "prediction_text": "Joseph Black"}
{"answers": {"text": ["latent heat", "latent heat", "latent heat"], "answer_start": [442, 442, 442]}, "prediction_text": "Latent heat."}
{"answers": {"text": ["during the compression stage relatively little work is required to drive the pump", "relatively little work is required to drive the pump,", "during the compression stage relatively little work is required to drive the pump"], "answer_start": [76, 105, 76]}, "prediction_text": "Low work required."}
{"answers": {"text": ["liquid", "liquid phase", "liquid"], "answer_start": [190, 190, 190]}, "prediction_text": "Liquid phase."}
{"answers": {"text": ["1% to 3%", "1% to 3%", "1% to 3%"], "answer_start": [287, 287, 287]}, "prediction_text": "1% to 3%"}
{"answers": {"text": ["1500 \u00b0C", "1500 \u00b0C", "1500 \u00b0C"], "answer_start": [532, 532, 532]}, "prediction_text": "1500 \u00b0C"}
{"answers": {"text": ["injector", "condensers", "injector"], "answer_start": [54, 112, 54]}, "prediction_text": "Injector"}
{"answers": {"text": ["recover the latent heat of vaporisation", "recover the latent heat of vaporisation", "recover the latent heat of vaporisation"], "answer_start": [152, 152, 152]}, "prediction_text": "Recycle the water."}
{"answers": {"text": ["superheaters", "superheaters", "superheaters"], "answer_start": [197, 197, 197]}, "prediction_text": "Superheaters."}
{"answers": {"text": ["bunker", "bunker", "bunker"], "answer_start": [478, 478, 478]}, "prediction_text": "bunker"}
{"answers": {"text": ["stoking", "Mechanical stoker", "a chain or screw stoking mechanism"], "answer_start": [378, 507, 361]}, "prediction_text": "Stoking mechanism"}
{"answers": {"text": ["feed water", "water", "feed water"], "answer_start": [63, 68, 63]}, "prediction_text": "feed water"}
{"answers": {"text": ["British", "British", "British"], "answer_start": [265, 265, 265]}, "prediction_text": "British"}
{"answers": {"text": ["dreadnought battleships", "dreadnought battleships", "dreadnought battleships"], "answer_start": [361, 361, 361]}, "prediction_text": "HMS Dreadnought"}
{"answers": {"text": ["ocean liners", "ocean liners", "ocean liners"], "answer_start": [390, 390, 390]}, "prediction_text": "Ocean liners."}
{"answers": {"text": ["1905", "1905", "1905"], "answer_start": [423, 423, 423]}, "prediction_text": "1905"}
{"answers": {"text": ["water", "water", "water"], "answer_start": [67, 67, 67]}, "prediction_text": "steam"}
{"answers": {"text": ["turbine", "turbine", "a turbine connected to an electrical generator"], "answer_start": [104, 104, 102]}, "prediction_text": "A turbine."}
{"answers": {"text": ["electrical generator", "electrical generator", "a turbo generator set with propulsion provided by electric motors"], "answer_start": [128, 128, 354]}, "prediction_text": "Electrical generator."}
{"answers": {"text": ["turbo-electric transmission", "turbo-electric transmission,", "turbo-electric transmission"], "answer_start": [302, 302, 302]}, "prediction_text": "Turbo generator set."}
{"answers": {"text": ["Britain", "Britain", "Britain"], "answer_start": [644, 644, 644]}, "prediction_text": "Sweden and Britain."}
{"answers": {"text": ["practical Carnot cycle", "practical Carnot cycle", "a practical Carnot cycle"], "answer_start": [48, 48, 46]}, "prediction_text": "Carnot cycle"}
{"answers": {"text": ["in the condenser", "in the condenser", "in the condenser"], "answer_start": [240, 240, 240]}, "prediction_text": "In the condenser."}
{"answers": {"text": ["constant pressure", "constant pressure", "constant pressure"], "answer_start": [272, 272, 272]}, "prediction_text": "Constant pressure."}
{"answers": {"text": ["isothermal", "isothermal", "isothermal"], "answer_start": [326, 326, 326]}, "prediction_text": "Theoretical Carnot cycle"}
{"answers": {"text": ["liquid", "a liquid", "gaseous"], "answer_start": [506, 504, 704]}, "prediction_text": "Liquid"}
{"answers": {"text": ["8", "8", "8", "8", "8"], "answer_start": [61, 61, 61, 61, 61]}, "prediction_text": "8"}
{"answers": {"text": ["helium", "helium", "helium", "helium", "helium"], "answer_start": [331, 331, 331, 331, 331]}, "prediction_text": "Hydrogen"}
{"answers": {"text": ["two atoms", "two", "two", "two", "two"], "answer_start": [377, 377, 377, 377, 377]}, "prediction_text": "2 atoms."}
{"answers": {"text": ["almost half", "almost half", "half", "almost half", "half"], "answer_start": [788, 788, 795, 788, 795]}, "prediction_text": "20.8%"}
{"answers": {"text": ["Diatomic oxygen", "Diatomic oxygen", "Diatomic oxygen gas", "Diatomic oxygen", "Diatomic oxygen gas"], "answer_start": [485, 485, 485, 485, 485]}, "prediction_text": "Dioxygen"}
{"answers": {"text": ["20.8%", "20.8%", "20.8%", "20.8%", "20.8%"], "answer_start": [517, 517, 517, 517, 517]}, "prediction_text": "20.8%"}
{"answers": {"text": ["Oxygen", "Oxygen", "Oxygen", "Oxygen", "Oxygen"], "answer_start": [661, 661, 661, 661, 661]}, "prediction_text": "Oxygen"}
{"answers": {"text": ["8", "8", "8", "8", "8"], "answer_start": [61, 61, 61, 61, 61]}, "prediction_text": "8"}
{"answers": {"text": ["monitoring of atmospheric oxygen levels show a global downward trend", "down", "down", "down", "downward"], "answer_start": [559, 613, 613, 613, 613]}, "prediction_text": "Going up."}
{"answers": {"text": ["By mass, oxygen is the third-most abundant element in the universe, after hydrogen and helium", "oxygen is the third-most abundant element in the universe, after hydrogen and helium", "oxygen is the third-most abundant element in the universe, after hydrogen and helium", "oxygen is the third-most abundant element in the universe, after hydrogen and helium", "By mass, oxygen is the third-most abundant element in the universe, after hydrogen and helium."], "answer_start": [244, 253, 253, 253, 244]}, "prediction_text": "Hydrogen, helium, and oxygen."}
{"answers": {"text": ["8", "8", "8", "8", "8"], "answer_start": [61, 61, 61, 61, 61]}, "prediction_text": "8"}
{"answers": {"text": ["chalcogen", "chalcogen", "chalcogen", "chalcogen", "the chalcogen group"], "answer_start": [86, 86, 86, 86, 82]}, "prediction_text": "Chalcogen group"}
{"answers": {"text": ["oxides", "oxides", "oxides", "oxide compounds", "oxide"], "answer_start": [216, 216, 216, 737, 737]}, "prediction_text": "Oxides."}
{"answers": {"text": ["third", "third-most", "third", "third-most", "third"], "answer_start": [267, 267, 267, 267, 267]}, "prediction_text": "Third."}
{"answers": {"text": ["dioxygen", "diatomic gas", "dioxygen", "dioxygen", "dioxygen"], "answer_start": [415, 450, 415, 415, 415]}, "prediction_text": "dioxygen"}
{"answers": {"text": ["photosynthesis", "photosynthesis", "photosynthesis", "photosynthesis", "photosynthesis"], "answer_start": [392, 392, 392, 392, 392]}, "prediction_text": "Photosynthesis."}
{"answers": {"text": ["sunlight", "sunlight", "sunlight", "the energy of sunlight", "sunlight"], "answer_start": [433, 433, 433, 419, 433]}, "prediction_text": "Sunlight."}
{"answers": {"text": ["high-altitude ozone layer", "ozone", "ozone layer", "ozone"], "answer_start": [724, 659, 738, 659]}, "prediction_text": "Ozone."}
{"answers": {"text": ["oxygen", "oxygen", "oxygen", "oxygen", "oxygen"], "answer_start": [127, 268, 127, 127, 268]}, "prediction_text": "Oxygen"}
{"answers": {"text": ["water", "water", "organic molecules", "inorganic compounds", "mass"], "answer_start": [294, 294, 22, 151, 240]}, "prediction_text": "Proteins, nucleic acids, carbohydrates, and fats."}
{"answers": {"text": ["photosynthesis", "living organisms", "photosynthesis", "photosynthesis", "photosynthesis"], "answer_start": [392, 605, 392, 392, 392]}, "prediction_text": "Photosynthesis."}
{"answers": {"text": ["water", "water", "water", "water", "water"], "answer_start": [465, 465, 465, 465, 465]}, "prediction_text": "Water."}
{"answers": {"text": ["ozone", "ozone", "ozone", "ozone"], "answer_start": [738, 659, 659, 659]}, "prediction_text": "Allotrope"}
{"answers": {"text": ["Robert Boyle", "Robert Boyle", "Boyle", "Robert Boyle"], "answer_start": [26, 26, 33, 26]}, "prediction_text": "John Mayow"}
{"answers": {"text": ["John Mayow", "John Mayow", "Mayow", "John Mayow"], "answer_start": [100, 100, 105, 100]}, "prediction_text": "John Mayow"}
{"answers": {"text": ["nitroaereus", "nitroaereus", "nitroaereus", "nitroaereus"], "answer_start": [485, 485, 485, 485]}, "prediction_text": "Nitroaereus."}
{"answers": {"text": ["1679", "1679", "1679", "1679"], "answer_start": [117, 117, 117, 117]}, "prediction_text": "1679"}
{"answers": {"text": ["Robert Boyle", "Robert Boyle", "Boyle", "Robert Boyle"], "answer_start": [26, 26, 33, 26]}, "prediction_text": "John Mayow"}
{"answers": {"text": ["nitroaereus", "spiritus nitroaereus", "spiritus nitroaereus", "spiritus nitroaereus or just nitroaereus"], "answer_start": [234, 205, 205, 205]}, "prediction_text": "spiritus nitroaereus"}
{"answers": {"text": ["17th century", "17th", "17th", "late 17th century"], "answer_start": [12, 12, 12, 7]}, "prediction_text": "17th century"}
{"answers": {"text": ["respiration", "respiration", "respiration", "respiration"], "answer_start": [517, 517, 517, 517]}, "prediction_text": "Respiration."}
{"answers": {"text": ["John Mayow", "John Mayow", "Mayow", "John Mayow"], "answer_start": [100, 100, 105, 100]}, "prediction_text": "John Mayow"}
{"answers": {"text": ["Joseph Priestley", "Priestley", "Joseph Priestley", "Joseph Priestley", "Priestley"], "answer_start": [85, 551, 85, 85, 551]}, "prediction_text": "Joseph Priestley"}
{"answers": {"text": ["clergyman", "clergyman", "clergyman", "clergyman", "clergyman"], "answer_start": [75, 75, 75, 75, 75]}, "prediction_text": "Clergyman"}
{"answers": {"text": ["HgO", "HgO", "HgO", "HgO", "HgO"], "answer_start": [138, 138, 138, 138, 138]}, "prediction_text": "HgO"}
{"answers": {"text": ["mercuric oxide (HgO)", "mercuric oxide", "mercuric oxide", "mercuric oxide (HgO)", "mercuric oxide"], "answer_start": [122, 122, 122, 122, 122]}, "prediction_text": "HgO"}
{"answers": {"text": ["mercuric oxide", "mercuric oxide", "mercuric oxide", "mercuric oxide (HgO)", "mercuric oxide"], "answer_start": [122, 122, 122, 122, 122]}, "prediction_text": "Mercuric oxide (HgO)"}
{"answers": {"text": ["dephlogisticated air", "dephlogisticated air", "dephlogisticated air", "dephlogisticated air", "\"dephlogisticated air"], "answer_start": [196, 196, 196, 196, 195]}, "prediction_text": "dephlogisticated air"}
{"answers": {"text": ["1775", "1775", "1775", "1775", "1775"], "answer_start": [587, 587, 587, 587, 587]}, "prediction_text": "1775"}
{"answers": {"text": ["published his findings first", "he published his findings first", "he published his findings first", "he published his findings first", "Because he published his findings first"], "answer_start": [779, 776, 776, 776, 768]}, "prediction_text": "He published his findings first."}
{"answers": {"text": ["active", "more active and lived longer", "was more active and lived longer", "more active and lived longer while breathing it", "was more active and lived longer"], "answer_start": [294, 289, 285, 289, 285]}, "prediction_text": "Longer lived."}
{"answers": {"text": ["Leonardo da Vinci", "Leonardo da Vinci", "Leonardo da Vinci", "Leonardo da Vinci", "da Vinci"], "answer_start": [529, 529, 529, 529, 538]}, "prediction_text": "Leonardo da Vinci"}
{"answers": {"text": ["Philo of Byzantium", "Philo of Byzantium", "Philo", "Philo", "Philo of Byzantium"], "answer_start": [146, 146, 190, 146, 146]}, "prediction_text": "Philo of Byzantium"}
{"answers": {"text": ["2nd century BCE", "2nd century BCE", "2nd century BCE", "2nd century BCE", "2nd century BCE"], "answer_start": [103, 103, 103, 103, 103]}, "prediction_text": "2nd century BCE"}
{"answers": {"text": ["incorrectly", "incorrectly", "incorrectly", "incorrectly", "incorrectly"], "answer_start": [347, 347, 347, 347, 347]}, "prediction_text": "Leonardo da Vinci"}
{"answers": {"text": ["Philo of Byzantium", "Philo of Byzantium", "Philo", "Philo", "Philo of Byzantium"], "answer_start": [146, 146, 190, 146, 146]}, "prediction_text": "Philo of Byzantium"}
{"answers": {"text": ["fire", "the classical element fire", "fire", "the classical element fire", "fire"], "answer_start": [446, 424, 446, 424, 446]}, "prediction_text": "classical element fire"}
{"answers": {"text": ["Pneumatica", "Pneumatica", "Pneumatica", "Pneumatica", "Pneumatica"], "answer_start": [178, 178, 178, 178, 178]}, "prediction_text": "Pneumatica"}
{"answers": {"text": ["Leonardo da Vinci", "Leonardo da Vinci", "Leonardo da Vinci", "Leonardo da Vinci", "da Vinci"], "answer_start": [529, 529, 529, 529, 538]}, "prediction_text": "Leonardo da Vinci"}
{"answers": {"text": ["air", "air", "air", "air"], "answer_start": [600, 600, 600, 600]}, "prediction_text": "Air."}
{"answers": {"text": ["heat or a spark", "oxygen", "an ignition event", "an ignition event, such as heat or a spark", "an ignition event"], "answer_start": [195, 31, 168, 168, 168]}, "prediction_text": "ignition event"}
{"answers": {"text": ["Oxygen is the oxidant", "the oxidant", "oxidant", "oxidant", "oxidant"], "answer_start": [245, 255, 259, 259, 259]}, "prediction_text": "Oxidant."}
{"answers": {"text": ["compounds of oxygen with a high oxidative", "compounds of oxygen with a high oxidative potential", "compounds of oxygen with a high oxidative potential", "compounds of oxygen with a high oxidative potential", "compounds of oxygen with a high oxidative potential"], "answer_start": [398, 398, 398, 398, 398]}, "prediction_text": "Oxygen-containing compounds."}
{"answers": {"text": ["Oxygen", "Oxygen", "Oxygen", "Oxygen", "Oxygen"], "answer_start": [245, 245, 245, 245, 245]}, "prediction_text": "Oxygen"}
{"answers": {"text": ["ignition event", "ignition event", "an ignition event", "ignition event", "an ignition event"], "answer_start": [171, 171, 168, 171, 168]}, "prediction_text": "ignition event"}
{"answers": {"text": ["oxidant", "the oxidant", "oxidant", "the oxidant", "oxidant"], "answer_start": [259, 255, 259, 255, 259]}, "prediction_text": "Oxygen."}
{"answers": {"text": ["rapid combustion", "rapid combustion", "rapid combustion", "rapid combustion", "rapid combustion"], "answer_start": [46, 46, 46, 46, 46]}, "prediction_text": "Fire and explosion hazards."}
{"answers": {"text": ["chemical energy", "chemical energy", "chemical energy", "most of the chemical energy"], "answer_start": [325, 325, 325, 313]}, "prediction_text": "Oxidant"}
{"answers": {"text": ["compounds of oxygen", "peroxides, chlorates, nitrates, perchlorates, and dichromates", "peroxides, chlorates, nitrates, perchlorates, and dichromates", "peroxides, chlorates, nitrates, perchlorates, and dichromates", "peroxides, chlorates, nitrates, perchlorates, and dichromates"], "answer_start": [398, 459, 459, 459, 459]}, "prediction_text": "Peroxides, chlorates, nitrates, perchlorates, and dichromates."}
{"answers": {"text": ["pure O"], "answer_start": [435]}, "prediction_text": "Oxygen"}
{"answers": {"text": ["oxygen", "oxygen", "oxygen"], "answer_start": [160, 160, 160]}, "prediction_text": "Oxygen."}
{"answers": {"text": ["1\u20443", "1\u20443", "1\u20443", "1\u20443", "1\u20443 normal pressure"], "answer_start": [507, 507, 507, 507, 507]}, "prediction_text": "1\u20443 normal pressure."}
{"answers": {"text": ["special training", "special training", "special training to ensure that ignition sources are minimized", "special training", "special training"], "answer_start": [252, 252, 252, 252, 252]}, "prediction_text": "training."}
{"answers": {"text": ["combustion", "combustion", "combustion", "combustion", "combustion"], "answer_start": [28, 28, 28, 28, 28]}, "prediction_text": "combustion"}
{"answers": {"text": ["storage vessels", "fire", "ensure that ignition sources are minimized", "special training to ensure that ignition sources are minimized"], "answer_start": [93, 320, 272, 252]}, "prediction_text": "Ignition sources."}
{"answers": {"text": ["special training", "special training", "special training to ensure that ignition sources are minimized", "special training", "special training"], "answer_start": [252, 252, 252, 252, 252]}, "prediction_text": "Steel pipes and storage vessels."}
{"answers": {"text": ["Apollo 1 crew", "the Apollo 1 crew", "the Apollo 1 crew", "Apollo 1 crew", "the Apollo 1 crew"], "answer_start": [341, 337, 337, 341, 337]}, "prediction_text": "Apollo 1 crew."}
{"answers": {"text": ["oxides of silicon", "silicon", "silicon"], "answer_start": [148, 158, 158]}, "prediction_text": "Silica SiO 2."}
{"answers": {"text": ["carbon dioxide", "carbon dioxide", "carbon dioxide", "carbon dioxide"], "answer_start": [71, 71, 71, 71]}, "prediction_text": "carbon dioxide (CO 2)"}
{"answers": {"text": ["mantle", "mantle", "mantle", "The Earth's mantle", "mantle"], "answer_start": [508, 508, 508, 496, 508]}, "prediction_text": "mantle"}
{"answers": {"text": ["carbon dioxide", "carbon dioxide"], "answer_start": [71, 71]}, "prediction_text": "Carbon dioxide (CO 2)"}
{"answers": {"text": ["Earth's crustal rock", "crustal rock", "Earth's crust", "The Earth's crust", "crustal rock"], "answer_start": [98, 106, 384, 94, 106]}, "prediction_text": "Earth's crust."}
{"answers": {"text": ["Earth's mantle", "The Earth's mantle", "mantle", "The Earth's mantle", "mantle"], "answer_start": [500, 496, 508, 496, 508]}, "prediction_text": "The Earth's mantle."}
{"answers": {"text": ["mantle", "The Earth's mantle", "mantle", "The Earth's mantle", "mantle"], "answer_start": [508, 496, 508, 496, 508]}, "prediction_text": "The mantle."}
{"answers": {"text": ["complex silicates", "complex silicates", "silicates", "silicates", "silicates (in silicate minerals)"], "answer_start": [454, 454, 462, 462, 462]}, "prediction_text": "Silicates."}
{"answers": {"text": ["monatomic", "monatomic", "monatomic", "monatomic", "monatomic"], "answer_start": [72, 72, 72, 72, 72]}, "prediction_text": "Monatomic."}
{"answers": {"text": ["simplest", "simplest", "simplest", "simplest", "the simplest"], "answer_start": [138, 138, 138, 138, 134]}, "prediction_text": "Simple atomic ratios."}
{"answers": {"text": ["HO", "HO", "HO", "HO", "HO"], "answer_start": [243, 243, 243, 243, 243]}, "prediction_text": "HO."}
{"answers": {"text": ["hydrogen", "hydrogen", "hydrogen", "hydrogen", "hydrogen"], "answer_start": [456, 456, 456, 456, 456]}, "prediction_text": "Hydrogen."}
{"answers": {"text": ["Avogadro's law", "Avogadro's law", "Avogadro's law", "the correct interpretation of water's composition", "Avogadro's law"], "answer_start": [613, 613, 613, 534, 613]}, "prediction_text": "Amedeo Avogadro's law."}
{"answers": {"text": ["phlogiston", "phlogiston", "phlogiston", "phlogiston", "phlogiston"], "answer_start": [237, 112, 112, 112, 112]}, "prediction_text": "wood, coal, iron."}
{"answers": {"text": ["non-combustible", "non-combustible substances that corrode", "wood", "wood"], "answer_start": [132, 132, 530, 530]}, "prediction_text": "Wood and coal."}
{"answers": {"text": ["Air", "Air", "Air", "Air", "a substance like wood gains overall weight in burning"], "answer_start": [210, 210, 210, 210, 513]}, "prediction_text": "Air."}
{"answers": {"text": ["metals", "metals", "metals", "metals", "metals"], "answer_start": [711, 711, 711, 711, 711]}, "prediction_text": "metals"}
{"answers": {"text": ["become lighter", "appear to become lighter", "appear to become lighter", "appear to become lighter and seem to lose something in the process", "lighter"], "answer_start": [441, 431, 431, 431, 448]}, "prediction_text": "Lighter."}
{"answers": {"text": ["covalent double bond", "a covalent double bond", "a covalent double bond that results from the filling of molecular orbitals formed from the atomic orbitals of the individual oxygen atoms", "a covalent double bond", "a covalent double bond that results from the filling of molecular orbitals formed from the atomic orbitals of the individual oxygen atoms"], "answer_start": [182, 180, 180, 180, 180]}, "prediction_text": "As a covalent double bond."}
{"answers": {"text": ["two", "two", "two", "two", "two"], "answer_start": [367, 367, 367, 367, 367]}, "prediction_text": "Two."}
{"answers": {"text": ["Aufbau", "Aufbau", "Aufbau", "Aufbau", "Aufbau"], "answer_start": [459, 459, 459, 459, 459]}, "prediction_text": "Aufbau filling of orbitals."}
{"answers": {"text": ["chemically", "chemically", "a covalent double bond", "a covalent double bond", "a covalent double bond"], "answer_start": [43, 43, 180, 180, 180]}, "prediction_text": "chemically bonded"}
{"answers": {"text": ["molecular orbitals", "filling of molecular orbitals", "the filling of molecular orbitals formed from the atomic orbitals of the individual oxygen atoms", "the filling of molecular orbitals formed from the atomic orbitals of the individual oxygen atoms", "the filling of molecular orbitals formed from the atomic orbitals of the individual oxygen atoms"], "answer_start": [236, 225, 221, 221, 221]}, "prediction_text": "The covalent double bond results from the filling of molecular orbitals formed from the atomic orbitals of the individual oxygen atoms."}
{"answers": {"text": ["1773", "1773", "1773", "1773", "1773 or earlier"], "answer_start": [76, 76, 76, 76, 76]}, "prediction_text": "1773"}
{"answers": {"text": ["1774", "1774", "1774", "1774", "1774"], "answer_start": [131, 131, 131, 131, 131]}, "prediction_text": "1774"}
{"answers": {"text": ["work was published first", "his work was published first", "his work was published first", "his work was published first", "published first"], "answer_start": [187, 183, 183, 183, 196]}, "prediction_text": "Joseph Priestley's experiments with oxygen."}
{"answers": {"text": ["Antoine Lavoisier", "Antoine Lavoisier", "Antoine Lavoisier", "Antoine Lavoisier", "Lavoisier"], "answer_start": [251, 251, 251, 251, 259]}, "prediction_text": "Carl Wilhelm Scheele"}
{"answers": {"text": ["phlogiston theory", "phlogiston theory of combustion and corrosion", "phlogiston theory of combustion and corrosion", "phlogiston theory of combustion and corrosion", "phlogiston theory"], "answer_start": [337, 337, 337, 337, 337]}, "prediction_text": "phlogiston theory of combustion and corrosion."}
{"answers": {"text": ["spin triplet state", "spin triplet state", "spin triplet state", "spin triplet state", "a spin triplet state"], "answer_start": [353, 353, 353, 353, 351]}, "prediction_text": "spin triplet state"}
{"answers": {"text": ["triplet oxygen", "O", "triplet oxygen", "triplet oxygen", "triplet oxygen"], "answer_start": [435, 404, 435, 435, 435]}, "prediction_text": "triplet oxygen"}
{"answers": {"text": ["unpaired electrons", "its unpaired electrons", "its unpaired electrons", "Because of its unpaired electrons", "unpaired electrons"], "answer_start": [595, 591, 591, 580, 595]}, "prediction_text": "unpaired electrons"}
{"answers": {"text": ["spontaneous", "spontaneous combustion", "spontaneous", "spontaneous", "spontaneous combustion"], "answer_start": [726, 726, 726, 726, 726]}, "prediction_text": "Spontaneous combustion."}
{"answers": {"text": ["antibonding", "antibonding", "antibonding", "antibonding", "antibonding"], "answer_start": [504, 504, 504, 504, 504]}, "prediction_text": "\u03c0* orbitals"}
{"answers": {"text": ["air", "air", "air", "part of the trapped air", "air"], "answer_start": [149, 234, 234, 214, 234]}, "prediction_text": "air"}
{"answers": {"text": ["weight", "weight", "weight", "that increase was the same as the weight of the air that rushed back in", "the tin had increased in weight and that increase was the same as the weight of the air that rushed back in"], "answer_start": [301, 301, 301, 312, 276]}, "prediction_text": "increased in weight."}
{"answers": {"text": ["weight", "weight", "weight", "weight", "weight"], "answer_start": [346, 301, 346, 346, 346]}, "prediction_text": "air rushed in."}
{"answers": {"text": ["1777", "1777", "1777", "1777", "1777"], "answer_start": [507, 507, 507, 507, 507]}, "prediction_text": "1777"}
{"answers": {"text": ["azote", "azote", "azote", "azote", "azote"], "answer_start": [640, 640, 640, 640, 640]}, "prediction_text": "Azote"}
{"answers": {"text": ["ozone", "ozone", "ozone", "Trioxygen", "Ozone"], "answer_start": [36, 36, 36, 0, 118]}, "prediction_text": "Ozone"}
{"answers": {"text": ["allotrope", "allotrope", "allotrope", "allotrope", "allotrope"], "answer_start": [65, 65, 65, 65, 65]}, "prediction_text": "Ozone."}
{"answers": {"text": ["lung tissue", "lung tissue", "lung tissue", "lung", "lung tissue"], "answer_start": [105, 105, 105, 105, 105]}, "prediction_text": "lung tissue"}
{"answers": {"text": ["protective radiation shield", "protective radiation shield", "protective radiation shield", "radiation shield", "a protective radiation shield"], "answer_start": [375, 375, 375, 386, 373]}, "prediction_text": "Protects the planet."}
{"answers": {"text": ["UV", "ultraviolet", "UV", "ultraviolet (UV)", "ultraviolet"], "answer_start": [293, 229, 293, 229, 229]}, "prediction_text": "UV radiation."}
{"answers": {"text": ["dioxygen", "dioxygen", "dioxygen", "dioxygen"], "answer_start": [60, 60, 60, 60]}, "prediction_text": "O2"}
{"answers": {"text": ["O2"], "answer_start": [155]}, "prediction_text": "Oxygen"}
{"answers": {"text": ["major", "major", "major", "a major part", "major"], "answer_start": [100, 100, 100, 98, 100]}, "prediction_text": "Oxygen."}
{"answers": {"text": ["energy content", "its energy content", "energy content", "energy content", "its energy content"], "answer_start": [406, 402, 406, 406, 402]}, "prediction_text": "Energy content."}
{"answers": {"text": ["cellular respiration", "cellular respiration", "cellular respiration", "in cellular respiration", "cellular respiration"], "answer_start": [479, 479, 479, 476, 479]}, "prediction_text": "Cellular respiration."}
{"answers": {"text": ["James Dewar", "James Dewar", "James Dewar", "Dewar", "James Dewar"], "answer_start": [25, 25, 25, 31, 25]}, "prediction_text": "James Dewar"}
{"answers": {"text": ["1891", "1891", "1891", "1891", "1891"], "answer_start": [3, 3, 3, 3, 3]}, "prediction_text": "1891"}
{"answers": {"text": ["1895", "1895", "1895", "1895", "1895"], "answer_start": [185, 185, 185, 185, 185]}, "prediction_text": "1895"}
{"answers": {"text": ["oxyacetylene", "oxyacetylene welding", "oxyacetylene welding", "oxyacetylene welding", "oxyacetylene"], "answer_start": [430, 430, 430, 430, 430]}, "prediction_text": "Oxyacetylene welding."}
{"answers": {"text": ["Oxygen", "Oxygen", "Oxygen", "Oxygen", "Oxygen"], "answer_start": [0, 0, 0, 0, 0]}, "prediction_text": "Oxygen."}
{"answers": {"text": ["temperature", "temperature", "temperature", "temperature", "temperature"], "answer_start": [253, 253, 253, 253, 253]}, "prediction_text": "Temperature."}
{"answers": {"text": ["6.04 milliliters", "6.04 milliliters", "6.04 milliliters", "6.04 milliliters", "6.04 milliliters"], "answer_start": [441, 441, 441, 441, 441]}, "prediction_text": "6.04 mL."}
{"answers": {"text": ["seawater", "seawater", "seawater", "seawater", "sea water"], "answer_start": [492, 492, 492, 492, 659]}, "prediction_text": "Sea water."}
{"answers": {"text": ["twice", "50% more", "about twice as much", "twice as much", "twice"], "answer_start": [286, 579, 280, 286, 286]}, "prediction_text": "14.6 mg\u00b7L\u22121."}
{"answers": {"text": ["most abundant", "most", "most abundant", "most abundant", "most abundant"], "answer_start": [14, 14, 14, 14, 14]}, "prediction_text": "Oxygen is the most abundant chemical element in the biosphere."}
{"answers": {"text": ["third", "third", "third", "third most abundant", "third"], "answer_start": [112, 112, 112, 112, 112]}, "prediction_text": "Oxygen is ranked as abundant in the universe as the third most abundant chemical element."}
{"answers": {"text": ["0.9%", "0.9%", "About 0.9%", "0.9%", "0.9%"], "answer_start": [199, 199, 193, 199, 199]}, "prediction_text": "0.9%"}
{"answers": {"text": ["world's oceans", "the world's oceans", "in the Earth's biosphere, air, sea and land", "oceans", "the world's oceans"], "answer_start": [321, 317, 53, 329, 317]}, "prediction_text": "Earth's crust."}
{"answers": {"text": ["ultraviolet radiation", "ultraviolet radiation", "ultraviolet radiation impacting oxygen-containing molecules", "ultraviolet radiation impacting oxygen-containing molecules", "ultraviolet radiation impacting oxygen-containing molecules such as carbon dioxide"], "answer_start": [760, 760, 760, 760, 760]}, "prediction_text": "ultraviolet radiation impacting oxygen-containing molecules such as carbon dioxide."}
{"answers": {"text": ["late 19th", "19th", "19th", "late 19th century", "19th"], "answer_start": [7, 12, 12, 7, 12]}, "prediction_text": "Late 19th century."}
{"answers": {"text": ["compressing and cooling", "compressing and cooling", "cascade method", "compressing and cooling it", "compressing and cooling"], "answer_start": [106, 106, 142, 106, 106]}, "prediction_text": "Compressing and cooling."}
{"answers": {"text": ["Raoul Pierre Pictet", "Raoul Pierre Pictet", "Pierre Pictet", "Pictet", "Raoul Pierre Pictet"], "answer_start": [186, 186, 192, 199, 186]}, "prediction_text": "Raoul Pierre Pictet"}
{"answers": {"text": ["few drops", "a few drops", "a few drops", "a few drops", "Only a few drops"], "answer_start": [594, 592, 592, 592, 587]}, "prediction_text": "Few drops."}
{"answers": {"text": ["March 29, 1883", "March 29, 1883", "March 29, 1883", "March 29, 1883", "March 29, 1883"], "answer_start": [752, 752, 752, 752, 752]}, "prediction_text": "March 29, 1883"}
{"answers": {"text": ["Sun", "the Sun", "the Sun", "Sun", "the Sun"], "answer_start": [212, 208, 208, 212, 208]}, "prediction_text": "The Sun."}
{"answers": {"text": ["oxygen-16", "oxygen-16", "oxygen-16", "oxygen-16", "oxygen-16"], "answer_start": [442, 442, 442, 442, 442]}, "prediction_text": "Oxygen-16"}
{"answers": {"text": ["Genesis spacecraft", "Genesis", "Genesis", "Genesis", "Genesis"], "answer_start": [373, 373, 373, 373, 373]}, "prediction_text": "Genesis spacecraft"}
{"answers": {"text": ["unknown", "an unknown process", "unknown", "unknown", "an unknown process"], "answer_start": [505, 502, 505, 505, 502]}, "prediction_text": "Decomposition of protoplanetary material."}
{"answers": {"text": ["Earth", "Earth", "Earth", "the Earth", "Earth"], "answer_start": [643, 643, 643, 639, 643]}, "prediction_text": "Earth"}
{"answers": {"text": ["Singlet", "Singlet oxygen", "Singlet oxygen", "Singlet", "Singlet oxygen"], "answer_start": [0, 0, 0, 0, 0]}, "prediction_text": "Singlet oxygen."}
{"answers": {"text": ["organic molecules", "common organic molecules", "common organic molecules", "common organic molecules", "common organic molecules"], "answer_start": [165, 158, 158, 158, 158]}, "prediction_text": "Common organic molecules."}
{"answers": {"text": ["photosynthesis", "photosynthesis", "photosynthesis", "photosynthesis", "photosynthesis"], "answer_start": [279, 279, 279, 279, 279]}, "prediction_text": "Photosynthesis."}
{"answers": {"text": ["photolysis of ozone", "photolysis of ozone", "photolysis of ozone by light of short wavelength", "photolysis of ozone by light of short wavelength", "photolysis"], "answer_start": [371, 371, 371, 371, 371]}, "prediction_text": "Ozone photolysis."}
{"answers": {"text": ["Carotenoids", "Carotenoids", "Carotenoids", "Carotenoids", "Carotenoids"], "answer_start": [476, 476, 476, 476, 476]}, "prediction_text": "Carotenoids."}
{"answers": {"text": ["Paleoclimatologists", "Paleoclimatologists", "Paleoclimatologists", "Paleoclimatologists", "Paleoclimatologists"], "answer_start": [0, 0, 0, 0, 0]}, "prediction_text": "Paleoclimatologists"}
{"answers": {"text": ["climate", "climate", "temperatures", "climate", "what the climate was like millions of years ago"], "answer_start": [135, 135, 401, 135, 126]}, "prediction_text": "Global temperatures."}
{"answers": {"text": ["12%", "12%", "12%", "12%", "12%"], "answer_start": [344, 344, 344, 344, 344]}, "prediction_text": "12%"}
{"answers": {"text": ["oxygen-18", "oxygen-18", "oxygen-16", "oxygen-18", "oxygen-18"], "answer_start": [585, 585, 523, 585, 585]}, "prediction_text": "Oxygen-18."}
{"answers": {"text": ["lower global temperatures", "periods of lower global temperatures", "lower temperatures", "periods of lower global temperatures", "lower global temperatures"], "answer_start": [433, 422, 395, 422, 433]}, "prediction_text": "Lower global temperatures."}
{"answers": {"text": ["687 and 760 nm", "687 and 760 nm", "687 and 760 nm", "687 and 760 nm", "687 and 760 nm"], "answer_start": [83, 83, 83, 83, 83]}, "prediction_text": "687 and 760 nm."}
{"answers": {"text": ["carbon cycle", "plant health status", "the carbon cycle", "plant health status", "carbon cycle"], "answer_start": [635, 245, 631, 245, 635]}, "prediction_text": "Carbon cycle."}
{"answers": {"text": ["satellite platform", "a satellite platform", "satellites", "a satellite platform", "satellite"], "answer_start": [272, 270, 653, 270, 272]}, "prediction_text": "Satellite platforms."}
{"answers": {"text": ["global", "a global scale", "global", "global", "a global scale"], "answer_start": [669, 667, 669, 669, 667]}, "prediction_text": "Global scale."}
{"answers": {"text": ["remote sensing", "remote", "remote", "remote sensing", "remote"], "answer_start": [104, 104, 104, 104, 104]}, "prediction_text": "Remote sensing."}
{"answers": {"text": ["paramagnetic", "paramagnetic", "paramagnetic", "paramagnetic", "paramagnetic"], "answer_start": [39, 39, 39, 39, 39]}, "prediction_text": "Magnetic."}
{"answers": {"text": ["Liquid oxygen", "liquid oxygen", "liquid oxygen", "oxygen", "liquid oxygen"], "answer_start": [299, 409, 409, 416, 409]}, "prediction_text": "Liquid oxygen"}
{"answers": {"text": ["unpaired electrons", "unpaired electrons", "unpaired electrons", "unpaired electrons in the molecule", "unpaired electrons"], "answer_start": [195, 195, 195, 195, 195]}, "prediction_text": "unpaired electrons"}
{"answers": {"text": ["magnetic field", "magnetic", "magnetic", "magnetic field", "magnetic"], "answer_start": [135, 171, 135, 135, 135]}, "prediction_text": "Magnetic field."}
{"answers": {"text": ["powerful magnet", "a powerful magnet", "a powerful magnet", "a powerful magnet", "magnet"], "answer_start": [486, 484, 484, 484, 495]}, "prediction_text": "Magnetometer"}
{"answers": {"text": ["dangerous by-products", "the immune system", "hypersensitive response of plants against pathogen attack", "dangerous by-products of oxygen use in organisms"], "answer_start": [91, 150, 325, 91]}, "prediction_text": "Peroxide, superoxide, and singlet oxygen."}
{"answers": {"text": ["destroy invading microbes", "to destroy invading microbes", "destroy invading microbes", "to destroy invading microbes", "destroy invading microbes"], "answer_start": [239, 236, 239, 236, 239]}, "prediction_text": "Destroy invading microbes."}
{"answers": {"text": ["pathogen attack", "pathogen attack", "pathogen attack", "pathogen attack", "pathogen"], "answer_start": [367, 367, 367, 367, 367]}, "prediction_text": "Oxygen."}
{"answers": {"text": ["anaerobic", "obligately anaerobic organisms", "obligately anaerobic", "obligately anaerobic", "obligately anaerobic organisms"], "answer_start": [414, 403, 403, 403, 403]}, "prediction_text": "Anaerobic organisms."}
{"answers": {"text": ["2.5 billion years ago", "2.5 billion years ago", "about 2.5 billion years ago", "about 2.5 billion years ago", "2.5 billion years ago during the Great Oxygenation Event"], "answer_start": [541, 541, 535, 535, 541]}, "prediction_text": "2.5 billion years ago."}
{"answers": {"text": ["90.20 K", "90.20 K", "90.20 K (\u2212182.95 \u00b0C, \u2212297.31 \u00b0F)", "90.20 K", "90.20 K (\u2212182.95 \u00b0C, \u2212297.31 \u00b0F)"], "answer_start": [20, 20, 20, 20, 20]}, "prediction_text": "90.20 K."}
{"answers": {"text": ["clear", "clear substances with a light sky-blue color", "clear", "clear", "clear"], "answer_start": [133, 133, 133, 133, 133]}, "prediction_text": "Light sky-blue."}
{"answers": {"text": ["liquefied air", "liquefied air", "liquefied air", "air", "liquefied air"], "answer_start": [384, 384, 384, 394, 384]}, "prediction_text": "Liquid nitrogen."}
{"answers": {"text": ["liquid nitrogen", "liquid nitrogen", "liquid nitrogen", "liquid nitrogen", "nitrogen"], "answer_start": [468, 468, 468, 468, 475]}, "prediction_text": "Liquid nitrogen"}
{"answers": {"text": ["combustible materials", "combustible materials", "combustible", "combustible", "combustible materials"], "answer_start": [560, 560, 560, 560, 560]}, "prediction_text": "Air."}
{"answers": {"text": ["water", "in solution in the world's water bodies", "the world's water bodies"], "answer_start": [51, 24, 39]}, "prediction_text": "World's water bodies."}
{"answers": {"text": ["lower", "lower", "lower", "lower", "lower temperatures"], "answer_start": [100, 100, 100, 100, 100]}, "prediction_text": "Lower temperatures."}
{"answers": {"text": ["higher oxygen content", "higher oxygen content", "higher oxygen content", "higher oxygen content", "due to their higher oxygen content"], "answer_start": [255, 255, 255, 255, 242]}, "prediction_text": "Higher oxygen content."}
{"answers": {"text": ["algae", "algae", "algae", "algae", "algae"], "answer_start": [369, 369, 369, 369, 369]}, "prediction_text": "Algae."}
{"answers": {"text": ["biochemical oxygen demand", "biochemical oxygen demand", "measuring the water's biochemical oxygen demand", "biochemical oxygen demand", "measuring the water's biochemical oxygen demand"], "answer_start": [591, 591, 569, 591, 569]}, "prediction_text": "Biochemical oxygen demand."}
{"answers": {"text": ["3.5 billion years ago", "about 3.5 billion years ago", "about 3.5 billion years ago", "3.5 billion years ago", "about 3.5 billion years ago"], "answer_start": [128, 122, 122, 128, 122]}, "prediction_text": "3.5 billion years ago."}
{"answers": {"text": ["Paleoproterozoic", "Paleoproterozoic", "Paleoproterozoic", "Paleoproterozoic", "the Paleoproterozoic eon"], "answer_start": [215, 215, 215, 215, 211]}, "prediction_text": "Paleoproterozoic"}
{"answers": {"text": ["banded iron formations", "banded iron formations", "banded iron formations", "banded iron formations", "banded iron"], "answer_start": [401, 401, 401, 401, 401]}, "prediction_text": "Banded iron formations."}
{"answers": {"text": ["1.7 billion years ago", "1.7 billion years ago", "1.7 billion years ago", "1.7 billion years ago", "around 1.7 billion years ago"], "answer_start": [576, 576, 576, 576, 569]}, "prediction_text": "1.7 billion years ago."}
{"answers": {"text": ["3\u20132.7 billion years ago", "3\u20132.7 billion years ago", "3\u20132.7 billion years ago", "3\u20132.7 billion years ago", "3\u20132.7 billion years ago"], "answer_start": [510, 510, 510, 510, 510]}, "prediction_text": "3-2.7 billion years ago."}
{"answers": {"text": ["oxygen cycle", "the oxygen cycle", "oxygen cycle", "oxygen cycle", "the oxygen cycle"], "answer_start": [77, 73, 77, 77, 73]}, "prediction_text": "Photosynthesis"}
{"answers": {"text": ["biogeochemical", "biogeochemical", "biogeochemical", "biogeochemical cycle", "biogeochemical"], "answer_start": [96, 96, 96, 96, 96]}, "prediction_text": "Photosynthesis"}
{"answers": {"text": ["three", "three", "three", "three main reservoirs", "three"], "answer_start": [173, 173, 173, 173, 173]}, "prediction_text": "3"}
{"answers": {"text": ["photosynthesis", "photosynthesis", "photosynthesis", "photosynthesis", "photosynthesis"], "answer_start": [304, 304, 304, 304, 304]}, "prediction_text": "Photosynthesis"}
{"answers": {"text": ["oxygen", "oxygen", "oxygen", "oxygen", "oxygen"], "answer_start": [396, 288, 396, 396, 396]}, "prediction_text": "Oxygen."}
{"answers": {"text": ["zeolite molecular sieves", "zeolite molecular sieves", "passing a stream of clean, dry air through one bed of a pair of identical zeolite molecular sieves", "passing a stream of clean, dry air through one bed of a pair of identical zeolite molecular sieves, which absorbs the nitrogen", "passing a stream of clean, dry air through one bed of a pair of identical zeolite molecular sieves"], "answer_start": [127, 127, 53, 53, 53]}, "prediction_text": "Pressure swing adsorption."}
{"answers": {"text": ["90% to 93%", "90% to 93%", "90% to 93%", "90% to 93%", "90% to 93%"], "answer_start": [214, 214, 214, 214, 214]}, "prediction_text": "90% to 93% O2."}
{"answers": {"text": ["nitrogen", "nitrogen", "nitrogen", "nitrogen", "nitrogen"], "answer_start": [171, 246, 171, 171, 171]}, "prediction_text": "Nitrogen."}
{"answers": {"text": ["non-cryogenic", "non-cryogenic", "non-cryogenic", "non-cryogenic", "non-cryogenic"], "answer_start": [717, 717, 717, 717, 717]}, "prediction_text": "Pressure swing adsorption."}
{"answers": {"text": ["major method", "increasingly", "major method", "increasingly"], "answer_start": [10, 686, 10, 686]}, "prediction_text": "90% to 93% O2."}
{"answers": {"text": ["water", "water", "water", "water"], "answer_start": [56, 56, 56, 56]}, "prediction_text": "Water."}
{"answers": {"text": ["oxygen and hydrogen", "oxygen and hydrogen", "molecular oxygen and hydrogen", "molecular oxygen and hydrogen", "Oxygen gas"], "answer_start": [77, 77, 67, 67, 0]}, "prediction_text": "Oxygen gas."}
{"answers": {"text": ["DC", "DC electricity", "DC", "DC electricity"], "answer_start": [98, 98, 98, 98]}, "prediction_text": "DC electricity."}
{"answers": {"text": ["oxides and oxoacids", "oxides and oxoacids", "oxides and oxoacids", "oxides and oxoacids", "oxides and oxoacids"], "answer_start": [535, 535, 535, 535, 535]}, "prediction_text": "Water electrolysis."}
{"answers": {"text": ["Chemical", "Chemical", "Chemical", "chemical oxygen generators or oxygen candles", "Chemical"], "answer_start": [556, 556, 556, 607, 556]}, "prediction_text": "Chemical catalysts."}
{"answers": {"text": ["recreational", "recreational", "recreational"], "answer_start": [54, 54, 54]}, "prediction_text": "As a mild euphoric."}
{"answers": {"text": ["mild euphoric", "a supposed mild euphoric", "a supposed mild euphoric", "euphoric"], "answer_start": [22, 11, 11, 27]}, "prediction_text": "Performance boost."}
{"answers": {"text": ["performance", "performance", "performance", "performance", "a \"boost\" in performance"], "answer_start": [406, 406, 406, 406, 393]}, "prediction_text": "Performance boost."}
{"answers": {"text": ["placebo", "placebo", "placebo", "a placebo effect", "placebo"], "answer_start": [461, 461, 461, 459, 461]}, "prediction_text": "Performance boost."}
{"answers": {"text": ["aerobic", "aerobic", "aerobic", "aerobic", "aerobic exercise"], "answer_start": [612, 612, 612, 612, 612]}, "prediction_text": "Aerobic exercise."}
{"answers": {"text": ["Hyperbaric (high-pressure) medicine", "special oxygen chambers", "oxygen chambers", "oxygen chambers", "oxygen chambers"], "answer_start": [0, 41, 49, 49, 49]}, "prediction_text": "Hyperbaric oxygen chamber."}
{"answers": {"text": ["carbon monoxide", "carbon monoxide", "carbon monoxide", "carbon monoxide", "carbon monoxide"], "answer_start": [345, 345, 345, 345, 345]}, "prediction_text": "Carbon monoxide."}
{"answers": {"text": ["anaerobic bacteria", "anaerobic bacteria", "anaerobic bacteria", "anaerobic bacteria", "anaerobic bacteria"], "answer_start": [427, 427, 427, 427, 427]}, "prediction_text": "Anaerobic bacteria."}
{"answers": {"text": ["Decompression sickness", "bubbles of inert gas, mostly nitrogen and helium, forming in their blood", "Decompression sickness", "decompression sickness (the 'bends')", "Decompression sickness"], "answer_start": [523, 617, 523, 206, 523]}, "prediction_text": "Bubbles of inert gas."}
{"answers": {"text": ["Oxygen therapy", "Oxygen therapy", "Oxygen therapy", "Oxygen therapy", "Oxygen therapy"], "answer_start": [314, 314, 314, 314, 314]}, "prediction_text": "Oxygen therapy."}
{"answers": {"text": ["heart", "the heart", "heart", "the heart", "heart"], "answer_start": [307, 303, 307, 303, 373]}, "prediction_text": "Heart."}
{"answers": {"text": ["oxygen supplementation", "oxygen supplementation", "oxygen supplementation", "Oxygen therapy", "Oxygen therapy"], "answer_start": [71, 71, 71, 314, 314]}, "prediction_text": "Oxygen supplementation."}
{"answers": {"text": ["respiration", "respiration", "respiration", "respiration", "respiration"], "answer_start": [55, 55, 55, 55, 55]}, "prediction_text": "Oxygen supplementation."}
{"answers": {"text": ["gaseous oxygen.", "gaseous oxygen", "gaseous oxygen", "oxygen", "gaseous oxygen"], "answer_start": [547, 547, 547, 555, 547]}, "prediction_text": "gaseous oxygen."}
{"answers": {"text": ["electronegativity", "electronegativity", "electronegativity", "electronegativity", "electronegativity"], "answer_start": [11, 11, 11, 11, 11]}, "prediction_text": "Electronegativity."}
{"answers": {"text": ["oxides", "oxides", "oxides", "oxides", "oxides"], "answer_start": [111, 111, 111, 111, 111]}, "prediction_text": "Oxides"}
{"answers": {"text": ["FeO", "FeO", "FeO", "FeO", "FeO"], "answer_start": [473, 473, 473, 473, 473]}, "prediction_text": "FeO (w\u00fcstite)"}
{"answers": {"text": ["oxide", "a thin film of oxide", "a thin film of oxide", "oxide", "a thin film of oxide"], "answer_start": [253, 238, 238, 253, 238]}, "prediction_text": "Oxides."}
{"answers": {"text": ["corrosion", "further corrosion", "further corrosion", "further corrosion", "corrosion"], "answer_start": [303, 295, 295, 295, 303]}, "prediction_text": "corrosion."}
{"answers": {"text": ["cabin depressurization", "cabin depressurization", "cabin depressurization", "cabin depressurization", "depressurization"], "answer_start": [251, 251, 251, 251, 257]}, "prediction_text": "Cabin depressurization."}
{"answers": {"text": ["chemical", "chemical oxygen", "chemical", "chemical oxygen generators", "chemical"], "answer_start": [312, 312, 312, 312, 312]}, "prediction_text": "Chemical oxygen generators."}
{"answers": {"text": ["exothermic", "exothermic", "exothermic", "exothermic", "exothermic reaction"], "answer_start": [595, 595, 595, 595, 595]}, "prediction_text": "Exothermic reaction."}
{"answers": {"text": ["oxygen gas", "oxygen", "oxygen", "oxygen", "oxygen"], "answer_start": [560, 560, 560, 560, 560]}, "prediction_text": "Oxygen gas."}
{"answers": {"text": ["storage", "storage", "storage methods", "storage", "storage methods"], "answer_start": [7, 7, 7, 7, 7]}, "prediction_text": "Oxygen storage methods."}
{"answers": {"text": ["insulated tankers", "insulated tankers", "insulated tankers", "specially insulated tankers"], "answer_start": [179, 179, 179, 169]}, "prediction_text": "In specially insulated tankers."}
{"answers": {"text": ["liquid", "as a liquid", "liquid", "as a liquid in specially insulated tankers"], "answer_start": [159, 154, 159, 154]}, "prediction_text": "Liquefied."}
{"answers": {"text": ["compressed gas", "compressed gas", "compressed gas;", "compressed gas", "compressed gas"], "answer_start": [691, 691, 691, 691, 691]}, "prediction_text": "Liquefied oxygen."}
{"answers": {"text": ["hospitals", "hospitals", "hospitals", "hospitals", "hospitals"], "answer_start": [414, 414, 414, 414, 414]}, "prediction_text": "Hospitals and other institutions."}
{"answers": {"text": ["organic solvents", "organic solvents", "organic solvents", "solvents", "organic solvents"], "answer_start": [309, 309, 309, 317, 309]}, "prediction_text": "alcohols"}
{"answers": {"text": ["organic compounds", "organic", "solvents", "organic solvents"], "answer_start": [36, 36, 317, 309]}, "prediction_text": "Organic compounds."}
{"answers": {"text": ["feeder materials", "as feeder materials", "feeder materials", "feeder materials", "as feeder materials"], "answer_start": [546, 543, 546, 546, 543]}, "prediction_text": "Feeder materials."}
{"answers": {"text": ["Epoxides", "Epoxides", "Epoxides", "Epoxides", "Epoxides"], "answer_start": [755, 755, 755, 755, 755]}, "prediction_text": "Epoxides"}
{"answers": {"text": ["important", "important", "important", "important"], "answer_start": [15, 616, 299, 299]}, "prediction_text": "Organics."}
{"answers": {"text": ["biomolecules", "biomolecules", "biomolecules", "biomolecules", "almost all biomolecules that are important to (or generated by) life"], "answer_start": [35, 35, 35, 35, 24]}, "prediction_text": "Organic compounds."}
{"answers": {"text": ["Only a few", "Only a few", "a few", "Only a few", "a few"], "answer_start": [94, 94, 99, 94, 99]}, "prediction_text": "Few."}
{"answers": {"text": ["carbohydrates", "carbohydrates", "carbohydrates", "carbohydrates", "carbohydrates"], "answer_start": [241, 241, 241, 241, 241]}, "prediction_text": "carbohydrates"}
{"answers": {"text": ["proteins", "proteins", "proteins", "proteins", "proteins"], "answer_start": [345, 345, 345, 345, 345]}, "prediction_text": "carbohydrates"}
{"answers": {"text": ["bones", "bones", "bones", "bones", "bones"], "answer_start": [656, 656, 656, 656, 656]}, "prediction_text": "Bone"}
{"answers": {"text": ["Oxygen toxicity", "Oxygen toxicity", "Oxygen toxicity", "Oxygen toxicity", "Oxygen toxicity to the lungs and central nervous system"], "answer_start": [0, 0, 0, 0, 0]}, "prediction_text": "Pulmonary fibrosis."}
{"answers": {"text": ["pulmonary fibrosis", "permanent pulmonary fibrosis", "permanent pulmonary fibrosis", "permanent pulmonary fibrosis", "pulmonary fibrosis"], "answer_start": [238, 228, 228, 228, 238]}, "prediction_text": "Pulmonary fibrosis."}
{"answers": {"text": ["160 kPa", "160 kPa", "160 kPa (about 1.6 atm)", "160 kPa (about 1.6 atm)", "160 kPa"], "answer_start": [307, 307, 307, 307, 307]}, "prediction_text": "1.6 atm."}
{"answers": {"text": ["Acute oxygen toxicity", "Acute oxygen toxicity", "seizures", "seizures", "seizures"], "answer_start": [384, 384, 415, 415, 415]}, "prediction_text": "Oxygen toxicity."}
{"answers": {"text": ["seizures", "seizures", "seizures", "seizures", "seizures"], "answer_start": [415, 415, 415, 415, 415]}, "prediction_text": "Convulsions."}
{"answers": {"text": ["low total pressures", "low total pressures used", "low total pressures", "low total pressures", "the low total pressures used"], "answer_start": [145, 145, 145, 145, 141]}, "prediction_text": "Low total pressures."}
{"answers": {"text": ["30 kPa", "about 30 kPa", "about 30 kPa", "30 kPa", "about 30 kPa (1.4 times normal)"], "answer_start": [266, 260, 260, 266, 260]}, "prediction_text": "30 kPa (1.4 times normal)"}
{"answers": {"text": ["1.4 times normal", "1.4 times", "1.4 times", "1.4 times normal", "1.4 times normal"], "answer_start": [274, 274, 274, 274, 274]}, "prediction_text": "1.4 times normal."}
{"answers": {"text": ["no damage", "no damage", "no", "no", "no damage"], "answer_start": [124, 124, 124, 124, 124]}, "prediction_text": "No damage."}
{"answers": {"text": ["only marginally more", "marginally more than normal", "marginally more", "marginally more", "only marginally more"], "answer_start": [369, 374, 374, 374, 369]}, "prediction_text": "Oxygen partial pressure in the astronaut's arterial blood."}
{"answers": {"text": ["at elevated partial pressures", "elevated partial pressures", "at elevated partial pressures", "at elevated partial pressures", "elevated partial pressures"], "answer_start": [30, 33, 30, 30, 33]}, "prediction_text": "At partial pressures more than 50 kPa."}
{"answers": {"text": ["50 kilopascals", "at partial pressures more than 50 kilopascals", "50 kilopascals", "more than 50 kilopascals", "at partial pressures more than 50 kilopascals"], "answer_start": [185, 154, 185, 175, 154]}, "prediction_text": "At partial pressures more than 50 kPa."}
{"answers": {"text": ["50% oxygen", "about 50% oxygen composition at standard pressure", "about 50% oxygen composition at standard pressure"], "answer_start": [222, 216, 216]}, "prediction_text": "50 kPa."}
{"answers": {"text": ["mechanical ventilators", "mechanical ventilators", "mechanical ventilators", "oxygen masks", "mechanical ventilators"], "answer_start": [383, 383, 383, 434, 383]}, "prediction_text": "Mechanical ventilators."}
{"answers": {"text": ["30%\u201350%", "30%\u201350%", "30%\u201350%", "30%\u201350%"], "answer_start": [501, 501, 501, 501]}, "prediction_text": "30%\u201350% O"}
{"answers": {"text": ["October 1973", "October 1973", "October 1973", "October", "1973"], "answer_start": [29, 29, 29, 29, 37]}, "prediction_text": "October 1973"}
{"answers": {"text": ["nearly $12", "$12", "$12", "$12", "$12"], "answer_start": [305, 312, 312, 312, 312]}, "prediction_text": "$12"}
{"answers": {"text": ["1979", "1979", "1979", "1979", "1979"], "answer_start": [550, 550, 550, 550, 550]}, "prediction_text": "1979"}
{"answers": {"text": ["first oil shock", "shock", "shock", "first oil shock", "shock"], "answer_start": [516, 401, 401, 516, 401]}, "prediction_text": "1979 oil crisis"}
{"answers": {"text": ["members of the Organization of Arab Petroleum Exporting Countries", "members of the Organization of Arab Petroleum Exporting Countries", "Organization of Arab Petroleum Exporting Countries", "members of the Organization of Arab Petroleum Exporting Countries", "OAPEC"], "answer_start": [51, 51, 66, 51, 118]}, "prediction_text": "OAPEC"}
{"answers": {"text": ["to avoid being targeted by the boycott", "to avoid being targeted by the boycott", "to avoid being targeted by the boycott", "to avoid being targeted by the boycott", "to avoid being targeted by the boycott"], "answer_start": [208, 208, 208, 208, 208]}, "prediction_text": "To avoid being targeted by the boycott."}
{"answers": {"text": ["They arranged for Israel to pull back from the Sinai Peninsula and the Golan Heights.", "multilateral", "They arranged for Israel to pull back from the Sinai Peninsula and the Golan Heights", "multilateral negotiations", "multilateral"], "answer_start": [430, 383, 430, 383, 383]}, "prediction_text": "Multilateral negotiations."}
{"answers": {"text": ["January 18, 1974,", "January 18, 1974", "January 18, 1974", "January 18, 1974", "January 18, 1974"], "answer_start": [519, 519, 519, 519, 519]}, "prediction_text": "January 18, 1974."}
{"answers": {"text": ["March 1974", "March 1974", "March 1974", "March 1974.", "March 1974"], "answer_start": [782, 782, 782, 782, 782]}, "prediction_text": "March 1974."}
{"answers": {"text": ["On August 15, 1971", "August 15, 1971", "August 15, 1971", "August 15, 1971", "August 15, 1971"], "answer_start": [0, 3, 3, 3, 3]}, "prediction_text": "August 15, 1971."}
{"answers": {"text": ["to \"float\" (rise and fall according to market demand)", "(rise and fall according to market demand", "rise and fall according to market demand", "rise and fall according to market demand", "rise and fall according to market demand"], "answer_start": [277, 288, 289, 289, 289]}, "prediction_text": "The value of the dollar is left to \"float\" according to market demand."}
{"answers": {"text": ["industrialized nations increased their reserves", "industrialized nations increased their reserves (by expanding their money supplies) in amounts far greater than before", "industrialized nations increased their reserves", "industrialized nations increased their reserves", "the industrialized nations increased their reserves"], "answer_start": [560, 560, 560, 560, 556]}, "prediction_text": "Oil was priced in dollars."}
{"answers": {"text": ["In September 1971", "September 1971", "September 1971", "September 1971", "September 1971"], "answer_start": [843, 846, 846, 846, 846]}, "prediction_text": "September 1971"}
{"answers": {"text": ["oil was priced in dollars, oil producers' real income decreased", "Because oil was priced in dollars, oil producers' real income decreased.", "Because oil was priced in dollars, oil producers' real income decreased", "Because oil was priced in dollars, oil producers' real income decreased", "Because oil was priced in dollars, oil producers' real income decreased"], "answer_start": [778, 770, 770, 770, 770]}, "prediction_text": "To stabilize the value of the dollar."}
{"answers": {"text": ["risen by less than two percent per year", "less than two percent per year", "less than two percent per year", "by less than two percent per year", "less than two percent per year"], "answer_start": [159, 168, 168, 165, 168]}, "prediction_text": "2% per year."}
{"answers": {"text": ["After 1971", "1973\u20131974", "1971", "After 1971", "1971"], "answer_start": [37, 485, 43, 37, 43]}, "prediction_text": "1971"}
{"answers": {"text": ["1973\u20131974", "1974", "1973\u20131974", "1973\u20131974", "1973\u20131974"], "answer_start": [485, 490, 485, 485, 485]}, "prediction_text": "1973-1974"}
{"answers": {"text": ["Until the oil shock", "the oil shock", "the oil shock", "Until the oil shock", "the oil shock"], "answer_start": [200, 206, 206, 200, 206]}, "prediction_text": "1973-1974"}
{"answers": {"text": ["On October 6, 1973", "October 6, 1973", "October 6, 1973", "October 6, 1973", "October 6, 1973"], "answer_start": [0, 3, 3, 3, 3]}, "prediction_text": "October 6, 1973"}
{"answers": {"text": ["Iran", "Iran", "Iran", "Iran", "Iran"], "answer_start": [255, 255, 255, 255, 255]}, "prediction_text": "Iran"}
{"answers": {"text": ["ten times more", "ten", "a hundred", "ten"], "answer_start": [766, 766, 649, 766]}, "prediction_text": "Ten times more."}
{"answers": {"text": ["Iran", "Iran", "Iran", "Iran", "Iran"], "answer_start": [255, 255, 255, 255, 350]}, "prediction_text": "Iran"}
{"answers": {"text": ["renewal of hostilities in the Arab\u2013Israeli conflict", "renewal of hostilities", ".", "the Arab\u2013Israeli conflict released the underlying economic pressure on oil prices"], "answer_start": [133, 133, 323, 159]}, "prediction_text": "To increase the price of oil."}
{"answers": {"text": ["In response to American aid to Israel", "In response to American aid to Israel", "In response to American aid to Israel", "In response to American aid to Israel", "American aid to Israel"], "answer_start": [0, 0, 0, 0, 15]}, "prediction_text": "To prevent Israel's collapse."}
{"answers": {"text": ["October 16, 1973,", "October 16, 1973", "October 16, 1973", "October 16, 1973", "October 16, 1973"], "answer_start": [42, 42, 42, 42, 42]}, "prediction_text": "October 16, 1973"}
{"answers": {"text": ["until their economic and political objectives were met", "In response to American aid to Israel", "OPEC raised the posted price of oil", "until their economic and political objectives were met"], "answer_start": [308, 0, 60, 308]}, "prediction_text": "To meet economic and political objectives."}
{"answers": {"text": ["$2.2 billion", "$2.2 billion", "$2.2 billion", "$2.2 billion", "$2.2 billion"], "answer_start": [419, 419, 419, 419, 419]}, "prediction_text": "$2.2 billion"}
{"answers": {"text": ["American aid to Israel", "Nixon requested Congress to appropriate $2.2 billion in emergency aid to Israel", "OAPEC proclaimed the embargo that curbed exports to various countries and blocked all oil deliveries to the US as a \"principal hostile country", "OAPEC proclaimed the embargo that curbed exports to various countries", "American aid to Israel"], "answer_start": [15, 379, 879, 879, 15]}, "prediction_text": "To block exports to various countries."}
{"answers": {"text": ["over 100 billion dollars", "over 100 billion dollars", "over 100 billion dollars", "over 100 billion dollars", "100 billion dollars"], "answer_start": [342, 342, 342, 342, 347]}, "prediction_text": "100 billion dollars."}
{"answers": {"text": ["Al-Qaeda and the Taliban", "violent Sunni extremist groups such as Al-Qaeda and the Taliban", "Al-Qaeda and the Taliban", "Al-Qaeda and the Taliban", "Al-Qaeda and the Taliban"], "answer_start": [625, 586, 625, 625, 625]}, "prediction_text": "Al-Qaeda and the Taliban."}
{"answers": {"text": ["Middle East", "Middle East", "Middle East", "Middle East", "Middle East"], "answer_start": [310, 310, 310, 310, 310]}, "prediction_text": "Middle East"}
{"answers": {"text": ["shrinking Western demand", "shrinking Western demand", "economies had been caught between higher oil prices and lower prices for their own export commodities", "economies had been caught between higher oil prices and lower prices for their own export commodities", "shrinking Western demand"], "answer_start": [198, 198, 90, 90, 198]}, "prediction_text": "Higher oil prices."}
{"answers": {"text": ["Wahhabism", "Wahhabism", "Wahhabism", "Wahhabism", "Wahhabism"], "answer_start": [462, 462, 462, 462, 462]}, "prediction_text": "Wahhabism"}
{"answers": {"text": ["distribution and price disruptions", "reduced productivity", "distribution and price disruptions", "distribution and price disruptions"], "answer_start": [417, 528, 417, 417]}, "prediction_text": "Energy disruptions."}
{"answers": {"text": ["USSR", "USSR", "USSR", "USSR", "USSR"], "answer_start": [234, 234, 234, 234, 234]}, "prediction_text": "USSR"}
{"answers": {"text": ["1973", "1973", "1973", "1973"], "answer_start": [136, 136, 136, 136]}, "prediction_text": "1973"}
{"answers": {"text": ["Kissinger", "Kissinger", "Kissinger", "Kissinger's", "Kissinger"], "answer_start": [372, 372, 372, 372, 372]}, "prediction_text": "Kissinger"}
{"answers": {"text": ["The embargo", "The embargo", "The embargo", "The embargo", "embargo"], "answer_start": [0, 0, 0, 0, 4]}, "prediction_text": "Embargo"}
{"answers": {"text": ["automobiles", "automobiles", "automobiles", "automobiles", "automobiles"], "answer_start": [237, 237, 237, 237, 237]}, "prediction_text": "Automobile industry."}
{"answers": {"text": ["Macroeconomic problems", "Macroeconomic", "Macroeconomic", "Macroeconomic", "Macroeconomic"], "answer_start": [250, 250, 250, 250, 250]}, "prediction_text": "Macroeconomic problems."}
{"answers": {"text": ["Arctic", "Arctic", "Arctic", "the Arctic", "the Arctic"], "answer_start": [445, 445, 445, 441, 441]}, "prediction_text": "Arctic"}
{"answers": {"text": ["five to ten years", "five to ten years", "five to ten years", "five to ten years", "five to ten years"], "answer_start": [508, 508, 508, 508, 508]}, "prediction_text": "Five to ten years."}
{"answers": {"text": ["Netherlands", "Netherlands", "the Netherlands", "the Netherlands"], "answer_start": [109, 109, 105, 105]}, "prediction_text": "Netherlands"}
{"answers": {"text": ["America", "America", "America", "America"], "answer_start": [229, 229, 229, 229]}, "prediction_text": "America"}
{"answers": {"text": ["UK", "UK", "The UK", "The UK"], "answer_start": [377, 377, 373, 373]}, "prediction_text": "Britain"}
{"answers": {"text": ["Israel", "Israel", "Israelis", "the Israelis"], "answer_start": [568, 467, 467, 463]}, "prediction_text": "Israel"}
{"answers": {"text": ["Ted Heath", "Ted Heath", "Ted Heath", "Ted Heath"], "answer_start": [515, 515, 515, 515]}, "prediction_text": "Ted Heath"}
{"answers": {"text": ["UK", "UK", "UK", "the UK", "UK"], "answer_start": [56, 56, 56, 52, 56]}, "prediction_text": "Germany"}
{"answers": {"text": ["a series of strikes", "a series of strikes by coal miners and railroad workers", "a series of strikes by coal miners and railroad workers", "strikes by coal miners and railroad workers", "a series of strikes"], "answer_start": [104, 104, 104, 116, 104]}, "prediction_text": "Strikes by coal miners and railroad workers."}
{"answers": {"text": ["winter of 1973\u201374", "over the winter of 1973\u201374", "1973\u201374", "winter of 1973\u201374", "the winter of 1973\u201374"], "answer_start": [169, 160, 179, 169, 165]}, "prediction_text": "1973-74"}
{"answers": {"text": ["Germany", "Germany", "Italy", "Norway"], "answer_start": [325, 325, 334, 357]}, "prediction_text": "Netherlands"}
{"answers": {"text": ["Sweden", "Sweden", "Sweden", "Sweden", "Sweden"], "answer_start": [411, 411, 411, 411, 411]}, "prediction_text": "Sweden"}
{"answers": {"text": ["Price controls", "Price controls", "Price controls", "Price controls", "Price controls"], "answer_start": [0, 0, 0, 0, 0]}, "prediction_text": "Price controls."}
{"answers": {"text": ["encourage investment", "to encourage investment", "to encourage investment", "to encourage investment", "The system limited the price of \"old oil\""], "answer_start": [200, 197, 197, 197, 49]}, "prediction_text": "To encourage investment."}
{"answers": {"text": ["Price controls", "Price controls", "promote oil exploration", "discouraged development of alternative energies", "The system limited the price of \"old oil\""], "answer_start": [0, 0, 394, 315, 49]}, "prediction_text": "To encourage investment."}
{"answers": {"text": ["rationing", "rationing", "rationing", "rationing", "rationing"], "answer_start": [445, 445, 445, 445, 445]}, "prediction_text": "Rationing."}
{"answers": {"text": ["William E. Simon", "William E. Simon", "William E. Simon", "William E. Simon", "William E. Simon"], "answer_start": [21, 21, 21, 21, 21]}, "prediction_text": "William E. Simon"}
{"answers": {"text": ["In 1973", "1973", "1973", "1973", "1973"], "answer_start": [0, 3, 3, 3, 3]}, "prediction_text": "1973"}
{"answers": {"text": ["coordinate the response to the embargo", "to coordinate the response to the embargo", "to coordinate the response to the embargo", "to coordinate the response to the embargo", "to coordinate the response to the embargo"], "answer_start": [132, 129, 129, 129, 129]}, "prediction_text": "To coordinate the response to the embargo."}
{"answers": {"text": ["last week of February 1974,", "20%", "20", "20%", "20%"], "answer_start": [445, 473, 473, 473, 473]}, "prediction_text": "20%"}
{"answers": {"text": ["55 mph", "55 mph", "55 mph", "55 mph", "55 mph"], "answer_start": [70, 70, 70, 70, 70]}, "prediction_text": "55 mph (88 km/h)"}
{"answers": {"text": ["Emergency Highway Energy Conservation Act", "Emergency Highway Energy Conservation Act", "Emergency Highway Energy Conservation Act", "Emergency Highway Energy Conservation Act", "Emergency Highway Energy Conservation Act"], "answer_start": [117, 117, 117, 117, 117]}, "prediction_text": "Emergency Highway Energy Conservation Act"}
{"answers": {"text": ["Bill Clinton", "Bill Clinton", "Bill Clinton", "Bill Clinton", "Bill Clinton"], "answer_start": [370, 370, 370, 370, 370]}, "prediction_text": "Bill Clinton"}
{"answers": {"text": ["November 28, 1995", "November 28, 1995", "November 28, 1995", "November 28, 1995", "November 28, 1995"], "answer_start": [351, 351, 351, 351, 351]}, "prediction_text": "November 28, 1995"}
{"answers": {"text": ["1977", "1977", "1977", "1977", "1977"], "answer_start": [229, 229, 229, 229, 229]}, "prediction_text": "1978"}
{"answers": {"text": ["energy crisis", "The energy crisis", "energy crisis", "energy crisis", "The energy crisis"], "answer_start": [4, 0, 4, 4, 0]}, "prediction_text": "Energy crisis"}
{"answers": {"text": ["market and technology realities", "market and technology realities", "market and technology realities", "market and technology realities", "market and technology realities"], "answer_start": [290, 290, 290, 290, 290]}, "prediction_text": "market and technology realities."}
{"answers": {"text": ["congresses and presidents", "congresses and presidents", "congresses and presidents", "congresses and presidents", "congresses and presidents"], "answer_start": [452, 452, 452, 452, 452]}, "prediction_text": "Congresses and presidents."}
{"answers": {"text": ["U.S", "U.S.", "U.S.", "U.S", "the U.S."], "answer_start": [50, 50, 50, 50, 46]}, "prediction_text": "Britain"}
{"answers": {"text": ["British Prime Minister Edward Heath", "British", "British", "British"], "answer_start": [523, 523, 363, 363]}, "prediction_text": "Britain"}
{"answers": {"text": ["10 years", "10 years", "10 years", "10 years", "10 years"], "answer_start": [1121, 1121, 1121, 1121, 1121]}, "prediction_text": "10 years."}
{"answers": {"text": ["Arabs and much of the rest of the Third World", "the Arabs and much of the rest of the Third World", "Arabs", "Arabs and much of the rest of the Third World", "the Arabs and much of the rest of the Third World"], "answer_start": [1230, 1226, 1230, 1230, 1226]}, "prediction_text": "Arabs and much of the rest of the Third World."}
{"answers": {"text": ["Japan", "Japan", "Japan", "Japan", "Japan"], "answer_start": [60, 60, 60, 60, 60]}, "prediction_text": "Japan"}
{"answers": {"text": ["71%", "71%", "71", "71%", "71%"], "answer_start": [110, 110, 110, 110, 110]}, "prediction_text": "71%"}
{"answers": {"text": ["5% production cut", "declared Japan a \"nonfriendly\" country", "declared Japan a \"nonfriendly\" country", "declared Japan a \"nonfriendly\" country", "declared Japan a \"nonfriendly\" country"], "answer_start": [330, 224, 224, 224, 224]}, "prediction_text": "Declared Japan a \"nonfriendly\" country."}
{"answers": {"text": ["November 22", "November 22", "November 22", "November 22,", "November 22"], "answer_start": [381, 381, 381, 381, 381]}, "prediction_text": "November 22."}
{"answers": {"text": ["December 25", "December 25", "December 25", "December 25", "December 25"], "answer_start": [643, 643, 643, 643, 643]}, "prediction_text": "November 7, 1973."}
{"answers": {"text": ["USSR's invasion", "Afghanistan", "USSR's", "USSR", "Afghanistan"], "answer_start": [4, 23, 4, 4, 23]}, "prediction_text": "USSR"}
{"answers": {"text": ["Saudi Arabia and Iran", "Saudi Arabia and Iran", "Saudi Arabia and Iran", "Saudi Arabia and Iran", "Saudi Arabia and Iran"], "answer_start": [175, 175, 175, 175, 175]}, "prediction_text": "Saudi Arabia and Iran."}
{"answers": {"text": ["Saudi Arabia", "Saudi Arabia", "Saudi Arabia", "Saudi Arabia", "Saudi Arabia"], "answer_start": [175, 175, 175, 175, 175]}, "prediction_text": "Saudi Arabia"}
{"answers": {"text": ["January 1979", "1979", "1979", "1979", "January 1979"], "answer_start": [696, 1296, 704, 704, 696]}, "prediction_text": "January 1979"}
{"answers": {"text": ["November 1979", "November 1979", "November 1979", "November 1979", "November 1979"], "answer_start": [1287, 1287, 1287, 1287, 1287]}, "prediction_text": "November 1979"}
{"answers": {"text": ["large cars", "large", "large", "large"], "answer_start": [34, 34, 34, 34]}, "prediction_text": "Large cars."}
{"answers": {"text": ["Japanese imports", "Japan", "Japanese", "Japanese", "Japanese"], "answer_start": [46, 46, 399, 399, 46]}, "prediction_text": "Japan"}
{"answers": {"text": ["V8 and six cylinder engines", "V8 and six cylinder", "V8 and six cylinder", "V8 and six cylinder", "V8 and six cylinder"], "answer_start": [370, 370, 370, 370, 370]}, "prediction_text": "V8 and six cylinder engines."}
{"answers": {"text": ["Japan", "Japan", "Japanese", "Japanese", "Japanese"], "answer_start": [399, 399, 399, 399, 46]}, "prediction_text": "Japan"}
{"answers": {"text": ["A decade after the 1973", "1981", "1981", "A decade after the 1973 oil crisis", "1981"], "answer_start": [413, 491, 491, 413, 491]}, "prediction_text": "1973"}
{"answers": {"text": ["Toyota Corona Mark II", "Corona Mark II", "Toyota Corona Mark II", "Corona Mark II"], "answer_start": [153, 160, 153, 160]}, "prediction_text": "Toyota Corona Mark II"}
{"answers": {"text": ["power steering", "air conditioning", "air conditioning", "power windows"], "answer_start": [295, 277, 277, 334]}, "prediction_text": "Air conditioning."}
{"answers": {"text": ["Lexus", "Lexus", "Lexus", "Acura", "Lexus"], "answer_start": [598, 598, 598, 591, 598]}, "prediction_text": "Acura"}
{"answers": {"text": ["Toyota Hilux", "Hilux", "Hilux", "Toyota Hilux", "Hilux"], "answer_start": [44, 51, 51, 44, 51]}, "prediction_text": "Hilux"}
{"answers": {"text": ["Dodge D-50", "Dodge D-50", "Dodge D-50", "Dodge D-50", "Dodge D-50"], "answer_start": [208, 208, 208, 208, 208]}, "prediction_text": "Dodge D-50"}
{"answers": {"text": ["Ford, Chrysler, and GM", "Ford, Chrysler, and GM", "Ford, Chrysler, and GM, respectively", "Ford", "Ford, Chrysler, and GM"], "answer_start": [309, 309, 309, 309, 309]}, "prediction_text": "Ford"}
{"answers": {"text": ["captive import policy", "captive import", "captive import", "captive import", "captive import"], "answer_start": [485, 485, 485, 485, 485]}, "prediction_text": "Captive import policy."}
{"answers": {"text": ["An increase in imported cars", "An increase in imported cars into North America", "An increase in imported cars into North America", "An increase in imported cars", "An increase in imported cars"], "answer_start": [0, 0, 0, 0, 0]}, "prediction_text": "To reduce the number of cars in the US."}
{"answers": {"text": ["at least four passengers", "four", "at least four passengers", "at least four", "four"], "answer_start": [291, 300, 291, 291, 300]}, "prediction_text": "Four."}
{"answers": {"text": ["1985", "1985", "1985", "1985", "1985"], "answer_start": [338, 338, 338, 338, 338]}, "prediction_text": "1985"}
{"answers": {"text": ["Lincoln Continental,", "Cadillac DeVille", "Cadillac DeVille", "Cadillac DeVille"], "answer_start": [720, 658, 658, 658]}, "prediction_text": "Cadillac DeVille"}
{"answers": {"text": ["Chevrolet Bel Air", "lower price models such as the Chevrolet Bel Air, and Ford Galaxie 500", "lower price models", "Chevrolet Bel Air, and Ford Galaxie 500", "Chevrolet Bel Air, and Ford Galaxie 500"], "answer_start": [921, 890, 890, 921, 921]}, "prediction_text": "Lincoln Continental, Mercury Marquis, Cadillac DeVille, Fleetwood, Buick Electra, Oldsmobile 98, Chevrolet Bel Air, Ford Galaxie 500."}
{"answers": {"text": ["1979", "1979", "1979", "1979", "1979"], "answer_start": [275, 275, 275, 275, 275]}, "prediction_text": "1979"}
{"answers": {"text": ["1981", "1981", "1981", "1981", "1981"], "answer_start": [469, 469, 469, 469, 469]}, "prediction_text": "1981"}
{"answers": {"text": ["Mustang I", "the 1974 Mustang I", "Mustang I", "1974 Mustang I", "1974 Mustang I"], "answer_start": [141, 132, 141, 136, 136]}, "prediction_text": "1974 Mustang I"}
{"answers": {"text": ["1981", "1981", "1981", "1981", "1981"], "answer_start": [47, 47, 47, 47, 47]}, "prediction_text": "1981"}
{"answers": {"text": ["1980s", "during the 1980s", "1980s", "1980s", "1980s"], "answer_start": [419, 408, 419, 419, 419]}, "prediction_text": "1980"}
{"answers": {"text": ["recover market share", "trying to recover market share", "recover market share", "to recover market share", "recover market share"], "answer_start": [185, 175, 185, 182, 185]}, "prediction_text": "To recover market share."}
{"answers": {"text": ["nearly $40 per barrel", "nearly $40 per barrel", "$40 per barrel", "$40 per barrel", "$40 per barrel"], "answer_start": [375, 375, 382, 382, 382]}, "prediction_text": "$40 per barrel"}
{"answers": {"text": ["Project Mercury", "spacecraft", "Project Mercury", "Apollo", "Project Apollo"], "answer_start": [361, 328, 361, 4, 34]}, "prediction_text": "Project Mercury"}
{"answers": {"text": ["National Aeronautics and Space Administration (NASA)", "National Aeronautics and Space Administration (NASA", "Apollo", "Project Mercury", "Apollo program"], "answer_start": [123, 123, 4, 361, 4]}, "prediction_text": "Project Apollo"}
{"answers": {"text": ["1968", "1969", "1962", "1968", "1969"], "answer_start": [752, 238, 701, 752, 238]}, "prediction_text": "1968"}
{"answers": {"text": ["Dwight D. Eisenhower", "John F. Kennedy's", "John F. Kennedy's", "Dwight D. Eisenhower", "Dwight D. Eisenhower"], "answer_start": [275, 457, 457, 275, 275]}, "prediction_text": "John F. Kennedy"}
{"answers": {"text": ["two", "rst", "two", "two", "two-man"], "answer_start": [677, 393, 677, 677, 677]}, "prediction_text": "Three."}
{"answers": {"text": ["1961 to 1972", "1961 to 1972", "1961 to 1972", "1961 to 1972", "Apollo ran from 1961 to 1972"], "answer_start": [16, 16, 16, 16, 0]}, "prediction_text": "1961-1972"}
{"answers": {"text": ["Gemini program", "Gemini", "Gemini", "Gemini", "Gemini program"], "answer_start": [63, 63, 128, 63, 63]}, "prediction_text": "Gemini"}
{"answers": {"text": ["Soviet Union", "Apollo Applications Program", "Soviet Union", "Soviet Union", "Soviet Union"], "answer_start": [542, 349, 542, 542, 542]}, "prediction_text": "Soviet Union"}
{"answers": {"text": ["Skylab", "Skylab", "Skylab", "Skylab", "Skylab"], "answer_start": [397, 397, 397, 397, 397]}, "prediction_text": "Skylab"}
{"answers": {"text": ["1967", "1967", "1967", "1967", "1967"], "answer_start": [107, 107, 107, 107, 107]}, "prediction_text": "1967"}
{"answers": {"text": ["prelaunch test", "prelaunch", "prelaunch test", "prelaunch test"], "answer_start": [169, 169, 169, 169]}, "prediction_text": "Prelaunch test."}
{"answers": {"text": ["Budget cuts", "oxygen tank explosion", "Budget cuts", "Budget cuts", "Budget cuts"], "answer_start": [347, 513, 347, 347, 347]}, "prediction_text": "Budget cuts."}
{"answers": {"text": ["Five", "six", "Five", "Five", "Five of the remaining six missions"], "answer_start": [402, 424, 402, 402, 402]}, "prediction_text": "Five."}
{"answers": {"text": ["oxygen tank explosion in transit to the Moon", "disabled the command spacecraft's propulsion and life support", "oxygen tank explosion in transit to the Moon", "oxygen tank explosion", "oxygen tank explosion in transit"], "answer_start": [513, 565, 513, 513, 513]}, "prediction_text": "The crew returned to Earth safely."}
{"answers": {"text": ["Apollo 8", "Apollo 8", "Apollo 8", "Apollo 8", "Apollo 8"], "answer_start": [122, 122, 122, 122, 122]}, "prediction_text": "Apollo 8"}
{"answers": {"text": ["Apollo 17", "Apollo 17", "Apollo 17", "Apollo 17", "Apollo 17"], "answer_start": [212, 212, 212, 212, 212]}, "prediction_text": "Apollo 17"}
{"answers": {"text": ["382 kg", "382", "382", "382", "382 kg"], "answer_start": [346, 346, 346, 346, 346]}, "prediction_text": "842 kgs."}
{"answers": {"text": ["avionics, telecommunications, and computers", "NASA's current human spaceflight capability, and funded construction of its Johnson Space Center and Kennedy Space Center", "avionics, telecommunications, and computers", "avionics, telecommunications, and computers.", "rocketry and manned spaceflight, including avionics, telecommunications, and computers"], "answer_start": [753, 516, 753, 753, 710]}, "prediction_text": "avionics, telecommunications, computers."}
{"answers": {"text": ["one", "three", "one", "one", "one astronaut"], "answer_start": [165, 234, 165, 165, 165]}, "prediction_text": "One astronaut."}
{"answers": {"text": ["three", "three", "three", "three astronauts"], "answer_start": [234, 234, 234, 234]}, "prediction_text": "Three."}
{"answers": {"text": ["Abe Silverstein", "Abe Silverstein", "Abe Silverstein", "Abe Silverstein", "NASA manager Abe Silverstein"], "answer_start": [458, 458, 458, 458, 445]}, "prediction_text": "Abe Silverstein"}
{"answers": {"text": ["manned lunar landings", "manned lunar landings", "lunar landings", "ferrying crews to a space station, circumlunar flights, and eventual manned lunar landings"], "answer_start": [348, 348, 355, 279]}, "prediction_text": "Manned lunar landings."}
{"answers": {"text": ["early 1960", "1960", "1960", "1960", "early 1960"], "answer_start": [73, 605, 605, 79, 73]}, "prediction_text": "Early 1960"}
{"answers": {"text": ["1960", "1960", "1960", "1960", "July 1960"], "answer_start": [8, 8, 8, 8, 3]}, "prediction_text": "1960"}
{"answers": {"text": ["Maxime Faget", "Hugh L. Dryden", "Maxime Faget", "Maxime Faget", "Maxime Faget"], "answer_start": [617, 40, 617, 617, 617]}, "prediction_text": "Maxime Faget"}
{"answers": {"text": ["three", "three", "three", "three", "three study contracts"], "answer_start": [426, 426, 426, 426, 426]}, "prediction_text": "Three."}
{"answers": {"text": ["Hugh L. Dryden", "Maxime Faget", "Hugh L. Dryden", "Hugh L. Dryden", "Hugh L. Dryden"], "answer_start": [40, 617, 40, 40, 40]}, "prediction_text": "Hugh L. Dryden"}
{"answers": {"text": ["John F. Kennedy", "John F. Kennedy", "John F. Kennedy", ", John F. Kennedy", "John F. Kennedy"], "answer_start": [18, 18, 18, 16, 18]}, "prediction_text": "John F. Kennedy"}
{"answers": {"text": ["Soviet Union", "Soviet Union", "Soviet Union", "Soviet Union", "Soviet Union"], "answer_start": [117, 117, 117, 117, 117]}, "prediction_text": "Soviet Union"}
{"answers": {"text": ["massive financial commitment", "massive financial commitment", "massive financial commitment", "financial commitment", "massive financial commitment"], "answer_start": [789, 789, 789, 797, 789]}, "prediction_text": "The massive financial commitment required by a manned Moon landing."}
{"answers": {"text": ["James E. Webb", "James E. Webb", "James E. Webb", "James E. Webb", "James E. Webb"], "answer_start": [903, 903, 903, 903, 903]}, "prediction_text": "James E. Webb"}
{"answers": {"text": ["missile gap", "first but, first and, first if, but first period", "missile gap", "missile gap"], "answer_start": [257, 518, 257, 257]}, "prediction_text": "\"missile gap\""}
{"answers": {"text": ["Yuri Gagarin", "Yuri Gagarin", "Yuri Gagarin", "Yuri Gagarin", "Yuri Gagarin"], "answer_start": [36, 36, 36, 36, 36]}, "prediction_text": "Yuri Gagarin"}
{"answers": {"text": ["Soviet Union", "Soviet", "Soviet Union", "Soviet Union", "Soviet Union"], "answer_start": [181, 19, 181, 181, 181]}, "prediction_text": "Soviet Union"}
{"answers": {"text": ["one day", "one", "one", "one", "one day"], "answer_start": [262, 262, 262, 262, 262]}, "prediction_text": "One day."}
{"answers": {"text": ["refusing to make a commitment", "refusing to make a commitment", "refusing to make a commitment", "refusing to make a commitment on America's response"], "answer_start": [453, 453, 453, 453]}, "prediction_text": "Declined."}
{"answers": {"text": ["April 20", "April 20", "April", "April", "April 20"], "answer_start": [3, 3, 3, 3, 3]}, "prediction_text": "April 20."}
{"answers": {"text": ["Lyndon B. Johnson", "Lyndon B. Johnson", "Lyndon B. Johnson", "Lyndon B. Johnson", "Lyndon B. Johnson"], "answer_start": [51, 51, 51, 51, 51]}, "prediction_text": "Lyndon B. Johnson"}
{"answers": {"text": ["approximately one week", "one week", "one week", "one week", "one week"], "answer_start": [224, 238, 238, 238, 238]}, "prediction_text": "One week."}
{"answers": {"text": ["neither making maximum effort nor achieving results necessary", "are neither making maximum effort nor achieving results necessary", "we are neither making maximum effort nor achieving results necessary if this country is to reach a position of leadership", "we are neither making maximum effort nor achieving results necessary"], "answer_start": [278, 274, 271, 271]}, "prediction_text": "Johnson concluded that a manned Moon landing was far enough in the future that it was likely the United States would achieve it first."}
{"answers": {"text": ["Robert R. Gilruth", "Robert R. Gilruth's", "Robert R. Gilruth's", "Robert R. Gilruth", "Robert R. Gilruth"], "answer_start": [82, 82, 82, 82, 82]}, "prediction_text": "Robert R. Gilruth"}
{"answers": {"text": ["NASA's Langley Research Center", "Houston", "NASA's Langley Research Center", "Langley Research Center", "Langley"], "answer_start": [184, 353, 184, 191, 191]}, "prediction_text": "Langley Research Center"}
{"answers": {"text": ["Houston, Texas", "Houston", "Houston, Texas", "Houston, Texas", "Houston"], "answer_start": [353, 353, 353, 353, 353]}, "prediction_text": "Houston, Texas"}
{"answers": {"text": ["Rice University", "Rice", "Rice University", "Rice University"], "answer_start": [388, 388, 388, 388]}, "prediction_text": "Rice University"}
{"answers": {"text": ["Florida", "Florida", "Florida", "Florida"], "answer_start": [618, 618, 618, 618]}, "prediction_text": "Florida"}
{"answers": {"text": ["Merritt Island", "Florida", "Merritt Island", "Merritt Island", "Merritt Island"], "answer_start": [444, 82, 444, 444, 444]}, "prediction_text": "Merritt Island."}
{"answers": {"text": ["Kurt H. Debus", "construction of the center was conducted by Kurt H. Debus, a member of Dr. Wernher von Braun's", "Kurt H. Debus", "Kurt H. Debus", "Kurt H. Debus,"], "answer_start": [532, 488, 532, 532, 532]}, "prediction_text": "Kurt H. Debus"}
{"answers": {"text": ["Director", "Director", "Director", "first Director", "Director"], "answer_start": [653, 653, 653, 647, 653]}, "prediction_text": "Director"}
{"answers": {"text": ["Kennedy", "Kennedy", "Kennedy", "Kennedy", "Kennedy"], "answer_start": [837, 837, 837, 837, 837]}, "prediction_text": "Kurt H. Debus"}
{"answers": {"text": ["three", "several", "three", "three pads", "three"], "answer_start": [338, 299, 338, 338, 338]}, "prediction_text": "2"}
{"answers": {"text": ["Apollo spacecraft", "Apollo", "Apollo", "Apollo"], "answer_start": [602, 602, 602, 602]}, "prediction_text": "Apollo spacecraft"}
{"answers": {"text": ["250,000 feet", "250,000 feet", "250,000", "250,000 feet", "250,000 feet ("], "answer_start": [721, 721, 721, 721, 721]}, "prediction_text": "76 km."}
{"answers": {"text": ["130 million cubic foot", "130 million cubic foot", "3.7 million cubic meter", "3.7 million cubic meter", "130 million cubic foot"], "answer_start": [67, 67, 91, 91, 67]}, "prediction_text": "130 million cubic feet (3.7 million cubic meter)"}
{"answers": {"text": ["Dr. George E. Mueller", "Mueller", "George E. Mueller", "Dr. George E. Mueller", "George E. Mueller"], "answer_start": [167, 216, 171, 167, 171]}, "prediction_text": "Dr. George E. Mueller"}
{"answers": {"text": ["July 23, 1963", "1963", "July 23, 1963", "July 23, 1963", "July 23, 1963,"], "answer_start": [486, 495, 486, 486, 486]}, "prediction_text": "July 23, 1963"}
{"answers": {"text": ["D. Brainerd Holmes", "Robert Seamans", "Brainerd Holmes", "D. Brainerd Holmes", "D. Brainerd Holmes"], "answer_start": [637, 412, 640, 637, 637]}, "prediction_text": "D. Brainerd Holmes"}
{"answers": {"text": ["Mueller", "Brainerd Holmes", "Mueller", "Mueller", "Mueller"], "answer_start": [888, 640, 888, 888, 888]}, "prediction_text": "Robert Seamans"}
{"answers": {"text": ["Air Force missile projects", "Air Force missile projects", "Air Force missile projects", "missile projects", "Air Force missile projects"], "answer_start": [36, 36, 36, 46, 36]}, "prediction_text": "Air Force missile projects."}
{"answers": {"text": ["United States Air Force", "Air Force", "Air Force", "Air Force", "Air Force"], "answer_start": [153, 167, 167, 36, 167]}, "prediction_text": "Air Force"}
{"answers": {"text": ["General Samuel C. Phillips", "Bernard A. Schriever", "Samuel C. Phillips", "Samuel C. Phillips", "General Samuel C. Phillips"], "answer_start": [217, 379, 225, 225, 217]}, "prediction_text": "General Samuel C. Phillips"}
{"answers": {"text": ["January 1964, until it achieved the first manned landing in July 1969", "1964, until it achieved the first manned landing in July 1969,", "from January 1964, until it achieved the first manned landing in July 1969", "January 1964, until it achieved the first manned landing in July 1969", "January 1964, until it achieved the first manned landing in July 1969,"], "answer_start": [588, 596, 583, 588, 588]}, "prediction_text": "Until July 1969."}
{"answers": {"text": ["Apollo Program Director", "Apollo Program Director", "Apollo Program Director", "Apollo Program Director", "Apollo Program Director"], "answer_start": [514, 514, 514, 514, 514]}, "prediction_text": "OMSF program controller"}
{"answers": {"text": ["a rendezvous \u2014let alone a docking", "rendezvous", "rendezvous", "rendezvous", "docking"], "answer_start": [105, 107, 107, 107, 131]}, "prediction_text": "rendezvous."}
{"answers": {"text": ["1961", "1960", "1961", "1961", "1961"], "answer_start": [9, 396, 9, 9, 9]}, "prediction_text": "1961"}
{"answers": {"text": ["Robert Seamans", "Robert Seamans", "Robert Seamans", "Robert Seamans", "Robert Seamans;"], "answer_start": [599, 599, 599, 599, 599]}, "prediction_text": "Robert Seamans"}
{"answers": {"text": ["Nicholas E. Golovin", "Nicholas E. Golovin", "Nicholas E. Golovin", "Nicholas E. Golovin", "Nicholas E. Golovin", "Nicholas E. Golovin"], "answer_start": [89, 89, 89, 89, 89, 89]}, "prediction_text": "Nicholas E. Golovin"}
{"answers": {"text": ["July 1961", "1961", "1961", "July 1961", "July 1961", "1961"], "answer_start": [112, 117, 117, 112, 112, 117]}, "prediction_text": "July 1961"}
{"answers": {"text": ["Manned Spacecraft Center", "Manned Spacecraft Center", "Manned Spacecraft Center", "Manned Spacecraft Center"], "answer_start": [578, 578, 578, 578]}, "prediction_text": "Manned Spacecraft Center (MSFC)"}
{"answers": {"text": ["Joseph Shea", "Joseph Shea,", "Joseph Shea,", "Joseph Shea", "Joseph Shea", ", Joseph Shea"], "answer_start": [720, 720, 720, 720, 720, 718]}, "prediction_text": "Joseph Shea"}
{"answers": {"text": ["Marshall Space Flight Center", "Marshall Space Flight Center", "Marshall Space Flight Center", "Marshall Space Flight Center", "Marshall Space Flight Center (MSFC)", "Marshall Space Flight Center"], "answer_start": [780, 780, 780, 780, 780, 780]}, "prediction_text": "Marshall Space Flight Center (MSFC)"}
{"answers": {"text": ["Jerome Wiesner", "Wiesner", "Jerome Wiesner", "Jerome Wiesner", "Jerome Wiesner"], "answer_start": [106, 113, 106, 106, 106]}, "prediction_text": "Jerome Wiesner"}
{"answers": {"text": ["Golovin", "Golovin", "Golovin", "Golovin", "Golovin"], "answer_start": [282, 282, 282, 282, 282]}, "prediction_text": "Jerome Wiesner"}
{"answers": {"text": ["NASA", "NASA", "NASA", "NASA"], "answer_start": [304, 304, 304, 304]}, "prediction_text": "NASA's Science Advisor Jerome Wiesner."}
{"answers": {"text": ["July 11, 1962", "1962", "July 11, 1962", "July 11, 1962", "July 11, 1962"], "answer_start": [569, 578, 569, 569, 569]}, "prediction_text": "July 11, 1962."}
{"answers": {"text": ["Wiesner", "Wiesner", "Wiesner", "Wiesner", "Wiesner"], "answer_start": [0, 149, 0, 0, 0]}, "prediction_text": "Wiesner"}
{"answers": {"text": ["\"No, that's no good\"", "\"No, that's no good", "No, that's no good", "No, that's no good", "\"No, that's no good"], "answer_start": [169, 169, 170, 170, 169]}, "prediction_text": "\"No, that's no good\""}
{"answers": {"text": ["Lunar Excursion Module", "Lunar Excursion Module", "Lunar Excursion Module", "Lunar Excursion Module", "Lunar Excursion Module"], "answer_start": [448, 448, 448, 448, 448]}, "prediction_text": "Lunar Excursion Module."}
{"answers": {"text": ["Grumman", "Grumman", "Grumman", "Grumman", "Grumman"], "answer_start": [736, 736, 736, 736, 736]}, "prediction_text": "Grumman"}
{"answers": {"text": ["spacecraft to be used as a \"lifeboat\"", "allowing the lander spacecraft to be used as a \"lifeboat", "allowing the lander spacecraft to be used as a \"lifeboat\"", "lifeboat", "allowing the lander spacecraft to be used as a \"lifeboat\""], "answer_start": [56, 36, 36, 84, 36]}, "prediction_text": "Lifeboat."}
{"answers": {"text": ["Apollo 13", "13", "Apollo 13", "Apollo 13", "Apollo 13"], "answer_start": [445, 452, 445, 445, 445]}, "prediction_text": "Apollo 13"}
{"answers": {"text": ["propulsion, electrical power and life support", "propulsion", "propulsion", "propulsion, electrical power and life support", "propulsion, electrical power and life support"], "answer_start": [559, 559, 559, 559, 559]}, "prediction_text": "Electrical power."}
{"answers": {"text": ["1964", "1964", "1964", "1964", "1964"], "answer_start": [230, 230, 230, 230, 230]}, "prediction_text": "1964"}
{"answers": {"text": ["cone-shaped", "cone", "cone", "cone-shaped", "cone"], "answer_start": [52, 52, 52, 52, 52]}, "prediction_text": "Cone-shaped."}
{"answers": {"text": ["Command/Service Module", "Command/Service Module (", "Command/Service Module", "Command/Service Module", "Command/Service Module"], "answer_start": [325, 325, 325, 325, 325]}, "prediction_text": "Command/Service Module (CSM)"}
{"answers": {"text": ["two", "two", "two men", "two"], "answer_start": [771, 771, 771, 771]}, "prediction_text": "Two."}
{"answers": {"text": ["three", "three", "three", "three astronauts", "three"], "answer_start": [70, 70, 70, 70, 70]}, "prediction_text": "3 astronauts."}
{"answers": {"text": ["ocean", "splashdown", "ocean", "ocean", "ocean"], "answer_start": [135, 526, 135, 135, 135]}, "prediction_text": "Earth ocean landing."}
{"answers": {"text": ["ablative heat shield", "ablative heat shield", "ablative heat shield", "ablative heat shield", "ablative heat shield"], "answer_start": [341, 341, 341, 341, 341]}, "prediction_text": "Ablative heat shield."}
{"answers": {"text": ["Parachutes", "Parachutes", "Parachutes", "Parachutes", "Parachutes"], "answer_start": [479, 479, 479, 479, 479]}, "prediction_text": "Parachutes."}
{"answers": {"text": ["5,560 kg", "5,560", "5,560", "5,560 kg", "5,560"], "answer_start": [653, 653, 653, 653, 653]}, "prediction_text": "5,560 kg"}
{"answers": {"text": ["Service Module (SM)", "cylindrical Service Module", "Service Module", "cylindrical Service Module", "Service Module"], "answer_start": [14, 2, 14, 2, 14]}, "prediction_text": "A high-gain S-band antenna."}
{"answers": {"text": ["high-gain S-band antenna", "high-gain S-band", "high-gain S-band", "high-gain S-band antenna", "S-band"], "answer_start": [218, 218, 218, 218, 228]}, "prediction_text": "High-gain S-band antenna."}
{"answers": {"text": ["discarded", "discarded", "discarded", "discarded", "discarded"], "answer_start": [416, 416, 416, 416, 416]}, "prediction_text": "Discarded."}
{"answers": {"text": ["51,300 pounds", "51,300", "51,300 pounds", "23,300 kg", "51,300"], "answer_start": [578, 578, 578, 593, 578]}, "prediction_text": "51,300 pounds (23,300 kg)"}
{"answers": {"text": ["orbital scientific instrument package", "orbital scientific instrument", "orbital scientific instrument package", "scientific instrument package"], "answer_start": [342, 342, 342, 672]}, "prediction_text": "Orbital scientific instrument package."}
{"answers": {"text": ["North American Aviation", "North American Aviation", "North American Aviation", "North American Aviation", "North American Aviation"], "answer_start": [0, 0, 0, 0, 0]}, "prediction_text": "North American Aviation"}
{"answers": {"text": ["twice the thrust", "twice the thrust required", "twice the thrust", "twice the thrust"], "answer_start": [320, 320, 320, 320]}, "prediction_text": "twice the thrust required for translunar flight."}
{"answers": {"text": ["1964", "1964", "1964", "1964", "1964"], "answer_start": [435, 435, 435, 435, 435]}, "prediction_text": "1964"}
{"answers": {"text": ["Saturn V", "Saturn V", "Saturn V", "Saturn V"], "answer_start": [92, 92, 92, 92]}, "prediction_text": "Saturn V launch vehicle"}
{"answers": {"text": ["two", "two", "two", "two astronauts", "two"], "answer_start": [71, 71, 71, 71, 71]}, "prediction_text": "Two."}
{"answers": {"text": ["Not", "Command Module", "Not", "take them back to orbit to rendezvous with the Command Module", "Not designed to fly through the Earth's atmosphere or return to Earth"], "answer_start": [165, 149, 165, 102, 165]}, "prediction_text": "No."}
{"answers": {"text": ["15,100 kg", "(15,100", "15,100", "15,100 kg", "15,100"], "answer_start": [710, 709, 710, 710, 710]}, "prediction_text": "33,300 pounds (15,100 kg)"}
{"answers": {"text": ["3 days", "3 days.", "over 3 days", "over 3 days", "3 days"], "answer_start": [870, 870, 865, 865, 870]}, "prediction_text": "3 days."}
{"answers": {"text": ["Wernher von Braun", "Wernher von Braun", "Wernher von Braun", "Wernher von Braun", "Wernher von Braun"], "answer_start": [33, 33, 33, 33, 33]}, "prediction_text": "Wernher von Braun"}
{"answers": {"text": ["Army", "Army", "Army", "Army", "Army"], "answer_start": [258, 258, 258, 258, 258]}, "prediction_text": "Army"}
{"answers": {"text": ["June 11, 1962", "1962", "June 11, 1962", "June 11, 1962", "June 11, 1962"], "answer_start": [590, 599, 590, 590, 590]}, "prediction_text": "June 11, 1962"}
{"answers": {"text": ["dummy upper stages filled with water", "dummy upper stages", "dummy upper stages", "dummy upper stages filled with water", "dummy upper stages filled with water."], "answer_start": [101, 101, 101, 101, 101]}, "prediction_text": "Upper stages filled with water."}
{"answers": {"text": ["1964 and 1965", "1964", "1964 and 1965", "1964 and 1965", "1964 and 1965"], "answer_start": [302, 302, 302, 302, 302]}, "prediction_text": "1964 and 1965."}
{"answers": {"text": ["Pegasus satellites", "Pegasus satellites,", "Pegasus satellites", "Pegasus satellites", "Pegasus satellites"], "answer_start": [395, 395, 395, 395, 395]}, "prediction_text": "Pegasus satellites."}
{"answers": {"text": ["frequency and severity of micrometeorite impacts", "translunar environment by measuring the frequency and severity of micrometeorite impacts.", "micrometeorite impacts", "safety of the translunar environment", "micrometeorite impacts."], "answer_start": [488, 448, 514, 434, 514]}, "prediction_text": "Micrometeorite impacts."}
{"answers": {"text": ["Saturn IB", "S-IB", "Saturn IB", "Saturn IB", "Saturn IB"], "answer_start": [4, 59, 4, 4, 4]}, "prediction_text": "Saturn I"}
{"answers": {"text": ["200,000 lbf", "200,000 lbf", "200,000 lbf", "200,000 lbf (890 kN) of thrust", "200,000 lbf"], "answer_start": [275, 275, 275, 275, 275]}, "prediction_text": "200,000 lbf (890 kN)"}
{"answers": {"text": ["third stage", "third", "third stage", "second stage"], "answer_start": [358, 358, 358, 143]}, "prediction_text": "Third stage."}
{"answers": {"text": ["40,000 pounds", "40,000 pounds", "40,000 pounds", "40,000 pounds (18,100 kg)", "over 40,000 pounds"], "answer_start": [417, 417, 417, 417, 412]}, "prediction_text": "40,000 pounds (18,100 kg)"}
{"answers": {"text": ["three-stage Saturn V", "Saturn V", "Saturn V", "Saturn V", "Saturn V"], "answer_start": [4, 16, 16, 16, 16]}, "prediction_text": "Saturn V"}
{"answers": {"text": ["33 feet", "33", "33", "33"], "answer_start": [92, 92, 92, 92]}, "prediction_text": "33 feet."}
{"answers": {"text": ["three", "third", "three", "three-stage", "three"], "answer_start": [4, 506, 4, 4, 4]}, "prediction_text": "3"}
{"answers": {"text": ["burned liquid hydrogen", "burned liquid hydrogen", "burned liquid hydrogen", "burned liquid hydrogen", "burned liquid hydrogen"], "answer_start": [474, 474, 474, 474, 474]}, "prediction_text": "Burning liquid hydrogen."}
{"answers": {"text": ["Mercury and Gemini", "Mercury", "Mercury and Gemini", "Project Mercury and Gemini", "Mercury and Gemini"], "answer_start": [51, 51, 51, 43, 51]}, "prediction_text": "Project Mercury veterans, Gemini veterans, and two later astronaut groups."}
{"answers": {"text": ["All missions", "17", "All missions", "All"], "answer_start": [118, 517, 118, 118]}, "prediction_text": "2"}
{"answers": {"text": ["Dr. Harrison Schmitt", "Harrison Schmitt", "Harrison Schmitt", "Harrison Schmitt", "Harrison Schmitt"], "answer_start": [375, 379, 379, 379, 379]}, "prediction_text": "Dr. Harrison Schmitt"}
{"answers": {"text": ["Apollo 17", "17", "Apollo 17", "Apollo 17", "Apollo 17"], "answer_start": [510, 517, 510, 510, 510]}, "prediction_text": "Apollo 17"}
{"answers": {"text": ["last mission", "last mission", "first NASA scientist astronaut to fly in space", "last mission,"], "answer_start": [496, 496, 418, 496]}, "prediction_text": "Apollo 17 was significant for the lunar geology training of the Apollo landing crews."}
{"answers": {"text": ["32", "32", "32", "32", "32"], "answer_start": [17, 17, 17, 17, 17]}, "prediction_text": "32"}
{"answers": {"text": ["Distinguished Service Medal", "Distinguished Service Medal", "Distinguished Service Medal", "Distinguished Service Medal", "Distinguished Service Medal,"], "answer_start": [63, 63, 63, 63, 63]}, "prediction_text": "Distinguished Service Medal"}
{"answers": {"text": ["1969", "1969", "1969", "1969", "1969"], "answer_start": [302, 302, 302, 302, 302]}, "prediction_text": "1969"}
{"answers": {"text": ["discipline problems", "discipline problems", "discipline problems", "discipline problems"], "answer_start": [552, 552, 552, 552]}, "prediction_text": "Discipline problems with the Flight Director's orders."}
{"answers": {"text": ["Apollo 8", "Apollo 8", "Apollo 7", "Apollo 8", "Apollo 8"], "answer_start": [347, 347, 420, 347, 347]}, "prediction_text": "Apollo 8 onward."}
{"answers": {"text": ["1966", "1966", "1966", "1966", "1966"], "answer_start": [67, 67, 67, 67, 67]}, "prediction_text": "1966"}
{"answers": {"text": ["265.7 nautical miles", "265.7", "492.1 km", "265.7"], "answer_start": [158, 158, 180, 158]}, "prediction_text": "265.7 nautical miles (492.1 km)"}
{"answers": {"text": ["25,700 km", "617.1", "1,142.9", "8,477 km", "(25,700 km"], "answer_start": [398, 314, 336, 230, 397]}, "prediction_text": "13,900 km."}
{"answers": {"text": ["heat shield", "validated the Service Module engine and the Command Module heat shield.", "heat shield", "Service Module engine and the Command Module heat shield", "heat shield."], "answer_start": [514, 455, 514, 469, 514]}, "prediction_text": "Engine and heat shield."}
{"answers": {"text": ["unmanned", "unmanned", "unmanned", "unmanned"], "answer_start": [9, 9, 9, 9]}, "prediction_text": "2"}
{"answers": {"text": ["new Apollo spacesuit", "new Apollo spacesuit", "new Apollo spacesuit, designed to accommodate lunar extravehicular activity", "new Apollo spacesuit", "new Apollo spacesuit"], "answer_start": [348, 348, 348, 348, 348]}, "prediction_text": "Apollo spacesuit."}
{"answers": {"text": ["traditional visor helmet", "visor helmet", "visor helmet", "visor helmet", "visor helmet"], "answer_start": [435, 447, 447, 447, 447]}, "prediction_text": "The traditional visor helmet."}
{"answers": {"text": ["a water-cooled undergarment", "water-cooled undergarment.", "water-cooled", "water-cooled undergarment", "water-cooled undergarment."], "answer_start": [571, 573, 573, 573, 573]}, "prediction_text": "Water-cooled undergarment."}
{"answers": {"text": ["Lunar Module Pilot", "Lunar Module Pilot", "Lunar Module Pilot", "Lunar Module Pilot", "Lunar Module Pilot"], "answer_start": [285, 285, 285, 285, 285]}, "prediction_text": "Lunar Module Pilot (LMP)"}
{"answers": {"text": ["Deke Slayton", "Deke Slayton,", "Deke Slayton", "Deke Slayton", "Deke Slayton"], "answer_start": [0, 0, 0, 0, 0]}, "prediction_text": "Deke Slayton"}
{"answers": {"text": ["Mercury", "Eisele", "Mercury", "Mercury astronaut", "Mercury"], "answer_start": [27, 248, 27, 27, 27]}, "prediction_text": "Gemini and Apollo programs."}
{"answers": {"text": ["1966", "1966", "1966", "January 1966", "1966"], "answer_start": [169, 498, 498, 161, 169]}, "prediction_text": "1966"}
{"answers": {"text": ["Donn F. Eisele", "Slayton", "Donn F. Eisele", "Grissom", "Eisele"], "answer_start": [240, 399, 240, 180, 269]}, "prediction_text": "Grissom"}
{"answers": {"text": ["AS-205", "Chaffee", "prime", "prime crew", "AS-205"], "answer_start": [759, 425, 744, 744, 759]}, "prediction_text": "Chaffee"}
{"answers": {"text": ["canceled", "canceled", "canceled", "canceled", "canceled"], "answer_start": [41, 41, 41, 41, 41]}, "prediction_text": "Canceled."}
{"answers": {"text": ["August 1967", "August 1967", "August 1967", "1967"], "answer_start": [355, 355, 355, 362]}, "prediction_text": "August 1967"}
{"answers": {"text": ["AS-205/208", "AS-205/208", "AS-205/208", "AS-205/208", "AS-205/208"], "answer_start": [321, 321, 321, 321, 321]}, "prediction_text": "AS-205/208 or AS-258."}
{"answers": {"text": ["Apollo 1 backup crew", "promoted", "Apollo 1 backup crew", "Apollo 1 backup crew", "Apollo 1 backup crew."], "answer_start": [498, 405, 498, 498, 498]}, "prediction_text": "Apollo 1 backup crew."}
{"answers": {"text": ["Samuel Phillips", "Mueller", "Samuel Phillips", "Samuel Phillips", "Samuel Phillips"], "answer_start": [151, 115, 151, 151, 151]}, "prediction_text": "Samuel Phillips"}
{"answers": {"text": ["\"tiger team\"", "tiger", "tiger team", "tiger team", "\"tiger team\""], "answer_start": [177, 178, 178, 178, 177]}, "prediction_text": "Tiger team"}
{"answers": {"text": ["1967", "1965", "1967", "1967", "1967"], "answer_start": [611, 60, 611, 611, 611]}, "prediction_text": "1967"}
{"answers": {"text": ["George Mueller", "Seamans", "George Mueller", "George Mueller", "Mueller"], "answer_start": [108, 472, 108, 108, 115]}, "prediction_text": "George Mueller"}
{"answers": {"text": ["altitude chamber", "altitude chamber", "altitude chamber", "altitude chamber"], "answer_start": [201, 201, 201, 201]}, "prediction_text": "North American."}
{"answers": {"text": ["Grissom, White, and Chaffee", "Apollo 1", "Grissom, White, and Chaffee", "Grissom, White, and Chaffee"], "answer_start": [0, 57, 0, 0]}, "prediction_text": "Grissom, White, and Chaffee."}
{"answers": {"text": ["launch countdown", "simulate a launch countdown on", "simulate a launch countdown", "simulate a launch countdown", "launch countdown"], "answer_start": [314, 303, 303, 303, 314]}, "prediction_text": "Transfer from pad-supplied to internal power."}
{"answers": {"text": ["North American", "North American", "North American", "North American"], "answer_start": [174, 174, 174, 174]}, "prediction_text": "North American"}
{"answers": {"text": ["strange odor in their spacesuits", "odor", "strange odor in their spacesuits", "strange odor", "strange odor in their spacesuits"], "answer_start": [129, 137, 129, 129, 129]}, "prediction_text": "Odor in spacesuits."}
{"answers": {"text": ["January 27, 1967", "1967", "January 27, 1967", "January 27, 1967", "January 27, 1967"], "answer_start": [43, 55, 43, 43, 43]}, "prediction_text": "January 27, 1967"}
{"answers": {"text": ["electrical fire", "ectrical fire", "delayed the sealing of the hatch", "electrical fire", "communications problems"], "answer_start": [326, 328, 169, 326, 209]}, "prediction_text": "The crew noticed a strange odor in their spacesuits."}
{"answers": {"text": ["asphyxiated", "asphyxiated", "asphyxiated", "asphyxiated"], "answer_start": [589, 589, 589, 589]}, "prediction_text": "Asphyxiated."}
{"answers": {"text": ["100% oxygen", "electrical", "100% oxygen", "oxygen atmosphere", "100% oxygen"], "answer_start": [403, 326, 403, 408, 403]}, "prediction_text": "100% oxygen atmosphere"}
{"answers": {"text": ["both houses of Congress", "Congress", "both houses of Congress", "both houses of Congress", "both houses of Congress."], "answer_start": [64, 79, 64, 64, 64]}, "prediction_text": "Congress"}
{"answers": {"text": ["deficiencies", "workmanship and quality control", "deficiencies existed in Command Module design, workmanship and quality control", "\"deficiencies existed in Command Module design, workmanship and quality control.\""], "answer_start": [194, 241, 194, 193]}, "prediction_text": "Command Module design."}
{"answers": {"text": ["George Low", "Low", "George Low", "George Low", "George Low."], "answer_start": [504, 511, 504, 504, 504]}, "prediction_text": "George Low"}
{"answers": {"text": ["immediately", "immediately", "immediately", "immediately"], "answer_start": [5, 5, 5, 5]}, "prediction_text": "Immediately."}
{"answers": {"text": ["nitrogen/oxygen mixture", "nitrogen/oxygen", "nitrogen/oxygen", "nitrogen/oxygen mixture", "nitrogen/oxygen"], "answer_start": [149, 149, 149, 149, 149]}, "prediction_text": "Nitrogen/oxygen mixture."}
{"answers": {"text": ["flammable cabin and space suit materials", "flammable cabin", "flammable", "flammable cabin and space suit materials", "flammable cabin and space suit materials."], "answer_start": [237, 237, 237, 237, 237]}, "prediction_text": "Flammable cabin and space suit materials."}
{"answers": {"text": ["quick-release, outward opening door", "Block I plug-type hatch cover", "quick-release", "quick-release, outward opening door", "quick-release, outward opening door"], "answer_start": [374, 337, 374, 374, 374]}, "prediction_text": "Quick-release, outward opening door."}
{"answers": {"text": ["discontinued", "discontinued", "unmanned Saturn V flights", "discontinued", "discontinued"], "answer_start": [416, 416, 495, 416, 416]}, "prediction_text": "NASA discontinued the manned Block I program."}
{"answers": {"text": ["fire-resistant Block II", "Crew members", "fire-resistant Block II", "fire-resistant", "modified, fire-resistant Block II space suits,"], "answer_start": [573, 522, 573, 573, 563]}, "prediction_text": "Modified, fire-resistant Block II space suits."}
{"answers": {"text": ["sequence", "manned lunar landing.", "had to be successfully accomplished", "sequence of mission types", "sequence"], "answer_start": [38, 130, 70, 38, 38]}, "prediction_text": "Mission types."}
{"answers": {"text": ["successful", "successfully accomplished", "successfully accomplished", "successfully accomplished"], "answer_start": [172, 172, 80, 80]}, "prediction_text": "Successfully accomplished."}
{"answers": {"text": ["letters", "letters", "letters were used instead of numbers", "letters"], "answer_start": [319, 319, 319, 319]}, "prediction_text": "Letters."}
{"answers": {"text": ["AS-501", "AS-501", "AS-501", "AS-501", "AS-501"], "answer_start": [110, 110, 110, 110, 110]}, "prediction_text": "AS-501"}
{"answers": {"text": ["heat shield", "Service Module engine", "heat shield", "capability of the Command Module's heat shield to survive a trans-lunar reentry", "capability of the Command Module's heat shield to survive a trans-lunar reentry"], "answer_start": [248, 323, 248, 213, 213]}, "prediction_text": "The CM's heat shield."}
{"answers": {"text": ["April 4, 1968", "1968", "April 4, 1968", "April 4, 1968", "April 4, 1968"], "answer_start": [450, 459, 450, 450, 450]}, "prediction_text": "April 4, 1968."}
{"answers": {"text": ["third unmanned test", "Apollo 6", "third unmanned test", "cancelling a third unmanned test", "third unmanned test."], "answer_start": [1365, 1288, 1365, 1352, 1365]}, "prediction_text": "The third unmanned test."}
{"answers": {"text": ["Apollo 5", "Apollo 5", "Apollo 5", "Apollo 5", "Apollo 5"], "answer_start": [0, 0, 0, 0, 0]}, "prediction_text": "Apollo 5 (AS-204)"}
{"answers": {"text": ["pad 37", "37", "37", "pad 37", "pad 37"], "answer_start": [89, 93, 93, 89, 89]}, "prediction_text": "Pad 37"}
{"answers": {"text": ["Grumman", "Low", "Grumman", "Grumman", "Grumman"], "answer_start": [474, 520, 474, 474, 474]}, "prediction_text": "George Low"}
{"answers": {"text": ["success", "success", "LM engines were successfully test-fired and restarted", "successfully"], "answer_start": [194, 194, 178, 194]}, "prediction_text": "Success"}
{"answers": {"text": ["\"fire-in-the-hole\"", "fire-in-the-hole", "fire-in-the-hole", "fire-in-the-hole", "\"fire-in-the-hole\""], "answer_start": [372, 373, 373, 373, 372]}, "prediction_text": "Fire-in-the-hole test."}
{"answers": {"text": ["two Saturn IBs", "V", "two Saturn IBs", "Saturn V", "two Saturn IBs"], "answer_start": [136, 123, 136, 116, 136]}, "prediction_text": "Saturn V"}
{"answers": {"text": ["Zond 5", "Zond 5", "Zond 5", "Zond 5", "Zond 5"], "answer_start": [602, 602, 602, 602, 602]}, "prediction_text": "Zond 5"}
{"answers": {"text": ["Christmas Eve", "Christmas Eve", "Christmas Eve", "Christmas Eve"], "answer_start": [966, 966, 966, 966]}, "prediction_text": "Christmas"}
{"answers": {"text": ["orbit the Moon", "orbit the Moon", "to orbit the Moon", "orbit the Moon", "orbit the Moon"], "answer_start": [370, 370, 367, 370, 370]}, "prediction_text": "orbiting the Moon."}
{"answers": {"text": ["human cosmonauts", "human cosmonauts", "human cosmonauts", "human cosmonauts"], "answer_start": [667, 667, 667, 667]}, "prediction_text": "Animals."}
{"answers": {"text": ["Gemini", "all-Gemini veteran", "Gemini", "all-Gemini veteran crew", "Gemini"], "answer_start": [63, 59, 63, 59, 63]}, "prediction_text": "Gemini"}
{"answers": {"text": ["July 1969", "July", "July 1969", "July 20, 1969", "July 1969"], "answer_start": [43, 43, 43, 240, 43]}, "prediction_text": "July 1969"}
{"answers": {"text": ["black-and-white television", "automated scientific instruments", "black-and-white television", "black-and-white television", "black-and-white television"], "answer_start": [516, 455, 516, 516, 516]}, "prediction_text": "Black-and-white television."}
{"answers": {"text": ["Neil Armstrong, Michael Collins and Buzz Aldrin", "Neil Armstrong, Michael Collins and Buzz Aldrin", "Neil Armstrong, Michael Collins and Buzz Aldrin", "Neil Armstrong, Michael Collins and Buzz Aldrin", "Neil Armstrong, Michael Collins and Buzz Aldrin."], "answer_start": [97, 97, 97, 97, 97]}, "prediction_text": "Neil Armstrong, Michael Collins, Buzz Aldrin."}
{"answers": {"text": ["July 24", "July 24.", "July 24", "July 24", "July 24"], "answer_start": [592, 592, 592, 592, 592]}, "prediction_text": "July 24."}
{"answers": {"text": ["Apollo 12", "Apollo 12", "Apollo 12", "Apollo 12", "Apollo 12"], "answer_start": [107, 107, 107, 107, 107]}, "prediction_text": "Apollo 12"}
{"answers": {"text": ["Surveyor 3", "Surveyor 3", "Surveyor 3", "Surveyor 3", "Surveyor 3"], "answer_start": [148, 148, 148, 148, 148]}, "prediction_text": "Surveyor 3"}
{"answers": {"text": ["returned to Earth", "color television camera", "returned to Earth", "removed some parts", "returned to Earth."], "answer_start": [575, 351, 575, 545, 575]}, "prediction_text": "They returned them to Earth."}
{"answers": {"text": ["the Sun", "Sun", "the Sun", "the Sun", "pointed into the Sun."], "answer_start": [426, 430, 426, 426, 413]}, "prediction_text": "Sun."}
{"answers": {"text": ["Lunar Roving Vehicle (LRV)", "payload capacity", "Lunar Roving Vehicle", "lunar orbital sensors and cameras", "Lunar Roving Vehicle"], "answer_start": [577, 260, 577, 406, 577]}, "prediction_text": "The Lunar Roving Vehicle (LRV)"}
{"answers": {"text": ["Block II spacesuit", "Block II spacesuit", "the mass of the CSM and LM", "Block II spacesuit"], "answer_start": [688, 688, 190, 688]}, "prediction_text": "Block II spacesuit."}
{"answers": {"text": ["eight", "five", "five", "eight", "eight"], "answer_start": [133, 230, 230, 133, 133]}, "prediction_text": "8"}
{"answers": {"text": ["over three days", "three days.", "over three days", "over three days", "over three days"], "answer_start": [524, 529, 524, 524, 524]}, "prediction_text": "Three days."}
{"answers": {"text": ["mass", "exploration area", "mass", "mass", "mass"], "answer_start": [194, 619, 194, 194, 194]}, "prediction_text": "Mass."}
{"answers": {"text": ["liquid oxygen tank exploded", "liquid oxygen tank exploded,", "liquid oxygen tank exploded", "liquid oxygen tank exploded", "liquid oxygen tank exploded, disabling the Service Module"], "answer_start": [263, 263, 263, 263, 263]}, "prediction_text": "The crew was grounded again."}
{"answers": {"text": ["rookies", "Jack Swigert, and Fred Haise", "two rookies", "rookies"], "answer_start": [127, 163, 123, 127]}, "prediction_text": "Two rookies."}
{"answers": {"text": ["grounded", "oxygen tank was redesigned", "grounded", "Apollo was grounded", "grounded"], "answer_start": [634, 686, 634, 623, 634]}, "prediction_text": "Grounded."}
{"answers": {"text": ["oxygen tank", "oxygen tank", "oxygen tank", "oxygen tank", "oxygen tank"], "answer_start": [686, 686, 686, 686, 686]}, "prediction_text": "The oxygen tank."}
{"answers": {"text": ["April 1970", "April", "April 1970", "April 1970", "April 1970,"], "answer_start": [195, 195, 195, 195, 195]}, "prediction_text": "April 1970"}
{"answers": {"text": ["Apollo 20", "Saturn V to", "Apollo 20", "Apollo 20", "Apollo 20."], "answer_start": [255, 79, 255, 255, 255]}, "prediction_text": "Apollo 20"}
{"answers": {"text": ["began to shrink", "NASA also had to make funds available", "began to shrink", "began to shrink", "shrink"], "answer_start": [292, 348, 292, 292, 301]}, "prediction_text": "Shrinkage."}
{"answers": {"text": ["museum exhibits", "museum exhibits", "museum exhibits", "museum exhibits", "museum exhibits"], "answer_start": [535, 535, 535, 535, 535]}, "prediction_text": "Museum exhibits."}
{"answers": {"text": ["1971", "1971", "1971", "1971", "1971"], "answer_start": [440, 440, 440, 440, 440]}, "prediction_text": "1971"}
{"answers": {"text": ["extremely old", "old", "extremely old", "extremely old", "old"], "answer_start": [38, 48, 38, 38, 48]}, "prediction_text": "Older."}
{"answers": {"text": ["4.6 billion years", "4.6 billion years", ".2 billion years for the basaltic samples derived from the lunar maria, to about 4.6 billion years for samples derived from the highlands crust", "4.6 billion"], "answer_start": [243, 243, 162, 243]}, "prediction_text": "3.2 billion years."}
{"answers": {"text": ["KREEP", "calcium-rich feldspar mineral anorthite,", "anorthite", "feldspar mineral anorthite", "KREEP"], "answer_start": [791, 659, 689, 672, 791]}, "prediction_text": "KREEP"}
{"answers": {"text": ["Genesis Rock", "Genesis", "Genesis Rock", "Genesis Rock", "anorthosite"], "answer_start": [503, 503, 503, 503, 604]}, "prediction_text": "Genesis Rock"}
{"answers": {"text": ["micrometeoroid impact craters", "micrometeoroid impact craters", "micrometeoroid impact craters", "micrometeoroid impact craters", "micrometeoroid impact craters"], "answer_start": [100, 100, 100, 100, 100]}, "prediction_text": "micrometeoroid impact craters"}
{"answers": {"text": ["impact process effects", "micrometeoroid impact craters", "high pressure shock waves", "subjected to high pressure shock waves", "evidence of impact process effects"], "answer_start": [38, 100, 234, 221, 26]}, "prediction_text": "Impact craters."}
{"answers": {"text": ["materials melted near an impact crater.", "materials melted near an impact crater", "materials melted near an impact crater", "materials melted near an impact crater", "materials melted near an impact crater."], "answer_start": [350, 350, 350, 350, 350]}, "prediction_text": "Materials melted near an impact crater."}
{"answers": {"text": ["$170 billion", "170 billion", "170 billion", "$170 billion", "roughly $170 billion."], "answer_start": [131, 132, 132, 131, 123]}, "prediction_text": "$170 billion"}
{"answers": {"text": ["15", "15", "15", "15 Saturn V rockets", "15"], "answer_start": [214, 214, 214, 214, 214]}, "prediction_text": "16"}
{"answers": {"text": ["$20.4 billion", "20.4 billion,", "20.4 billion", "$20.4 billion", "$20.4 billion"], "answer_start": [615, 616, 616, 615, 615]}, "prediction_text": "$20.4 billion"}
{"answers": {"text": ["Apollo X", "Apollo Extension Series", "Apollo Extension Series", "Apollo X", "Apollo Extension Series"], "answer_start": [142, 117, 117, 142, 117]}, "prediction_text": "Apollo X"}
{"answers": {"text": ["Apollo Applications Program", "Apollo Applications Program", "Apollo Applications Program", "Apollo Applications Program", "Apollo Applications Program"], "answer_start": [514, 514, 514, 514, 514]}, "prediction_text": "Apollo Applications Program"}
{"answers": {"text": ["Venus", "Venus", "Venus", "Venus"], "answer_start": [809, 809, 809, 809]}, "prediction_text": "Venus"}
{"answers": {"text": ["1973", "1973", "1973", "1973", "1973"], "answer_start": [188, 188, 188, 188, 188]}, "prediction_text": "1973"}
{"answers": {"text": ["on the ground", "on the ground", "constructed complete on the ground", "on the ground"], "answer_start": [136, 136, 115, 136]}, "prediction_text": "Earth's surface."}
{"answers": {"text": ["February 8, 1974", "February 8, 1974,", "February 8, 1974", "February 8, 1974", "February 8, 1974"], "answer_start": [406, 406, 406, 406, 406]}, "prediction_text": "February 8, 1974"}
{"answers": {"text": ["Apollo Telescope Mount", "Apollo Telescope Mount", "solar telescope", "Apollo Telescope Mount", "Apollo Telescope Mount"], "answer_start": [259, 259, 287, 259, 259]}, "prediction_text": "Apollo Telescope Mount"}
{"answers": {"text": ["Lunar Reconnaissance Orbiter", "Japan Aerospace Exploration Agency's SELENE", "Reconnaissance Orbiter", "robotic Lunar Reconnaissance Orbiter", "Lunar Reconnaissance Orbiter"], "answer_start": [205, 9, 211, 197, 205]}, "prediction_text": "Lunar Reconnaissance Orbiter"}
{"answers": {"text": ["Apollo 11", "Apollo 11", "Apollo 11", "Apollo 11"], "answer_start": [579, 579, 579, 579]}, "prediction_text": "Apollo 11."}
{"answers": {"text": ["unknown", "retain their original colors", "unknown", "degree to which these flags retain their original colors remains unknown", "unknown"], "answer_start": [799, 762, 799, 734, 799]}, "prediction_text": "Faded."}
{"answers": {"text": ["Apollo 8", "Apollo 8", "Apollo 8", "Apollo 8", "Apollo 8"], "answer_start": [12, 12, 12, 12, 12]}, "prediction_text": "Apollo 8"}
{"answers": {"text": ["Book of Genesis", "Genesis", "Genesis", "Book of Genesis", "Genesis"], "answer_start": [141, 149, 149, 141, 149]}, "prediction_text": "Genesis"}
{"answers": {"text": ["one-quarter", "one-quarter", "one-quarter", "one-quarter of the population", "one-quarter"], "answer_start": [195, 195, 195, 195, 195]}, "prediction_text": "One-quarter."}
{"answers": {"text": ["inspiring end", "inspiring", "inspiring", "inspiring end", "inspiring"], "answer_start": [370, 370, 370, 370, 370]}, "prediction_text": "Inspiring."}
{"answers": {"text": ["special Apollo TV camera", "Apollo TV camera", "Apollo TV", "Apollo TV camera", "Apollo TV camera"], "answer_start": [40, 48, 48, 48, 48]}, "prediction_text": "Apollo TV camera"}
{"answers": {"text": ["incompatible", "had to be converted", "incompatible", "format incompatible with broadcast TV", "incompatible"], "answer_start": [92, 160, 92, 85, 92]}, "prediction_text": "Incompatible."}
{"answers": {"text": ["magnetic tape shortage", "remove massive numbers of magnetic tapes from the National Archives and Records Administration to be recorded over", "magnetic tape shortage", "magnetic tape shortage", "magnetic tape shortage"], "answer_start": [284, 324, 284, 284, 284]}, "prediction_text": "Magnetic tape shortage."}
{"answers": {"text": ["newer satellite data", "the National Archives and Records Administration", "newer satellite data", "satellite data", "satellite data"], "answer_start": [444, 370, 444, 450, 450]}, "prediction_text": "Satellite data."}
{"answers": {"text": ["Stan Lebar", "Nafzger", "Stan Lebar", "Stan Lebar", "Stan Lebar"], "answer_start": [466, 602, 466, 466, 466]}, "prediction_text": "Stan Lebar"}
{"answers": {"text": ["Nafzger", "Nafzger", "Nafzger", "Nafzger", "Nafzger"], "answer_start": [102, 102, 102, 102, 102]}, "prediction_text": "Nafzger"}
{"answers": {"text": ["without destroying historical legitimacy", "Lowry Digital f", "without destroying historical legitimacy", "processed to remove random noise and camera shake without destroying historical legitimacy", "without destroying historical legitimacy"], "answer_start": [221, 126, 221, 171, 221]}, "prediction_text": "Destroy historical legitimacy."}
{"answers": {"text": ["kinescope recordings", "kinescope", "kinescope recordings", "kinescope"], "answer_start": [330, 330, 330, 330]}, "prediction_text": "kinescope recordings"}
{"answers": {"text": ["Lowry Digital", "CBS", "Lowry Digital", "Lowry Digital", "Lowry Digital"], "answer_start": [126, 308, 126, 126, 126]}, "prediction_text": "Nafzger"}
{"answers": {"text": ["black and white", "black and white,", "black and white", "remaining in black and white", "remaining in black and white,"], "answer_start": [414, 414, 414, 401, 401]}, "prediction_text": "Black and white."}
{"answers": {"text": ["primary law, secondary law and supplementary law.", "primary law, secondary law and supplementary law", "primary law, secondary law and supplementary law", "primary law, secondary law and supplementary law"], "answer_start": [227, 227, 227, 227]}, "prediction_text": "Treaties, regulations, directives."}
{"answers": {"text": ["a body of treaties and legislation", "a body of treaties and legislation, such as Regulations and Directives, which have direct effect or indirect effect on the laws of European Union member states", "a body of treaties and legislation, such as Regulations and Directives", "a body of treaties and legislation, such as Regulations and Directives"], "answer_start": [22, 22, 22, 22]}, "prediction_text": "European Union law."}
{"answers": {"text": ["Treaties establishing the European Union", "the Treaties establishing the European Union", "primary law, secondary law and supplementary law", "the Treaties establishing the European Union"], "answer_start": [317, 313, 227, 313]}, "prediction_text": "Treaties."}
{"answers": {"text": ["regulations and directives", "regulations and directives which are based on the Treaties", "regulations and directives which are based on the Treaties", "regulations and directives which are based on the Treaties"], "answer_start": [385, 385, 385, 385]}, "prediction_text": "Regulations and directives."}
{"answers": {"text": ["European Parliament and the Council of the European Union", "the European Parliament and the Council of the European Union", "European Parliament and the Council of the European Union", "European Parliament and the Council of the European Union"], "answer_start": [514, 510, 514, 514]}, "prediction_text": "European Parliament and Council of the European Union."}
{"answers": {"text": ["a body of treaties and legislation", "a body of treaties and legislation, such as Regulations and Directives, which have direct effect or indirect effect on the laws of European Union member states", "a body of treaties and legislation, such as Regulations and Directives", "a body of treaties and legislation, such as Regulations and Directives"], "answer_start": [22, 22, 22, 22]}, "prediction_text": "European Union law."}
{"answers": {"text": ["direct effect or indirect effect", "direct effect or indirect effect on the laws of European Union member states", "direct effect or indirect effect", "direct effect or indirect effect"], "answer_start": [105, 105, 105, 105]}, "prediction_text": "Direct effect."}
{"answers": {"text": ["primary law, secondary law and supplementary law", "primary law, secondary law and supplementary law", "primary law, secondary law and supplementary law", "primary law, secondary law and supplementary law"], "answer_start": [227, 227, 227, 227]}, "prediction_text": "Treaties, regulations, directives."}
{"answers": {"text": ["European Parliament and the Council of the European Union", "the Treaties establishing the European Union", "European Parliament and the Council of the European Union", "European Parliament and the Council of the European Union"], "answer_start": [514, 313, 514, 514]}, "prediction_text": "European Parliament and Council of the European Union."}
{"answers": {"text": ["primary law, secondary law and supplementary law", "primary law, secondary law and supplementary law", "primary law, secondary law and supplementary law", "primary law, secondary law and supplementary law"], "answer_start": [227, 227, 227, 227]}, "prediction_text": "Treaties, regulations, directives."}
{"answers": {"text": ["the Treaties establishing the European Union", "the Treaties establishing the European Union", "the Treaties establishing the European Union", "the Treaties establishing the European Union"], "answer_start": [313, 313, 313, 313]}, "prediction_text": "Treaties."}
{"answers": {"text": ["the European Parliament and the Council of the European Union", "the European Parliament and the Council of the European Union", "European Parliament and the Council of the European Union", "European Parliament and the Council of the European Union"], "answer_start": [510, 510, 514, 514]}, "prediction_text": "European Parliament and Council of the European Union."}
{"answers": {"text": ["three", "three", "three", "three"], "answer_start": [187, 187, 187, 187]}, "prediction_text": "3"}
{"answers": {"text": ["courts of member states and the Court of Justice of the European Union", "the courts of member states and the Court of Justice of the European Union", "the courts of member states and the Court of Justice of the European Union", "the courts of member states and the Court of Justice of the European Union"], "answer_start": [37, 33, 33, 33]}, "prediction_text": "Courts of member states."}
{"answers": {"text": ["courts of member states", "the courts of member states", "the courts of member states", "the courts of member states"], "answer_start": [209, 205, 205, 205]}, "prediction_text": "Courts of member states."}
{"answers": {"text": ["Treaty on the Functioning of the European Union", "the Treaty on the Functioning of the European Union", "Treaty on the Functioning of the European Union", "Treaty on the Functioning of the European Union"], "answer_start": [430, 426, 430, 430]}, "prediction_text": "Treaty on the Functioning of the European Union."}
{"answers": {"text": ["The European Court of Justice", "The European Court of Justice", "The European Court of Justice", "The European Court of Justice"], "answer_start": [479, 479, 479, 479]}, "prediction_text": "European Court of Justice"}
{"answers": {"text": ["international law", "case law by the Court of Justice", "case law by the Court of Justice", "case law by the Court of Justice"], "answer_start": [654, 620, 620, 620]}, "prediction_text": "case law by the Court of Justice."}
{"answers": {"text": ["courts of member states and the Court of Justice of the European Union", "the courts of member states and the Court of Justice of the European Union", "the courts of member states and the Court of Justice of the European Union", "the courts of member states and the Court of Justice of the European Union"], "answer_start": [37, 33, 33, 33]}, "prediction_text": "Courts of member states and Court of Justice of the European Union."}
{"answers": {"text": ["the Treaty on the Functioning of the European Union", "the Treaty on the Functioning of the European Union", "the Treaty on the Functioning of the European Union.", "the Treaty on the Functioning of the European Union"], "answer_start": [426, 426, 426, 426]}, "prediction_text": "Treaty on the Functioning of the European Union."}
{"answers": {"text": ["The European Court of Justice", "The European Court of Justice", "The European Court of Justice", "The European Court of Justice"], "answer_start": [479, 479, 479, 479]}, "prediction_text": "European Court of Justice"}
{"answers": {"text": ["international law", "international law", "international law", "international law"], "answer_start": [654, 654, 654, 654]}, "prediction_text": "Case law by the Court of Justice."}
{"answers": {"text": ["the courts of member states and the Court of Justice of the European Union", "the courts of member states and the Court of Justice of the European Union", "the courts of member states and the Court of Justice of the European Union", "the courts of member states and the Court of Justice of the European Union"], "answer_start": [33, 33, 33, 33]}, "prediction_text": "Courts of member states."}
{"answers": {"text": ["the courts of member states", "the courts of member states", "the courts of member states", "the courts of member states"], "answer_start": [205, 205, 205, 205]}, "prediction_text": "Courts of member states."}
{"answers": {"text": ["The European Court of Justice", "The European Court of Justice", "The European Court of Justice", "The European Court of Justice"], "answer_start": [479, 479, 479, 479]}, "prediction_text": "European Court of Justice"}
{"answers": {"text": ["case law by the Court of Justice, international law and general principles of European Union law", "general principles of European Union law", "case law by the Court of Justice", "case law by the Court of Justice"], "answer_start": [620, 676, 620, 620]}, "prediction_text": "case law by the Court of Justice, international law, general principles of European Union law."}
{"answers": {"text": ["Treaty on European Union (TEU) and the Treaty on the Functioning of the European Union (TFEU)", "the Treaty on European Union (TEU) and the Treaty on the Functioning of the European Union (TFEU)", "the Treaty on European Union (TEU) and the Treaty on the Functioning of the European Union (TFEU)", "the Treaty on European Union (TEU) and the Treaty on the Functioning of the European Union (TFEU)"], "answer_start": [204, 200, 200, 200]}, "prediction_text": "TEU and TFEU."}
{"answers": {"text": ["The European Commission", "The European Commission", "The European Commission", "The European Commission"], "answer_start": [553, 553, 553, 553]}, "prediction_text": "The European Commission."}
{"answers": {"text": ["citizens", "citizens", "citizens", "citizens"], "answer_start": [767, 767, 767, 767]}, "prediction_text": "Citizens."}
{"answers": {"text": ["The European Court of Justice", "The European Court of Justice", "The European Court of Justice", "The European Court of Justice"], "answer_start": [1179, 1179, 1179, 1179]}, "prediction_text": "European Court of Justice (ECJ)"}
{"answers": {"text": ["The \"European Council\"", "The \"European Council\"", "The \"European Council\"", "The \"European Council\""], "answer_start": [932, 932, 932, 932]}, "prediction_text": "European Council"}
{"answers": {"text": ["Treaty on European Union (TEU)", "Treaty on European Union (TEU)", "Treaty on European Union (TEU)", "Treaty on European Union (TEU)"], "answer_start": [98, 98, 98, 98]}, "prediction_text": "Treaty on European Union (TEU)"}
{"answers": {"text": ["the Faroe Islands", "the Faroe Islands", "the Faroe Islands", "the Faroe Islands"], "answer_start": [1002, 1002, 1002, 1002]}, "prediction_text": "Faroe Islands."}
{"answers": {"text": ["can interpret the Treaties, but it cannot rule on their validity", "The Court of Justice of the European Union can interpret the Treaties", "The Court of Justice of the European Union can interpret the Treaties", "The Court of Justice of the European Union can interpret the Treaties"], "answer_start": [1617, 1574, 1574, 1574]}, "prediction_text": "Interpretation."}
{"answers": {"text": ["if the Treaty provisions have a direct effect and they are sufficiently clear, precise and unconditional.", "if the Treaty provisions have a direct effect and they are sufficiently clear, precise and unconditional", "if the Treaty provisions have a direct effect and they are sufficiently clear, precise and unconditional", "if the Treaty provisions have a direct effect and they are sufficiently clear, precise and unconditional"], "answer_start": [1804, 1804, 1804, 1804]}, "prediction_text": "The Court of Justice of the European Union can interpret the Treaties."}
{"answers": {"text": ["as soon as they enter into force, unless stated otherwise", "Treaties apply as soon as they enter into force", "as soon as they enter into force", "as soon as they enter into force"], "answer_start": [1081, 1066, 1081, 1081]}, "prediction_text": "When they enter into force."}
{"answers": {"text": ["Treaty on European Union (TEU) and the Treaty on the Functioning of the European Union (TFEU)", "the Treaty on European Union (TEU) and the Treaty on the Functioning of the European Union (TFEU)", "the Treaty on European Union (TEU) and the Treaty on the Functioning of the European Union (TFEU)", "the Treaty on European Union (TEU) and the Treaty on the Functioning of the European Union (TFEU)"], "answer_start": [98, 94, 94, 94]}, "prediction_text": "The core treaties are the Treaty on European Union (TEU) and the Treaty on the Functioning of the European Union (TFEU)."}
{"answers": {"text": ["Gibraltar and the \u00c5land islands", "Gibraltar and the \u00c5land islands", "Gibraltar and the \u00c5land islands", "Gibraltar and the \u00c5land islands"], "answer_start": [750, 750, 750, 750]}, "prediction_text": "Gibraltar, Saint-Pierre-et-Miquelon, Madeira, the Canary Islands, the French overseas departments."}
{"answers": {"text": ["Treaties apply as soon as they enter into force, unless stated otherwise", "Treaties apply as soon as they enter into force, unless stated otherwise, and are generally concluded for an unlimited period", "as soon as they enter into force", "as soon as they enter into force"], "answer_start": [1066, 1066, 1081, 1081]}, "prediction_text": "When they enter into force."}
{"answers": {"text": ["The Court of Justice of the European Union can interpret the Treaties", "The Court of Justice of the European Union can interpret the Treaties", "The Court of Justice of the European Union", "The Court of Justice of the European Union"], "answer_start": [1574, 1574, 1574, 1574]}, "prediction_text": "The Court of Justice of the European Union."}
{"answers": {"text": ["with common rules for coal and steel, and then atomic energy", "with common rules for coal and steel", "with common rules for coal and steel", "with common rules for coal and steel"], "answer_start": [58, 58, 58, 58]}, "prediction_text": "Common rules for coal and steel, atomic energy, and minor amendments."}
{"answers": {"text": ["Treaty of Rome 1957 and the Maastricht Treaty 1992", "the Treaty of Rome 1957 and the Maastricht Treaty 1992", "the Treaty of Rome 1957 and the Maastricht Treaty 1992 (now: TFEU)", "the Treaty of Rome 1957 and the Maastricht Treaty 1992 (now: TFEU)"], "answer_start": [191, 187, 187, 187]}, "prediction_text": "TFEU and Maastricht."}
{"answers": {"text": ["1985", "1985", "1985", "1985"], "answer_start": [889, 889, 889, 889]}, "prediction_text": "1985 and 1994."}
{"answers": {"text": ["in 1972 (though Norway did not end up joining)", "not", "not", "not"], "answer_start": [806, 833, 833, 833]}, "prediction_text": "No."}
{"answers": {"text": ["Greenland", "Greenland", "Greenland", "Greenland"], "answer_start": [1184, 1184, 1184, 1184]}, "prediction_text": "Greenland"}
{"answers": {"text": ["common rules for coal and steel, and then atomic energy", "common rules for coal and steel, and then atomic energy", "with common rules for coal and steel", "with common rules for coal and steel"], "answer_start": [63, 63, 58, 58]}, "prediction_text": "The need for principal Treaties that ended up forming the EU was caused by the need for more complete and formal institutions."}
{"answers": {"text": ["1992", "1992", "1992", "1992"], "answer_start": [237, 237, 237, 237]}, "prediction_text": "1992"}
{"answers": {"text": ["1986", "1986", "1986", "1986"], "answer_start": [430, 430, 430, 430]}, "prediction_text": "1986"}
{"answers": {"text": ["1972", "1972", "1972", "1972"], "answer_start": [809, 809, 809, 809]}, "prediction_text": "1979"}
{"answers": {"text": ["1985", "1985", "1985", "1985"], "answer_start": [1213, 1213, 1213, 1213]}, "prediction_text": "1985"}
{"answers": {"text": ["Following the Nice Treaty", "2004", "Following the Nice Treaty", "Following the Nice Treaty"], "answer_start": [0, 289, 0, 0]}, "prediction_text": "Nice Treaty"}
{"answers": {"text": ["referendum in France and the referendum in the Netherlands", "the referendum in France and the referendum in the Netherlands", "referendum in France and the referendum in the Netherlands", "the referendum in France and the referendum in the Netherlands"], "answer_start": [225, 221, 225, 221]}, "prediction_text": "France and the Netherlands."}
{"answers": {"text": ["very similar", "very similar", "very similar", "very similar"], "answer_start": [421, 421, 421, 421]}, "prediction_text": "Similar."}
{"answers": {"text": ["an amending treaty", "an amending treaty", "an amending treaty", "an amending treaty"], "answer_start": [493, 493, 493, 493]}, "prediction_text": "Amending treaty"}
{"answers": {"text": ["altered the existing treaties", "it significantly altered the existing treaties", "significantly altered the existing treaties", "altered the existing treaties"], "answer_start": [543, 526, 529, 543]}, "prediction_text": "Amending Treaty"}
{"answers": {"text": ["there was an attempt to reform the constitutional law of the European Union and make it more transparent", "an attempt to reform the constitutional law of the European Union and make it more transparent", "there was an attempt to reform the constitutional law of the European Union and make it more transparent", "an attempt to reform the constitutional law of the European Union and make it more transparent"], "answer_start": [27, 37, 27, 37]}, "prediction_text": "Reform of the constitutional law of the European Union."}
{"answers": {"text": ["this would have also produced a single constitutional document", "this would have also produced a single constitutional document", "would have also produced a single constitutional document", "this would have also produced a single constitutional document"], "answer_start": [133, 133, 138, 133]}, "prediction_text": "The additional projected effect of the attempted reform was the creation of a single constitutional document."}
{"answers": {"text": ["the referendum in France and the referendum in the Netherlands", "the referendum in France and the referendum in the Netherlands", "the referendum in France and the referendum in the Netherlands", "the referendum in France and the referendum in the Netherlands"], "answer_start": [221, 221, 221, 221]}, "prediction_text": "The 2004 Treaty establishing a Constitution for Europe."}
{"answers": {"text": ["the Lisbon Treaty", "the Lisbon Treaty", "the Lisbon Treaty", "the Lisbon Treaty"], "answer_start": [372, 372, 372, 372]}, "prediction_text": "Lisbon Treaty"}
{"answers": {"text": ["The European Commission", "The European Commission", "The European Commission", "The European Commission"], "answer_start": [0, 0, 0, 0]}, "prediction_text": "European Commission"}
{"answers": {"text": ["the Commission", "The European Commission", "the Commission", "the Commission"], "answer_start": [487, 0, 487, 487]}, "prediction_text": "The Parliament."}
{"answers": {"text": ["The Commission's President", "The Commission's President", "The Commission's President", "The Commission's President ("], "answer_start": [793, 793, 793, 793]}, "prediction_text": "Jean-Claude Juncker"}
{"answers": {"text": ["one Commissioner for each of the 28 member states", "one", "one", "one"], "answer_start": [1180, 1180, 1180, 1180]}, "prediction_text": "One."}
{"answers": {"text": ["Federica Mogherini", "Jean-Claude Juncker", "Jean-Claude Juncker", "Jean-Claude Juncker"], "answer_start": [1326, 864, 864, 864]}, "prediction_text": "Jean-Claude Juncker"}
{"answers": {"text": ["Article 17(3)", "Article 17(3)", "Article 17(3)", "Article 17(3)"], "answer_start": [199, 199, 199, 199]}, "prediction_text": "Article 17(3)"}
{"answers": {"text": ["The Commission's President", "The Commission's President", "The Commission's President", "The Commission's President"], "answer_start": [793, 793, 793, 793]}, "prediction_text": "The Commissioners."}
{"answers": {"text": ["simple majority vote", "simple majority vote", "a simple majority vote", "a simple majority"], "answer_start": [945, 945, 943, 943]}, "prediction_text": "Through a simple majority vote."}
{"answers": {"text": ["Ireland", "Ireland", "Ireland", "Ireland"], "answer_start": [1098, 1098, 1098, 1098]}, "prediction_text": "Ireland"}
{"answers": {"text": ["Commissioners", "Commissioners", "Commissioners", "Commissioners"], "answer_start": [1746, 1746, 1746, 1746]}, "prediction_text": "Commissioners."}
{"answers": {"text": ["the Santer Commission", "the Santer Commission", "the Santer Commission", "the Santer Commission"], "answer_start": [255, 255, 255, 255]}, "prediction_text": "Santer Commission"}
{"answers": {"text": ["did in fact not break any law", "not", "not", "not"], "answer_start": [555, 567, 567, 567]}, "prediction_text": "Yes."}
{"answers": {"text": ["Committee of Independent Experts", "a Committee of Independent Experts", "Committee of Independent Experts", "Committee of Independent Experts"], "answer_start": [631, 629, 631, 631]}, "prediction_text": "Committee of Independent Experts."}
{"answers": {"text": ["European Council", "the European Council", "the European Council", "the European Council"], "answer_start": [1220, 1216, 1216, 1216]}, "prediction_text": "European Council"}
{"answers": {"text": ["do not have voting rights", "not", "not", "not"], "answer_start": [1351, 1354, 1354, 1354]}, "prediction_text": "No."}
{"answers": {"text": ["1999", "1999", "1999", "1999"], "answer_start": [307, 307, 307, 307]}, "prediction_text": "1999"}
{"answers": {"text": ["Commission v Edith Cresson", "Commission v Edith Cresson", "Commission v Edith Cresson", "Commission v Edith Cresson"], "answer_start": [403, 403, 403, 403]}, "prediction_text": "Commission v Edith Cresson"}
{"answers": {"text": ["a Committee of Independent Experts", "a Committee of Independent Experts", "Committee of Independent Experts", "Committee of Independent Experts"], "answer_start": [629, 629, 631, 631]}, "prediction_text": "Committee of Independent Experts."}
{"answers": {"text": ["the European Anti-fraud Office", "the European Anti-fraud Office", "the European Anti-fraud Office", "the European Anti-fraud Office"], "answer_start": [801, 801, 801, 801]}, "prediction_text": "European Anti-fraud Office (EAFO)"}
{"answers": {"text": ["2012", "2012", "2012", "2012"], "answer_start": [836, 836, 836, 836]}, "prediction_text": "2012"}
{"answers": {"text": ["the European Parliament and the Council of the European Union", "the European Parliament and the Council of the European Union", "the European Parliament and the Council of the European Union", "the European Parliament and the Council of the European Union"], "answer_start": [63, 63, 63, 63]}, "prediction_text": "European Parliament and Council of the European Union."}
{"answers": {"text": ["cannot initiate legislation against the Commission's wishes", "Parliament cannot initiate legislation against the Commission's wishes", "initiate legislation against the Commission's wishes", "cannot initiate legislation against the Commission's wishes"], "answer_start": [474, 463, 481, 474]}, "prediction_text": "The EU."}
{"answers": {"text": ["every five years", "every five years", "every five years", "every five years"], "answer_start": [1698, 1698, 1698, 1698]}, "prediction_text": "Every five years."}
{"answers": {"text": ["two-thirds majority", "a two-thirds majority", "a two-thirds majority", "a two-thirds majority"], "answer_start": [2742, 2740, 2740, 2740]}, "prediction_text": "Two-thirds."}
{"answers": {"text": ["the Commission and Council", "the Commission and Council", "the Commission and Council", "the European Parliament and the Council of the European Union"], "answer_start": [3090, 3090, 3090, 63]}, "prediction_text": "The Commission and Council."}
{"answers": {"text": ["the Commission", "the Commission", "the Commission", "the Commission"], "answer_start": [6, 6, 6, 6]}, "prediction_text": "European Parliament and Council of the European Union."}
{"answers": {"text": ["the European Parliament and the Council of the European Union", "the European Parliament and the Council of the European Union", "the European Parliament and the Council of the European Union", "the European Parliament and the Council of the European Union"], "answer_start": [63, 63, 63, 63]}, "prediction_text": "European Parliament and Council of the European Union."}
{"answers": {"text": ["1979", "1979", "1979", "1979"], "answer_start": [1184, 1184, 1184, 1184]}, "prediction_text": "1979"}
{"answers": {"text": ["every five years", "every five years", "every five years,", "every five years"], "answer_start": [1698, 1698, 1698, 1698]}, "prediction_text": "Every five years."}
{"answers": {"text": ["the conservative European People's Party", "European People's Party", "European People's Party", "European People's Party"], "answer_start": [2235, 2252, 2252, 2252]}, "prediction_text": "European People's Party (EPP)"}
{"answers": {"text": ["different ministers of the member states", "ministers", "different ministers of the member states", "different ministers of the member states"], "answer_start": [70, 80, 70, 70]}, "prediction_text": "Ministers of the member states."}
{"answers": {"text": ["Donald Tusk", "Poland Prime Minister Donald Tusk", "Donald Tusk", "Donald Tusk"], "answer_start": [443, 421, 443, 443]}, "prediction_text": "Donald Tusk"}
{"answers": {"text": ["inversely", "it is weighted inversely to member state size", "it is weighted inversely to member state size", "weighted inversely to member state size"], "answer_start": [980, 965, 965, 971]}, "prediction_text": "Less votes."}
{"answers": {"text": ["352", "352 votes", "352", "352"], "answer_start": [1099, 1099, 1099, 1099]}, "prediction_text": "352 votes."}
{"answers": {"text": ["260", "260", "260", "260"], "answer_start": [1403, 1403, 1403, 1403]}, "prediction_text": "55 per cent."}
{"answers": {"text": ["the Council", "the Council", "the Council", "the Council"], "answer_start": [36, 36, 36, 36]}, "prediction_text": "Council"}
{"answers": {"text": ["each six months", "each six months", "each six months", "each six months"], "answer_start": [369, 369, 369, 369]}, "prediction_text": "Six months."}
{"answers": {"text": ["352", "352", "352", "352"], "answer_start": [1099, 1099, 1414, 1414]}, "prediction_text": "352 votes."}
{"answers": {"text": ["at least 55 per cent of the Council members (not votes) representing 65 per cent of the population of the EU", "74 per cent, or 260 of the 352 votes", "74 per cent, or 260 of the 352 votes", "74 per cent, or 260 of the 352 votes"], "answer_start": [1249, 1387, 1387, 1387]}, "prediction_text": "55 per cent."}
{"answers": {"text": ["a majority", "a Commission proposal", "a Commission proposal", "a Commission proposal"], "answer_start": [230, 173, 173, 173]}, "prediction_text": "majority of all MEPs (not just those present) to block or suggest changes."}
{"answers": {"text": ["qualified majority", "qualified majority", "a majority", "a majority"], "answer_start": [336, 336, 230, 230]}, "prediction_text": "Qualified majority."}
{"answers": {"text": ["harder", "harder", "harder", "harder"], "answer_start": [861, 861, 861, 861]}, "prediction_text": "harder."}
{"answers": {"text": ["TEU articles 4 and 5", "TEU articles 4 and 5", "TEU articles 4 and 5", "TEU articles 4 and 5"], "answer_start": [1264, 1264, 1264, 1264]}, "prediction_text": "TEU articles 4 and 5."}
{"answers": {"text": ["Court of Justice", "the Court of Justice", "the Court of Justice", "the Court of Justice"], "answer_start": [1625, 1621, 1621, 1621]}, "prediction_text": "Court of Justice."}
{"answers": {"text": ["TFEU article 294", "TFEU article 294", "TFEU article 294", "TFEU article 294"], "answer_start": [25, 25, 25, 25]}, "prediction_text": "Article 294."}
{"answers": {"text": ["legislation can be blocked by a majority in Parliament, a minority in the Council, and a majority in the Commission", "unanimity", "unanimity", "a majority in Parliament"], "answer_start": [738, 382, 382, 768]}, "prediction_text": "Commission proposal."}
{"answers": {"text": ["TEU articles 4 and 5", "TEU articles 4 and 5", "TEU articles 4 and 5", "TEU articles 4 and 5"], "answer_start": [1264, 1264, 1264, 1264]}, "prediction_text": "TEU articles 4 and 5."}
{"answers": {"text": ["Conciliation Committee", "a \"Conciliation Committee\"", "a \"Conciliation Committee\"", "a \"Conciliation Committee\""], "answer_start": [486, 483, 483, 483]}, "prediction_text": "Conciliation Committee."}
{"answers": {"text": ["judicial branch", "The judicial branch", "The judicial branch", "The judicial branch"], "answer_start": [4, 0, 0, 0]}, "prediction_text": "Judicial branch"}
{"answers": {"text": ["Court of Justice of the European Union (CJEU)", "the Court of Justice of the European Union (CJEU)", "the Court of Justice of the European Union (CJEU)", "the Court of Justice of the European Union (CJEU)"], "answer_start": [203, 199, 199, 199]}, "prediction_text": "CJEU"}
{"answers": {"text": ["28", "28", "28", "28"], "answer_start": [707, 707, 707, 707]}, "prediction_text": "28 judges."}
{"answers": {"text": ["member state courts", "member state courts", "member state courts", "member state courts"], "answer_start": [1095, 1095, 1095, 1095]}, "prediction_text": "Member state courts."}
{"answers": {"text": ["ensure that in the interpretation and application of the Treaties the law is observed", "\"ensure that in the interpretation and application of the Treaties the law is observed\"", "\"ensure that in the interpretation and application of the Treaties the law is observed\"", "\"ensure that in the interpretation and application of the Treaties the law is observed\""], "answer_start": [1304, 1303, 1303, 1303]}, "prediction_text": "To ensure that in the interpretation and application of the Treaties the law is observed."}
{"answers": {"text": ["by assuming the task of interpreting the treaties, and accelerating economic and political integration", "assuming the task of interpreting the treaties, and accelerating economic and political integration", "has the ability to expand and develop the law according to the principles it deems to be appropriate", "has the ability to expand and develop the law according to the principles it deems to be appropriate"], "answer_start": [89, 92, 1418, 1418]}, "prediction_text": "Interpretation and application of the treaties."}
{"answers": {"text": ["the Court of Justice of the European Union", "the Court of Justice of the European Union (CJEU)", "the Court of Justice of the European Union (CJEU)", "the Court of Justice of the European Union (CJEU)"], "answer_start": [199, 199, 199, 199]}, "prediction_text": "CJEU"}
{"answers": {"text": ["Civil Service Tribunal", "Civil Service Tribunal", "Civil Service Tribunal", "Civil Service Tribunal"], "answer_start": [523, 523, 523, 523]}, "prediction_text": "Civil Service Tribunal"}
{"answers": {"text": ["three years", "three years", "three years", "three years"], "answer_start": [961, 961, 961, 961]}, "prediction_text": "Three years."}
{"answers": {"text": ["to \"ensure that in the interpretation and application of the Treaties the law is observed\"", "\"ensure that in the interpretation and application of the Treaties the law is observed\"", "\"ensure that in the interpretation and application of the Treaties the law is observed\"", "\"ensure that in the interpretation and application of the Treaties the law is observed\""], "answer_start": [1300, 1303, 1303, 1303]}, "prediction_text": "To ensure that in the interpretation and application of the Treaties the law is observed."}
{"answers": {"text": ["EU law", "EU law has primacy", "EU law has primacy", "EU law"], "answer_start": [399, 399, 399, 343]}, "prediction_text": "The law of the EU."}
{"answers": {"text": ["nationalisation law was from 1962, and the treaty was in force from 1958", "because the nationalisation law was from 1962, and the treaty was in force from 1958, Costa had no claim", "because the nationalisation law was from 1962, and the treaty was in force from 1958, Costa had no claim", "because the nationalisation law was from 1962, and the treaty was in force from 1958, Costa had no claim"], "answer_start": [936, 924, 924, 924]}, "prediction_text": "The Court of Justice said that because the Italian nationalization law was from 1962, and the Treaty was in force from 1958, Costa had no claim."}
{"answers": {"text": ["1964 and 1968", "1964 and 1968", "1964 and 1968", "1964 and 1968"], "answer_start": [2214, 2214, 2214, 2214]}, "prediction_text": "1964 and 1968."}
{"answers": {"text": ["the European Court of Justice and the highest national courts", "the European Court of Justice and the highest national courts", "the European Court of Justice and the highest national courts", "the European Court of Justice and the highest national courts"], "answer_start": [133, 133, 133, 133]}, "prediction_text": "European Court of Justice, highest national courts."}
{"answers": {"text": ["1964", "1964", "1964", "1964"], "answer_start": [446, 446, 446, 446]}, "prediction_text": "1964"}
{"answers": {"text": ["the Court of Justice", "the Court of Justice", "The Italian Constitutional Court", "The Italian Constitutional Court"], "answer_start": [1043, 1043, 870, 870]}, "prediction_text": "Court of Justice."}
{"answers": {"text": ["EU law", "EU law", "EU law", "EU law"], "answer_start": [59, 59, 59, 59]}, "prediction_text": "EU law."}
{"answers": {"text": ["foundational constitutional questions affecting democracy and human rights", "foundational constitutional questions affecting democracy and human rights", "foundational constitutional questions affecting democracy and human rights", "on foundational constitutional questions affecting democracy and human rights"], "answer_start": [199, 199, 199, 196]}, "prediction_text": "democracy, the rule of law, and the social state principles."}
{"answers": {"text": ["1972", "1972", "1972", "1972"], "answer_start": [1297, 742, 742, 742]}, "prediction_text": "1972"}
{"answers": {"text": ["the ultimate authority of member states, its factual commitment to human rights, and the democratic will of the people.", "the ultimate authority of member states, its factual commitment to human rights, and the democratic will of the people", "the ultimate authority of member states", "the ultimate authority of member states"], "answer_start": [1966, 1966, 1966, 1966]}, "prediction_text": "The ultimate authority of member states."}
{"answers": {"text": ["if the EU does not comply with its basic constitutional rights and principles", "if the EU does not comply with its basic constitutional rights and principles", "if the EU does not comply with its basic constitutional rights and principles"], "answer_start": [1399, 1399, 1399]}, "prediction_text": "In R (HS2 Action Alliance Ltd) v Secretary of State for Transport, the UK Supreme Court noted that \"whatever limitation of its sovereignty Parliament accepted when it enacted the European Communities Act 1972 was entirely voluntary\" and so \"it has always been clear\" that UK courts have a duty \"to override any rule of national law found to be in conflict with any directly enforceable rule of Community law.\" More recently the UK Supreme Court noted that in R (HS2 Action Alliance Ltd) v Secretary of State for Transport, although the UK constitution is uncodified, there could be \"fundamental principles\" of common law, and Parliament \"did not either contemplate or authorise the abrogation\" of those principles when it enacted the European Communities Act"}
{"answers": {"text": ["administrative law", "administrative law", "administrative law"], "answer_start": [77, 77, 77]}, "prediction_text": "Administrative law."}
{"answers": {"text": ["1986", "1986", "1986"], "answer_start": [688, 688, 688]}, "prediction_text": "1986"}
{"answers": {"text": ["All actions", "All actions by EU institutions can be subject to judicial review", "All actions"], "answer_start": [1095, 1095, 1095]}, "prediction_text": "All actions by EU institutions can be subject to judicial review."}
{"answers": {"text": ["constitutional law", "constitutional law", "constitutional law"], "answer_start": [6, 6, 6]}, "prediction_text": "Administrative law."}
{"answers": {"text": ["Van Gend en Loos v Nederlandse Administratie der Belastingen", "Van Gend en Loos v Nederlandse Administratie der Belastingen", "Van Gend en Loos v Nederlandse Administratie der Belastingen"], "answer_start": [165, 165, 165]}, "prediction_text": "Van Gend en Loos."}
{"answers": {"text": ["article 30", "TFEU article 30", "TFEU article 30"], "answer_start": [1087, 530, 530]}, "prediction_text": "TFEU article 30."}
{"answers": {"text": ["a postal company", "a postal company", "a postal company"], "answer_start": [487, 487, 487]}, "prediction_text": "Postal company"}
{"answers": {"text": ["Treaty provisions", "EU Regulations are the same as Treaty provisions in this sense, because as TFEU article 288 states, they are \u2018directly applicable in all Member States\u2019", "they are \u2018directly applicable in all Member States\u2019"], "answer_start": [1332, 1301, 1401]}, "prediction_text": "Treaty provisions."}
{"answers": {"text": ["Directives", "Directives", "Directives"], "answer_start": [100, 100, 100]}, "prediction_text": "Directives."}
{"answers": {"text": ["4 weeks", "4 weeks paid holidays each year", "4 weeks paid"], "answer_start": [594, 594, 594]}, "prediction_text": "4 weeks."}
{"answers": {"text": ["28 days", "more than 28 days", "more than 28 days"], "answer_start": [668, 658, 658]}, "prediction_text": "28 days."}
{"answers": {"text": ["early 1990s", "the early 1990s", "early 1990s"], "answer_start": [1027, 1023, 1027]}, "prediction_text": "1990s"}
{"answers": {"text": ["the member state cannot enforce conflicting laws, and a citizen may rely on the Directive in such an action", "the member state cannot enforce conflicting laws", "the member state cannot enforce conflicting laws"], "answer_start": [64, 64, 64]}, "prediction_text": "The member state cannot enforce conflicting laws."}
{"answers": {"text": ["a citizen or company can invoke a Directive, not just in a dispute with a public authority, but in a dispute with another citizen or company", "a citizen may rely on the Directive in such an action (so called \"vertical\" direct effect)", "a citizen may rely on the Directive in such an action"], "answer_start": [632, 118, 118]}, "prediction_text": "The member state cannot enforce conflicting laws."}
{"answers": {"text": ["10 years", "10 years", "10 years"], "answer_start": [1506, 1506, 1506]}, "prediction_text": "10 years."}
{"answers": {"text": ["British Gas plc", "British Gas plc", "British Gas plc"], "answer_start": [2117, 2117, 2117]}, "prediction_text": "British Gas plc"}
{"answers": {"text": ["women retire at age 60 and men at 65", "women retire at age 60 and men at 65", "women retire at age 60 and men at 65"], "answer_start": [2145, 2145, 2145]}, "prediction_text": "60."}
{"answers": {"text": ["national courts", "national courts", "national courts"], "answer_start": [8, 8, 8]}, "prediction_text": "National courts."}
{"answers": {"text": ["incorporations would only be nullified for a fixed list of reasons", "incorporations would only be nullified for a fixed list of reasons", "incorporations would only be nullified for a fixed list of reasons"], "answer_start": [478, 478, 478]}, "prediction_text": "Incorporations."}
{"answers": {"text": ["failed to set up an insurance fund for employees to claim unpaid wages if their employers had gone insolvent", "to set up an insurance fund for employees to claim unpaid wages if their employers had gone insolvent, as the Insolvency Protection Directive required", "to set up an insurance fund for employees to claim unpaid wages if their employers had gone insolvent, as the Insolvency Protection Directive required"], "answer_start": [939, 946, 946]}, "prediction_text": "Set up an insurance fund for employees to claim unpaid wages if their employers had gone insolvent."}
{"answers": {"text": ["6 million Lira", "6 million Lira", "6 million Lira"], "answer_start": [1190, 1190, 1190]}, "prediction_text": "6 million Lira."}
{"answers": {"text": ["the European Court of Justice", "the European Court of Justice", "the European Court"], "answer_start": [83, 83, 83]}, "prediction_text": "European Court of Justice"}
{"answers": {"text": ["fundamental rights (see human rights), proportionality, legal certainty, equality before the law and subsidiarity", "fundamental rights (see human rights), proportionality, legal certainty, equality before the law and subsidiarity", "fundamental rights (see human rights), proportionality, legal certainty, equality before the law and subsidiarity"], "answer_start": [600, 600, 600]}, "prediction_text": "fundamental rights, proportionality, legal certainty, equality before the law, subsidiarity."}
{"answers": {"text": ["since the 1950s", "since the 1950s", "since the 1950s"], "answer_start": [115, 115, 115]}, "prediction_text": "1950s"}
{"answers": {"text": ["in Article 5", "the lawfulness of an action depends on whether it was appropriate and necessary to achieve the objectives legitimately pursued", "Article 5"], "answer_start": [539, 186, 542]}, "prediction_text": "Article 5."}
{"answers": {"text": ["the least onerous", "the least onerous must be adopted", "the least onerous must be adopted"], "answer_start": [374, 374, 374]}, "prediction_text": "The least onerous measure."}
{"answers": {"text": ["since the 1960s", "since the 1960s", "since the 1960s"], "answer_start": [130, 130, 130]}, "prediction_text": "1960s"}
{"answers": {"text": ["international law and public law", "international law and public law", "international law and public law"], "answer_start": [187, 187, 187]}, "prediction_text": "Legislation in member states which implements European Union law."}
{"answers": {"text": ["a proper legal basis", "a proper legal basis", "a proper legal basis"], "answer_start": [544, 544, 544]}, "prediction_text": "A proper legal basis."}
{"answers": {"text": ["the principles of legal certainty and good faith", "the principles of legal certainty and good faith", "the principles of legal certainty and good faith"], "answer_start": [936, 936, 936]}, "prediction_text": "The legitimate expectation doctrine."}
{"answers": {"text": ["from the constitutional traditions common to the member states", "the constitutional traditions common to the member states", "the constitutional traditions common to the member states"], "answer_start": [284, 289, 289]}, "prediction_text": "The European Court of Justice."}
{"answers": {"text": ["fundamental rights recognised and protected in the constitutions of member states", "fundamental rights recognised and protected in the constitutions of member states", "fundamental rights recognised and protected in the constitutions of member states"], "answer_start": [440, 440, 440]}, "prediction_text": "fundamental rights."}
{"answers": {"text": ["None", "None", "None"], "answer_start": [0, 0, 0]}, "prediction_text": "None."}
{"answers": {"text": ["member states", "the European Convention on Human Rights", "the European Convention on Human Rights in 1950 and the establishment of the European Court of Human Rights"], "answer_start": [303, 393, 393]}, "prediction_text": "European Union institutions."}
{"answers": {"text": ["1950", "1950", "1950"], "answer_start": [436, 436, 436]}, "prediction_text": "1950"}
{"answers": {"text": ["European Court of Human Rights.", "the European Court of Human Rights", "European Court of Human Rights"], "answer_start": [470, 466, 470]}, "prediction_text": "European Court of Justice"}
{"answers": {"text": ["1999", "1999", "1999"], "answer_start": [764, 764, 764]}, "prediction_text": "1999"}
{"answers": {"text": ["2007", "2007", "2007"], "answer_start": [4, 4, 4]}, "prediction_text": "2007"}
{"answers": {"text": ["the Charter of Fundamental Rights of the European Union", "the Charter of Fundamental Rights of the European Union of 7", "the Charter of Fundamental Rights of the European Union"], "answer_start": [356, 167, 167]}, "prediction_text": "Charter of Fundamental Rights of the European Union."}
{"answers": {"text": ["European Union law", "European Union law", "European Union law"], "answer_start": [657, 657, 657]}, "prediction_text": "European Union law."}
{"answers": {"text": ["European Court of Justice", "European Court of Justice", "the European Court of Justice"], "answer_start": [714, 714, 710]}, "prediction_text": "European Court of Justice"}
{"answers": {"text": ["1997 Treaty of Amsterdam", "the 1997 Treaty of Amsterdam", "the 1997 Treaty of Amsterdam"], "answer_start": [39, 35, 35]}, "prediction_text": "Treaty of Amsterdam"}
{"answers": {"text": ["1997", "1997", "1997"], "answer_start": [39, 39, 39]}, "prediction_text": "1997"}
{"answers": {"text": ["1989", "1989", "1989"], "answer_start": [167, 167, 167]}, "prediction_text": "1989"}
{"answers": {"text": ["30", "30", "30"], "answer_start": [481, 481, 481]}, "prediction_text": "30"}
{"answers": {"text": ["40", "40", "40"], "answer_start": [784, 784, 784]}, "prediction_text": "40 pieces of legislation."}
{"answers": {"text": ["11 of the then 12 member states", "11 of the then 12 member states", "11"], "answer_start": [55, 55, 55]}, "prediction_text": "11"}
{"answers": {"text": ["The UK", "UK", "UK"], "answer_start": [88, 92, 92]}, "prediction_text": "UK"}
{"answers": {"text": ["the \"Social Chapter\"", "the \"Social Chapter\"", "\"Social Chapter\""], "answer_start": [337, 337, 341]}, "prediction_text": "Social Chapter"}
{"answers": {"text": ["1992", "1992", "1992"], "answer_start": [365, 365, 365]}, "prediction_text": "1992"}
{"answers": {"text": ["the election of the UK Labour Party to government", "the election of the UK Labour Party to government in 1997", "the election of the UK Labour Party to government"], "answer_start": [10, 10, 10]}, "prediction_text": "Election of the UK Labour Party to government in 1997."}
{"answers": {"text": ["1997", "1997", "1997"], "answer_start": [63, 63, 63]}, "prediction_text": "1997"}
{"answers": {"text": ["Works Council Directive", "the 1994 Works Council Directive", "Works Council Directive"], "answer_start": [354, 345, 354]}, "prediction_text": "Works Council Directive"}
{"answers": {"text": ["1996", "1996", "1996"], "answer_start": [440, 440, 440]}, "prediction_text": "1996"}
{"answers": {"text": ["workforce consultation in businesses", "workforce consultation in businesses", "workforce consultation in businesses"], "answer_start": [394, 394, 394]}, "prediction_text": "workforce consultation in businesses."}
{"answers": {"text": ["France, Italy, Belgium, the Netherlands, Luxembourg and Germany", "France, Italy, Belgium, the Netherlands, Luxembourg and Germany", "France, Italy, Belgium, the Netherlands, Luxembourg and Germany"], "answer_start": [101, 101, 101]}, "prediction_text": "France, Italy, Belgium, Netherlands, Luxembourg, Germany."}
{"answers": {"text": ["1951", "1951", "1951"], "answer_start": [168, 168, 168]}, "prediction_text": "1951"}
{"answers": {"text": ["cartels", "cartels", "cartels"], "answer_start": [425, 425, 425]}, "prediction_text": "Cartels."}
{"answers": {"text": ["article 66", "66", "66"], "answer_start": [437, 445, 445]}, "prediction_text": "Article 66."}
{"answers": {"text": ["1957", "1957", "1957"], "answer_start": [718, 718, 718]}, "prediction_text": "1957"}
{"answers": {"text": ["Article 101(1)", "Article 101(1)", "Article 101(1)"], "answer_start": [69, 69, 69]}, "prediction_text": "Article 101(1)"}
{"answers": {"text": ["the abuse of dominant position", "the abuse of dominant position", "abuse of dominant position"], "answer_start": [528, 528, 532]}, "prediction_text": "Price discrimination and exclusive dealing."}
{"answers": {"text": ["Articles 106 and 107", "Articles 106 and 107", "Articles 106 and 107"], "answer_start": [949, 949, 949]}, "prediction_text": "Article 106."}
{"answers": {"text": ["Article 102", "Article 102", "Article 102"], "answer_start": [612, 612, 612]}, "prediction_text": "Regulation 139/2004/EC."}
{"answers": {"text": ["2007", "2007", "2007"], "answer_start": [82, 82, 82]}, "prediction_text": "2007"}
{"answers": {"text": ["1957", "1957", "since the Treaty of Rome 1957"], "answer_start": [174, 174, 149]}, "prediction_text": "Since the Treaty of Rome 1957."}
{"answers": {"text": ["consumer prices", "consumer prices", "reduce consumer prices"], "answer_start": [589, 589, 582]}, "prediction_text": "consumer prices."}
{"answers": {"text": ["free trade", "free trade", "free trade"], "answer_start": [1975, 1975, 1975]}, "prediction_text": "Free trade."}
{"answers": {"text": ["the Court of Justice", "the Court of Justice", "the Court of Justice"], "answer_start": [2135, 2135, 2135]}, "prediction_text": "The Court of Justice."}
{"answers": {"text": ["a customs union, and the principle of non-discrimination", "a customs union", "a customs union"], "answer_start": [64, 64, 64]}, "prediction_text": "Customs union."}
{"answers": {"text": ["parallel importers like Mr Dassonville", "parallel importers", "parallel importers"], "answer_start": [839, 839, 839]}, "prediction_text": "parallel importers like Mr Dassonville."}
{"answers": {"text": ["private actors", "private actors", "private actors"], "answer_start": [1229, 1229, 1229]}, "prediction_text": "private actors."}
{"answers": {"text": ["Commission v France", "Commission v France French", "Commission v France French"], "answer_start": [1262, 1262, 1262]}, "prediction_text": "Spain."}
{"answers": {"text": ["a protest that blocked heavy traffic", "a protest", "a protest"], "answer_start": [2366, 2366, 2366]}, "prediction_text": "protest that blocked heavy traffic passing over the A13, Brenner Autobahn."}
{"answers": {"text": ["25", "25 per cent", "25 per cent"], "answer_start": [710, 710, 710]}, "prediction_text": "25 per cent."}
{"answers": {"text": ["France", "France", "France"], "answer_start": [921, 921, 921]}, "prediction_text": "France"}
{"answers": {"text": ["2003", "2003", "2003"], "answer_start": [1790, 1790, 1790]}, "prediction_text": "2003"}
{"answers": {"text": ["cocoa butter", "cocoa butter", "cocoa butter"], "answer_start": [2007, 2007, 2007]}, "prediction_text": "Cocoa butter."}
{"answers": {"text": ["motorcycles or mopeds pulling trailers", "motorcycles or mopeds pulling trailers", "motorcycles or mopeds pulling trailers"], "answer_start": [2568, 2568, 2568]}, "prediction_text": "Motorcycles or mopeds pulling trailers."}
{"answers": {"text": ["Keck and Mithouard", "Keck and Mithouard", "Keck and Mithouard"], "answer_start": [291, 291, 291]}, "prediction_text": "Keck and Mithouard."}
{"answers": {"text": ["cut throat competition", "prevent cut throat competition", "prevent cut throat competition"], "answer_start": [498, 490, 490]}, "prediction_text": "To prevent cut throat competition."}
{"answers": {"text": ["Konsumentombudsmannen v De Agostini", "Konsumentombudsmannen v De Agostini", "Konsumentombudsmannen v De Agostini"], "answer_start": [990, 990, 990]}, "prediction_text": "Konsumentombudsmannen v De Agostini."}
{"answers": {"text": ["the Unfair Commercial Practices Directive", "the Unfair Commercial Practices Directive", "the Unfair Commercial Practices Directive"], "answer_start": [1866, 1866, 1866]}, "prediction_text": "Unfair Commercial Practices Directive."}
{"answers": {"text": ["to enable people to pursue their life goals in any country through free movement", "people to pursue their life goals in any country through free movement", "Since its foundation"], "answer_start": [42, 52, 0]}, "prediction_text": "People to pursue their life goals in any country through free movement."}
{"answers": {"text": ["the European Community", "the European Community", "the European Community"], "answer_start": [171, 171, 171]}, "prediction_text": "European Community"}
{"answers": {"text": ["citizenship", "\"citizenship\"", "\"citizenship\""], "answer_start": [401, 400, 400]}, "prediction_text": "Citizenship."}
{"answers": {"text": ["Steymann v Staatssecretaris van Justitie", "Steymann v Staatssecretaris van Justitie", "Steymann v Staatssecretaris van Justitie"], "answer_start": [1014, 1014, 1014]}, "prediction_text": "Bhagwan community."}
{"answers": {"text": ["to stay, so long as there was at least an \"indirect quid pro quo\" for the work he did", "stay", "stay"], "answer_start": [1335, 1338, 1338]}, "prediction_text": "Residence in the Netherlands."}
{"answers": {"text": ["articles 1 to 7", "articles 1 to 7", "articles 1 to 7"], "answer_start": [40, 40, 40]}, "prediction_text": "1 to 7."}
{"answers": {"text": ["Jean-Marc Bosman", "the Belgian Football Association v Bosman", "Jean-Marc Bosman"], "answer_start": [374, 304, 374]}, "prediction_text": "Jean-Marc Bosman"}
{"answers": {"text": ["Gaelic", "Gaelic", "Gaelic"], "answer_start": [866, 866, 866]}, "prediction_text": "Gaelic."}
{"answers": {"text": ["Hendrix v Employee", "Hendrix v Employee Insurance Institute", "Hendrix v Employee Insurance Institute"], "answer_start": [2525, 2525, 2525]}, "prediction_text": "Hendrix v Employee Insurance Institute."}
{"answers": {"text": ["between 3 and 14 hours a week", "3 and 14 hours a week", "between 3 and 14 hours a week"], "answer_start": [2922, 2930, 2922]}, "prediction_text": "3-14 hours a week."}
{"answers": {"text": ["Citizenship of the EU", "Citizenship of the EU", "Citizenship of the EU"], "answer_start": [0, 0, 0]}, "prediction_text": "Citizenship of the EU."}
{"answers": {"text": ["the number of social services that people can access wherever they move", "the number of social services that people can access wherever they move", "the number of social services that people can access wherever they move"], "answer_start": [156, 156, 156]}, "prediction_text": "Social services."}
{"answers": {"text": ["Commission v Austria", "Commission v Austria the Court", "Commission v Austria"], "answer_start": [380, 380, 380]}, "prediction_text": "Commission v Austria."}
{"answers": {"text": ["higher education", "higher education", "higher education"], "answer_start": [257, 257, 257]}, "prediction_text": "Higher education."}
{"answers": {"text": ["the Treaty on the Functioning of the European Union", "the Treaty on the Functioning of the European Union", "the Treaty on the Functioning of the European Union"], "answer_start": [92, 92, 92]}, "prediction_text": "Treaty on the Functioning of the European Union."}
{"answers": {"text": ["if they were non-discriminatory", "if they were non-discriminatory, \"justified by imperative requirements in the general interest\" and proportionately applied", "participate in economic life \"on a stable and continuous basis\""], "answer_start": [866, 866, 387]}, "prediction_text": "Non-discriminatory, \"justified by imperative requirements in the general interest\" and proportionately applied."}
{"answers": {"text": ["Reyners v Belgium", "Reyners v Belgium the Court of Justice", "Reyners v Belgium"], "answer_start": [1389, 1389, 1389]}, "prediction_text": "Reyners v Belgium."}
{"answers": {"text": ["article 49", "TFEU article 49", "article 49"], "answer_start": [1545, 1540, 1545]}, "prediction_text": "TFEU article 49."}
{"answers": {"text": ["Commission v Italy", "Commission v Italy the Court of Justice", "Commission v Italy"], "answer_start": [1760, 1760, 1760]}, "prediction_text": "Commission v Italy."}
{"answers": {"text": ["2006", "2006", "2006"], "answer_start": [3, 3, 3]}, "prediction_text": "2006"}
{"answers": {"text": ["shipping toxic waste", "shipping toxic waste", "toxic waste"], "answer_start": [334, 334, 140]}, "prediction_text": "shipping toxic waste."}
{"answers": {"text": ["October 2007", "2007", "2007"], "answer_start": [1024, 1032, 1032]}, "prediction_text": "October 2007."}
{"answers": {"text": ["2005", "2005", "2005"], "answer_start": [552, 552, 552]}, "prediction_text": "2005"}
{"answers": {"text": ["to people who give services \"for remuneration\"", "people who give services \"for remuneration\", especially commercial or professional activity", "people who give services \"for remuneration\""], "answer_start": [64, 67, 67]}, "prediction_text": "People who give services \"for remuneration\"."}
{"answers": {"text": ["because Dutch law said only people established in the Netherlands could give legal advice", "Dutch law said only people established in the Netherlands could give legal advice", "Dutch law said only people established in the Netherlands could give legal advice"], "answer_start": [369, 377, 377]}, "prediction_text": "Dutch law said only people established in the Netherlands could give legal advice."}
{"answers": {"text": ["narcotic drugs", "narcotic drugs", "narcotic drugs"], "answer_start": [2318, 2318, 2318]}, "prediction_text": "Narcotic drugs."}
{"answers": {"text": ["the treatment", "treatment", "treatment"], "answer_start": [1124, 1128, 1128]}, "prediction_text": "Treatment."}
{"answers": {"text": ["the Daily Mail", "the Daily Mail", "Daily Mail"], "answer_start": [224, 224, 56]}, "prediction_text": "Daily Mail."}
{"answers": {"text": ["\u00a31", "\u00a31", "\u00a31"], "answer_start": [703, 703, 703]}, "prediction_text": "\u00a31."}
{"answers": {"text": ["200,000 Danish krone", "200,000 Danish krone", "200,000 Danish krone"], "answer_start": [827, 827, 827]}, "prediction_text": "\u00a31."}
{"answers": {"text": ["creditor protection, labour rights to participate in work, or the public interest in collecting taxes", "creditor protection, labour rights to participate in work, or the public interest in collecting taxes", "Denmark's minimum capital law"], "answer_start": [1888, 1888, 959]}, "prediction_text": "Creditor protection."}
{"answers": {"text": ["\u00dcberseering BV v Nordic Construction GmbH", "\u00dcberseering BV v Nordic Construction GmbH", "\u00dcberseering BV v Nordic Construction GmbH"], "answer_start": [1583, 1583, 1583]}, "prediction_text": "Because it was not validly incorporated in Germany."}
{"answers": {"text": ["also known in English as Amazonia or the Amazon Jungle,", "Amazonia or the Amazon Jungle", "Amazonia"], "answer_start": [176, 201, 201]}, "prediction_text": "Amazonia"}
{"answers": {"text": ["5,500,000 square kilometres (2,100,000 sq mi) are covered by the rainforest.", "5,500,000", "5,500,000"], "answer_start": [394, 394, 394]}, "prediction_text": "2,700,000 square kilometers."}
{"answers": {"text": ["This region includes territory belonging to nine nations.", "nine", "nine"], "answer_start": [471, 515, 515]}, "prediction_text": "Nine."}
{"answers": {"text": ["States or departments in four nations contain \"Amazonas\" in their names.", "four", "four"], "answer_start": [749, 774, 774]}, "prediction_text": "Four nations."}
{"answers": {"text": ["The Amazon represents over half of the planet's remaining rainforests", "over half"], "answer_start": [822, 844]}, "prediction_text": "Half."}
{"answers": {"text": ["Amazoneregenwoud", "Amazoneregenwoud", "Amazoneregenwoud"], "answer_start": [157, 157, 157]}, "prediction_text": "Amazoneregenwoud"}
{"answers": {"text": ["The Amazon rainforest", "Amazon rainforest", "The Amazon"], "answer_start": [0, 4, 0]}, "prediction_text": "Brazil"}
{"answers": {"text": ["Brazil", "Brazil", "Brazil"], "answer_start": [576, 576, 576]}, "prediction_text": "Brazil"}
{"answers": {"text": ["over half", "over half", "over half"], "answer_start": [844, 844, 844]}, "prediction_text": "5,500,000 square kilometers."}
{"answers": {"text": ["16,000", "16,000", "16,000"], "answer_start": [1042, 1042, 1042]}, "prediction_text": "16,000."}
{"answers": {"text": ["moist broadleaf forest", "moist broadleaf forest", "tropical"], "answer_start": [237, 237, 948]}, "prediction_text": "moist broadleaf forest"}
{"answers": {"text": ["7,000,000 square kilometres (2,70", "7,000,000", "7,000,000 square kilometres"], "answer_start": [338, 338, 338]}, "prediction_text": "2,700,000 square kilometers."}
{"answers": {"text": ["nine nations", "nine", "nine"], "answer_start": [515, 515, 515]}, "prediction_text": "Nine."}
{"answers": {"text": ["Brazil", "Brazil", "Brazil"], "answer_start": [576, 576, 576]}, "prediction_text": "Brazil"}
{"answers": {"text": ["16,000 species", "16,000", "16,000"], "answer_start": [1042, 1042, 1042]}, "prediction_text": "390 billion."}
{"answers": {"text": ["the wetter climate may have allowed the tropical rainforest to spread out across the continent.", "wetter", "wetter"], "answer_start": [89, 93, 93]}, "prediction_text": "Tropical rainforest"}
{"answers": {"text": ["Climate fluctuations during the last 34 million years have allowed savanna regions to expand into the tropics.", "Climate fluctuations during the last 34 million years", "Climate fluctuations"], "answer_start": [246, 246, 246]}, "prediction_text": "Climate fluctuations."}
{"answers": {"text": ["During the Oligocene, for example, the rainforest spanned a relatively narrow band.", "Oligocene", "Oligocene"], "answer_start": [357, 368, 368]}, "prediction_text": "66-34 Mya."}
{"answers": {"text": ["It expanded again during the Middle Miocene, then retracted to a mostly inland formation at the last glacial maximum.", "Middle Miocene", "at the last glacial maximum"], "answer_start": [441, 470, 530]}, "prediction_text": "Last glacial maximum."}
{"answers": {"text": ["However, the rainforest still managed to thrive during these glacial periods, allowing for the survival and evolution of a broad diversity of species.", "still managed to thrive"], "answer_start": [559, 583]}, "prediction_text": "Yes."}
{"answers": {"text": ["the extinction of the dinosaurs and the wetter climate", "extinction of the dinosaurs and the wetter climate", "extinction of the dinosaurs"], "answer_start": [53, 57, 57]}, "prediction_text": "Climate fluctuations."}
{"answers": {"text": ["45", "45\u00b0", "45\u00b0"], "answer_start": [241, 241, 241]}, "prediction_text": "45\u00b0."}
{"answers": {"text": ["Climate fluctuations", "Climate fluctuations", "Climate fluctuations"], "answer_start": [246, 246, 246]}, "prediction_text": "Climate fluctuations."}
{"answers": {"text": ["Oligocene", "Oligocene", "Oligocene"], "answer_start": [368, 368, 368]}, "prediction_text": "66-34 Mya"}
{"answers": {"text": ["It expanded", "expanded", "expanded"], "answer_start": [441, 444, 444]}, "prediction_text": "Expand."}
{"answers": {"text": ["Cretaceous\u2013Paleogene extinction event", "dinosaurs", "dinosaurs"], "answer_start": [14, 75, 75]}, "prediction_text": "Cretaceous-Paleogene extinction event."}
{"answers": {"text": ["66\u201334 Mya", "66\u201334"], "answer_start": [190, 190]}, "prediction_text": "66-34 Mya"}
{"answers": {"text": ["Middle Miocene", "Middle Miocene", "Middle Miocene"], "answer_start": [470, 470, 470]}, "prediction_text": "Middle Miocene."}
{"answers": {"text": ["last glacial maximum", "last glacial maximum", "last glacial maximum"], "answer_start": [537, 537, 537]}, "prediction_text": "Glacial periods."}
{"answers": {"text": ["34 million years", "34 million", "34 million"], "answer_start": [283, 283, 283]}, "prediction_text": "34 million years."}
{"answers": {"text": ["During the mid-Eocene, it is believed that the drainage basin of the Amazon was split along the middle of the continent by the Purus Arch.", "During the mid-Eocene", "mid-Eocene"], "answer_start": [0, 0, 11]}, "prediction_text": "Middle of the continent."}
{"answers": {"text": ["Water on the eastern side flowed toward the Atlantic,", "toward the Atlantic", "toward the Atlantic"], "answer_start": [139, 172, 172]}, "prediction_text": "Atlantic"}
{"answers": {"text": ["Solim\u00f5es Basin", "Solim\u00f5es Basin", "Solim\u00f5es Basin"], "answer_start": [373, 373, 373]}, "prediction_text": "Solim\u00f5es Basin"}
{"answers": {"text": ["Within the last 5\u201310 million years", "Within the last 5\u201310 million years", "last 5\u201310 million years"], "answer_start": [389, 389, 400]}, "prediction_text": "5-10 million years ago."}
{"answers": {"text": ["joining the easterly flow toward the Atlantic.", "the easterly flow", "easterly"], "answer_start": [479, 487, 491]}, "prediction_text": "Atlantic"}
{"answers": {"text": ["During the mid-Eocene", "During the mid-Eocene", "During the mid-Eocene"], "answer_start": [0, 0, 0]}, "prediction_text": "mid-Eocene"}
{"answers": {"text": ["the Atlantic", "the Atlantic", "Atlantic"], "answer_start": [179, 179, 516]}, "prediction_text": "Pacific."}
{"answers": {"text": ["the Pacific", "the Pacific", "Pacific"], "answer_start": [231, 231, 235]}, "prediction_text": "Pacific."}
{"answers": {"text": ["Amazonas Basin", "Amazonas Basin", "Andes Mountains"], "answer_start": [254, 254, 277]}, "prediction_text": "The Solim\u00f5es Basin."}
{"answers": {"text": ["the Solim\u00f5es Basin", "Solim\u00f5es Basin", "Solim\u00f5es Basin"], "answer_start": [369, 373, 373]}, "prediction_text": "Solim\u00f5es Basin"}
{"answers": {"text": ["the mid-Eocene", "the mid-Eocene", "mid-Eocene"], "answer_start": [7, 7, 11]}, "prediction_text": "mid-Eocene"}
{"answers": {"text": ["Purus Arch", "along the middle", "Purus Arch"], "answer_start": [127, 86, 127]}, "prediction_text": "Middle of the continent."}
{"answers": {"text": ["the Atlantic", "the Atlantic", "Atlantic"], "answer_start": [179, 179, 183]}, "prediction_text": "Atlantic"}
{"answers": {"text": ["the Pacific", "the Pacific", "Pacific"], "answer_start": [231, 231, 235]}, "prediction_text": "Pacific."}
{"answers": {"text": ["Solim\u00f5es Basin", "Solim\u00f5es Basin", "Solim\u00f5es Basin"], "answer_start": [373, 373, 373]}, "prediction_text": "Solim\u00f5es Basin"}
{"answers": {"text": ["Last Glacial Maximum", "Last Glacial Maximum", "Last Glacial Maximum"], "answer_start": [130, 130, 130]}, "prediction_text": "Last Glacial Maximum"}
{"answers": {"text": ["rainfall in the basin during the LGM was lower than for the present", "rainfall in the basin during the LGM was lower than for the present", "rainfall in the basin during the LGM was lower"], "answer_start": [283, 283, 283]}, "prediction_text": "Rainfall in the basin during the LGM was lower than for the present."}
{"answers": {"text": ["the rainforest was reduced to small, isolated refugia separated by open forest and grassland", "the rainforest was reduced to small, isolated refugia separated by open forest and grassland", "rainforest was reduced"], "answer_start": [544, 544, 548]}, "prediction_text": "Rainforest remained largely intact but extended less far to the north, south, and east."}
{"answers": {"text": ["This debate has proved difficult", "difficult to resolve"], "answer_start": [777, 800]}, "prediction_text": "Difficult to resolve."}
{"answers": {"text": ["explanations are reasonably well supported", "by the available data", "reasonably well"], "answer_start": [969, 1012, 986]}, "prediction_text": "The explanations are supported by the available data."}
{"answers": {"text": ["21,000", "21,000", "21,000"], "answer_start": [105, 105, 105]}, "prediction_text": "21,000 years."}
{"answers": {"text": ["the Last Glacial Maximum (LGM) and subsequent deglaciation", "rainfall in the basin during the LGM was lower than for the present", "rainfall"], "answer_start": [126, 283, 283]}, "prediction_text": "LGM and subsequent deglaciation."}
{"answers": {"text": ["sediment deposits", "sediment deposits", "sediment deposits"], "answer_start": [198, 198, 198]}, "prediction_text": "Sediment deposits from Amazon basin paleolakes."}
{"answers": {"text": ["reduced moist tropical vegetation cover in the basin", "reduced moist tropical vegetation cover in the basin", "reduced moist tropical vegetation cover"], "answer_start": [398, 398, 398]}, "prediction_text": "Reduced moist tropical vegetation cover in the basin."}
{"answers": {"text": ["21,000", "21,000", "21,000"], "answer_start": [105, 105, 105]}, "prediction_text": "21,000 years ago."}
{"answers": {"text": ["sediment deposits", "sediment", "sediment"], "answer_start": [198, 198, 198]}, "prediction_text": "Sediment deposits."}
{"answers": {"text": ["moist tropical vegetation cover", "moist tropical", "tropical"], "answer_start": [406, 406, 412]}, "prediction_text": "moist tropical vegetation cover"}
{"answers": {"text": ["open forest and grassland", "open forest and grassland", "open forest and grassland"], "answer_start": [611, 611, 611]}, "prediction_text": "open forest and grassland"}
{"answers": {"text": ["data sampling is biased away from the center of the Amazon basin", "practical limitations of working in the rainforest mean that data sampling is biased away from the center of the Amazon basin", "practical limitations of working in the rainforest"], "answer_start": [894, 833, 833]}, "prediction_text": "Data sampling bias."}
{"answers": {"text": ["CALIPSO", "CALIPSO", "CALIPSO"], "answer_start": [7, 7, 7]}, "prediction_text": "CALIPSO"}
{"answers": {"text": ["182 million tons", "182 million", "an average 182 million"], "answer_start": [119, 119, 108]}, "prediction_text": "182 million tons."}
{"answers": {"text": ["1,600 miles", "1,600", "1,600 miles"], "answer_start": [224, 224, 224]}, "prediction_text": "1,600 miles."}
{"answers": {"text": ["Amazon basin", "Amazon", "Amazon"], "answer_start": [427, 427, 427]}, "prediction_text": "Caribbean Sea"}
{"answers": {"text": ["132 million tons", "132 million tons", "132 million"], "answer_start": [441, 441, 441]}, "prediction_text": "43 million tons."}
{"answers": {"text": ["NASA's CALIPSO satellite", "NASA's CALIPSO satellite", "satellite"], "answer_start": [0, 0, 15]}, "prediction_text": "CALIPSO satellite"}
{"answers": {"text": ["182 million tons", "182 million tons", "182 million tons"], "answer_start": [119, 119, 119]}, "prediction_text": "182 million tons."}
{"answers": {"text": ["27.7 million tons", "27.7 million tons", "27.7 million tons"], "answer_start": [381, 381, 381]}, "prediction_text": "15%"}
{"answers": {"text": ["132 million tons", "132 million tons", "132 million tons"], "answer_start": [441, 441, 441]}, "prediction_text": "43 million tons."}
{"answers": {"text": ["43 million tons", "43 million tons", "43 million tons"], "answer_start": [485, 485, 485]}, "prediction_text": "43 million tons."}
{"answers": {"text": ["CALIPSO", "CALIPSO", "CALIPSO"], "answer_start": [7, 7, 7]}, "prediction_text": "CALIPSO"}
{"answers": {"text": ["NASA", "NASA", "NASA's"], "answer_start": [0, 0, 0]}, "prediction_text": "NASA"}
{"answers": {"text": ["182 million tons", "182 million tons", "182 million tons"], "answer_start": [119, 119, 119]}, "prediction_text": "182 million tons."}
{"answers": {"text": ["1,600 miles", "1,600", "1,600"], "answer_start": [224, 224, 224]}, "prediction_text": "1,600 miles."}
{"answers": {"text": ["27.7 million tons", "27.7 million", "27.7 million tons"], "answer_start": [381, 381, 381]}, "prediction_text": "15%"}
{"answers": {"text": ["Man and Culture in a Counterfeit Paradise", "Amazonia: Man and Culture in a Counterfeit Paradise", "Amazonia: Man and Culture in a Counterfeit Paradise."], "answer_start": [287, 277, 277]}, "prediction_text": "Amazonia: Man and Culture in a Counterfeit Paradise"}
{"answers": {"text": ["0.52/sq mi", "0.52", "0.52/sq mi"], "answer_start": [409, 409, 409]}, "prediction_text": "0.52/sq mi"}
{"answers": {"text": ["agriculture", "agriculture", "agriculture"], "answer_start": [498, 498, 498]}, "prediction_text": "Agriculture."}
{"answers": {"text": ["anthropological", "anthropological", "recent anthropological"], "answer_start": [562, 562, 555]}, "prediction_text": "Anthropological findings."}
{"answers": {"text": ["5 million", "5 million", "5 million"], "answer_start": [655, 655, 655]}, "prediction_text": "5 million."}
{"answers": {"text": ["the poor soil", "poor soil", "poor soil."], "answer_start": [170, 174, 174]}, "prediction_text": "Poor soil."}
{"answers": {"text": ["Betty Meggers", "Betty Meggers", "Betty Meggers"], "answer_start": [198, 198, 198]}, "prediction_text": "Betty Meggers"}
{"answers": {"text": ["0.2", "0.2", "0.2"], "answer_start": [371, 371, 371]}, "prediction_text": "0.52/sq mi"}
{"answers": {"text": ["Amazonia: Man and Culture in a Counterfeit Paradise", "Amazonia: Man and Culture in a Counterfeit Paradise", "Amazonia: Man and Culture in a Counterfeit Paradise"], "answer_start": [277, 277, 277]}, "prediction_text": "Amazonia: Man and Culture in a Counterfeit Paradise."}
{"answers": {"text": ["Betty Meggers", "Betty Meggers", "Betty Meggers"], "answer_start": [198, 198, 198]}, "prediction_text": "Betty Meggers"}
{"answers": {"text": ["Amazonia: Man and Culture in a Counterfeit Paradise", "Amazonia: Man and Culture in a Counterfeit Paradise", "Amazonia: Man and Culture in a Counterfeit Paradise"], "answer_start": [277, 277, 277]}, "prediction_text": "Amazonia: Man and Culture in a Counterfeit Paradise."}
{"answers": {"text": ["0.2 inhabitants per square kilometre", "0.2 inhabitants", "0.2 inhabitants per square kilometre"], "answer_start": [371, 371, 371]}, "prediction_text": "0.52/sq mi"}
{"answers": {"text": ["5 million people", "5 million", "Some 5 million"], "answer_start": [655, 655, 650]}, "prediction_text": "5 million."}
{"answers": {"text": ["200,000.", "less than 200,000", "less than 200,000"], "answer_start": [895, 885, 885]}, "prediction_text": "1 million"}
{"answers": {"text": ["Francisco de Orellana", "Francisco de Orellana", "Francisco de Orellana"], "answer_start": [64, 64, 64]}, "prediction_text": "Francisco de Orellana"}
{"answers": {"text": ["1540s", "1540s", "1540s"], "answer_start": [317, 317, 317]}, "prediction_text": "1540s"}
{"answers": {"text": ["diseases from Europe", "the spread of diseases from Europe", "spread of diseases from Europe"], "answer_start": [399, 385, 389]}, "prediction_text": "European diseases."}
{"answers": {"text": ["1970s", "Since the 1970s", "1970s"], "answer_start": [449, 439, 449]}, "prediction_text": "11,000 years."}
{"answers": {"text": ["AD 0\u20131250", "between AD 0\u20131250", "AD 0\u20131250"], "answer_start": [530, 522, 530]}, "prediction_text": "AD 0-1250."}
{"answers": {"text": ["Francisco de Orellana", "Francisco de Orellana", "Francisco de Orellana"], "answer_start": [64, 64, 64]}, "prediction_text": "Francisco de Orellana"}
{"answers": {"text": ["1542", "1542", "1542"], "answer_start": [89, 89, 89]}, "prediction_text": "1542"}
{"answers": {"text": ["AD 0\u20131250", "between AD 0\u20131250", "AD 0\u20131250"], "answer_start": [530, 522, 530]}, "prediction_text": "AD 0-1250."}
{"answers": {"text": ["Ondemar Dias", "Ondemar Dias", "Ondemar Dias"], "answer_start": [594, 594, 594]}, "prediction_text": "Alceu Ranzi"}
{"answers": {"text": ["11,000 years", "at least 11,000 years", "at least 11,000 years"], "answer_start": [892, 883, 883]}, "prediction_text": "11,000 years."}
{"answers": {"text": ["black earth", "black earth", "black earth"], "answer_start": [13, 13, 13]}, "prediction_text": "Terra preta (black earth)"}
{"answers": {"text": ["large areas", "over large areas", "over large areas"], "answer_start": [53, 48, 48]}, "prediction_text": "Large areas."}
{"answers": {"text": ["agriculture and silviculture", "allowed agriculture and silviculture", "agriculture and silviculture"], "answer_start": [199, 191, 199]}, "prediction_text": "Agriculture and silviculture."}
{"answers": {"text": ["Xingu tribe", "Xingu", "Xingu"], "answer_start": [464, 464, 464]}, "prediction_text": "Xingu tribe"}
{"answers": {"text": ["Michael Heckenberger and colleagues of the University of Florida", "Michael Heckenberger and colleagues of the University of Florida", "Michael Heckenberger and colleagues of the University of Florida"], "answer_start": [577, 577, 577]}, "prediction_text": "Michael Heckenberger"}
{"answers": {"text": ["Terra preta (black earth)", "Terra preta", "black earth"], "answer_start": [0, 0, 13]}, "prediction_text": "Terra preta (black earth)"}
{"answers": {"text": ["agriculture and silviculture", "agriculture and silviculture", "agriculture and silviculture"], "answer_start": [199, 199, 199]}, "prediction_text": "Agriculture and silviculture."}
{"answers": {"text": ["Xingu tribe", "Xingu", "Xingu"], "answer_start": [464, 464, 464]}, "prediction_text": "Xingu tribe"}
{"answers": {"text": ["Michael Heckenberger and colleagues", "Michael Heckenberger and colleagues of the University of Florida", "Michael Heckenberger"], "answer_start": [577, 577, 577]}, "prediction_text": "Michael Heckenberger"}
{"answers": {"text": ["roads, bridges and large plazas", "roads, bridges and large plazas", "roads, bridges and large plazas"], "answer_start": [672, 672, 672]}, "prediction_text": "roads, bridges, large plazas."}
{"answers": {"text": ["2.5 million", "2.5 million", "2.5 million"], "answer_start": [28, 28, 28]}, "prediction_text": "2.5 million"}
{"answers": {"text": ["One in five", "One in five", "One in five"], "answer_start": [283, 283, 283]}, "prediction_text": "One in five."}
{"answers": {"text": ["40,000", "40,000", "40,000"], "answer_start": [137, 137, 137]}, "prediction_text": "2.5 million"}
{"answers": {"text": ["one in five", "2,200", "2,200"], "answer_start": [375, 159, 159]}, "prediction_text": "2,200"}
{"answers": {"text": ["96,660 and 128,843", "between 96,660 and 128,843", "128,843"], "answer_start": [479, 471, 490]}, "prediction_text": "96,660"}
{"answers": {"text": ["2.5 million", "about 2.5 million", "2.5 million"], "answer_start": [28, 22, 28]}, "prediction_text": "2.5 million"}
{"answers": {"text": ["2,000", "some 2,000", "2,000"], "answer_start": [94, 89, 94]}, "prediction_text": "2,200"}
{"answers": {"text": ["40,000", "tens of thousands", "tens of thousands"], "answer_start": [137, 56, 56]}, "prediction_text": "40,000"}
{"answers": {"text": ["378", "378", "378"], "answer_start": [219, 219, 219]}, "prediction_text": "378"}
{"answers": {"text": ["One in five", "1,294", "One in five of all the bird species in the world"], "answer_start": [283, 173, 283]}, "prediction_text": "2,200"}
{"answers": {"text": ["62 acres", "quarter square", "quarter square", "62"], "answer_start": [114, 88, 88, 114]}, "prediction_text": "62 acres."}
{"answers": {"text": ["1,100", "more than 1,100", "more than 1,100", "1,100"], "answer_start": [168, 158, 158, 168]}, "prediction_text": "16,000"}
{"answers": {"text": ["90,790", "about 90,790", "about 90,790", "90,790"], "answer_start": [282, 276, 276, 282]}, "prediction_text": "90,790 tonnes."}
{"answers": {"text": ["356 \u00b1 47 tonnes per hectare", "356 \u00b1 47 tonnes per hectare", "356 \u00b1 47 tonnes per hectare", "356 \u00b1 47 tonnes per hectare"], "answer_start": [356, 356, 356, 356]}, "prediction_text": "356 \u00b1 47 tonnes per hectare."}
{"answers": {"text": ["438,000", "438,000", "438,000", "438,000 species"], "answer_start": [407, 407, 407, 407]}, "prediction_text": "438,000"}
{"answers": {"text": ["highest on Earth", "the highest", "the highest", "highest"], "answer_start": [41, 37, 37, 41]}, "prediction_text": "The Amazon region rates among the entire earth for its amount of biodiversity."}
{"answers": {"text": ["1,100", "more than 1,100", "more than 1,100", "1,100"], "answer_start": [168, 158, 158, 168]}, "prediction_text": "62 acres"}
{"answers": {"text": ["90,790 tonnes", "about 90,790", "about 90,790", "90,790 tonnes"], "answer_start": [282, 276, 276, 282]}, "prediction_text": "90,790 tonnes."}
{"answers": {"text": ["356 \u00b1 47 tonnes", "356 \u00b1 47 tonnes", "356 \u00b1 47 tonnes", "356 \u00b1 47 tonnes"], "answer_start": [356, 356, 356, 356]}, "prediction_text": "356 \u00b1 47 tonnes per hectare."}
{"answers": {"text": ["438,000", "438,000", "438,000", "438,000"], "answer_start": [407, 407, 407, 407]}, "prediction_text": "90,790 tonnes."}
{"answers": {"text": ["electric eels", "electric eels", "electric eels"], "answer_start": [168, 168, 168]}, "prediction_text": "Electric eels."}
{"answers": {"text": ["black caiman", "black caiman", "black caiman"], "answer_start": [110, 110, 110]}, "prediction_text": "Electric eels."}
{"answers": {"text": ["piranha", "piranha", "piranha"], "answer_start": [241, 241, 241]}, "prediction_text": "Electric eels."}
{"answers": {"text": ["lipophilic alkaloid toxins", "lipophilic alkaloid toxins", "lipophilic alkaloid toxins"], "answer_start": [331, 331, 331]}, "prediction_text": "Lipophilic alkaloid toxins."}
{"answers": {"text": ["Vampire bats", "Vampire", "Vampire"], "answer_start": [434, 434, 434]}, "prediction_text": "Vampire bats"}
{"answers": {"text": ["Deforestation", "Deforestation", "Deforestation"], "answer_start": [0, 0, 0]}, "prediction_text": "Deforestation"}
{"answers": {"text": ["the early 1960s", "early 1960s", "1960s"], "answer_start": [180, 184, 190]}, "prediction_text": "early 1960s"}
{"answers": {"text": ["slash and burn method", "slash and burn", "slash and burn"], "answer_start": [368, 368, 368]}, "prediction_text": "Slash and burn."}
{"answers": {"text": ["loss of soil fertility and weed invasion", "loss of soil fertility and weed invasion", "soil fertility and weed invasion"], "answer_start": [478, 478, 486]}, "prediction_text": "soil fertility and weed invasion."}
{"answers": {"text": ["areas cleared of forest are visible to the naked eye", "areas cleared of forest", "areas cleared of forest are visible to the naked eye"], "answer_start": [785, 785, 785]}, "prediction_text": "The Amazon forest is visible from outer space."}
{"answers": {"text": ["415,000", "415,000 to 587,000", "415,000"], "answer_start": [77, 77, 77]}, "prediction_text": "160,000 square kilometers."}
{"answers": {"text": ["587,000", "587,000", "587,000"], "answer_start": [88, 88, 88]}, "prediction_text": "587,000 square kilometers."}
{"answers": {"text": ["pasture for cattle", "pasture for cattle", "pasture for cattle"], "answer_start": [180, 180, 180]}, "prediction_text": "Livestock pasture."}
{"answers": {"text": ["second-largest global producer", "second", "second-largest"], "answer_start": [352, 352, 352]}, "prediction_text": "Second."}
{"answers": {"text": ["91%", "91", "91%"], "answer_start": [261, 261, 261]}, "prediction_text": "70%"}
{"answers": {"text": ["soy farmers", "soy", "soy"], "answer_start": [13, 13, 13]}, "prediction_text": "Soy farmers."}
{"answers": {"text": ["increased settlement and deforestation", "increased settlement and deforestation", "increased settlement and deforestation"], "answer_start": [218, 218, 218]}, "prediction_text": "Increased settlement and deforestation."}
{"answers": {"text": ["8,646 sq mi", "22,392 km2 or 8,646 sq mi", "8,646"], "answer_start": [326, 312, 326]}, "prediction_text": "8,646 sq mi per year."}
{"answers": {"text": ["deforestation has declined", "declined significantly", "declined significantly"], "answer_start": [442, 460, 460]}, "prediction_text": "Increased."}
{"answers": {"text": ["18% higher", "18%", "18%"], "answer_start": [352, 352, 352]}, "prediction_text": "18%"}
{"answers": {"text": ["loss of biodiversity", "biodiversity", "biodiversity"], "answer_start": [38, 46, 46]}, "prediction_text": "biodiversity"}
{"answers": {"text": ["destruction of the forest", "destruction of the forest", "destruction of the forest"], "answer_start": [81, 81, 81]}, "prediction_text": "destruction of the forest"}
{"answers": {"text": ["carbon contained within the vegetation", "carbon contained within the vegetation", "carbon"], "answer_start": [142, 142, 142]}, "prediction_text": "Global warming."}
{"answers": {"text": ["10% of the carbon stores", "10%", "10%"], "answer_start": [323, 323, 267]}, "prediction_text": "1.1 \u00d7 1011 metric tonnes."}
{"answers": {"text": ["1.1 \u00d7 1011 metric tonnes", "1.1 \u00d7 1011", "1.1 \u00d7 1011"], "answer_start": [378, 378, 378]}, "prediction_text": "1.1 \u00d7 1011 metric tonnes."}
{"answers": {"text": ["reduced rainfall and increased temperatures", "severely reduced rainfall and increased temperatures", "severely reduced rainfall and increased temperatures"], "answer_start": [168, 159, 159]}, "prediction_text": "Severely reduced rainfall and increased temperatures."}
{"answers": {"text": ["greenhouse gas emissions", "greenhouse gas", "greenhouse gas"], "answer_start": [54, 54, 54]}, "prediction_text": "greenhouse gas emissions"}
{"answers": {"text": ["2100", "by 2100", "2100"], "answer_start": [284, 281, 284]}, "prediction_text": "2100"}
{"answers": {"text": ["though the 21st century", "though the 21st century", "though the 21st century"], "answer_start": [546, 546, 546]}, "prediction_text": "2100."}
{"answers": {"text": ["climate change in addition to deforestation", "climate change in addition to deforestation", "climate change in addition to deforestation"], "answer_start": [573, 573, 573]}, "prediction_text": "Climate change, deforestation."}
{"answers": {"text": ["indigenous territories", "indigenous", "indigenous"], "answer_start": [3, 3, 3]}, "prediction_text": "Indigenous peoples' rainforest communities."}
{"answers": {"text": ["community-based conservation", "community-based", "community-based"], "answer_start": [502, 502, 502]}, "prediction_text": "Ethno-biology and community-based conservation efforts."}
{"answers": {"text": ["deforestation and ecocide", "deforestation and ecocide", "deforestation and ecocide"], "answer_start": [54, 54, 54]}, "prediction_text": "Deforestation and ecocide."}
{"answers": {"text": ["Urarina", "Urarina", "Urarina"], "answer_start": [201, 201, 201]}, "prediction_text": "Urarina"}
{"answers": {"text": ["lowland South American", "lowland South American peoples", "South American"], "answer_start": [413, 413, 421]}, "prediction_text": "Urarina"}
{"answers": {"text": ["remote sensing", "remote sensing", "remote sensing"], "answer_start": [11, 11, 11]}, "prediction_text": "handheld GPS devices and programs like Google Earth."}
{"answers": {"text": ["Trio Tribe", "Trio", "Trio"], "answer_start": [249, 249, 249]}, "prediction_text": "Trio Tribe"}
{"answers": {"text": ["southern Suriname", "southern Suriname", "southern Suriname"], "answer_start": [292, 292, 292]}, "prediction_text": "Suriname"}
{"answers": {"text": ["to help strengthen their territorial claims", "map out their ancestral lands to help strengthen their territorial claims", "map out their ancestral lands"], "answer_start": [341, 311, 311]}, "prediction_text": "Map out their ancestral lands."}
{"answers": {"text": ["to protect their tribal lands from commercial interests", "to protect their tribal lands from commercial interests", "protect their tribal lands from commercial interests"], "answer_start": [118, 118, 121]}, "prediction_text": "To protect their tribal lands."}
{"answers": {"text": ["tree growth", "tree growth"], "answer_start": [102, 102]}, "prediction_text": "Tree growth stages."}
{"answers": {"text": ["carbon related emissions", "carbon related", "carbon related"], "answer_start": [54, 54, 54]}, "prediction_text": "carbon emissions"}
{"answers": {"text": ["Tatiana Kuplich", "Tatiana Kuplich", "Tatiana Kuplich"], "answer_start": [178, 178, 178]}, "prediction_text": "Tatiana Kuplich"}
{"answers": {"text": ["2006", "2006", "2006"], "answer_start": [173, 173, 173]}, "prediction_text": "2006"}
{"answers": {"text": ["Synthetic aperture radar (SAR)", "Synthetic aperture", "Synthetic aperture radar"], "answer_start": [504, 504, 504]}, "prediction_text": "SAR"}
{"answers": {"text": ["2005", "2005", "2005"], "answer_start": [3, 3, 3]}, "prediction_text": "2005"}
{"answers": {"text": ["Brazilian National Institute of Amazonian Research", "Brazilian National Institute of Amazonian Research", "Brazilian National Institute of Amazonian Research"], "answer_start": [386, 386, 386]}, "prediction_text": "Brazilian National Institute of Amazonian Research"}
{"answers": {"text": ["deforestation", "deforestation on regional climate", "deforestation"], "answer_start": [514, 514, 514]}, "prediction_text": "Deforestation."}
{"answers": {"text": ["savanna or desert", "desert", "savanna or desert"], "answer_start": [710, 721, 710]}, "prediction_text": "Savanna or desert."}
{"answers": {"text": ["Woods Hole Research Center", "Woods Hole Research Center", "Woods Hole Research Center"], "answer_start": [246, 246, 246]}, "prediction_text": "Woods Hole Research Center"}
{"answers": {"text": ["2010", "2010", "2010"], "answer_start": [3, 3, 3]}, "prediction_text": "2010"}
{"answers": {"text": ["1,160,000", "1,160,000", "1,160,000"], "answer_start": [151, 151, 151]}, "prediction_text": "3,000,000 km2"}
{"answers": {"text": ["three epicenters", "three epicenters", "three"], "answer_start": [284, 284, 284]}, "prediction_text": "3,000,000 km2"}
{"answers": {"text": ["2005", "2005", "2005"], "answer_start": [339, 339, 339]}, "prediction_text": "2005"}
{"answers": {"text": ["1.5 gigatons", "1.5 gigatons", "1.5 gigatons"], "answer_start": [483, 483, 483]}, "prediction_text": "1.5 gigatons."}
{"answers": {"text": ["comb jellies", "comb jellies", "comb jellies"], "answer_start": [151, 151, 151]}, "prediction_text": "Ctenophores"}
{"answers": {"text": ["marine waters worldwide.", "marine waters worldwide", "marine waters"], "answer_start": [201, 201, 201]}, "prediction_text": "Marine waters worldwide."}
{"answers": {"text": ["a few millimeters to 1.5 m (4 ft 11 in) in size.", "a few millimeters to 1.5 m (4 ft 11 in) in size", "a few millimeters to 1.5 m"], "answer_start": [415, 415, 415]}, "prediction_text": "1.5 m (4 ft 11 in)"}
{"answers": {"text": ["phylum of animals that live in marine waters", "a phylum of animals", "comb jellies"], "answer_start": [170, 168, 151]}, "prediction_text": "Ctenophora (/t\u025bn\u0259f\u0254\u02d0r/ or /\u02c8ti\u02d0n\u0259f\u0254\u02d0r/; singular ctenophore, /\u02c8t\u025bn\u0259f\u0254\u02d0r/ or /\u02c8ti\u02d0n\u0259f\u0254\u02d0r/; from the Greek \u03ba\u03c4\u03b5\u03af\u03c2 kteis 'comb' and \u03c6\u03ad\u03c1\u03c9 pher\u014d 'carry') is a phylum of animals that live in marine waters worldwide. Their most distinctive feature is the \u2018combs\u2019 \u2013 groups of cilia which they use for swimming."}
{"answers": {"text": ["\u2018combs\u2019 \u2013 groups of cilia", "cilia", "cilia"], "answer_start": [264, 284, 371]}, "prediction_text": "Cilia."}
{"answers": {"text": ["water flow through the body cavity", "water flow", "water flow through the body cavity"], "answer_start": [801, 801, 801]}, "prediction_text": "Cilia."}
{"answers": {"text": ["1.5 m (4 ft 11 in)", "a few millimeters to 1.5 m"], "answer_start": [436, 415]}, "prediction_text": "4 ft 11 in."}
{"answers": {"text": ["\u2018combs\u2019 \u2013 groups of cilia", "combs", "the \u2018combs\u2019"], "answer_start": [264, 265, 260]}, "prediction_text": "Combs."}
{"answers": {"text": ["comb jellies", "comb jellies", "comb jellies"], "answer_start": [151, 151, 151]}, "prediction_text": "Ctenophores"}
{"answers": {"text": ["1.5 m (4 ft 11 in)", "1.5 m (4 ft 11 in)", "a few millimeters to 1.5 m"], "answer_start": [436, 436, 415]}, "prediction_text": "4 ft 11 in."}
{"answers": {"text": ["water flow through the body cavity", "\u03ba\u03c4\u03b5\u03af\u03c2 kteis 'comb' and \u03c6\u03ad\u03c1\u03c9 pher\u014d 'carry'", "water flow through the body cavity"], "answer_start": [801, 90, 801]}, "prediction_text": "Water flow through the body cavity."}
{"answers": {"text": ["\u03ba\u03c4\u03b5\u03af\u03c2 kteis 'comb' and \u03c6\u03ad\u03c1\u03c9 pher\u014d 'carry'", "\u03ba\u03c4\u03b5\u03af\u03c2 kteis 'comb' and \u03c6\u03ad\u03c1\u03c9 pher\u014d 'carry'", "kteis 'comb' and \u03c6\u03ad\u03c1\u03c9 pher\u014d 'carry'"], "answer_start": [90, 90, 96]}, "prediction_text": "Comb jellies."}
{"answers": {"text": ["marine waters", "marine waters worldwide", "marine waters"], "answer_start": [201, 201, 201]}, "prediction_text": "Marine waters worldwide."}
{"answers": {"text": ["ten times their own weight", "ten times their own weight", "ten times their own weight"], "answer_start": [300, 300, 300]}, "prediction_text": "100-150."}
{"answers": {"text": ["100\u2013150", "100\u2013150", "100\u2013150"], "answer_start": [342, 342, 342]}, "prediction_text": "100-150."}
{"answers": {"text": ["possibly another 25", "25", "25"], "answer_start": [383, 400, 400]}, "prediction_text": "25"}
{"answers": {"text": ["100\u2013150 species", "100\u2013150", "100\u2013150"], "answer_start": [342, 342, 342]}, "prediction_text": "100-150."}
{"answers": {"text": ["tentilla", "tentilla", "tentilla"], "answer_start": [552, 552, 552]}, "prediction_text": "Tentilla."}
{"answers": {"text": ["ten times their own weight", "ten times their own weight", "ten times their own weight"], "answer_start": [300, 300, 300]}, "prediction_text": "100-150."}
{"answers": {"text": ["tentacles", "tentacles", "tentacles"], "answer_start": [824, 824, 824]}, "prediction_text": "tentacles"}
{"answers": {"text": ["groups of large, stiffened cilia", "stiffened cilia", "stiffened cilia"], "answer_start": [896, 913, 913]}, "prediction_text": "Cilia."}
{"answers": {"text": ["ten times their own weight", "ten times their own weight", "ten times their own weight"], "answer_start": [300, 300, 300]}, "prediction_text": "100-150."}
{"answers": {"text": ["tentilla", "tentilla", "tentilla"], "answer_start": [552, 552, 552]}, "prediction_text": "Little tentacles."}
{"answers": {"text": ["groups of large, stiffened cilia", "stiffened cilia", "stiffened cilia"], "answer_start": [896, 913, 913]}, "prediction_text": "Groups of stiffened cilia."}
{"answers": {"text": ["colloblasts", "retractable tentacles fringed with tentilla", "colloblasts"], "answer_start": [604, 517, 604]}, "prediction_text": "Colloblasts."}
{"answers": {"text": ["100\u2013150 species", "100\u2013150", "100\u2013150 species have been validated"], "answer_start": [342, 342, 342]}, "prediction_text": "25"}
{"answers": {"text": ["Most species are hermaphrodites", "hermaphrodites", "hermaphrodites"], "answer_start": [0, 17, 17]}, "prediction_text": "Hermaphrodites"}
{"answers": {"text": ["miniature cydippids", "miniature cydippids", "miniature cydippids"], "answer_start": [533, 533, 533]}, "prediction_text": "Cydippids."}
{"answers": {"text": ["In at least some species, juveniles are capable of reproduction before reaching the adult size", "juveniles are capable of reproduction", "juveniles are capable of reproduction"], "answer_start": [886, 912, 912]}, "prediction_text": "Yes."}
{"answers": {"text": ["can produce both eggs and sperm, meaning it can fertilize its own egg", "it can fertilize its own egg", "a single animal can produce both eggs and sperm"], "answer_start": [48, 89, 32]}, "prediction_text": "The hermaphrodite can produce both eggs and sperm."}
{"answers": {"text": ["can produce both eggs and sperm at the same time", "produce both eggs and sperm at the same time", "produce both eggs and sperm at the same time"], "answer_start": [183, 187, 187]}, "prediction_text": "Produce both eggs and sperm."}
{"answers": {"text": ["sequential", "sequential hermaphrodites", "simultaneous"], "answer_start": [244, 244, 148]}, "prediction_text": "Simultaneous hermaphrodites."}
{"answers": {"text": ["platyctenids", "platyctenids", "platyctenids"], "answer_start": [372, 372, 372]}, "prediction_text": "Platyctenids."}
{"answers": {"text": ["hermaphroditism and early reproduction", "hermaphroditism and early reproduction", "combination of hermaphroditism and early reproduction"], "answer_start": [1011, 1011, 996]}, "prediction_text": "Early reproduction."}
{"answers": {"text": ["a single animal can produce both eggs and sperm", "a single animal can produce both eggs and sperm", "a single animal can produce both eggs and sperm"], "answer_start": [32, 32, 32]}, "prediction_text": "Animal."}
{"answers": {"text": ["can produce both eggs and sperm at the same time.", "can produce both eggs and sperm", "produce both eggs and sperm at the same time"], "answer_start": [183, 183, 187]}, "prediction_text": "They can produce both eggs and sperm at the same time."}
{"answers": {"text": ["the eggs and sperm mature at different times", "eggs and sperm mature at different times", "eggs and sperm mature at different times"], "answer_start": [280, 284, 284]}, "prediction_text": "The eggs and sperm mature at different times."}
{"answers": {"text": ["platyctenids", "platyctenids", "platyctenids"], "answer_start": [372, 372, 372]}, "prediction_text": "Platyctenids."}
{"answers": {"text": ["beroids", "beroids", "beroids"], "answer_start": [634, 634, 634]}, "prediction_text": "Beroids"}
{"answers": {"text": ["the Black Sea", "Black Sea", "Black Sea"], "answer_start": [474, 478, 478]}, "prediction_text": "Black Sea"}
{"answers": {"text": ["Mnemiopsis", "Mnemiopsis", "Mnemiopsis"], "answer_start": [424, 424, 424]}, "prediction_text": "Mnemiopsis"}
{"answers": {"text": ["over-fishing and long-term environmental changes", "over-fishing and long-term environmental changes", "long-term environmental changes"], "answer_start": [680, 680, 697]}, "prediction_text": "Over-fishing."}
{"answers": {"text": ["other ctenophores", "ctenophores", "other ctenophores"], "answer_start": [877, 883, 877]}, "prediction_text": "Fish larvae and organisms that would otherwise have fed the fish."}
{"answers": {"text": ["Mnemiopsis", "Mnemiopsis", "Mnemiopsis"], "answer_start": [424, 424, 424]}, "prediction_text": "Mnemiopsis."}
{"answers": {"text": ["fish larvae and organisms", "fish larvae and organisms that would otherwise have fed the fish", "fish larvae and organisms that would otherwise have fed the fish"], "answer_start": [559, 559, 559]}, "prediction_text": "Fish larvae and organisms that would otherwise have fed the fish."}
{"answers": {"text": ["In bays", "bays", "bays"], "answer_start": [141, 144, 144]}, "prediction_text": "Bays."}
{"answers": {"text": ["In bays", "bays", "bays"], "answer_start": [141, 144, 144]}, "prediction_text": "Bays."}
{"answers": {"text": ["planktonic plants", "planktonic plants", "a vital part of marine food chains"], "answer_start": [342, 342, 372]}, "prediction_text": "Planktonic plants."}
{"answers": {"text": ["Mnemiopsis", "Mnemiopsis", "Mnemiopsis"], "answer_start": [424, 424, 424]}, "prediction_text": "Mnemiopsis"}
{"answers": {"text": ["causing fish stocks to collapse", "causing fish stocks to collapse", "causing fish stocks to collapse"], "answer_start": [512, 512, 512]}, "prediction_text": "Fish stocks collapse."}
{"answers": {"text": ["introduction of Beroe", "accidental introduction of Beroe"], "answer_start": [805, 794]}, "prediction_text": "Overfishing and long-term environmental changes."}
{"answers": {"text": ["66 million years ago", "515 million years ago", "66 million years ago"], "answer_start": [760, 226, 760]}, "prediction_text": "66 million years ago."}
{"answers": {"text": ["monophyletic", "monophyletic", "\"cydippids\" are not monophyletic"], "answer_start": [859, 859, 839]}, "prediction_text": "Monophyletic"}
{"answers": {"text": ["515 million years", "66 million years ago", "515 million years"], "answer_start": [226, 760, 226]}, "prediction_text": "515 million years ago."}
{"answers": {"text": ["tentacles", "tentacles", "tentacles"], "answer_start": [100, 100, 100]}, "prediction_text": "tentacles."}
{"answers": {"text": ["515 million years", "66 million years", "515 million years"], "answer_start": [226, 760, 226]}, "prediction_text": "515 million years ago."}
{"answers": {"text": ["Cretaceous\u2013Paleogene extinction", "the Cretaceous\u2013Paleogene extinction", "Cretaceous\u2013Paleogene extinction"], "answer_start": [722, 718, 722]}, "prediction_text": "Cretaceous-Paleogene extinction event."}
{"answers": {"text": ["monophyletic", "monophyletic", "monophyletic"], "answer_start": [859, 859, 859]}, "prediction_text": "Monophyletic."}
{"answers": {"text": ["tentacles", "tentacles", "tentacles"], "answer_start": [100, 100, 100]}, "prediction_text": "Combs."}
{"answers": {"text": ["cnidarians", "Ctenophores", "cnidarians"], "answer_start": [89, 0, 89]}, "prediction_text": "Cnidarians"}
{"answers": {"text": ["by having colloblasts", "having colloblasts", "colloblasts"], "answer_start": [446, 449, 456]}, "prediction_text": "Colloblasts."}
{"answers": {"text": ["bilaterians", "bilaterians", "bilaterians"], "answer_start": [155, 155, 155]}, "prediction_text": "Bilateria"}
{"answers": {"text": ["Ctenophores", "Ctenophores", "Ctenophores"], "answer_start": [0, 0, 0]}, "prediction_text": "Ctenophores"}
{"answers": {"text": ["colloblasts", "colloblasts", "colloblasts"], "answer_start": [456, 456, 456]}, "prediction_text": "Colloblasts."}
{"answers": {"text": ["cnidarians", "cnidarians", "cnidarians"], "answer_start": [89, 89, 89]}, "prediction_text": "Bilaterians"}
{"answers": {"text": ["colloblasts", "colloblasts", "colloblasts"], "answer_start": [456, 456, 456]}, "prediction_text": "Colloblasts."}
{"answers": {"text": ["colloblasts", "colloblasts", "colloblasts"], "answer_start": [456, 456, 456]}, "prediction_text": "Colloblasts."}
{"answers": {"text": ["ctenophores and cnidarians", "ctenophores and cnidarians", "ctenophores and cnidarians"], "answer_start": [230, 230, 230]}, "prediction_text": "Ctenophores and cnidarians."}
{"answers": {"text": ["bilaterians", "bilaterians", "bilaterians"], "answer_start": [155, 155, 155]}, "prediction_text": "Bilaterians"}
{"answers": {"text": ["mesoglea", "mesoglea", "mesoglea"], "answer_start": [144, 144, 144]}, "prediction_text": "mesoglea"}
{"answers": {"text": ["diploblastic", "ctenophores", "diploblastic"], "answer_start": [337, 29, 337]}, "prediction_text": "Diploblastic"}
{"answers": {"text": ["sponges and cnidarians, ctenophores", "ctenophores", "cnidarians and ctenophores"], "answer_start": [5, 29, 156]}, "prediction_text": "Cnidarians and ctenophores."}
{"answers": {"text": ["sponges", "sponges", "sponges"], "answer_start": [362, 362, 362]}, "prediction_text": "Sponges"}
{"answers": {"text": ["cilia", "cilia", "cilia"], "answer_start": [133, 133, 133]}, "prediction_text": "Cilia."}
{"answers": {"text": ["method of locomotion", "their main method of locomotion", "locomotion"], "answer_start": [163, 152, 173]}, "prediction_text": "locomotion"}
{"answers": {"text": ["ctenes", "ctenes", "ctenes"], "answer_start": [314, 314, 314]}, "prediction_text": "ctenes"}
{"answers": {"text": ["comb-bearing", "\"comb\" and the Greek suffix -\u03c6\u03bf\u03c1\u03bf\u03c2 meaning \"carrying\"", "comb-bearing"], "answer_start": [450, 512, 450]}, "prediction_text": "Comb-bearing."}
{"answers": {"text": ["Pleurobrachia", "Pleurobrachia", "Pleurobrachia"], "answer_start": [590, 590, 590]}, "prediction_text": "Pleurobrachia"}
{"answers": {"text": ["oceanic species", "oceanic species", "oceanic species"], "answer_start": [190, 293, 190]}, "prediction_text": "Oceanic species."}
{"answers": {"text": ["to withstand waves and swirling sediment particles", "to withstand waves and swirling sediment particles", "to withstand waves and swirling sediment particles"], "answer_start": [127, 127, 127]}, "prediction_text": "They are tough enough to withstand waves and swirling sediment particles."}
{"answers": {"text": ["Pleurobrachia, Beroe and Mnemiopsis", "Pleurobrachia, Beroe and Mnemiopsis", "Pleurobrachia, Beroe and Mnemiopsis"], "answer_start": [475, 475, 475]}, "prediction_text": "Pleurobrachia, Beroe, Mnemiopsis."}
{"answers": {"text": ["epithelium", "epithelium", "epithelium"], "answer_start": [536, 536, 536]}, "prediction_text": "Epithelium."}
{"answers": {"text": ["bioluminescence", "bioluminescence", "bioluminescence"], "answer_start": [945, 945, 945]}, "prediction_text": "Bioluminescence."}
{"answers": {"text": ["pharynx", "pharynx", "pharynx"], "answer_start": [76, 76, 76]}, "prediction_text": "Pharynx"}
{"answers": {"text": ["a mouth that can usually be closed by muscles; a pharynx (\"throat\"); a wider area in the center that acts as a stomach; and a system of internal canals.", "a system of internal canals", "the gastrodermis"], "answer_start": [27, 151, 548]}, "prediction_text": "The internal cavity contains: a mouth that can usually be closed by muscles; a pharynx (\"throat\"); a wider area in the center that acts as a stomach; and a system of internal canals."}
{"answers": {"text": ["the mouth and pharynx;", "the mouth and pharynx", "the mouth and pharynx"], "answer_start": [254, 254, 254]}, "prediction_text": "mouth and pharynx"}
{"answers": {"text": ["swimming-plates", "swimming-plates", "swimming-plates"], "answer_start": [56, 56, 56]}, "prediction_text": "swimming-plates"}
{"answers": {"text": ["also called \"ctenes\" or \"comb plates", "\"ctenes\" or \"comb plates\"", "ctenes"], "answer_start": [424, 436, 437]}, "prediction_text": "ctenophores"}
{"answers": {"text": ["supporting function", "suspected to have a supporting function", "a supporting function"], "answer_start": [773, 753, 771]}, "prediction_text": "Reverse direction."}
{"answers": {"text": ["in the direction in which the mouth is pointing,", "the direction in which the mouth is pointing", "direction in which the mouth is pointing"], "answer_start": [941, 944, 948]}, "prediction_text": "Inward."}
{"answers": {"text": ["2 millimeters (0.079 in)", "2 millimeters", "2 millimeters"], "answer_start": [546, 546, 546]}, "prediction_text": "2 millimeters."}
{"answers": {"text": ["osmotic pressure", "It is uncertain", "rely on osmotic pressure"], "answer_start": [109, 0, 101]}, "prediction_text": "osmotic pressure."}
{"answers": {"text": ["the mesoglea", "mesoglea", "mesoglea"], "answer_start": [328, 332, 332]}, "prediction_text": "Mesoglea"}
{"answers": {"text": ["increase its bulk and decrease its density", "increase its bulk and decrease its density", "increase its bulk and decrease its density"], "answer_start": [344, 344, 344]}, "prediction_text": "Decreases its volume and increases its density."}
{"answers": {"text": ["pump water out of the mesoglea", "pump", "pump water out of the mesoglea"], "answer_start": [488, 313, 488]}, "prediction_text": "Pump water out of the mesoglea."}
{"answers": {"text": ["aboral organ", "aboral organ", "aboral organ"], "answer_start": [42, 42, 42]}, "prediction_text": "Aboral organ"}
{"answers": {"text": ["at the opposite end from the mouth", "the opposite end from the mouth", "opposite end from the mouth"], "answer_start": [56, 59, 63]}, "prediction_text": "At the opposite end from the mouth."}
{"answers": {"text": ["a transparent dome made of long, immobile cilia", "a transparent dome made of long, immobile cilia", "transparent dome made of long, immobile cilia"], "answer_start": [303, 303, 305]}, "prediction_text": "The transparent dome."}
{"answers": {"text": ["a statocyst", "statocyst", "statocyst"], "answer_start": [115, 117, 117]}, "prediction_text": "Statocyst"}
{"answers": {"text": ["a balance sensor", "a balance sensor consisting of a statolith", "a balance sensor consisting of a statolith"], "answer_start": [128, 128, 128]}, "prediction_text": "Balance sensor."}
{"answers": {"text": ["sea gooseberry", "sea gooseberry", "sea gooseberry"], "answer_start": [159, 159, 159]}, "prediction_text": "Sea gooseberry"}
{"answers": {"text": ["a pair of long, slender tentacles", "long, slender tentacles", "a pair of long, slender tentacles"], "answer_start": [350, 360, 350]}, "prediction_text": "A pair of long, slender tentacles."}
{"answers": {"text": ["more or less rounded", "egg-shaped", "more or less rounded"], "answer_start": [42, 208, 42]}, "prediction_text": "rounded"}
{"answers": {"text": ["a sheath", "a sheath", "a sheath into which it can be withdrawn"], "answer_start": [400, 400, 400]}, "prediction_text": "sheaths"}
{"answers": {"text": ["at the narrow end", "the narrow end", "at the narrow end"], "answer_start": [239, 242, 239]}, "prediction_text": "At the narrow end."}
{"answers": {"text": ["tentilla", "tentilla", "tentilla"], "answer_start": [65, 65, 65]}, "prediction_text": "tentilla"}
{"answers": {"text": ["specialized mushroom-shaped cells in the outer layer of the epidermis", "specialized mushroom-shaped cells in the outer layer of the epidermis", "specialized mushroom-shaped cells in the outer layer of the epidermis"], "answer_start": [297, 297, 297]}, "prediction_text": "Colloblasts."}
{"answers": {"text": ["they contain striated muscle,", "they contain striated muscle", "they contain striated muscle"], "answer_start": [1150, 1150, 1150]}, "prediction_text": "The tentilla of euplokamis contains striated muscle."}
{"answers": {"text": ["three types of movement", "three", "three"], "answer_start": [1369, 1369, 1369]}, "prediction_text": "3"}
{"answers": {"text": ["capturing prey", "capturing prey", "capturing prey"], "answer_start": [1410, 1410, 1410]}, "prediction_text": "Capturing prey."}
{"answers": {"text": ["eight rows", "eight", "eight"], "answer_start": [10, 10, 10]}, "prediction_text": "8"}
{"answers": {"text": ["from near the mouth to the opposite end", "near the mouth to the opposite end", "near the mouth to the opposite end"], "answer_start": [39, 44, 44]}, "prediction_text": "Near the mouth."}
{"answers": {"text": ["evenly round the body", "evenly", "evenly round the body"], "answer_start": [95, 95, 95]}, "prediction_text": "evenly round the body."}
{"answers": {"text": ["ciliary groove", "a ciliary groove", "a ciliary groove"], "answer_start": [233, 231, 231]}, "prediction_text": "The ciliary groove."}
{"answers": {"text": ["lobes", "lobes", "lobes"], "answer_start": [26, 26, 26]}, "prediction_text": "Lobates have a pair of lobes."}
{"answers": {"text": ["gelatinous projections edged with cilia that produce water currents", "gelatinous projections edged with cilia", "gelatinous projections edged with cilia"], "answer_start": [417, 417, 417]}, "prediction_text": "Four auricles."}
{"answers": {"text": ["four", "four", "four"], "answer_start": [402, 402, 402]}, "prediction_text": "Four."}
{"answers": {"text": ["help direct microscopic prey toward the mouth", "produce water currents that help direct microscopic prey toward the mouth", "produce water currents that help direct microscopic prey toward the mouth"], "answer_start": [490, 462, 462]}, "prediction_text": "Produce water currents."}
{"answers": {"text": ["suspended planktonic prey", "suspended planktonic prey", "planktonic prey"], "answer_start": [608, 608, 618]}, "prediction_text": "suspended planktonic prey"}
{"answers": {"text": ["by clapping their lobes", "clapping their lobes", "clapping their lobes"], "answer_start": [515, 518, 518]}, "prediction_text": "Clapping their lobes."}
{"answers": {"text": ["jet of expelled water drives them backwards very quickly.", "jet of expelled water drives them backwards very quickly", "expelled water drives them backwards very quickly"], "answer_start": [552, 552, 559]}, "prediction_text": "They escape from danger."}
{"answers": {"text": ["nerves", "nerves rather than by water disturbances created by the cilia", "nerves"], "answer_start": [679, 679, 679]}, "prediction_text": "nerves"}
{"answers": {"text": ["water disturbances created by the cilia", "water disturbances created by the cilia", "water disturbances created by the cilia"], "answer_start": [701, 701, 701]}, "prediction_text": "nerves"}
{"answers": {"text": ["Nuda", "Nuda", "Nuda"], "answer_start": [27, 27, 27]}, "prediction_text": "Nuda"}
{"answers": {"text": ["The Beroida", "Beroida", "Beroida"], "answer_start": [0, 4, 4]}, "prediction_text": "Beroida"}
{"answers": {"text": ["zip\" the mouth shut when the animal is not feeding,", "\"zip\" the mouth shut when the animal is not feeding", "\"zip\" the mouth shut when the animal is not feeding"], "answer_start": [514, 513, 513]}, "prediction_text": "Zips the mouth shut."}
{"answers": {"text": ["\"zip\" the mouth shut", "streamlines the front of the animal", "tight closure streamlines the front of the animal"], "answer_start": [513, 656, 642]}, "prediction_text": "Zips the mouth shut."}
{"answers": {"text": ["large pharynx", "large cilia", "\"macrocilia\""], "answer_start": [71, 228, 158]}, "prediction_text": "Pharynx."}
{"answers": {"text": ["The Cestida", "Cestida", "Cestida"], "answer_start": [0, 4, 4]}, "prediction_text": "Cestum veneris and Velamen parallelum."}
{"answers": {"text": ["Cestum veneris", "Cestum veneris", "up to 1.5 meters (4.9 ft) long"], "answer_start": [512, 512, 580]}, "prediction_text": "Venus' girdle"}
{"answers": {"text": ["belt animals", "belt animals", "\"belt animals\""], "answer_start": [14, 14, 13]}, "prediction_text": "Belt animals"}
{"answers": {"text": ["by undulating their bodies as well as by the beating of their comb-rows.", "undulating their bodies", "by undulating their bodies as well as by the beating of their comb-rows"], "answer_start": [346, 349, 346]}, "prediction_text": "undulating their bodies as well as by the beating of their comb-rows."}
{"answers": {"text": ["Velamen parallelum", "Velamen parallelum", "Velamen parallelum"], "answer_start": [654, 654, 654]}, "prediction_text": "Velamen parallelum"}
{"answers": {"text": ["a pair of tentilla-bearing tentacles", "tentilla-bearing tentacles", "a pair of tentilla-bearing tentacles"], "answer_start": [89, 99, 89]}, "prediction_text": "tentilla-bearing tentacles"}
{"answers": {"text": ["cling to and creep on surfaces", "as a muscular \"foot\"", "as a muscular \"foot\""], "answer_start": [154, 222, 222]}, "prediction_text": "Cling to surfaces."}
{"answers": {"text": ["comb-rows", "comb-rows", "comb-rows"], "answer_start": [294, 294, 294]}, "prediction_text": "comb-rows."}
{"answers": {"text": ["on rocks, algae, or the body surfaces of other invertebrates", "rocks, algae, or the body surfaces of other invertebrates", "rocks, algae, or the body surfaces of other invertebrates"], "answer_start": [356, 359, 359]}, "prediction_text": "On rocks, algae, or the body surfaces of other invertebrates."}
{"answers": {"text": ["via pores in the epidermis", "pores in the epidermis", "via pores in the epidermis"], "answer_start": [341, 345, 341]}, "prediction_text": "pores in the epidermis."}
{"answers": {"text": ["internal fertilization and keep the eggs in brood chambers until they hatch.", "internal fertilization", "internal fertilization"], "answer_start": [433, 433, 433]}, "prediction_text": "Internal fertilization."}
{"answers": {"text": ["Mnemiopsis", "Mnemiopsis", "Mnemiopsis"], "answer_start": [580, 580, 580]}, "prediction_text": "Mnemiopsis"}
{"answers": {"text": ["in the parts of the internal canal network under the comb rows", "the parts of the internal canal network under the comb rows", "internal canal network under the comb rows"], "answer_start": [245, 248, 265]}, "prediction_text": "Under the comb rows."}
{"answers": {"text": ["external", "external", "external"], "answer_start": [386, 386, 386]}, "prediction_text": "Internal."}
{"answers": {"text": ["tentacles and tentacle sheaths", "tentacles and tentacle sheaths", "tentacles and tentacle sheaths"], "answer_start": [228, 228, 228]}, "prediction_text": "tentacles and tentacle sheaths."}
{"answers": {"text": ["among the plankton", "among the plankton", "among the plankton"], "answer_start": [462, 462, 462]}, "prediction_text": "Plankton."}
{"answers": {"text": ["after dropping to the sea-floor", "after dropping to the sea-floor", "after dropping to the sea-floor"], "answer_start": [604, 604, 604]}, "prediction_text": "Gradually."}
{"answers": {"text": ["more like true larvae", "true larvae", "like true larvae"], "answer_start": [426, 436, 431]}, "prediction_text": "true larvae"}
{"answers": {"text": ["Beroe", "Beroe", "Beroe"], "answer_start": [185, 185, 185]}, "prediction_text": "Beroe"}
{"answers": {"text": ["they produce secretions (ink) that luminesce", "they produce secretions (ink) that luminesce at much the same wavelengths as their bodies", "they produce secretions (ink) that luminesce"], "answer_start": [112, 112, 112]}, "prediction_text": "They produce secretions (ink) that luminesce at much the same wavelengths as their bodies."}
{"answers": {"text": ["are disturbed,", "disturbed", "are disturbed"], "answer_start": [97, 101, 97]}, "prediction_text": "When disturbed."}
{"answers": {"text": ["ink", "ink", "ink"], "answer_start": [137, 137, 137]}, "prediction_text": "Ink"}
{"answers": {"text": ["Juveniles will luminesce more brightly", "Juveniles", "Juveniles"], "answer_start": [203, 203, 203]}, "prediction_text": "Juveniles."}
{"answers": {"text": ["Almost all ctenophores are predators", "predators", "predators"], "answer_start": [0, 27, 27]}, "prediction_text": "Both."}
{"answers": {"text": ["jellyfish", "jellyfish", "jellyfish"], "answer_start": [479, 479, 479]}, "prediction_text": "jellyfish"}
{"answers": {"text": ["incorporate their prey's nematocysts (stinging cells) into their own tentacles instead of colloblasts", "incorporate their prey's nematocysts (stinging cells) into their own tentacles", "incorporate their prey's nematocysts (stinging cells) into their own tentacles"], "answer_start": [493, 493, 493]}, "prediction_text": "They are incorporated into the tentacles."}
{"answers": {"text": ["smaller, weaker swimmers such as rotifers and mollusc and crustacean larvae.", "smaller, weaker swimmers such as rotifers and mollusc and crustacean larvae", "rotifers and mollusc and crustacean larvae"], "answer_start": [1556, 1556, 1589]}, "prediction_text": "Rotifers and mollusc and crustacean larvae."}
{"answers": {"text": ["Lampea", "Lampea", "Lampea"], "answer_start": [1018, 1018, 1018]}, "prediction_text": "Lampea"}
{"answers": {"text": ["their low ratio of organic matter to salt and water", "their low ratio of organic matter to salt and water", "low ratio of organic matter to salt and water"], "answer_start": [92, 92, 98]}, "prediction_text": "Low ratio of organic matter to salt and water."}
{"answers": {"text": ["chum salmon", "chum salmon", "chum salmon"], "answer_start": [386, 386, 386]}, "prediction_text": "Keta"}
{"answers": {"text": ["ctenophores", "other ctenophores", "other ctenophores"], "answer_start": [618, 612, 612]}, "prediction_text": "Other ctenophores."}
{"answers": {"text": ["the Red Sea", "the Red Sea", "blooms in the Red Sea"], "answer_start": [1089, 1089, 1079]}, "prediction_text": "Red Sea."}
{"answers": {"text": ["ctenophores,", "ctenophores", "ctenophores"], "answer_start": [682, 682, 682]}, "prediction_text": "Ctenophores."}
{"answers": {"text": ["ctenophore Mnemiopsis leidyi", "Mnemiopsis leidyi", "the Western Atlantic ctenophore Mnemiopsis leidyi was accidentally introduced"], "answer_start": [58, 69, 37]}, "prediction_text": "Mnemiopsis leidyi."}
{"answers": {"text": ["via the ballast tanks of ships", "via the ballast tanks of ships", "the ballast tanks of ships"], "answer_start": [150, 150, 154]}, "prediction_text": "Ballast tanks of ships."}
{"answers": {"text": ["by the accidental introduction of the Mnemiopsis-eating North American ctenophore Beroe ovata,", "the accidental introduction of the Mnemiopsis-eating North American ctenophore Beroe ovata", "the accidental introduction of the Mnemiopsis-eating North American ctenophore Beroe ovata, and by a cooling of the local climate from 1991 to 1993"], "answer_start": [925, 928, 928]}, "prediction_text": "Accidental introduction of the Mnemiopsis-eating North American ctenophore Beroe ovata."}
{"answers": {"text": ["in the late 1980s", "the late 1980s", "late 1980s"], "answer_start": [19, 22, 26]}, "prediction_text": "1980s"}
{"answers": {"text": ["significantly slowed the animal's metabolism", "slowed the animal's metabolism", "significantly slowed the animal's metabolism"], "answer_start": [1083, 1097, 1083]}, "prediction_text": "Slows metabolism."}
{"answers": {"text": ["Because of their soft, gelatinous bodies", "their soft, gelatinous bodies", "their soft, gelatinous bodies"], "answer_start": [0, 11, 11]}, "prediction_text": "Because of their soft, gelatinous bodies."}
{"answers": {"text": ["comb jelly.", "comb jelly", "a comb jelly"], "answer_start": [1213, 1213, 1211]}, "prediction_text": "Comb jelly."}
{"answers": {"text": ["Cambrian period.", "mid-Cambrian period", "mid-Cambrian period"], "answer_start": [564, 560, 560]}, "prediction_text": "Cambrian"}
{"answers": {"text": ["Three additional putative species", "Three", "Three"], "answer_start": [413, 413, 413]}, "prediction_text": "3"}
{"answers": {"text": ["lacked tentacles", "tentacles", "tentacles"], "answer_start": [602, 609, 609]}, "prediction_text": "tentacles."}
{"answers": {"text": ["515 million years", "about 515 million years", "515 million years"], "answer_start": [115, 109, 115]}, "prediction_text": "515 million years ago."}
{"answers": {"text": ["Cambrian sessile frond-like fossil Stromatoveris", "Stromatoveris", "sessile frond-like"], "answer_start": [10, 45, 19]}, "prediction_text": "Stromatoveris"}
{"answers": {"text": ["Stromatoveris", "Stromatoveris", "Stromatoveris"], "answer_start": [346, 346, 346]}, "prediction_text": "Stromatoveris"}
{"answers": {"text": ["Vendobionta", "ctenophores", "Vendobionta"], "answer_start": [157, 390, 157]}, "prediction_text": "Vendobionta"}
{"answers": {"text": ["Ediacaran period", "Ediacaran period", "Ediacaran period"], "answer_start": [186, 186, 186]}, "prediction_text": "Ediacaran period."}
{"answers": {"text": ["all other animals", "Bilateria", "Bilateria"], "answer_start": [929, 276, 344]}, "prediction_text": "All other animal phyla."}
{"answers": {"text": ["Porifera", "Porifera", "Porifera"], "answer_start": [1328, 1328, 1328]}, "prediction_text": "Porifera (sponges)"}
{"answers": {"text": ["beroids", "beroids", "beroids"], "answer_start": [40, 40, 40]}, "prediction_text": "Beroids"}
{"answers": {"text": ["monophyletic", "monophyletic", "monophyletic"], "answer_start": [819, 317, 317]}, "prediction_text": "Monophyletic."}
{"answers": {"text": ["65.5 million years ago", "65.5 million years ago", "65.5 million years ago"], "answer_start": [1272, 1272, 1272]}, "prediction_text": "65.5 million years ago."}
{"answers": {"text": ["Richard Harbison", "Richard Harbison", "Richard Harbison"], "answer_start": [223, 223, 223]}, "prediction_text": "Richard Harbison"}
{"answers": {"text": ["Fresno", "Fresno", "Fresno"], "answer_start": [0, 0, 0]}, "prediction_text": "Fresno"}
{"answers": {"text": ["220 miles (350 km)", "220 miles (350 km)", "220 miles (350 km)"], "answer_start": [432, 432, 432]}, "prediction_text": "350 km."}
{"answers": {"text": ["ash tree", "ash tree", "ash tree"], "answer_start": [605, 605, 605]}, "prediction_text": "Ash tree"}
{"answers": {"text": ["ash leaf", "ash leaf", "an ash leaf"], "answer_start": [634, 634, 631]}, "prediction_text": "Ash leaf"}
{"answers": {"text": ["(/\u02c8fr\u025bzno\u028a/ FREZ-noh)", "FREZ-noh", "FREZ-noh"], "answer_start": [7, 19, 19]}, "prediction_text": "FREZ-noh."}
{"answers": {"text": ["1872", "1872", "1872"], "answer_start": [3, 3, 3]}, "prediction_text": "1872"}
{"answers": {"text": ["the convenience of the railroad and worried about flooding", "drawn by the convenience of the railroad and worried about flooding", "the convenience of the railroad and worried about flooding"], "answer_start": [294, 285, 294]}, "prediction_text": "convenience of the railroad, flood control."}
{"answers": {"text": ["1885", "1885", "1885"], "answer_start": [420, 420, 420]}, "prediction_text": "1885"}
{"answers": {"text": ["47 streetcars", "47", "47"], "answer_start": [471, 471, 471]}, "prediction_text": "47"}
{"answers": {"text": ["store", "store", "store"], "answer_start": [199, 166, 199]}, "prediction_text": "Central Pacific Railroad"}
{"answers": {"text": ["2.7%", "2.7%", "2.7%"], "answer_start": [214, 214, 214]}, "prediction_text": "2.7%"}
{"answers": {"text": ["Chinatown", "Chinatown", "Chinatown"], "answer_start": [242, 242, 242]}, "prediction_text": "Little Armenia"}
{"answers": {"text": ["Pinedale", "Pinedale", "Pinedale"], "answer_start": [357, 357, 357]}, "prediction_text": "Pinedale"}
{"answers": {"text": ["an interim facility for the relocation of Fresno area Japanese Americans to internment camps", "an interim facility for the relocation of Fresno area Japanese Americans to internment camps", "an interim facility for the relocation of Fresno area Japanese Americans to internment camps"], "answer_start": [442, 442, 442]}, "prediction_text": "Pinedale Assembly Center"}
{"answers": {"text": ["an assembly center", "an assembly center", "an assembly center"], "answer_start": [580, 580, 580]}, "prediction_text": "Assembly center."}
{"answers": {"text": ["BankAmericard", "BankAmericard", "BankAmericard"], "answer_start": [65, 65, 65]}, "prediction_text": "BankAmericard"}
{"answers": {"text": ["BankAmericard", "BankAmericard", "BankAmericard"], "answer_start": [65, 152, 152]}, "prediction_text": "BankAmericard"}
{"answers": {"text": ["to revolve a balance", "a financial instrument that was usable across a large number of merchants and also allowed cardholders to revolve a balance", "revolve a balance"], "answer_start": [330, 227, 333]}, "prediction_text": "Revolve a balance."}
{"answers": {"text": ["1976", "1976", "1976"], "answer_start": [423, 423, 423]}, "prediction_text": "1976"}
{"answers": {"text": ["Visa Inc.", "Visa Inc.", "Visa Inc."], "answer_start": [507, 507, 507]}, "prediction_text": "Visa Inc."}
{"answers": {"text": ["Bill Aken", "Bill Aken", "Bill Aken"], "answer_start": [107, 107, 107]}, "prediction_text": "Bill Aken"}
{"answers": {"text": ["Bob Gallion", "Bob Gallion", "Bob Gallion"], "answer_start": [133, 133, 133]}, "prediction_text": "Bob Gallion"}
{"answers": {"text": ["Madera", "Madera", "Madera"], "answer_start": [323, 323, 323]}, "prediction_text": "Madera"}
{"answers": {"text": ["The Fresno Barn", "The Fresno Barn", "the old country-western show at The Fresno Barn"], "answer_start": [510, 510, 478]}, "prediction_text": "WWVA Jamboree"}
{"answers": {"text": ["Lupe Mayorga", "Lupe Mayorga", "Lupe Mayorga"], "answer_start": [274, 274, 274]}, "prediction_text": "Lupe Mayorga"}
{"answers": {"text": ["three", "three", "three"], "answer_start": [11, 11, 11]}, "prediction_text": "3"}
{"answers": {"text": ["Roeding Park", "Roeding Park", "Roeding Park"], "answer_start": [281, 281, 281]}, "prediction_text": "Roeding Park"}
{"answers": {"text": ["Kearney Park", "Kearney Park", "Kearney Park"], "answer_start": [387, 387, 387]}, "prediction_text": "Kearney Park"}
{"answers": {"text": ["Shinzen Japanese Gardens", "Shinzen Japanese Gardens"], "answer_start": [135, 135]}, "prediction_text": "Shinzen Japanese Gardens"}
{"answers": {"text": ["Kearney Park", "Kearney Park", "Kearney Park"], "answer_start": [387, 387, 387]}, "prediction_text": "Kearney Park"}
{"answers": {"text": ["Between the 1880s and World War II", "Between the 1880s and World War II", "Between the 1880s and World War II"], "answer_start": [0, 0, 0]}, "prediction_text": "1880s to World War II."}
{"answers": {"text": ["Fresno County Courthouse (demolished), the Fresno Carnegie Public Library", "Fresno County Courthouse (demolished), the Fresno Carnegie Public Library", "the original Fresno County Courthouse (demolished), the Fresno Carnegie Public Library (demolished)"], "answer_start": [210, 210, 197]}, "prediction_text": "Fresno County Courthouse, Fresno Carnegie Public Library, Fresno Water Tower, Bank of Italy Building, Pacific Southwest Building, San Joaquin Light & Power Building."}
{"answers": {"text": ["San Joaquin Light & Power Building", "San Joaquin Light & Power Building", "San Joaquin Light & Power Building"], "answer_start": [386, 386, 386]}, "prediction_text": "Hughes Hotel"}
{"answers": {"text": ["Hughes Hotel", "Hughes Hotel", "Hughes Hotel"], "answer_start": [466, 466, 466]}, "prediction_text": "Hughes Hotel"}
{"answers": {"text": ["1964", "1964", "1964"], "answer_start": [159, 159, 159]}, "prediction_text": "1964"}
{"answers": {"text": ["Fulton Mall", "Fulton Mall", "Fulton Mall"], "answer_start": [177, 177, 177]}, "prediction_text": "Fulton Mall"}
{"answers": {"text": ["Pierre-Auguste Renoir", "Pierre-Auguste Renoir", "Pierre-Auguste Renoir"], "answer_start": [431, 431, 431]}, "prediction_text": "Pierre-Auguste Renoir"}
{"answers": {"text": ["near their current locations", "near their current locations", "near their current locations"], "answer_start": [636, 636, 636]}, "prediction_text": "Near their current locations."}
{"answers": {"text": ["wide sidewalks", "wide sidewalks", "wide sidewalks"], "answer_start": [682, 682, 682]}, "prediction_text": "Wide sidewalks."}
{"answers": {"text": ["Fresno's far southeast side", "far southeast side", "far southeast side"], "answer_start": [36, 45, 45]}, "prediction_text": "Fresno's far southeast side."}
{"answers": {"text": ["Kings Canyon Avenue and Clovis Avenue", "Kings Canyon Avenue and Clovis Avenue", "Kings Canyon Avenue and Clovis Avenue"], "answer_start": [133, 133, 133]}, "prediction_text": "Kings Canyon Avenue and Clovis Avenue."}
{"answers": {"text": ["1950s through the 1970s", "1950s through the 1970s", "1950s through the 1970s"], "answer_start": [324, 324, 324]}, "prediction_text": "1950s"}
{"answers": {"text": ["Sunnyside", "Sunnyside", "Sunnyside"], "answer_start": [438, 190, 190]}, "prediction_text": "Sunnyside Country Club"}
{"answers": {"text": ["William P. Bell", "William P. Bell", "William P. Bell"], "answer_start": [504, 504, 504]}, "prediction_text": "William P. Bell"}
{"answers": {"text": ["Tower Theatre", "Tower Theatre", "Tower Theatre"], "answer_start": [85, 85, 85]}, "prediction_text": "The Tower Theatre"}
{"answers": {"text": ["1939", "1939", "1939"], "answer_start": [184, 184, 184]}, "prediction_text": "1939"}
{"answers": {"text": ["water tower", "water tower", "water tower"], "answer_start": [317, 317, 317]}, "prediction_text": "Well-known water tower."}
{"answers": {"text": ["Fresno Normal School", "Fresno Normal School", "Fresno Normal School"], "answer_start": [874, 874, 874]}, "prediction_text": "California State University at Fresno"}
{"answers": {"text": ["one-half mile", "one-half mile", "one-half mile"], "answer_start": [1024, 450, 1024]}, "prediction_text": "One-half mile."}
{"answers": {"text": ["late 1970s", "late 1970s", "late 1970s"], "answer_start": [296, 296, 296]}, "prediction_text": "late 1970s"}
{"answers": {"text": ["second and third run movies, along with classic films", "second and third run movies", "second and third run"], "answer_start": [334, 334, 334]}, "prediction_text": "Second and third run movies."}
{"answers": {"text": ["1978", "1978", "1978"], "answer_start": [463, 463, 463]}, "prediction_text": "1978"}
{"answers": {"text": ["Fresno", "Fresno", "Fresno"], "answer_start": [515, 515, 515]}, "prediction_text": "Fresno"}
{"answers": {"text": ["Evita and The Wiz", "Evita and The Wiz", "Evita and The Wiz"], "answer_start": [578, 578, 578]}, "prediction_text": "Evita and The Wiz."}
{"answers": {"text": ["live theater", "live", "live"], "answer_start": [39, 39, 39]}, "prediction_text": "Live theater."}
{"answers": {"text": ["all within a few hundred feet of each other", "few hundred feet", "within a few hundred feet of each other"], "answer_start": [170, 183, 174]}, "prediction_text": "Few hundred feet."}
{"answers": {"text": ["Tower District", "Tower District", "the Tower District"], "answer_start": [234, 234, 230]}, "prediction_text": "Olive Avenue"}
{"answers": {"text": ["Tower District", "Tower District", "Tower District"], "answer_start": [333, 333, 333]}, "prediction_text": "Tower District"}
{"answers": {"text": ["Tower District", "Tower District", "Tower District"], "answer_start": [433, 433, 433]}, "prediction_text": "Tower District"}
{"answers": {"text": ["early twentieth century homes", "early twentieth century homes", "early twentieth century"], "answer_start": [31, 31, 31]}, "prediction_text": "California Bungalow and American Craftsman style homes."}
{"answers": {"text": ["Storybook houses", "Storybook"], "answer_start": [325, 325]}, "prediction_text": "Hilliard, Taylor & Wheeler."}
{"answers": {"text": ["contrasts", "contrasts", "contrasts"], "answer_start": [452, 452, 452]}, "prediction_text": "Yes."}
{"answers": {"text": ["in recent decades", "recent decades", "recent decades"], "answer_start": [95, 98, 98]}, "prediction_text": "recent decades"}
{"answers": {"text": ["Huntington Boulevard", "Huntington Boulevard", "Huntington Boulevard"], "answer_start": [116, 116, 116]}, "prediction_text": "Huntington Boulevard"}
{"answers": {"text": ["William Stranahan", "William Stranahan", "William Stranahan"], "answer_start": [412, 412, 412]}, "prediction_text": "William Stranahan"}
{"answers": {"text": ["1914", "1914", "1914"], "answer_start": [1014, 1014, 1014]}, "prediction_text": "1914"}
{"answers": {"text": ["267", "267", "267"], "answer_start": [1268, 1268, 1268]}, "prediction_text": "267"}
{"answers": {"text": ["Fresno Traction Company", "Fresno Traction Company", "Fresno Traction Company"], "answer_start": [1331, 1331, 1331]}, "prediction_text": "Fresno Traction Company"}
{"answers": {"text": ["\"Southwest Fresno\"", "Southwest Fresno", "Southwest Fresno"], "answer_start": [45, 46, 46]}, "prediction_text": "Southwest Fresno"}
{"answers": {"text": ["southwest", "southwest", "southwest"], "answer_start": [135, 135, 135]}, "prediction_text": "Southwest."}
{"answers": {"text": ["African-American", "African-American", "African-American"], "answer_start": [420, 420, 420]}, "prediction_text": "African-American"}
{"answers": {"text": ["Hmong or Laotian", "Hmong or Laotian", "Hmong or Laotian"], "answer_start": [552, 552, 552]}, "prediction_text": "Hmong and Laotian."}
{"answers": {"text": ["\"West Side\"", "West Side", "West Side"], "answer_start": [4, 5, 5]}, "prediction_text": "West Side"}
{"answers": {"text": ["M. Theo Kearney", "M. Theo Kearney", "M. Theo Kearney"], "answer_start": [105, 105, 105]}, "prediction_text": "M. Theo Kearney"}
{"answers": {"text": ["tall palm trees", "tall palm trees", "palm"], "answer_start": [308, 308, 313]}, "prediction_text": "Palm trees."}
{"answers": {"text": ["Fresno Street and Thorne Ave", "Fresno Street and Thorne Ave", "Fresno Street and Thorne Ave"], "answer_start": [384, 384, 384]}, "prediction_text": "Fresno Street and Thorne Ave."}
{"answers": {"text": ["Brookhaven", "Brookhaven", "Brookhaven"], "answer_start": [519, 519, 519]}, "prediction_text": "Brookhaven"}
{"answers": {"text": ["The isolated subdivision", "Dogg Pound"], "answer_start": [701, 754]}, "prediction_text": "Brookhaven"}
{"answers": {"text": ["between the 1960s and 1990s", "between the 1960s and 1990s", "between the 1960s and 1990s"], "answer_start": [146, 146, 146]}, "prediction_text": "1960s and 1990s."}
{"answers": {"text": ["Fresno and B streets", "Fresno and B streets", "Fresno and B streets"], "answer_start": [510, 510, 510]}, "prediction_text": "Fresno Street"}
{"answers": {"text": ["Cargill Meat Solutions and Foster Farms", "Cargill Meat Solutions and Foster Farms", "Cargill Meat Solutions and Foster Farms"], "answer_start": [715, 715, 715]}, "prediction_text": "Cargill Meat Solutions and Foster Farms."}
{"answers": {"text": ["the West Side", "West Side", "on the West Side"], "answer_start": [962, 966, 959]}, "prediction_text": "West Side."}
{"answers": {"text": ["very little", "very little", "very little"], "answer_start": [1212, 1212, 1212]}, "prediction_text": "Very little."}
{"answers": {"text": ["Ralph Woodward", "Ralph Woodward", "Ralph Woodward"], "answer_start": [75, 75, 75]}, "prediction_text": "Ralph Woodward"}
{"answers": {"text": ["300 acres", "300", "300"], "answer_start": [454, 454, 454]}, "prediction_text": "235 acres."}
{"answers": {"text": ["2,500", "2,500", "2,500"], "answer_start": [759, 759, 759]}, "prediction_text": "2,500 seats."}
{"answers": {"text": ["22 miles", "22", "22"], "answer_start": [1114, 1114, 1114]}, "prediction_text": "22 miles (35 km)"}
{"answers": {"text": ["April through October", "April through October", "April through October, 6am to 10pm and November through March, 6am to 7pm"], "answer_start": [1447, 1447, 1447]}, "prediction_text": "April through October."}
{"answers": {"text": ["1946", "1946", "1946"], "answer_start": [10, 10, 10]}, "prediction_text": "1946"}
{"answers": {"text": ["William Smilie", "William Smilie", "William Smilie"], "answer_start": [370, 370, 370]}, "prediction_text": "William Smilie"}
{"answers": {"text": ["Sierra Sky Park", "Sierra Sky Park", "Sierra Sky Park"], "answer_start": [191, 191, 191]}, "prediction_text": "Sierra Sky Park"}
{"answers": {"text": ["automobiles", "automobiles", "automobiles"], "answer_start": [155, 155, 155]}, "prediction_text": "automobiles"}
{"answers": {"text": ["there are now numerous such communities across the United States", "numerous"], "answer_start": [273, 287]}, "prediction_text": "Yes."}
{"answers": {"text": ["hot and dry", "hot and dry", "hot and dry"], "answer_start": [83, 83, 83]}, "prediction_text": "Hot and dry."}
{"answers": {"text": ["July", "July", "July"], "answer_start": [368, 368, 368]}, "prediction_text": "July"}
{"answers": {"text": ["around 11.5 inches", "11.5 inches (292.1 mm)", "11.5 inches"], "answer_start": [960, 967, 967]}, "prediction_text": "11.5 inches (292.1 mm)"}
{"answers": {"text": ["northwest", "northwest", "northwest"], "answer_start": [1114, 1114, 1114]}, "prediction_text": "Northwest."}
{"answers": {"text": ["December, January and February", "December, January and February", "December, January and February"], "answer_start": [1206, 1206, 1206]}, "prediction_text": "December, January, and February."}
{"answers": {"text": ["115 \u00b0F", "115 \u00b0F (46.1 \u00b0C)", "115 \u00b0F"], "answer_start": [51, 51, 51]}, "prediction_text": "115 \u00b0F (46.1 \u00b0C)"}
{"answers": {"text": ["January 6, 1913", "January 6, 1913", "January 6, 1913"], "answer_start": [145, 145, 145]}, "prediction_text": "January 6, 1913."}
{"answers": {"text": ["1885", "July 1982 to June 1983"], "answer_start": [640, 487]}, "prediction_text": "1885"}
{"answers": {"text": ["2.2 inches", "2.2 inches (0.06 m)", "2.2 inches"], "answer_start": [848, 848, 848]}, "prediction_text": "2.2 inches (0.06 m) on January 21, 1962."}
{"answers": {"text": ["3.55 inches", "3.55 inches (90.2 mm)", "3.55 inches"], "answer_start": [679, 679, 679]}, "prediction_text": "3.55 inches (90.2 mm)"}
{"answers": {"text": ["494,665", "494,665", "494,665"], "answer_start": [71, 71, 71]}, "prediction_text": "494,665"}
{"answers": {"text": ["49.6%", "49.6%", "49.6%"], "answer_start": [194, 194, 194]}, "prediction_text": "30.0%"}
{"answers": {"text": ["8,525", "62,528", "8,525"], "answer_start": [240, 270, 240]}, "prediction_text": "8,525"}
{"answers": {"text": ["30.0%", "30.0%", "30.0%"], "answer_start": [738, 738, 738]}, "prediction_text": "30.0%"}
{"answers": {"text": ["4,404.5 people", "4,404.5", "4,404.5"], "answer_start": [107, 107, 107]}, "prediction_text": "1,700.6/km\u00b2"}
{"answers": {"text": ["68,511", "68,511", "68,511"], "answer_start": [40, 40, 40]}, "prediction_text": "68,511"}
{"answers": {"text": ["19.3%", "19.3%", "19.3%"], "answer_start": [178, 178, 178]}, "prediction_text": "69,284%"}
{"answers": {"text": ["1,388", "1,388", "1,388"], "answer_start": [360, 360, 360]}, "prediction_text": "30,547"}
{"answers": {"text": ["3.62", "3.62", "3.62"], "answer_start": [666, 666, 666]}, "prediction_text": "3.62"}
{"answers": {"text": ["3.07", "3.07", "3.07"], "answer_start": [577, 577, 577]}, "prediction_text": "3.07"}
{"answers": {"text": ["427,652", "427,652", "427,652"], "answer_start": [37, 37, 37]}, "prediction_text": "140,079"}
{"answers": {"text": ["149,025", "149,025", "149,025"], "answer_start": [199, 199, 199]}, "prediction_text": "149,025"}
{"answers": {"text": ["8.4%", "8.4%", "8.4%"], "answer_start": [327, 327, 327]}, "prediction_text": "8.4%"}
{"answers": {"text": ["a third", "about a third", "a third"], "answer_start": [400, 394, 400]}, "prediction_text": "0.1%"}
{"answers": {"text": ["4,097.9 people per square mile", "4,097.9 people per square mile", "4,097.9"], "answer_start": [142, 142, 142]}, "prediction_text": "1,582.2/km\u00b2"}
{"answers": {"text": ["To avoid interference with existing VHF television stations", "To avoid interference with existing VHF television stations", "To avoid interference with existing VHF television stations"], "answer_start": [0, 0, 0]}, "prediction_text": "To avoid interference with existing VHF television stations in the San Francisco Bay Area and those planned for Chico, Sacramento, Salinas, and Stockton."}
{"answers": {"text": ["KMJ-TV", "KMJ-TV", "KMJ-TV"], "answer_start": [320, 320, 320]}, "prediction_text": "KMJ-TV"}
{"answers": {"text": ["June 1, 1953", "June 1, 1953", "June 1, 1953"], "answer_start": [345, 345, 345]}, "prediction_text": "June 1, 1953"}
{"answers": {"text": ["NBC affiliate KSEE", "NBC affiliate KSEE", "KSEE"], "answer_start": [379, 379, 393]}, "prediction_text": "NBC affiliate KSEE"}
{"answers": {"text": ["KGPE", "KGPE", "KGPE"], "answer_start": [457, 457, 457]}, "prediction_text": "KGPE"}
{"answers": {"text": ["State Route 99", "State Route 99", "State Route 99"], "answer_start": [20, 20, 20]}, "prediction_text": "State Route 99"}
{"answers": {"text": ["the Sierra Freeway", "Sierra Freeway", "Sierra Freeway"], "answer_start": [159, 163, 163]}, "prediction_text": "Sierra Freeway"}
{"answers": {"text": ["State Route 41", "State Route 41"], "answer_start": [233, 233]}, "prediction_text": "Eisenhower Freeway"}
{"answers": {"text": ["west", "west", "west"], "answer_start": [422, 422, 422]}, "prediction_text": "Kings Canyon National Park"}
{"answers": {"text": ["Fresno", "Fresno", "Fresno"], "answer_start": [0, 0, 0]}, "prediction_text": "Fresno"}
{"answers": {"text": ["1950s", "in the 1950s"], "answer_start": [132, 125]}, "prediction_text": "1950s"}
{"answers": {"text": ["99", "State Route 99", "State Route 99"], "answer_start": [411, 330, 330]}, "prediction_text": "SR 99"}
{"answers": {"text": ["rapidly raising population and traffic in cities along SR 99", "rapidly raising population and traffic in cities along SR 99, as well as the desirability of Federal funding", "rapidly raising population and traffic in cities along SR 99, as well as the desirability of Federal funding"], "answer_start": [353, 353, 353]}, "prediction_text": "population, traffic, desirability of Federal funding."}
{"answers": {"text": ["Amtrak San Joaquins", "Amtrak San Joaquins", "Amtrak San Joaquins"], "answer_start": [38, 38, 38]}, "prediction_text": "Amtrak San Joaquins"}
{"answers": {"text": ["Downtown Fresno", "Downtown Fresno", "Downtown Fresno"], "answer_start": [153, 153, 153]}, "prediction_text": "Downtown Fresno"}
{"answers": {"text": ["Burlington Northern Santa Fe Railway and Union Pacific Railroad", "Burlington Northern Santa Fe Railway and Union Pacific Railroad", "Burlington Northern Santa Fe Railway and Union Pacific Railroad"], "answer_start": [212, 212, 212]}, "prediction_text": "San Joaquin Valley Railroad and Burlington Northern Santa Fe Railway."}
{"answers": {"text": ["San Joaquin Valley Railroad", "San Joaquin Valley Railroad", "San Joaquin Valley Railroad"], "answer_start": [362, 362, 362]}, "prediction_text": "Burlington Northern Santa Fe Railway"}
{"answers": {"text": ["Fresno", "Fresno", "Fresno"], "answer_start": [492, 492, 492]}, "prediction_text": "Fresno"}
{"answers": {"text": ["Paul Baran developed the concept Distributed Adaptive Message Block Switching", "the concept Distributed Adaptive Message Block Switching", "Distributed Adaptive Message Block Switching"], "answer_start": [56, 77, 89]}, "prediction_text": "Distributed Adaptive Message Block Switching"}
{"answers": {"text": ["provide a fault-tolerant, efficient routing method for telecommunication messages", "provide a fault-tolerant, efficient routing method for telecommunication messages", "fault-tolerant, efficient routing method"], "answer_start": [151, 151, 161]}, "prediction_text": "Provided fault-tolerant, efficient routing for telecommunication messages."}
{"answers": {"text": ["This concept contrasted and contradicted the theretofore established principles of pre-allocation of network bandwidth", "theretofore established principles of pre-allocation of network bandwidth", "principles of pre-allocation of network bandwidth"], "answer_start": [328, 373, 397]}, "prediction_text": "pre-allocation of network bandwidth."}
{"answers": {"text": ["Davies is credited with coining the modern name packet switching and inspiring numerous packet switching networks in Europe", "coining the modern name packet switching and inspiring numerous packet switching networks", "coining the modern name packet switching"], "answer_start": [715, 739, 739]}, "prediction_text": "Coining the modern name packet switching and inspiring numerous packet switching networks in Europe."}
{"answers": {"text": ["the concept Distributed Adaptive Message Block Switching", "the concept Distributed Adaptive Message Block Switching", "Distributed Adaptive Message Block Switching"], "answer_start": [77, 77, 89]}, "prediction_text": "Distributed Adaptive Message Block Switching"}
{"answers": {"text": ["to provide a fault-tolerant, efficient routing method for telecommunication messages", "provide a fault-tolerant, efficient routing method for telecommunication messages"], "answer_start": [148, 151]}, "prediction_text": "Provide fault-tolerant, efficient routing method for telecommunication messages."}
{"answers": {"text": ["Davies is credited with coining the modern name packet switching and inspiring numerous packet switching networks in Europe", "Donald Davies", "Donald Davies"], "answer_start": [715, 623, 623]}, "prediction_text": "Donald Davies"}
{"answers": {"text": ["circuit switching", "circuit switching", "circuit switching"], "answer_start": [71, 71, 71]}, "prediction_text": "circuit switching"}
{"answers": {"text": ["circuit switching is characterized by a fee per unit of connection time", "a method which pre-allocates dedicated network bandwidth specifically for each communication session", "fee per unit of connection time"], "answer_start": [323, 90, 363]}, "prediction_text": "Constant bit rate and latency between nodes."}
{"answers": {"text": ["by a fee per unit of information transmitted", "a fee per unit of information transmitted", "fee per unit of information transmitted"], "answer_start": [474, 477, 479]}, "prediction_text": "Fee per unit of connection time."}
{"answers": {"text": ["circuit switching", "circuit switching", "circuit switching"], "answer_start": [71, 71, 71]}, "prediction_text": "Circuit switching."}
{"answers": {"text": ["a method which pre-allocates dedicated network bandwidth", "pre-allocates dedicated network bandwidth specifically for each communication session", "pre-allocates"], "answer_start": [90, 105, 105]}, "prediction_text": "Dedicated network bandwidth."}
{"answers": {"text": ["by a fee per unit of connection time, even when no data is transferred", "a fee per unit of connection time", "fee per unit of connection time"], "answer_start": [358, 361, 363]}, "prediction_text": "Fee per unit of connection time."}
{"answers": {"text": ["by a fee per unit of information transmitted, such as characters, packets, or messages", "a fee per unit of information transmitted", "fee per unit of information transmitted"], "answer_start": [474, 477, 479]}, "prediction_text": "Fee per unit of connection time."}
{"answers": {"text": ["with or without intermediate forwarding nodes", "with or without intermediate forwarding nodes", "with or without intermediate forwarding nodes"], "answer_start": [45, 45, 45]}, "prediction_text": "Through intermediate network nodes."}
{"answers": {"text": ["asynchronously using first-in, first-out buffering, but may be forwarded according to some scheduling discipline for fair queuing", "asynchronously using first-in, first-out buffering", "intermediate network nodes"], "answer_start": [182, 182, 155]}, "prediction_text": "asynchronously using first-in, first-out buffering."}
{"answers": {"text": ["the packets may be delivered according to a multiple access scheme", "according to a multiple access scheme", "multiple access scheme"], "answer_start": [497, 526, 541]}, "prediction_text": "According to a multiple access scheme."}
{"answers": {"text": ["with or without intermediate forwarding nodes", "with or without intermediate forwarding nodes", "with or without intermediate forwarding nodes"], "answer_start": [45, 45, 45]}, "prediction_text": "Through intermediate network nodes."}
{"answers": {"text": ["by intermediate network nodes asynchronously using first-in, first-out buffering, but may be forwarded according to some scheduling discipline for fair queuing", "intermediate network nodes asynchronously using first-in, first-out buffering", "asynchronously"], "answer_start": [152, 155, 182]}, "prediction_text": "asynchronously using first-in, first-out buffering."}
{"answers": {"text": ["the packets may be delivered according to a multiple access scheme", "according to a multiple access scheme", "multiple access scheme"], "answer_start": [497, 526, 541]}, "prediction_text": "According to a multiple access scheme."}
{"answers": {"text": ["the concept of distributed adaptive message block switching", "the concept of distributed adaptive message block switching", "concept of distributed adaptive message block switching"], "answer_start": [16, 16, 20]}, "prediction_text": "Distributed adaptive message block switching."}
{"answers": {"text": ["survivable communications networks", "the concept of distributed adaptive message block switching", "survivable communications networks"], "answer_start": [146, 16, 146]}, "prediction_text": "RAND report P-2626"}
{"answers": {"text": ["use of a decentralized network with multiple paths between any two points, dividing user messages into message blocks", "ideas"], "answer_start": [493, 486]}, "prediction_text": "Use of a decentralized network with multiple paths between any two points, dividing user messages into message blocks, later called packets, and delivery of these messages by store and forward switching."}
{"answers": {"text": ["delivery of these messages by store and forward switching", "decentralized network with multiple paths between any two points", "store and forward switching"], "answer_start": [638, 502, 668]}, "prediction_text": "By store and forward switching."}
{"answers": {"text": ["a general architecture for a large-scale, distributed, survivable communications network", "a general architecture for a large-scale, distributed, survivable communications network", "briefing B-265"], "answer_start": [366, 366, 240]}, "prediction_text": "RAND report RM 3420."}
{"answers": {"text": ["by store and forward switching", "packets", "store and forward switching"], "answer_start": [665, 625, 668]}, "prediction_text": "packets"}
{"answers": {"text": ["distributed adaptive message block switching", "distributed adaptive message block switching", "distributed adaptive message block switching"], "answer_start": [31, 31, 31]}, "prediction_text": "Distributed adaptive message block switching."}
{"answers": {"text": ["use of a decentralized network with multiple paths between any two points, dividing user messages into message blocks, later called packets", "large-scale, distributed, survivable communications network", "multiple paths between any two points"], "answer_start": [493, 395, 529]}, "prediction_text": "user messages, message blocks, packets, and delivery."}
{"answers": {"text": ["independently developed the same message routing methodology as developed by Baran", "the same message routing methodology as developed by Baran", "message routing methodology"], "answer_start": [73, 97, 106]}, "prediction_text": "Packet switching methodology."}
{"answers": {"text": ["packet switching", "packet switching", "packet switching"], "answer_start": [170, 170, 170]}, "prediction_text": "packet switching"}
{"answers": {"text": ["proposed to build a nationwide network in the UK", "a nationwide network", "nationwide network"], "answer_start": [229, 247, 249]}, "prediction_text": "A nationwide network in the UK."}
{"answers": {"text": ["use in the ARPANET", "ARPANET", "ARPANET"], "answer_start": [548, 559, 559]}, "prediction_text": "ARPANET"}
{"answers": {"text": ["Donald Davies", "Donald Davies", "Donald Davies"], "answer_start": [18, 18, 18]}, "prediction_text": "Donald Davies"}
{"answers": {"text": ["packet switching", "packet switching", "packet switching"], "answer_start": [170, 170, 170]}, "prediction_text": "packet switching"}
{"answers": {"text": ["suggested it for use in the ARPANET", "use in the ARPANET", "use in the ARPANET"], "answer_start": [531, 548, 548]}, "prediction_text": "Lawrence Roberts"}
{"answers": {"text": ["each packet includes complete addressing information", "complete addressing information", "complete addressing information"], "answer_start": [23, 44, 44]}, "prediction_text": "complete addressing information."}
{"answers": {"text": ["individually, sometimes resulting in different paths and out-of-order delivery", "individually", "individually"], "answer_start": [100, 100, 100]}, "prediction_text": "individually."}
{"answers": {"text": ["Each packet is labeled with a destination address, source address, and port numbers. It may also be labeled with the sequence number of the packet", "destination address, source address, and port numbers", "destination address, source address, and port numbers"], "answer_start": [180, 210, 210]}, "prediction_text": "Destination address, source address, and port numbers."}
{"answers": {"text": ["the original message/data is reassembled in the correct order, based on the packet sequence number", "the original message/data is reassembled in the correct order", "reassembled"], "answer_start": [881, 881, 910]}, "prediction_text": "Reassembled in the correct order."}
{"answers": {"text": ["The packet header can be small, as it only needs to contain this code and any information, such as length, timestamp, or sequence number", "The packet header can be small", "small"], "answer_start": [816, 816, 841]}, "prediction_text": "No."}
{"answers": {"text": ["Routing a packet requires the node to look up the connection id in a table", "the connection id in a table", "connection id"], "answer_start": [740, 786, 790]}, "prediction_text": "The node needs to look up the connection id in a table."}
{"answers": {"text": ["a connection identifier rather than address information and are negotiated between endpoints so that they are delivered in order and with error checking", "negotiated between endpoints"], "answer_start": [177, 241]}, "prediction_text": "Connection identifier"}
{"answers": {"text": ["a setup phase in each involved node before any packet is transferred to establish the parameters of communication", "a setup phase in each involved node", "setup phase"], "answer_start": [42, 42, 44]}, "prediction_text": "A setup phase in each involved node before any packet is transferred to establish the parameters of communication."}
{"answers": {"text": ["connection-oriented operations. But X.25 does it at the network layer of the OSI Model. Frame Relay does it at level two, the data link layer", "a handshake between the communicating parties before any user packets are transmitted", "connection-oriented operations"], "answer_start": [34, 253, 34]}, "prediction_text": "handshakes."}
{"answers": {"text": ["supplanted by the Internet Protocol (IP) at the network layer, and the Asynchronous Transfer Mode (ATM) and or versions of Multi-Protocol Label Switching", "Internet Protocol (IP)", "Internet Protocol"], "answer_start": [2652, 2670, 2670]}, "prediction_text": "Internet Protocol (IP) at the network layer."}
{"answers": {"text": ["Frame Relay was used to interconnect LANs across wide area networks. However, X.25 and well as Frame Relay have been supplanted", "provide connection-oriented operations", "standardized interface"], "answer_start": [2535, 26, 1215]}, "prediction_text": "Interconnecting LANs across wide area networks."}
{"answers": {"text": ["A typical configuration is to run IP over ATM or a version of MPLS", "run IP over ATM or a version of MPLS", "run IP over ATM or a version of MPLS"], "answer_start": [2827, 2857, 2857]}, "prediction_text": "IP over ATM or a version of MPLS."}
{"answers": {"text": ["1969", "1969", "1969"], "answer_start": [43, 43, 43]}, "prediction_text": "1969"}
{"answers": {"text": ["Two fundamental differences involved the division of functions and tasks between the hosts at the edge of the network and the network core", "the division of functions and tasks between the hosts at the edge of the network and the network core.", "division of functions and tasks between the hosts at the edge of the network and the network core"], "answer_start": [154, 191, 195]}, "prediction_text": "The hosts have the responsibility to ensure orderly delivery of packets."}
{"answers": {"text": ["In the virtual call system, the network guarantees sequenced delivery of data to the host", "sequenced delivery of data to the host", "sequenced delivery of data"], "answer_start": [462, 513, 513]}, "prediction_text": "Sequenced delivery of data to the host."}
{"answers": {"text": ["User Datagram Protocol", "the datagram model", "UDP"], "answer_start": [395, 626, 419]}, "prediction_text": "Datagram protocol."}
{"answers": {"text": ["a proprietary suite of networking protocols developed by Apple Inc. in 1985", "a proprietary suite of networking protocols developed by Apple Inc", "proprietary suite of networking protocols"], "answer_start": [14, 14, 16]}, "prediction_text": "AppleTalk"}
{"answers": {"text": ["that allowed local area networks to be established ad hoc without the requirement for a centralized router or server", "allowed local area networks to be established ad hoc without the requirement for a centralized router or server", "allowed local area networks to be established ad hoc without the requirement for a centralized router or server"], "answer_start": [226, 231, 231]}, "prediction_text": "Local area networks."}
{"answers": {"text": ["automatically assigned addresses, updated the distributed namespace, and configured any required inter-network routing", "addresses", "addresses"], "answer_start": [365, 388, 388]}, "prediction_text": "addresses"}
{"answers": {"text": ["a plug-n-play system", "plug-n-play", "plug-n-play"], "answer_start": [492, 494, 494]}, "prediction_text": "AppleTalk"}
{"answers": {"text": ["CYCLADES packet switching network", "CYCLADES", "CYCLADES"], "answer_start": [4, 4, 4]}, "prediction_text": "CYCLADES"}
{"answers": {"text": ["to make the hosts responsible for reliable delivery of data, rather than the network itself", "the first network to make the hosts responsible for reliable delivery of data", "hosts responsible for reliable delivery of data"], "answer_start": [272, 254, 284]}, "prediction_text": "The first for this network was the development of reliable delivery of data."}
{"answers": {"text": ["using unreliable datagrams and associated end-to-end protocol mechanisms", "using unreliable datagrams and associated end-to-end protocol mechanisms", "unreliable datagrams and associated end-to-end protocol mechanisms"], "answer_start": [365, 365, 371]}, "prediction_text": "unreliable datagrams and associated end-to-end protocol mechanisms."}
{"answers": {"text": ["later ARPANET architecture", "ARPANET", "ARPANET"], "answer_start": [475, 481, 481]}, "prediction_text": "ARPANET architecture."}
{"answers": {"text": ["a suite of network protocols created by Digital Equipment Corporation", "a suite of network protocols created by Digital Equipment Corporation", "suite of network protocols created by Digital Equipment Corporation"], "answer_start": [10, 10, 12]}, "prediction_text": "DECnet is a suite of network protocols created by Digital Equipment Corporation."}
{"answers": {"text": ["connect two PDP-11 minicomputers", "connect two PDP-11 minicomputers", "connect two PDP-11 minicomputers"], "answer_start": [121, 121, 121]}, "prediction_text": "Connect two PDP-11 minicomputers."}
{"answers": {"text": ["Initially built with three layers, it later (1982) evolved into a seven-layer OSI-compliant networking protocol", "seven", "seven"], "answer_start": [289, 355, 355]}, "prediction_text": "7 layers."}
{"answers": {"text": ["were open standards with published specifications, and several implementations were developed outside DEC, including one for Linux", "Linux", "open standards"], "answer_start": [517, 642, 522]}, "prediction_text": "OSI-compliant networking protocol."}
{"answers": {"text": ["a data network based on this voice-phone network was designed to connect GE's four computer sales and service centers", "a data network", "a data network"], "answer_start": [47, 47, 47]}, "prediction_text": "High-level marketing manager"}
{"answers": {"text": ["the world's first commercial online service", "the world's first commercial online service", "world's first commercial online service"], "answer_start": [268, 268, 272]}, "prediction_text": "Schenectady, Phoenix, Chicago, and Phoenix."}
{"answers": {"text": ["They lost money from the beginning, and Sinback, a high-level marketing manager, was given the job of turning the business around", "They lost money", "lost money"], "answer_start": [430, 430, 435]}, "prediction_text": "No."}
{"answers": {"text": ["that a time-sharing system, based on Kemney's work at Dartmouth\u2014which used a computer on loan from GE\u2014could be profitable", "a time-sharing system, based on Kemney's work at Dartmouth\u2014which used a computer on loan from GE\u2014could be profitable", "could be profitable"], "answer_start": [572, 577, 674]}, "prediction_text": "To turn the business around."}
{"answers": {"text": ["as a means to help the state's educational and economic development", "to explore computer networking between three of Michigan's public universities", "explore computer networking"], "answer_start": [266, 187, 190]}, "prediction_text": "To explore computer networking between three of Michigan's public universities."}
{"answers": {"text": ["an interactive host to host connection was made between the IBM mainframe computer systems at the University of Michigan in Ann Arbor and Wayne State", "the CDC mainframe at Michigan State University in East Lansing", "1972 connections"], "answer_start": [499, 703, 683]}, "prediction_text": "CDC mainframe computers at Michigan State University in East Lansing."}
{"answers": {"text": ["Ethernet attached hosts, and eventually TCP/IP and additional public universities in Michigan join the network", "the network was enhanced", "TCP/IP"], "answer_start": [1166, 867, 1206]}, "prediction_text": "The mid-1980s."}
{"answers": {"text": ["the first FCC-licensed public data network in the United States", "the first FCC-licensed public data network in the United States", "first FCC-licensed public data network"], "answer_start": [12, 12, 16]}, "prediction_text": "Telenet was the first FCC-licensed public data network in the United States. It was founded by former ARPA IPTO director Larry Roberts as a means of making ARPANET technology public. He had tried to interest AT&T in buying the technology, but the monopoly's reaction was that this was incompatible with their future. Bolt, Beranack and Newman (BBN) provided the financing. It initially used ARPANET technology but changed the host interface to X.25 and the terminal interface to X.29. Telenet designed these protocols and helped standardize them in the CCITT. Telenet was incorporated in 1973 and started operations in 1975. It went public in 1979 and was then"}
{"answers": {"text": ["Larry Roberts", "ARPA IPTO director Larry Roberts", "Larry Roberts"], "answer_start": [121, 102, 121]}, "prediction_text": "Larry Roberts"}
{"answers": {"text": ["making ARPANET technology public", "a means of making ARPANET technology public", "making ARPANET technology public"], "answer_start": [149, 138, 149]}, "prediction_text": "To make ARPANET technology public."}
{"answers": {"text": ["host interface to X.25 and the terminal interface to X.29", "X.25", "ARPANET"], "answer_start": [426, 444, 391]}, "prediction_text": "X.25 and X.29."}
{"answers": {"text": ["Telenet was incorporated in 1973 and started operations in 1975. It went public in 1979 and was then sold to GTE", "GTE", "GTE"], "answer_start": [560, 669, 669]}, "prediction_text": "GTE"}
{"answers": {"text": ["an international data communications network headquartered in San Jose, CA", "an international data communications network", "international data communications network"], "answer_start": [11, 11, 14]}, "prediction_text": "Tymnet"}
{"answers": {"text": ["connect host computers (servers)at thousands of large companies, educational institutions, and government agencies", "host computers (servers)at thousands of large companies, educational institutions, and government agencies", "host computers"], "answer_start": [193, 201, 201]}, "prediction_text": "hundreds of large companies, educational institutions, and government agencies."}
{"answers": {"text": ["connected via dial-up connections or dedicated async connections", "dial-up connections or dedicated async connections", "dial-up"], "answer_start": [325, 339, 339]}, "prediction_text": "via dial-up connections or dedicated async connections."}
{"answers": {"text": ["government agencies and large companies (mostly banks and airlines) to build their own dedicated networks", "build their own dedicated networks", "build their own dedicated networks"], "answer_start": [513, 584, 584]}, "prediction_text": "Build their own dedicated networks."}
{"answers": {"text": ["private networks were often connected via gateways to the public network to reach locations not on the private network", "reach locations not on the private network", "reach locations not on the private network"], "answer_start": [624, 700, 700]}, "prediction_text": "Build their own dedicated networks."}
{"answers": {"text": ["There were two kinds of X.25 networks. Some such as DATAPAC and TRANSPAC", "two", "two"], "answer_start": [0, 11, 11]}, "prediction_text": "2"}
{"answers": {"text": ["DATAPAC was developed by Bell Northern Research", "Bell Northern Research", "Bell Northern Research"], "answer_start": [273, 298, 298]}, "prediction_text": "Bell Northern Research"}
{"answers": {"text": ["A user or host could call a host on a foreign network by including the DNIC of the remote network as part of the destination address", "the interconnection of national X.25 networks", "interconnection of national X.25 networks"], "answer_start": [611, 564, 568]}, "prediction_text": "The DNIC of the remote network."}
{"answers": {"text": ["AUSTPAC was an Australian public X.25 network operated by Telstra", "an Australian public X.25 network operated by Telstra", "an Australian public X.25 network"], "answer_start": [0, 12, 12]}, "prediction_text": "AUSTPAC was an Australian public X.25 network operated by Telstra."}
{"answers": {"text": ["supporting applications such as on-line betting, financial applications", "applications such as on-line betting, financial applications", "applications such as on-line betting, financial applications"], "answer_start": [183, 194, 194]}, "prediction_text": "Applications such as on-line betting, financial applications \u2014 the Australian Tax Office made use of AUSTPAC."}
{"answers": {"text": ["Access can be via a dial-up terminal to a PAD, or, by linking a permanent X.25 node to the network", "dial-up terminal to a PAD, or, by linking a permanent X.25 node", "dial-up terminal"], "answer_start": [445, 465, 465]}, "prediction_text": "via a dial-up terminal to a PAD."}
{"answers": {"text": ["was the public switched data network operated by the Dutch PTT Telecom", "the public switched data network operated by the Dutch PTT Telecom", "public switched data network"], "answer_start": [10, 14, 18]}, "prediction_text": "Datanet 1 was the public switched data network operated by the Dutch PTT Telecom (now known as KPN). Strictly speaking DATANET 1 only referred to the network and the connected users via leased lines (using the X.121 DNIC 2041), the name also referred to the public PAD service Telepad (using the DNIC 2049). And because the main Videotex service used the network and modified PAD devices as infrastructure the name Datanet 1 was used for these services as well. Although this use of the name was incorrect all these services were managed by the same people within one department of KPN contributed to the confusion."}
{"answers": {"text": ["Datanet 1 only referred to the network and the connected users via leased lines", "the network", "the network and the connected users"], "answer_start": [119, 146, 146]}, "prediction_text": "KPN"}
{"answers": {"text": ["public PAD service Telepad (using the DNIC 2049", "the public PAD service Telepad", "public PAD service Telepad"], "answer_start": [258, 254, 258]}, "prediction_text": "KPN"}
{"answers": {"text": ["use of the name was incorrect all these services were managed by the same people within one department of KPN contributed to the confusion", "the name was incorrect", "Dutch PTT Telecom"], "answer_start": [476, 483, 63]}, "prediction_text": "Yes."}
{"answers": {"text": ["The Computer Science Network", "a computer network funded by the U.S. National Science Foundation (NSF)", "Computer Science Network"], "answer_start": [0, 41, 4]}, "prediction_text": "Computer Science Network (CSNET)"}
{"answers": {"text": ["to extend networking benefits, for computer science departments at academic and research institutions that could not be directly connected to ARPANET", "to extend networking benefits", "extend networking benefits"], "answer_start": [159, 159, 162]}, "prediction_text": "Spread awareness of, and access to, national networking."}
{"answers": {"text": ["role in spreading awareness of, and access to, national networking and was a major milestone on the path to development of the global Internet", "not be directly connected to ARPANET", "not be directly connected to ARPANET"], "answer_start": [379, 272, 272]}, "prediction_text": "limited to ARPANET."}
{"answers": {"text": ["a not-for-profit United States computer networking consortium led by members from the research and education communities, industry, and government", "a not-for-profit United States computer networking consortium led by members from the research and education communities, industry, and government", "not-for-profit United States computer networking consortium"], "answer_start": [13, 13, 15]}, "prediction_text": "Internet2 Network"}
{"answers": {"text": ["The Internet2 community, in partnership with Qwest", "Qwest", "Qwest"], "answer_start": [161, 206, 206]}, "prediction_text": "Level 3 Communications"}
{"answers": {"text": ["Abilene", "Abilene", "Abilene"], "answer_start": [255, 255, 255]}, "prediction_text": "Abilene"}
{"answers": {"text": ["a partnership with Level 3 Communications to launch a brand new nationwide network", "Level 3 Communications", "Qwest"], "answer_start": [368, 387, 206]}, "prediction_text": "Level 3 Communications"}
{"answers": {"text": ["Internet2 officially retired Abilene and now refers to its new, higher capacity network as the Internet2 Network", "Internet2 Network", "Internet2 Network"], "answer_start": [522, 617, 617]}, "prediction_text": "Internet2 Network"}
{"answers": {"text": ["The National Science Foundation Network", "National Science Foundation Network", "National Science Foundation Network"], "answer_start": [0, 4, 4]}, "prediction_text": "National Science Foundation Network (NSFNET)"}
{"answers": {"text": ["advanced research and education networking in the United States", "advanced research and education networking", "advanced research and education networking"], "answer_start": [177, 177, 177]}, "prediction_text": "Advanced research and education networking."}
{"answers": {"text": ["it developed into a major part of the Internet backbone", "a major part of the Internet backbone", "major part of the Internet backbone"], "answer_start": [615, 633, 635]}, "prediction_text": "Internet backbone."}
{"answers": {"text": ["The Very high-speed Backbone Network Service", "Very high-speed Backbone Network Service", "Very high-speed Backbone Network Service"], "answer_start": [0, 4, 4]}, "prediction_text": "Very High-speed Backbone Network Service (vBNS)"}
{"answers": {"text": ["provide high-speed interconnection between NSF-sponsored supercomputing centers and select access points in the United States", "provide high-speed interconnection between NSF-sponsored supercomputing centers and select access points in the United States", "high-speed interconnection"], "answer_start": [147, 147, 155]}, "prediction_text": "Grew to connect more than 100 universities and research and engineering institutions via 12 national points of presence with DS-3 (45 Mbit/s), OC-3c (155 Mbit/s), and OC-12c (622 Mbit/s) links on an all OC-12c backbone."}
{"answers": {"text": ["The network was engineered and operated by MCI Telecommunications under a cooperative agreement with the NSF", "MCI Telecommunications", "National Science Foundation"], "answer_start": [274, 317, 92]}, "prediction_text": "MCI Telecommunications"}
{"answers": {"text": ["By 1998, the vBNS had grown to connect more than 100 universities and research and engineering institutions via 12 national points of presence with DS-3", "more than 100 universities", "100"], "answer_start": [384, 423, 433]}, "prediction_text": "100 universities."}
{"answers": {"text": ["vBNS installed one of the first ever production OC-48c (2.5 Gbit/s) IP links in February 1999 and went on to upgrade the entire backbone to OC-48c", "one of the first ever production OC-48c (2.5 Gbit/s) IP links", "OC-48c"], "answer_start": [677, 692, 725]}, "prediction_text": "One of the first ever production OC-48c (2.5 Gbit/s) IP links."}
{"answers": {"text": ["the arid plains of Central Asia", "Central Asia", "Central Asia"], "answer_start": [49, 68, 68]}, "prediction_text": "Central Asia"}
{"answers": {"text": ["merchant ships.", "merchant ships", "Silk Road"], "answer_start": [270, 270, 116]}, "prediction_text": "By carrying Oriental rat fleas."}
{"answers": {"text": ["30\u201360% of Europe's total population", "30\u201360% of Europe's total population", "30\u201360%"], "answer_start": [381, 381, 381]}, "prediction_text": "30-60%"}
{"answers": {"text": ["the 17th century", "17th century", "17th century"], "answer_start": [618, 622, 622]}, "prediction_text": "17th century."}
{"answers": {"text": ["until the 19th century", "until the 19th century", "19th century"], "answer_start": [679, 679, 689]}, "prediction_text": "Until the 19th century."}
{"answers": {"text": ["commonly present", "commonly present", "commonly present"], "answer_start": [60, 60, 60]}, "prediction_text": "Commonly present."}
{"answers": {"text": ["dating to 1338\u201339", "1338\u201339", "1338\u201339"], "answer_start": [258, 268, 268]}, "prediction_text": "1338-39."}
{"answers": {"text": ["China", "China", "China"], "answer_start": [594, 594, 594]}, "prediction_text": "China."}
{"answers": {"text": ["1331", "1331", "1331"], "answer_start": [867, 867, 867]}, "prediction_text": "1331"}
{"answers": {"text": ["an estimated 25 million", "25 million", "25 million"], "answer_start": [962, 975, 975]}, "prediction_text": "25 million."}
{"answers": {"text": ["Genoese traders", "Genoese traders", "Genoese traders"], "answer_start": [53, 53, 53]}, "prediction_text": "Genoese traders"}
{"answers": {"text": ["Jani Beg", "Jani Beg", "Jani Beg"], "answer_start": [179, 179, 179]}, "prediction_text": "Jani Beg"}
{"answers": {"text": ["infected corpses", "infected corpses", "infected corpses"], "answer_start": [244, 244, 244]}, "prediction_text": "corpses"}
{"answers": {"text": ["Sicily", "Sicily and the south of Europe", "Sicily"], "answer_start": [374, 374, 374]}, "prediction_text": "Sicily and the south of Europe."}
{"answers": {"text": ["war, famine, and weather", "war, famine, and weather", "war, famine, and weather"], "answer_start": [527, 527, 527]}, "prediction_text": "War, famine, and weather."}
{"answers": {"text": ["northwest across Europe", "northwest", "northwest"], "answer_start": [31, 31, 31]}, "prediction_text": "East."}
{"answers": {"text": ["northwestern Russia", "Russia", "Russia"], "answer_start": [333, 346, 346]}, "prediction_text": "Russia"}
{"answers": {"text": ["parts of Europe that had smaller trade relations with their neighbours", "smaller trade relations with their neighbours", "smaller trade relations with their neighbours"], "answer_start": [401, 426, 426]}, "prediction_text": "Kingdom of Poland, Basque Country, isolated parts of Belgium, isolated alpine villages."}
{"answers": {"text": ["Germany and Scandinavia", "Germany and Scandinavia", "Germany and Scandinavia"], "answer_start": [151, 151, 151]}, "prediction_text": "Germany and Scandinavia."}
{"answers": {"text": ["1349", "1349", "1349"], "answer_start": [225, 225, 225]}, "prediction_text": "1349"}
{"answers": {"text": ["serious depopulation and permanent change in both economic and social structures", "depopulation and permanent change in both economic and social structures", "depopulation and permanent change in both economic and social structures"], "answer_start": [87, 95, 95]}, "prediction_text": "Depopulation and permanent change in both economic and social structures."}
{"answers": {"text": ["autumn 1347", "1347", "1347"], "answer_start": [262, 269, 269]}, "prediction_text": "1347"}
{"answers": {"text": ["y through the port's trade with Constantinople, and ports on the Black Sea", "port's trade", "trade with Constantinople"], "answer_start": [322, 336, 343]}, "prediction_text": "Through the port's trade with Constantinople."}
{"answers": {"text": ["The city's residents fled to the north", "the north", "north"], "answer_start": [641, 670, 674]}, "prediction_text": "North."}
{"answers": {"text": ["Gasquet", "Gasquet", "Gasquet"], "answer_start": [0, 0, 0]}, "prediction_text": "J.I. Pontanus"}
{"answers": {"text": ["atra mors", "atra mors", "atra mors"], "answer_start": [43, 43, 43]}, "prediction_text": "atra mors (Black Death)"}
{"answers": {"text": ["J.I. Pontanus", "J.I. Pontanus", "J.I. Pontanus"], "answer_start": [167, 167, 167]}, "prediction_text": "J.I. Pontanus"}
{"answers": {"text": ["1823", "1823", "1823"], "answer_start": [456, 456, 456]}, "prediction_text": "1823"}
{"answers": {"text": ["Scandinavia", "Scandinavia", "Scandinavia"], "answer_start": [318, 318, 318]}, "prediction_text": "Scandinavia."}
{"answers": {"text": ["the heavens", "the heavens", "the heavens"], "answer_start": [184, 184, 184]}, "prediction_text": "The heavens."}
{"answers": {"text": ["the king of France", "king of France", "king of France"], "answer_start": [153, 157, 157]}, "prediction_text": "King of France"}
{"answers": {"text": ["That the plague was caused by bad air", "bad air", "great pestilence in the air"], "answer_start": [422, 452, 266]}, "prediction_text": "Miasma theory."}
{"answers": {"text": ["Miasma theory", "Miasma theory.", "Miasma theory"], "answer_start": [528, 528, 528]}, "prediction_text": "Miasma theory"}
{"answers": {"text": ["Yersinia pestis", "Yersinia pestis", "Yersinia pestis"], "answer_start": [100, 100, 100]}, "prediction_text": "Yersinia pestis"}
{"answers": {"text": ["Hong Kong in 1894", "Hong Kong", "Hong Kong in 1894"], "answer_start": [334, 334, 334]}, "prediction_text": "Hong Kong in 1894."}
{"answers": {"text": ["French-Swiss bacteriologist Alexandre Yersin", "Alexandre Yersin", "Alexandre Yersin"], "answer_start": [372, 400, 400]}, "prediction_text": "Alexandre Yersin"}
{"answers": {"text": ["The mechanism by which Y. pestis was usually transmitted", "mechanism by which Y. pestis was usually transmitted", "The mechanism by which Y. pestis was usually transmitted"], "answer_start": [469, 473, 469]}, "prediction_text": "The bubonic plague mechanism."}
{"answers": {"text": ["two populations of rodents", "two populations of rodents", "two populations of rodents"], "answer_start": [1024, 1024, 1024]}, "prediction_text": "Rodents."}
{"answers": {"text": ["Francis Aidan Gasquet", "Francis Aidan Gasquet", "Francis Aidan Gasquet"], "answer_start": [14, 14, 14]}, "prediction_text": "Francis Aidan Gasquet"}
{"answers": {"text": ["some form of the ordinary Eastern or bubonic plague", "bubonic plague", "ordinary Eastern or bubonic plague"], "answer_start": [121, 158, 138]}, "prediction_text": "The Great Pestilence."}
{"answers": {"text": ["1908", "1908", "1908"], "answer_start": [281, 281, 281]}, "prediction_text": "1908"}
{"answers": {"text": ["rats and fleas", "rats and fleas", "rats and fleas"], "answer_start": [299, 299, 299]}, "prediction_text": "rats and fleas."}
{"answers": {"text": ["the Justinian plague that was prevalent in the Eastern Roman Empire from 541 to 700 CE.", "Justinian plague", "Justinian plague"], "answer_start": [423, 427, 427]}, "prediction_text": "Justinian plague"}
{"answers": {"text": ["30\u201375%", "30\u201375%", "30\u201375%"], "answer_start": [115, 115, 115]}, "prediction_text": "30-75%"}
{"answers": {"text": ["100\u2013106 \u00b0F", "38\u201341 \u00b0C", "41 \u00b0C"], "answer_start": [164, 154, 157]}, "prediction_text": "High."}
{"answers": {"text": ["80 percent", "80", "80"], "answer_start": [325, 325, 325]}, "prediction_text": "80%"}
{"answers": {"text": ["90 to 95 percent", "90 to 95", "90 to 95 percent"], "answer_start": [400, 400, 400]}, "prediction_text": "90 to 95%"}
{"answers": {"text": ["purple skin patches", "purple skin patches", "purple skin patches"], "answer_start": [665, 665, 665]}, "prediction_text": "Purple skin patches."}
{"answers": {"text": ["In October 2010", "October 2010", "October 2010"], "answer_start": [0, 3, 3]}, "prediction_text": "October 2010"}
{"answers": {"text": ["a new investigation into the role of Yersinia pestis in the Black Death", "role of Yersinia pestis in the Black Death", "the role of Yersinia pestis in the Black Death"], "answer_start": [123, 152, 148]}, "prediction_text": "The Plos Pathogens paper was about the role of Yersinia pestis in the Black Death."}
{"answers": {"text": ["with Polymerase Chain Reaction (PCR)", "Polymerase Chain Reaction (PCR) techniques", "Polymerase Chain Reaction (PCR) techniques"], "answer_start": [304, 309, 309]}, "prediction_text": "Polymerase Chain Reaction (PCR) techniques."}
{"answers": {"text": ["from the tooth sockets in human skeletons", "mass graves in northern, central and southern Europe", "tooth sockets in human skeletons"], "answer_start": [366, 413, 375]}, "prediction_text": "tooth sockets in human skeletons from mass graves in northern, central, and southern Europe."}
{"answers": {"text": ["unambiguously demonstrates that Y. pestis was the causative agent of the epidemic plague", "Y. pestis was the causative agent of the epidemic plague", "Y. pestis was the causative agent of the epidemic plague that devastated Europe during the Middle Ages"], "answer_start": [732, 764, 764]}, "prediction_text": "The plos pathogen paper claims that the Yersinia pestis paper claims that Yersinia pestis was the causative agent of the epidemic plague that devastated Europe during the Middle Ages."}
{"answers": {"text": ["genetic branches", "genetic branches", "genetic branches"], "answer_start": [80, 80, 80]}, "prediction_text": "Genetic branches."}
{"answers": {"text": ["Y. p. orientalis and Y. p. medievalis", "Y. pestis", "Y. p. orientalis and Y. p. medievalis"], "answer_start": [285, 105, 285]}, "prediction_text": "Y. p. orientalis and Y. p. medievalis."}
{"answers": {"text": ["the plague may have entered Europe in two waves", "the plague may have entered Europe in two waves", "may have entered Europe in two waves"], "answer_start": [335, 335, 346]}, "prediction_text": "Two clades (genetic branches) of the Y. pestis genome associated with medieval mass graves."}
{"answers": {"text": ["through the port of Marseille around November 1347", "the port of Marseille around November 1347", "1347"], "answer_start": [478, 486, 524]}, "prediction_text": "November 1347."}
{"answers": {"text": ["spring of 1349", "1349", "1349"], "answer_start": [615, 625, 625]}, "prediction_text": "1349"}
{"answers": {"text": ["confirmed and amended", "confirmed and amended", "confirmed and amended"], "answer_start": [49, 49, 49]}, "prediction_text": "Confirmed and amended."}
{"answers": {"text": ["East Smithfield", "England", "East Smithfield"], "answer_start": [138, 169, 138]}, "prediction_text": "East Smithfield"}
{"answers": {"text": ["may no longer exist", "may no longer exist", "may no longer exist"], "answer_start": [298, 298, 298]}, "prediction_text": "The strain that caused the Black Death is ancestral to most modern strains of the disease."}
{"answers": {"text": ["October 2011", "October 2011", "October 2011"], "answer_start": [351, 351, 351]}, "prediction_text": "October 2011"}
{"answers": {"text": ["British bacteriologist J. F. D. Shrewsbury", "J. F. D. Shrewsbury", "J. F. D. Shrewsbury"], "answer_start": [68, 91, 91]}, "prediction_text": "J. F. D. Shrewsbury"}
{"answers": {"text": ["rates of mortality in rural areas during the 14th-century pandemic were inconsistent with the modern bubonic plague", "the reported rates of mortality in rural areas during the 14th-century pandemic were inconsistent with the modern bubonic plague", "reported rates of mortality in rural areas during the 14th-century pandemic were inconsistent with the modern bubonic plague"], "answer_start": [148, 135, 139]}, "prediction_text": "inconsistent rates of mortality."}
{"answers": {"text": ["contemporary accounts were exaggerations", "contemporary accounts were exaggerations", "contemporary accounts were exaggerations"], "answer_start": [294, 294, 294]}, "prediction_text": "The conclusion was that contemporary accounts were exaggerations."}
{"answers": {"text": ["the first major work to challenge the bubonic plague theory directly", "the first major work to challenge the bubonic plague theory directly", "first major work to challenge the bubonic plague theory directly,"], "answer_start": [376, 376, 380]}, "prediction_text": "The Black Death."}
{"answers": {"text": ["Samuel K. Cohn, Jr.", "Samuel K. Cohn, Jr.", "Samuel K. Cohn, Jr."], "answer_start": [552, 552, 552]}, "prediction_text": "David Herlihy"}
{"answers": {"text": ["epidemiological account of the plague", "an epidemiological account", "epidemiological account of the plague"], "answer_start": [25, 22, 25]}, "prediction_text": "Epidemiological account."}
{"answers": {"text": ["the lack of reliable statistics from this period", "the lack of reliable statistics", "lack of reliable statistics"], "answer_start": [145, 145, 149]}, "prediction_text": "Lack of reliable statistics."}
{"answers": {"text": ["by over 100%", "by over 100%", "over 100%"], "answer_start": [318, 318, 321]}, "prediction_text": "Over 100%."}
{"answers": {"text": ["the clergy", "figures from the clergy", "figures from the clergy"], "answer_start": [502, 489, 489]}, "prediction_text": "Clergy"}
{"answers": {"text": ["between the time of publication of the Domesday Book and the year 1377", "1377", "between the time of publication of the Domesday Book and the year 1377"], "answer_start": [359, 425, 359]}, "prediction_text": "1377"}
{"answers": {"text": ["the rat population was insufficient", "rat population was insufficient", "rat population was insufficient to account for a bubonic plague pandemic"], "answer_start": [28, 32, 32]}, "prediction_text": "The rats were not responsible for the plague."}
{"answers": {"text": ["of marginal significance", "marginal", "marginal"], "answer_start": [324, 327, 327]}, "prediction_text": "Marginal."}
{"answers": {"text": ["temperatures that are too cold in northern Europe for the survival of fleas", "too cold in northern Europe for the survival of fleas", "too cold in northern Europe for the survival of fleas"], "answer_start": [613, 635, 635]}, "prediction_text": "The temperature affects the theory of plague spreading because it affects the theory of transference via fleas in goods."}
{"answers": {"text": ["the Black Death was much faster than that of modern bubonic plague", "faster", "faster"], "answer_start": [747, 772, 772]}, "prediction_text": "faster"}
{"answers": {"text": ["5 to 15 years", "5 to 15", "5 to 15"], "answer_start": [1108, 1108, 1108]}, "prediction_text": "5 to 15 years."}
{"answers": {"text": ["a form of anthrax", "was a form of anthrax", "the cause was a form of anthrax"], "answer_start": [101, 97, 87]}, "prediction_text": "Twigg proposes that the cause of the pandemic was a form of anthrax."}
{"answers": {"text": ["a combination of anthrax and other pandemics", "a combination of anthrax and other pandemics", "a combination of anthrax and other pandemics"], "answer_start": [170, 170, 170]}, "prediction_text": "That the plague was a combination of anthrax and other pandemics."}
{"answers": {"text": ["typhus, smallpox and respiratory infections", "typhus, smallpox and respiratory infections", "typhus, smallpox and respiratory infections"], "answer_start": [982, 982, 982]}, "prediction_text": "Bubonic plague, typhus, smallpox, respiratory infections."}
{"answers": {"text": ["(a type of \"blood poisoning\"", "a type of \"blood poisoning\"", "a type of \"blood poisoning\""], "answer_start": [1103, 1104, 1104]}, "prediction_text": "Blood poisoning."}
{"answers": {"text": ["25", "25", "25"], "answer_start": [1473, 1473, 1473]}, "prediction_text": "25 bodies."}
{"answers": {"text": ["about a third.", "about a third", "about a third"], "answer_start": [128, 128, 128]}, "prediction_text": "40%"}
{"answers": {"text": ["Half of Paris's population of 100,000 people", "100,000", "Half"], "answer_start": [199, 229, 199]}, "prediction_text": "Half."}
{"answers": {"text": ["at least some pre-planning and Christian burials", "some pre-planning and Christian burials", "some pre-planning and Christian burials"], "answer_start": [795, 804, 804]}, "prediction_text": "pre-planning and Christian burials."}
{"answers": {"text": ["as much as 50%", "50%", "50%"], "answer_start": [1202, 1213, 1213]}, "prediction_text": "50%"}
{"answers": {"text": ["most isolated areas", "isolated areas", "isolated"], "answer_start": [1284, 1289, 1289]}, "prediction_text": "Monks and priests."}
{"answers": {"text": ["throughout the 14th to 17th centuries", "14th to 17th centuries", "14th to 17th centuries"], "answer_start": [69, 84, 84]}, "prediction_text": "1346-1671"}
{"answers": {"text": ["the plague was present somewhere in Europe in every year between 1346 and 1671.", "was present somewhere in Europe in every year between 1346 and 1671", "the plague was present somewhere in Europe in every year between 1346 and 1671"], "answer_start": [130, 141, 130]}, "prediction_text": "The plague returned to Europe in every year between 1346 and 1671."}
{"answers": {"text": ["almost a million people", "almost a million people", "a million"], "answer_start": [609, 609, 616]}, "prediction_text": "Almost a million."}
{"answers": {"text": ["propose a range of preincident population figures from as high as 7 million to as low as 4 million", "propose a range of preincident population figures from as high as 7 million to as low as 4 million", "propose a range of preincident population figures"], "answer_start": [57, 57, 57]}, "prediction_text": "Proposed a range of preincident population figures from as high as 7 million to as low as 4 million in 1300."}
{"answers": {"text": ["By the end of 1350", "1350", "By the end of 1350"], "answer_start": [223, 237, 223]}, "prediction_text": "1350"}
{"answers": {"text": ["10\u201315% of the population", "10\u201315% of the population", "10\u201315% of the population"], "answer_start": [493, 493, 493]}, "prediction_text": "10-15%"}
{"answers": {"text": ["1665", "1665", "1665"], "answer_start": [771, 771, 771]}, "prediction_text": "1665"}
{"answers": {"text": ["40,000", "40,000", "40,000"], "answer_start": [17, 17, 17]}, "prediction_text": "40,000"}
{"answers": {"text": ["Russia", "Russia", "Russia"], "answer_start": [235, 235, 235]}, "prediction_text": "Russia"}
{"answers": {"text": ["the Italian Plague of 1629\u20131631", "Italian Plague", "Italian Plague"], "answer_start": [732, 736, 736]}, "prediction_text": "The Great Plague of Vienna."}
{"answers": {"text": ["The last plague outbreak ravaged Oslo in 1654.", "1654", "1654"], "answer_start": [925, 966, 966]}, "prediction_text": "1654"}
{"answers": {"text": ["22 times between 1361 and 1528", "22", "22"], "answer_start": [577, 577, 577]}, "prediction_text": "22 times."}
{"answers": {"text": ["some 1.7 million victims", "1.7 million", "1.7 million"], "answer_start": [56, 61, 61]}, "prediction_text": "1.7 million"}
{"answers": {"text": ["about half of Naples' 300,000 inhabitants", "half of Naples' 300,000 inhabitants", "half of Naples' 300,000 inhabitants"], "answer_start": [150, 156, 156]}, "prediction_text": "300,000"}
{"answers": {"text": ["reduced the population of Seville by half", "half", "half"], "answer_start": [320, 357, 357]}, "prediction_text": "1.25 million"}
{"answers": {"text": ["Sweden v. Russia and allies", "Sweden v. Russia and allies", "Sweden v. Russia and allies"], "answer_start": [440, 440, 440]}, "prediction_text": "Sweden"}
{"answers": {"text": ["1720 in Marseille.", "1720", "1720"], "answer_start": [674, 674, 674]}, "prediction_text": "1720"}
{"answers": {"text": ["between 1500 and 1850", "1500 and 1850", "1500 and 1850"], "answer_start": [137, 145, 145]}, "prediction_text": "1500-1750"}
{"answers": {"text": ["30 to 50 thousand inhabitants", "30 to 50 thousand", "30 to 50 thousand"], "answer_start": [226, 226, 226]}, "prediction_text": "30 to 50 thousand."}
{"answers": {"text": ["until the second quarter of the 19th century.", "until the second quarter of the 19th century", "until the second quarter of the 19th century"], "answer_start": [370, 370, 370]}, "prediction_text": "Between 1701 and 1750."}
{"answers": {"text": ["two-thirds of its population", "two-thirds of its population", "two-thirds of its population"], "answer_start": [642, 642, 642]}, "prediction_text": "Two-thirds."}
{"answers": {"text": ["melt (magma and/or lava)", "melt", "rock crystallizes from melt (magma and/or lava)", "melt (magma and/or lava)"], "answer_start": [238, 238, 215, 238]}, "prediction_text": "melt (magma and/or lava)"}
{"answers": {"text": ["metamorphic rock", "metamorphic", "metamorphic rock", "metamorphic"], "answer_start": [592, 592, 592, 592]}, "prediction_text": "Metamorphic rock."}
{"answers": {"text": ["new magma", "igneous", "new magma", "magma"], "answer_start": [914, 949, 914, 918]}, "prediction_text": "Magma."}
{"answers": {"text": ["igneous, sedimentary, and metamorphic", "igneous, sedimentary, and metamorphic", "igneous, sedimentary, and metamorphic", "igneous, sedimentary, and metamorphic"], "answer_start": [37, 37, 37, 37]}, "prediction_text": "igneous, sedimentary, metamorphic."}
{"answers": {"text": ["heat and pressure", "heat and pressure", "heat and pressure", "heat and pressure"], "answer_start": [430, 430, 430, 430]}, "prediction_text": "Heat and pressure."}
{"answers": {"text": ["seafloor spreading", "seafloor spreading", "seafloor spreading", "seafloor spreading"], "answer_start": [71, 71, 71, 71]}, "prediction_text": "Seafloor spreading."}
{"answers": {"text": ["the crust and rigid uppermost portion of the upper mantle", "crust and rigid uppermost portion of the upper mantle", "crust and rigid uppermost portion of the upper mantle", "crust and rigid uppermost portion of the upper mantle"], "answer_start": [143, 147, 147, 147]}, "prediction_text": "crust, upper mantle, asthenosphere."}
{"answers": {"text": ["asthenosphere", "asthenosphere", "asthenosphere", "asthenosphere"], "answer_start": [333, 333, 333, 333]}, "prediction_text": "Asthenosphere"}
{"answers": {"text": ["the convecting mantle", "convection of the mantle", "convection of the mantle", "the convecting mantle"], "answer_start": [724, 436, 436, 724]}, "prediction_text": "The convecting mantle."}
{"answers": {"text": ["the 1960s", "1960s", "1960s", "1960s"], "answer_start": [3, 7, 7, 7]}, "prediction_text": "1960s"}
{"answers": {"text": ["divergent boundaries", "divergent boundaries", "divergent boundaries", "divergent boundaries"], "answer_start": [295, 295, 295, 295]}, "prediction_text": "Mid-ocean ridges."}
{"answers": {"text": ["convergent boundaries", "convergent boundaries", "convergent boundaries", "convergent boundaries"], "answer_start": [398, 398, 398, 398]}, "prediction_text": "Arcs of volcanoes and earthquakes."}
{"answers": {"text": ["Transform boundaries", "Transform", "Transform boundaries", "Transform boundaries"], "answer_start": [461, 461, 461, 461]}, "prediction_text": "Transform boundaries."}
{"answers": {"text": ["Alfred Wegener", "Alfred Wegener", "Alfred Wegener", "Alfred Wegener"], "answer_start": [612, 612, 612, 612]}, "prediction_text": "Alfred Wegener"}
{"answers": {"text": ["the convecting mantle", "convecting mantle", "convecting", "convecting"], "answer_start": [1016, 1020, 1020, 1020]}, "prediction_text": "The convecting mantle."}
{"answers": {"text": ["seismic waves", "seismic", "seismic", "seismic"], "answer_start": [43, 43, 43, 43]}, "prediction_text": "seismic waves"}
{"answers": {"text": ["crust", "crust", "lithosphere", "crust and lithosphere"], "answer_start": [332, 332, 342, 332]}, "prediction_text": "crust"}
{"answers": {"text": ["the mantle", "mantle", "mantle", "mantle"], "answer_start": [362, 366, 366, 366]}, "prediction_text": "crust and lithosphere."}
{"answers": {"text": ["wave speeds", "wave speeds", "wave speeds"], "answer_start": [578, 578, 578]}, "prediction_text": "wave speeds."}
{"answers": {"text": ["the outer core and inner core", "outer core and inner core", "outer core and inner core", "outer core and inner core"], "answer_start": [463, 467, 467, 467]}, "prediction_text": "The mantle has two layers below it."}
{"answers": {"text": ["second scale shows the most recent eon with an expanded scale", "compresses the most recent era", "compresses the most recent era"], "answer_start": [195, 275, 275]}, "prediction_text": "To show the most recent era."}
{"answers": {"text": ["Quaternary", "Quaternary", "Quaternary"], "answer_start": [372, 372, 372]}, "prediction_text": "Holocene"}
{"answers": {"text": ["The Holocene", "Holocene", "The Holocene", "Holocene"], "answer_start": [595, 599, 595, 599]}, "prediction_text": "Holocene (the latest epoch)"}
{"answers": {"text": ["the Quaternary period", "Quaternary period", "Quaternary", "Quaternary"], "answer_start": [783, 787, 787, 787]}, "prediction_text": "Q. Q stands for the Quaternary period."}
{"answers": {"text": ["The principle of cross-cutting relationships", "cross-cutting relationships", "cross-cutting relationships"], "answer_start": [0, 17, 17]}, "prediction_text": "Cross-cutting relationships."}
{"answers": {"text": ["younger than the fault", "younger", "younger"], "answer_start": [369, 145, 369]}, "prediction_text": "Younger."}
{"answers": {"text": ["the key bed", "key bed", "key bed"], "answer_start": [401, 405, 405]}, "prediction_text": "Key bed."}
{"answers": {"text": ["older than the fault", "older", "younger"], "answer_start": [309, 309, 145]}, "prediction_text": "Younger."}
{"answers": {"text": ["xenoliths", "xenoliths", "inclusions", "xenoliths"], "answer_start": [530, 386, 141, 530]}, "prediction_text": "xenoliths"}
{"answers": {"text": ["magma or lava flows", "magma or lava flows", "magma or lava", "magma or lava flows"], "answer_start": [445, 445, 445, 445]}, "prediction_text": "lava flows"}
{"answers": {"text": ["clasts", "clasts", "clasts", "clasts"], "answer_start": [98, 98, 98, 98]}, "prediction_text": "Clasts"}
{"answers": {"text": ["The principle of inclusions and components", "principle of inclusions and components", "The principle of inclusions and components", "inclusions and components"], "answer_start": [0, 4, 0, 17]}, "prediction_text": "Inclusions."}
{"answers": {"text": ["gravel", "gravel", "gravel", "gravel"], "answer_start": [257, 257, 257, 257]}, "prediction_text": "Gravel"}
{"answers": {"text": ["The principle of faunal succession", "principle of faunal succession", "The principle of faunal succession", "faunal succession"], "answer_start": [0, 4, 0, 17]}, "prediction_text": "Faunal succession."}
{"answers": {"text": ["William Smith", "William Smith", "William Smith", "William Smith"], "answer_start": [311, 311, 311, 311]}, "prediction_text": "William Smith"}
{"answers": {"text": ["complex", "complex", "complex", "quite complex"], "answer_start": [523, 523, 523, 517]}, "prediction_text": "complex"}
{"answers": {"text": ["organisms", "organisms", "organisms", "organisms"], "answer_start": [98, 98, 98, 98]}, "prediction_text": "fossilization"}
{"answers": {"text": ["Charles Darwin", "Charles Darwin", "Charles Darwin", "Charles Darwin"], "answer_start": [374, 374, 374, 374]}, "prediction_text": "Charles Darwin"}
{"answers": {"text": ["At the beginning of the 20th century", "20th century", "At the beginning of the 20th century", "beginning of the 20th century"], "answer_start": [0, 24, 0, 7]}, "prediction_text": "20th century"}
{"answers": {"text": ["stratigraphic correlation", "stratigraphic", "stratigraphic", "stratigraphic"], "answer_start": [311, 311, 311, 311]}, "prediction_text": "Stratigraphic correlation."}
{"answers": {"text": ["absolute ages", "rock units", "new absolute"], "answer_start": [606, 453, 602]}, "prediction_text": "New absolute ages."}
{"answers": {"text": ["to one another", "one another", "one another"], "answer_start": [371, 374, 374]}, "prediction_text": "One another."}
{"answers": {"text": ["fossil sequences", "fossil sequences", "fossil sequences", "fossil sequences"], "answer_start": [510, 510, 510, 510]}, "prediction_text": "Fossils."}
{"answers": {"text": ["Thermochemical techniques", "Thermochemical", "Thermochemical", "Thermochemical"], "answer_start": [830, 830, 830, 830]}, "prediction_text": "Thermochemical techniques."}
{"answers": {"text": ["particular closure temperature", "closure temperature", "closure temperature"], "answer_start": [173, 184, 184]}, "prediction_text": "closure temperature"}
{"answers": {"text": ["isotope ratios of radioactive elements", "isotope", "isotope", "isotope"], "answer_start": [32, 32, 32, 32]}, "prediction_text": "Isotope ratios."}
{"answers": {"text": ["Dating of lava and volcanic ash layers found within a stratigraphic sequence", "Dating of lava and volcanic ash layers", "stratigraphic sequence", "Dating of lava and volcanic ash layers found within a stratigraphic sequence"], "answer_start": [540, 540, 594, 540]}, "prediction_text": "Dating of lava and volcanic ash layers found within a stratigraphic sequence."}
{"answers": {"text": ["horizontal compression", "horizontal", "horizontal"], "answer_start": [33, 33, 33]}, "prediction_text": "Thicker and shorter."}
{"answers": {"text": ["In the shallow crust", "shallow crust", "shallow crust"], "answer_start": [238, 245, 245]}, "prediction_text": "Shallow crust."}
{"answers": {"text": ["antiforms", "antiforms", "antiforms"], "answer_start": [877, 877, 877]}, "prediction_text": "Antiforms."}
{"answers": {"text": ["synforms", "synforms", "synforms"], "answer_start": [930, 930, 930]}, "prediction_text": "Synforms."}
{"answers": {"text": ["anticlines and synclines", "overturned anticline", "anticlines and synclines"], "answer_start": [1029, 1150, 1029]}, "prediction_text": "Anticons."}
{"answers": {"text": ["Extension", "Extension", "Extension"], "answer_start": [0, 0, 0]}, "prediction_text": "longer and thinner"}
{"answers": {"text": ["boudins", "boudins", "boudins"], "answer_start": [683, 683, 683]}, "prediction_text": "Boudins"}
{"answers": {"text": ["within the Maria Fold and Thrust Belt", "Maria Fold and Thrust Belt", "Maria Fold and Thrust Belt"], "answer_start": [402, 413, 413]}, "prediction_text": "The entire sedimentary sequence of the Grand Canyon can be seen in less than a meter."}
{"answers": {"text": ["metamorphosed", "metamorphosed", "metamorphosed"], "answer_start": [609, 609, 609]}, "prediction_text": "metamorphosed"}
{"answers": {"text": ["normal faulting and through the ductile stretching and thinning", "normal faulting", "normal faulting and through the ductile stretching and thinning"], "answer_start": [112, 112, 112]}, "prediction_text": "Normal faulting and ductile stretching and thinning."}
{"answers": {"text": ["Dikes", "Dikes", "Dikes"], "answer_start": [724, 724, 724]}, "prediction_text": "Dikes"}
{"answers": {"text": ["in areas that are being actively deformed", "areas that are being actively deformed", "in areas that are being actively deformed"], "answer_start": [826, 829, 826]}, "prediction_text": "Cracks."}
{"answers": {"text": ["topographic gradients", "topographic", "topographic"], "answer_start": [171, 171, 171]}, "prediction_text": "Topographic gradients."}
{"answers": {"text": ["Continual motion along the fault", "Continual motion", "Continual motion along the fault"], "answer_start": [368, 368, 368]}, "prediction_text": "Faulting and other deformational processes."}
{"answers": {"text": ["Deformational events", "Deformational", "Deformational"], "answer_start": [547, 547, 547]}, "prediction_text": "Volcanic ashes and lavas, igneous intrusions."}
{"answers": {"text": ["layered basaltic lava flows", "layered basaltic lava flows", "layered basaltic lava flows"], "answer_start": [183, 183, 183]}, "prediction_text": "layered basaltic lava flows."}
{"answers": {"text": ["Acasta gneiss", "Acasta gneiss", "Acasta gneiss of the Slave craton in northwestern Canada"], "answer_start": [645, 645, 645]}, "prediction_text": "Acasta gneiss"}
{"answers": {"text": ["sedimentary rocks", "sedimentary", "sedimentary"], "answer_start": [366, 366, 366]}, "prediction_text": "Sedimentary rocks."}
{"answers": {"text": ["Cambrian time", "Cambrian time", "Cambrian time"], "answer_start": [418, 418, 418]}, "prediction_text": "Cambrian time."}
{"answers": {"text": ["Slave craton in northwestern Canada", "Canada", "northwestern Canada"], "answer_start": [666, 695, 682]}, "prediction_text": "The Acasta gneiss of the Slave craton in northwestern Canada."}
{"answers": {"text": ["the study of rocks", "study of rocks", "the study of rocks"], "answer_start": [263, 267, 263]}, "prediction_text": "The study of rocks."}
{"answers": {"text": ["the study of sedimentary layers", "study of sedimentary layers", "the study of sedimentary layers"], "answer_start": [298, 302, 298]}, "prediction_text": "Sedimentary layers."}
{"answers": {"text": ["the study of positions of rock units and their deformation", "study of positions of rock units and their deformation", "the study of positions of rock units and their deformation"], "answer_start": [356, 360, 356]}, "prediction_text": "Positions of rock units."}
{"answers": {"text": ["modern soils", "soils, rivers, landscapes, and glaciers", "soils, rivers, landscapes, and glaciers"], "answer_start": [454, 461, 461]}, "prediction_text": "soils, rivers, landscapes, glaciers."}
{"answers": {"text": ["identifying rocks", "identifying rocks", "identifying rocks"], "answer_start": [15, 132, 132]}, "prediction_text": "Identifying rocks in the laboratory."}
{"answers": {"text": ["birefringence, pleochroism, twinning, and interference properties", "birefringence, pleochroism, twinning, and interference", "birefringence, pleochroism, twinning, and interference"], "answer_start": [483, 483, 483]}, "prediction_text": "Birefringence, pleochroism, twinning, and interference."}
{"answers": {"text": ["geochemical evolution of rock units", "geochemical evolution of rock units", "the geochemical evolution of rock units"], "answer_start": [794, 794, 790]}, "prediction_text": "Geochemical evolution of rock units."}
{"answers": {"text": ["the laboratory", "laboratory", "laboratory"], "answer_start": [85, 89, 89]}, "prediction_text": "Laboratory."}
{"answers": {"text": ["petrographic microscope", "petrographic", "petrographic"], "answer_start": [324, 324, 324]}, "prediction_text": "Optical microscopy"}
{"answers": {"text": ["pressure physical experiments", "fluid inclusion data", "fluid inclusion data"], "answer_start": [80, 26, 26]}, "prediction_text": "Fluid inclusion data."}
{"answers": {"text": ["physical experiments", "high temperature and pressure physical experiments", "pressure physical experiments"], "answer_start": [89, 59, 80]}, "prediction_text": "Fluid inclusion data."}
{"answers": {"text": ["metamorphic processes", "metamorphic", "metamorphic"], "answer_start": [321, 321, 321]}, "prediction_text": "metamorphic processes and conditions of crystallization of igneous rocks."}
{"answers": {"text": ["Structural geologists", "Structural", "Structural"], "answer_start": [0, 0, 0]}, "prediction_text": "Structural geologists."}
{"answers": {"text": ["microscopic analysis of oriented thin sections", "microscopic analysis", "use microscopic analysis of oriented thin sections of geologic samples"], "answer_start": [26, 26, 22]}, "prediction_text": "Microscopic analysis."}
{"answers": {"text": ["plot and combine", "plot and combine", "plot and combine"], "answer_start": [226, 226, 226]}, "prediction_text": "Plot and combine measurements of geological structures."}
{"answers": {"text": ["analog and numerical experiments", "analog and numerical", "analog and numerical"], "answer_start": [443, 443, 443]}, "prediction_text": "Analog and numerical experiments."}
{"answers": {"text": ["orogenic wedges", "orogenic wedges", "orogenic wedges"], "answer_start": [80, 80, 80]}, "prediction_text": "Orogenic wedges."}
{"answers": {"text": ["those involving orogenic wedges", "orogenic wedges", "involving orogenic wedges"], "answer_start": [64, 80, 70]}, "prediction_text": "Orogenic wedges."}
{"answers": {"text": ["sand", "sand", "sand"], "answer_start": [252, 252, 252]}, "prediction_text": "Sand."}
{"answers": {"text": ["all angles remain the same", "all angles remain the same", "all angles remain the same"], "answer_start": [404, 404, 404]}, "prediction_text": "All angles remain the same."}
{"answers": {"text": ["Numerical models", "Numerical", "Numerical models"], "answer_start": [448, 448, 448]}, "prediction_text": "Numerical models."}
{"answers": {"text": ["stratigraphers", "stratigraphers", "stratigraphers"], "answer_start": [19, 19, 19]}, "prediction_text": "Stratigraphers"}
{"answers": {"text": ["geophysical surveys", "geophysical", "geophysical"], "answer_start": [183, 183, 183]}, "prediction_text": "Geophysical surveys."}
{"answers": {"text": ["well logs", "well logs", "well logs"], "answer_start": [290, 290, 290]}, "prediction_text": "Well logs."}
{"answers": {"text": ["computer programs", "computer programs", "computer programs"], "answer_start": [389, 389, 389]}, "prediction_text": "Computer programs."}
{"answers": {"text": ["water, coal, and hydrocarbon extraction", "hydrocarbon", "hydrocarbon"], "answer_start": [600, 617, 617]}, "prediction_text": "Water, coal, and hydrocarbon extraction."}
{"answers": {"text": ["provide better absolute bounds on the timing and rates of deposition", "gain information about past climate", "provide better absolute bounds on the timing and rates of deposition"], "answer_start": [327, 568, 327]}, "prediction_text": "To provide better absolute bounds on the timing and rates of deposition."}
{"answers": {"text": ["biostratigraphers", "biostratigraphers", "biostratigraphers"], "answer_start": [19, 19, 19]}, "prediction_text": "Biostratigraphers"}
{"answers": {"text": ["Magnetic stratigraphers", "Magnetic stratigraphers", "Magnetic stratigraphers"], "answer_start": [397, 397, 397]}, "prediction_text": "Geochronologists, magnetic stratigraphers, and other scientists."}
{"answers": {"text": ["Geochronologists", "Geochronologists", "Geochronologists"], "answer_start": [244, 244, 244]}, "prediction_text": "Geochronologists."}
{"answers": {"text": ["Persia", "Persia", "Persia after the Muslim conquests had come to an end"], "answer_start": [130, 130, 130]}, "prediction_text": "Persia."}
{"answers": {"text": ["Abu al-Rayhan al-Biruni", "Abu al-Rayhan al-Biruni", "Abu al-Rayhan al-Biruni"], "answer_start": [184, 184, 184]}, "prediction_text": "Abu al-Rayhan al-Biruni"}
{"answers": {"text": ["Shen Kuo", "Shen Kuo", "Shen Kuo"], "answer_start": [782, 782, 782]}, "prediction_text": "Shen Kuo"}
{"answers": {"text": ["Ibn Sina", "Ibn Sina", "Ibn Sina"], "answer_start": [514, 514, 514]}, "prediction_text": "Shen Kuo"}
{"answers": {"text": ["his observation of fossil animal shells", "fossil animal shells", "based on his observation of fossil animal shells in a geological stratum in a mountain hundreds of miles from the ocean"], "answer_start": [871, 890, 862]}, "prediction_text": "Observation of fossil animal shells."}
{"answers": {"text": ["James Hutton", "James Hutton", "James Hutton"], "answer_start": [0, 0, 0]}, "prediction_text": "James Hutton"}
{"answers": {"text": ["Theory of the Earth", "Theory of the Earth", "Theory of the Earth"], "answer_start": [98, 98, 98]}, "prediction_text": "Theory of the Earth"}
{"answers": {"text": ["1795", "1795", "1795"], "answer_start": [481, 481, 481]}, "prediction_text": "1795"}
{"answers": {"text": ["Earth must be much older than had previously been supposed", "Earth must be much older than had previously been supposed", "the Earth must be much older than had previously been supposed"], "answer_start": [200, 200, 196]}, "prediction_text": "Theory of the Earth."}
{"answers": {"text": ["William Maclure", "William Maclure", "William Maclure"], "answer_start": [61, 61, 61]}, "prediction_text": "William Maclure"}
{"answers": {"text": ["1809", "1809", "1809"], "answer_start": [53, 53, 53]}, "prediction_text": "1807"}
{"answers": {"text": ["1807", "1807", "1807"], "answer_start": [81, 81, 81]}, "prediction_text": "1807"}
{"answers": {"text": ["Observations on the Geology of the United States explanatory of a Geological Map", "Observations on the Geology of the United States explanatory of a Geological Map", "Observations on the Geology of the United States"], "answer_start": [420, 420, 420]}, "prediction_text": "Observations on the Geology of the United States."}
{"answers": {"text": ["the American Philosophical Society", "American Philosophical Society", "American Philosophical Society"], "answer_start": [364, 368, 368]}, "prediction_text": "American Philosophical Society"}
{"answers": {"text": ["Principles of Geology", "Principles of Geology", "Principles of Geology"], "answer_start": [51, 51, 51]}, "prediction_text": "Principles of Geology"}
{"answers": {"text": ["uniformitarianism", "uniformitarianism", "uniformitarianism"], "answer_start": [180, 180, 180]}, "prediction_text": "Uniformitarianism"}
{"answers": {"text": ["uniformitarianism", "uniformitarianism", "uniformitarianism"], "answer_start": [180, 180, 180]}, "prediction_text": "Catastrophism"}
{"answers": {"text": ["catastrophism", "catastrophism", "catastrophism"], "answer_start": [338, 338, 338]}, "prediction_text": "Catastrophism"}
{"answers": {"text": ["Charles Darwin", "Charles Darwin", "Charles Darwin"], "answer_start": [126, 126, 126]}, "prediction_text": "Charles Darwin"}
{"answers": {"text": ["103 miles", "103 miles", "103 miles"], "answer_start": [167, 167, 167]}, "prediction_text": "8.5 mi (13.7 km)"}
{"answers": {"text": ["8.5 mi", "8.5 mi", "8.5 mi"], "answer_start": [284, 284, 284]}, "prediction_text": "8.5 mi (13.7 km)"}
{"answers": {"text": ["Eurocities", "Eurocities", "the English Core Cities Group"], "answer_start": [526, 526, 473]}, "prediction_text": "Eurocities"}
{"answers": {"text": ["Northumberland", "Northumberland", "county of Northumberland"], "answer_start": [601, 601, 591]}, "prediction_text": "Northumberland"}
{"answers": {"text": ["Geordie", "Geordie", "Geordie"], "answer_start": [838, 838, 838]}, "prediction_text": "Geordie"}
{"answers": {"text": ["Robert Curthose", "Robert Curthose", "Robert Curthose"], "answer_start": [107, 107, 107]}, "prediction_text": "Robert Curthose"}
{"answers": {"text": ["wool", "wool", "the wool trade"], "answer_start": [205, 205, 201]}, "prediction_text": "Wool trade"}
{"answers": {"text": ["coal", "coal", "coal"], "answer_start": [262, 262, 262]}, "prediction_text": "Coal."}
{"answers": {"text": ["16th century", "the 16th", "the 16th century"], "answer_start": [306, 302, 302]}, "prediction_text": "16th century"}
{"answers": {"text": ["the Great North Run", "the Great North Run", "the Great North Run"], "answer_start": [796, 796, 796]}, "prediction_text": "Great North Run"}
{"answers": {"text": ["Pons Aelius", "Pons Aelius", "Pons Aelius"], "answer_start": [59, 59, 59]}, "prediction_text": "Pons Aelius"}
{"answers": {"text": ["Tyne", "the River Tyne", "the River Tyne"], "answer_start": [113, 103, 103]}, "prediction_text": "Tyne"}
{"answers": {"text": ["2,000", "2,000", "2,000"], "answer_start": [393, 393, 393]}, "prediction_text": "2,000."}
{"answers": {"text": ["Hadrian's", "Hadrian's", "Hadrian's Wall"], "answer_start": [413, 413, 677]}, "prediction_text": "Hadrian's Wall"}
{"answers": {"text": ["Pictish", "Pictish", "Pictish tribes"], "answer_start": [921, 921, 921]}, "prediction_text": "Pictish tribes."}
{"answers": {"text": ["England's", "England", "England"], "answer_start": [42, 42, 42]}, "prediction_text": "England"}
{"answers": {"text": ["Elizabeth", "Elizabeth", "Elizabeth"], "answer_start": [141, 141, 141]}, "prediction_text": "Elizabeth"}
{"answers": {"text": ["25-foot", "25-foot", "25-foot (7.6 m) high"], "answer_start": [162, 162, 162]}, "prediction_text": "25 feet (7.6 m)"}
{"answers": {"text": ["William the Lion", "William the Lion", "William the Lion"], "answer_start": [323, 323, 323]}, "prediction_text": "William the Lion"}
{"answers": {"text": ["three times", "three", "three times"], "answer_start": [515, 515, 515]}, "prediction_text": "3 times."}
{"answers": {"text": ["coal", "coal", "coal"], "answer_start": [50, 50, 50]}, "prediction_text": "Coal."}
{"answers": {"text": ["the Hostmen", "the Hostmen", "the Hostmen"], "answer_start": [172, 172, 172]}, "prediction_text": "Hostmen"}
{"answers": {"text": ["a pointless pursuit", "a pointless pursuit", "a pointless pursuit"], "answer_start": [396, 396, 396]}, "prediction_text": "A pointless pursuit."}
{"answers": {"text": ["an eccentric", "an eccentric", "an eccentric,"], "answer_start": [498, 498, 498]}, "prediction_text": "eccentric"}
{"answers": {"text": ["ruin him", "ruin him", "to ruin him"], "answer_start": [613, 613, 610]}, "prediction_text": "Ruin him."}
{"answers": {"text": ["their families", "their families", "their families"], "answer_start": [116, 116, 116]}, "prediction_text": "Families."}
{"answers": {"text": ["boats", "keels", "keels"], "answer_start": [186, 179, 179]}, "prediction_text": "By boat."}
{"answers": {"text": ["7,000", "7,000", "7,000"], "answer_start": [325, 325, 325]}, "prediction_text": "47%"}
{"answers": {"text": ["47%", "more than one-third", "47%"], "answer_start": [538, 386, 538]}, "prediction_text": "47%"}
{"answers": {"text": ["devastating loss", "devastating loss", "devastating loss"], "answer_start": [635, 635, 635]}, "prediction_text": "devastating loss."}
{"answers": {"text": ["the King", "the King", "for the King"], "answer_start": [53, 53, 49]}, "prediction_text": "King."}
{"answers": {"text": ["the Scots", "the Scots", "the Scots"], "answer_start": [123, 123, 123]}, "prediction_text": "Scots"}
{"answers": {"text": ["drummes", "drummes", "drummes"], "answer_start": [366, 366, 366]}, "prediction_text": "Drumming."}
{"answers": {"text": ["Triumphing by a brave defence", "Triumphing by a brave defence", "Triumphing by a brave defence"], "answer_start": [479, 479, 479]}, "prediction_text": "Triumphing by a brave defence."}
{"answers": {"text": ["Charles I", "Charles I", "Charles I"], "answer_start": [526, 526, 526]}, "prediction_text": "Charles I"}
{"answers": {"text": ["urbanization", "urbanization", "the urbanization of the city"], "answer_start": [187, 187, 183]}, "prediction_text": "Urbanization."}
{"answers": {"text": ["the Maling company", "Maling", "Maling company"], "answer_start": [221, 225, 225]}, "prediction_text": "Maling Company"}
{"answers": {"text": ["electric lighting", "the incandescent lightbulb", "the incandescent lightbulb"], "answer_start": [706, 611, 611]}, "prediction_text": "Electric lighting."}
{"answers": {"text": ["prosperity", "the city's prosperity", "the city's prosperity;"], "answer_start": [83, 72, 72]}, "prediction_text": "Prosperity."}
{"answers": {"text": ["the steam turbine", "the steam turbine", "the steam turbine"], "answer_start": [946, 946, 946]}, "prediction_text": "Steam turbine."}
{"answers": {"text": ["medieval", "medieval", "medieval street layout."], "answer_start": [42, 42, 42]}, "prediction_text": "Medieval street layout."}
{"answers": {"text": ["Narrow alleys", "Narrow alleys", "Narrow alleys"], "answer_start": [66, 66, 66]}, "prediction_text": "Narrow alleys or 'chares'"}
{"answers": {"text": ["Stairs", "Stairs", "chares'"], "answer_start": [199, 199, 84]}, "prediction_text": "Stairs."}
{"answers": {"text": ["modern", "modern buildings as well as structures dating from the 15th\u201318th centuries", "modern buildings"], "answer_start": [391, 391, 391]}, "prediction_text": "Modern buildings."}
{"answers": {"text": ["a restaurant", "a restaurant", "a restaurant situated at a Grade"], "answer_start": [580, 580, 580]}, "prediction_text": "Restaurant situated at a Grade I-listed 16th century merchant's house."}
{"answers": {"text": ["Tyneside Classical", "Tyneside Classical", "Newcastle"], "answer_start": [61, 61, 231]}, "prediction_text": "Tyneside Classical"}
{"answers": {"text": ["England's best-looking city", "England's best-looking city", "England's best-looking city"], "answer_start": [244, 244, 244]}, "prediction_text": "England's best-looking city."}
{"answers": {"text": ["Grey Street", "Grey Street", "Grey Street"], "answer_start": [358, 358, 358]}, "prediction_text": "Grey Street"}
{"answers": {"text": ["in the 1960s", "the 1960s", "the 1960s"], "answer_start": [771, 774, 774]}, "prediction_text": "1960s"}
{"answers": {"text": ["Shopping Centre", "Shopping Centre", "the Eldon Square Shopping Centre"], "answer_start": [817, 817, 800]}, "prediction_text": "Shopping center"}
{"answers": {"text": ["Town Moor", "the Town Moor", "the Town Moor"], "answer_start": [40, 36, 36]}, "prediction_text": "Town Moor"}
{"answers": {"text": ["graze", "graze", "graze cattle on it."], "answer_start": [218, 218, 218]}, "prediction_text": "graze cattle."}
{"answers": {"text": ["The Hoppings funfair", "The Hoppings funfair", "The Hoppings funfair"], "answer_start": [586, 586, 586]}, "prediction_text": "Hoppings funfair"}
{"answers": {"text": ["June", "June", "annually in June"], "answer_start": [686, 686, 674]}, "prediction_text": "June."}
{"answers": {"text": ["freemen", "freemen", "Honorary freemen"], "answer_start": [446, 446, 437]}, "prediction_text": "Honorary freeman"}
{"answers": {"text": ["Large-scale regeneration", "Large-scale regeneration", "new office developments"], "answer_start": [0, 0, 77]}, "prediction_text": "Large-scale regeneration."}
{"answers": {"text": ["Gateshead Council", "Gateshead Council", "Gateshead Council"], "answer_start": [184, 184, 184]}, "prediction_text": "Gateshead Council"}
{"answers": {"text": ["Norman Foster", "Norman Foster", "Norman Foster"], "answer_start": [404, 404, 404]}, "prediction_text": "Norman Foster"}
{"answers": {"text": ["tourist promotion", "to spearhead the regeneration of the North-East", "to spearhead the regeneration of the North-East"], "answer_start": [583, 686, 686]}, "prediction_text": "To spearhead the regeneration of the North-East."}
{"answers": {"text": ["ten", "ten", "for ten days"], "answer_start": [795, 795, 791]}, "prediction_text": "Ten days."}
{"answers": {"text": ["the Grainger Town area", "Grainger Town area", "the Grainger Town area"], "answer_start": [35, 39, 35]}, "prediction_text": "Grainger Town"}
{"answers": {"text": ["between 1835 and 1842", "between 1835 and 1842", "between 1835 and 1842,"], "answer_start": [144, 144, 144]}, "prediction_text": "1835-1842"}
{"answers": {"text": ["four stories", "four stories", "four stories high"], "answer_start": [391, 391, 391]}, "prediction_text": "Four stories."}
{"answers": {"text": ["244", "244", "244 are listed,"], "answer_start": [589, 589, 589]}, "prediction_text": "29 are grade I and 49 are grade II*."}
{"answers": {"text": ["the Butcher Market", "the Butcher Market", "the Butcher Market"], "answer_start": [79, 79, 79]}, "prediction_text": "Butcher Market"}
{"answers": {"text": ["1835", "1835", "in 1835"], "answer_start": [141, 141, 138]}, "prediction_text": "1835"}
{"answers": {"text": ["2000", "2000", "2000 guests"], "answer_start": [356, 356, 356]}, "prediction_text": "2000 guests."}
{"answers": {"text": ["a painting", "a painting", "a painting"], "answer_start": [399, 399, 399]}, "prediction_text": "A painting of the opening."}
{"answers": {"text": ["English Heritage", "English Heritage", "English Heritage."], "answer_start": [719, 719, 719]}, "prediction_text": "English Heritage"}
{"answers": {"text": ["oceanic", "oceanic", "significantly milder than some other locations"], "answer_start": [28, 28, 53]}, "prediction_text": "Oceanic (K\u00f6ppen Cfb)"}
{"answers": {"text": ["warming", "warming", "warming influence"], "answer_start": [147, 147, 147]}, "prediction_text": "Warming."}
{"answers": {"text": ["rain", "rain", "the rain shadow"], "answer_start": [229, 229, 225]}, "prediction_text": "Rain shadow."}
{"answers": {"text": ["January 1982", "1982", "\u221212.6 \u00b0C (9.3 \u00b0F)"], "answer_start": [441, 449, 416]}, "prediction_text": "January 1982"}
{"answers": {"text": ["the British Isles", "the British Isles", "the remainder of the British Isles"], "answer_start": [618, 618, 601]}, "prediction_text": "British Isles"}
{"answers": {"text": ["2010", "2010", "In 2010"], "answer_start": [3, 3, 0]}, "prediction_text": "2010"}
{"answers": {"text": ["Eldon Square Shopping Centre,", "the Eldon Square Shopping Centre", "the Eldon Square Shopping Centre"], "answer_start": [184, 180, 180]}, "prediction_text": "Eldon Square Shopping Centre"}
{"answers": {"text": ["Bainbridge's", "Bainbridge's", "Bainbridges"], "answer_start": [456, 456, 427]}, "prediction_text": "Bainbridges"}
{"answers": {"text": ["by department", "by department", "by department,"], "answer_start": [733, 733, 733]}, "prediction_text": "Department."}
{"answers": {"text": ["2007", "2007", "2007"], "answer_start": [943, 943, 943]}, "prediction_text": "2007"}
{"answers": {"text": ["shopping", "shopping", "shopping destinations"], "answer_start": [6, 6, 6]}, "prediction_text": "Modern"}
{"answers": {"text": ["suburban", "suburban", "the largest suburban shopping areas"], "answer_start": [275, 275, 263]}, "prediction_text": "Suburban"}
{"answers": {"text": ["Tesco", "Tesco", "Tesco store"], "answer_start": [335, 335, 335]}, "prediction_text": "Tesco"}
{"answers": {"text": ["the MetroCentre", "the MetroCentre", "the MetroCentre"], "answer_start": [489, 489, 489]}, "prediction_text": "MetroCentre"}
{"answers": {"text": ["Gateshead", "Gateshead", "in Gateshead"], "answer_start": [520, 520, 517]}, "prediction_text": "Gateshead"}
{"answers": {"text": ["The Tyneside flat", "The Tyneside flat", "The Tyneside flat"], "answer_start": [0, 0, 0]}, "prediction_text": "Tyneside flats"}
{"answers": {"text": ["terraces", "terraces", "terraces"], "answer_start": [297, 297, 297]}, "prediction_text": "Terraces."}
{"answers": {"text": ["the Ouseburn valley", "the Ouseburn valley", "the Ouseburn valley"], "answer_start": [454, 454, 454]}, "prediction_text": "Ouseburn valley"}
{"answers": {"text": ["Architects", "Architects", "Architects"], "answer_start": [494, 494, 494]}, "prediction_text": "Architects"}
{"answers": {"text": ["high density", "high density", "high density"], "answer_start": [571, 571, 571]}, "prediction_text": "High density."}
{"answers": {"text": ["7.8%", "to 7.8%", "(to 7.8%"], "answer_start": [135, 132, 131]}, "prediction_text": "7.8%"}
{"answers": {"text": ["5.9%", "highest", "5.9%"], "answer_start": [380, 335, 380]}, "prediction_text": "5.9%"}
{"answers": {"text": ["overinflated", "converted or shared houses", "historic densely occupied, arguably overinflated markets"], "answer_start": [521, 262, 485]}, "prediction_text": "Overinflated markets."}
{"answers": {"text": ["authorities", "authorities", "authorities"], "answer_start": [555, 555, 555]}, "prediction_text": "Harrogate"}
{"answers": {"text": ["Tunbridge Wells.", "Tunbridge Wells", "Tunbridge Wells"], "answer_start": [634, 634, 634]}, "prediction_text": "Tunbridge Wells"}
{"answers": {"text": ["2001", "2001", "2001"], "answer_start": [42, 42, 42]}, "prediction_text": "2001"}
{"answers": {"text": ["metropolitan", "the metropolitan", "the metropolitan borough"], "answer_start": [130, 126, 126]}, "prediction_text": "Newcastle Borough"}
{"answers": {"text": ["student", "student", "student"], "answer_start": [905, 905, 905]}, "prediction_text": "Student"}
{"answers": {"text": ["Universities", "Newcastle and Northumbria Universities", "Universities"], "answer_start": [955, 929, 955]}, "prediction_text": "Universities"}
{"answers": {"text": ["student populations", "student", "student populations"], "answer_start": [1010, 1010, 1010]}, "prediction_text": "Students"}
{"answers": {"text": ["37.8", "37.8", "37.8"], "answer_start": [83, 83, 83]}, "prediction_text": "37.8"}
{"answers": {"text": ["ancestors", "ancestors", "ancestors"], "answer_start": [170, 170, 170]}, "prediction_text": "Scottish or Irish."}
{"answers": {"text": ["Border Reiver", "Border Reiver", "Border Reiver surnames"], "answer_start": [211, 211, 211]}, "prediction_text": "Border Reiver"}
{"answers": {"text": ["500", "500", "500"], "answer_start": [468, 468, 468]}, "prediction_text": "500."}
{"answers": {"text": ["1%", "up to 1%", "1% of the population"], "answer_start": [520, 514, 520]}, "prediction_text": "1%"}
{"answers": {"text": ["Geordie", "Geordie", "Geordie,"], "answer_start": [37, 37, 37]}, "prediction_text": "Geordie"}
{"answers": {"text": ["Anglo-Saxon populations", "Anglo-Saxon populations", "the Anglo-Saxon populations"], "answer_start": [245, 245, 241]}, "prediction_text": "Anglo-Saxons"}
{"answers": {"text": ["many elements", "many elements", "many elements of the old language."], "answer_start": [589, 589, 589]}, "prediction_text": "Words with Anglo-Saxon origins."}
{"answers": {"text": ["strong", "strang", "strong"], "answer_start": [710, 760, 710]}, "prediction_text": "Go."}
{"answers": {"text": ["stream", "stream", "stream"], "answer_start": [951, 951, 951]}, "prediction_text": "Burn."}
{"answers": {"text": ["Scandinavia", "Scandinavia", "Scandinavia"], "answer_start": [108, 108, 108]}, "prediction_text": "Scandinavia"}
{"answers": {"text": ["Northern United Kingdom", "elsewhere in the Northern United Kingdom", "the Northern United Kingdom"], "answer_start": [257, 240, 253]}, "prediction_text": "Northern United Kingdom."}
{"answers": {"text": ["Scots", "Scots", "Scots"], "answer_start": [431, 431, 431]}, "prediction_text": "Scots."}
{"answers": {"text": ["Many words", "Many words", "Canny"], "answer_start": [544, 544, 643]}, "prediction_text": "\"Canny\" (a versatile word meaning \"good\", \"nice\" or \"very\")."}
{"answers": {"text": ["Dutch", "Dutch", "the Dutch gooien"], "answer_start": [767, 767, 763]}, "prediction_text": "Dutch"}
{"answers": {"text": ["a report", "a report", "a report"], "answer_start": [3, 3, 3]}, "prediction_text": "Ear Institute at the University College London."}
{"answers": {"text": ["noisiest", "noisiest", "the noisiest city in the whole of the UK"], "answer_start": [174, 174, 170]}, "prediction_text": "Ear Institute"}
{"answers": {"text": ["80.4", "80.4", "80.4 decibels"], "answer_start": [237, 237, 237]}, "prediction_text": "80.4 decibels."}
{"answers": {"text": ["negative", "negative long-term", "negative long-term impact on the health"], "answer_start": [308, 308, 308]}, "prediction_text": "Negative long-term impact."}
{"answers": {"text": ["a motorway underpass", "a motorway underpass", "a motorway underpass"], "answer_start": [521, 521, 521]}, "prediction_text": "Motorway underpass."}
{"answers": {"text": ["Collingwood Street", "Collingwood Street", "Collingwood Street,"], "answer_start": [201, 201, 201]}, "prediction_text": "Neville Street"}
{"answers": {"text": ["indoor complex", "a new indoor complex", "a new indoor complex"], "answer_start": [464, 458, 458]}, "prediction_text": "The Gate."}
{"answers": {"text": ["12", "12", "12-screen"], "answer_start": [533, 533, 533]}, "prediction_text": "12"}
{"answers": {"text": ["'The Pink Triangle'", "The Pink Triangle", "The Pink Triangle"], "answer_start": [592, 593, 593]}, "prediction_text": "The Pink Triangle"}
{"answers": {"text": ["bars, caf\u00e9s and clubs", "Times Square", "bars, caf\u00e9s and clubs"], "answer_start": [694, 632, 694]}, "prediction_text": "Bars, caf\u00e9s, clubs."}
{"answers": {"text": ["theatre", "theatre", "theatre."], "answer_start": [32, 32, 32]}, "prediction_text": "Theatre"}
{"answers": {"text": ["Stephen Kemble", "Stephen Kemble"], "answer_start": [41, 41]}, "prediction_text": "Stephen Kemble"}
{"answers": {"text": ["many celebrated seasons", "many celebrated seasons", "many celebrated seasons."], "answer_start": [329, 329, 329]}, "prediction_text": "Seasoned seasons."}
{"answers": {"text": ["1788", "21 January 1788", "21 January 1788"], "answer_start": [419, 408, 408]}, "prediction_text": "1788"}
{"answers": {"text": ["Grey Street", "Mosley Street", "Grey Street"], "answer_start": [492, 443, 492]}, "prediction_text": "Grey Street"}
{"answers": {"text": ["theatres", "theatres", "theatres."], "answer_start": [29, 29, 29]}, "prediction_text": "theatres"}
{"answers": {"text": ["the Theatre Royal", "the Theatre Royal", "the Theatre Royal"], "answer_start": [52, 52, 52]}, "prediction_text": "Theatre Royal on Grey Street"}
{"answers": {"text": ["Royal Shakespeare", "the Royal Shakespeare Company", "the Royal Shakespeare Company"], "answer_start": [193, 189, 189]}, "prediction_text": "Royal Shakespeare Company"}
{"answers": {"text": ["local talent", "local talent", "local talent"], "answer_start": [382, 382, 382]}, "prediction_text": "local talent"}
{"answers": {"text": ["arts capital of the UK", "arts capital of the UK", "the arts capital of the UK"], "answer_start": [741, 741, 737]}, "prediction_text": "Arts capital of the UK"}
{"answers": {"text": ["The Literary and Philosophical Society of Newcastle", "The Literary and Philosophical Society of Newcastle upon Tyne", "The Literary and Philosophical Society of Newcastle"], "answer_start": [0, 0, 0]}, "prediction_text": "Newcastle upon Tyne"}
{"answers": {"text": ["8000", "8000", "8000"], "answer_start": [211, 211, 211]}, "prediction_text": "8000"}
{"answers": {"text": ["Green", "Green", "John and Benjamin Green"], "answer_start": [340, 340, 322]}, "prediction_text": "John and Benjamin Green"}
{"answers": {"text": ["lecture theatre", "lecture theatre", "Lit and Phil premises"], "answer_start": [410, 410, 248]}, "prediction_text": "Joseph Swan"}
{"answers": {"text": ["Joseph Swan", "Joseph Swan", "Joseph Swan"], "answer_start": [505, 505, 505]}, "prediction_text": "Joseph Swan"}
{"answers": {"text": ["The Newcastle Beer Festival", "The Newcastle Beer Festival", "The Newcastle Beer Festival"], "answer_start": [0, 0, 0]}, "prediction_text": "Newcastle Beer Festival"}
{"answers": {"text": ["May", "Newcastle and Gateshead", "over the Spring bank holiday"], "answer_start": [74, 79, 195]}, "prediction_text": "May"}
{"answers": {"text": ["biennial", "biennial", "biennial"], "answer_start": [302, 302, 302]}, "prediction_text": "March."}
{"answers": {"text": ["EAT!", "EAT!", "EAT! NewcastleGateshead"], "answer_start": [554, 554, 554]}, "prediction_text": "EAT! NewcastleGateshead"}
{"answers": {"text": ["2", "2", "2 weeks each year"], "answer_start": [618, 618, 618]}, "prediction_text": "2 weeks."}
{"answers": {"text": ["The Hoppings", "The Hoppings", "The Hoppings"], "answer_start": [0, 0, 0]}, "prediction_text": "Hoppings"}
{"answers": {"text": ["every June", "every June", "every June"], "answer_start": [98, 98, 98]}, "prediction_text": "June."}
{"answers": {"text": ["Temperance", "the Temperance Movement", "the Temperance Movement"], "answer_start": [143, 139, 139]}, "prediction_text": "Temperance Movement"}
{"answers": {"text": ["a cycling festival", "a cycling festival", "cycling festival,"], "answer_start": [432, 432, 434]}, "prediction_text": "Cycling festival"}
{"answers": {"text": ["The Northern Pride Festival", "The Northern Pride Festival and Parade", "The Northern Pride Festival"], "answer_start": [509, 509, 509]}, "prediction_text": "Northern Pride Festival"}
{"answers": {"text": ["Newcastle Mela", "Newcastle Mela", "Newcastle Mela"], "answer_start": [0, 0, 0]}, "prediction_text": "Newcastle Mela"}
{"answers": {"text": ["Sage Gateshead Music and Arts Centre", "Sage Gateshead Music and Arts Centre", "Gateshead Music and Arts Centre"], "answer_start": [306, 306, 311]}, "prediction_text": "Sage Gateshead Music and Arts Centre."}
{"answers": {"text": ["Design Event festival", "Design Event festival", "Design Event festival"], "answer_start": [382, 382, 382]}, "prediction_text": "Design Event Festival"}
{"answers": {"text": ["East Asian", "East Asian", "East Asian"], "answer_start": [549, 549, 549]}, "prediction_text": "Design Event"}
{"answers": {"text": ["NewcastleGateshead", "NewcastleGateshead", "NewcastleGateshead"], "answer_start": [188, 188, 188]}, "prediction_text": "NewcastleGateshead"}
{"answers": {"text": ["folk-rock", "folk-rock", "folk-rock"], "answer_start": [18, 18, 18]}, "prediction_text": "Folk rock"}
{"answers": {"text": ["1971", "1971", "1971"], "answer_start": [112, 112, 112]}, "prediction_text": "1971"}
{"answers": {"text": ["Venom", "Venom", "Venom"], "answer_start": [180, 180, 180]}, "prediction_text": "Venom"}
{"answers": {"text": ["Skyclad", "Skyclad", "Skyclad"], "answer_start": [351, 351, 351]}, "prediction_text": "Venom"}
{"answers": {"text": ["Duran Duran", "Duran Duran", "Duran Duran"], "answer_start": [533, 533, 533]}, "prediction_text": "Duran Duran"}
{"answers": {"text": ["November 2006 and May 2008", "November 2006 and May 2008", "between November 2006 and May 2008"], "answer_start": [52, 52, 44]}, "prediction_text": "November 2006 and May 2008."}
{"answers": {"text": ["Old Town Hall", "the Old Town Hall", "the Old Town Hall,"], "answer_start": [140, 136, 136]}, "prediction_text": "Old Town Hall"}
{"answers": {"text": ["three", "three", "three cinemas"], "answer_start": [284, 284, 284]}, "prediction_text": "3"}
{"answers": {"text": ["Classic", "Classic", "Classic"], "answer_start": [322, 322, 322]}, "prediction_text": "Classic"}
{"answers": {"text": ["roof", "a roof extension", "a roof"], "answer_start": [437, 435, 435]}, "prediction_text": "Tyneside Bar"}
{"answers": {"text": ["Centre for Life", "the Centre for Life", "Tyneside"], "answer_start": [68, 64, 161]}, "prediction_text": "Centre for Life"}
{"answers": {"text": ["life on Tyneside", "life on Tyneside", "life on Tyneside,"], "answer_start": [153, 153, 153]}, "prediction_text": "Life on Tyneside."}
{"answers": {"text": ["shipbuilding", "shipbuilding", "shipbuilding"], "answer_start": [192, 192, 192]}, "prediction_text": "Shipbuilding heritage."}
{"answers": {"text": ["2009", "2009", "2009"], "answer_start": [282, 282, 282]}, "prediction_text": "2009"}
{"answers": {"text": ["Seven Stories", "Seven Stories", "Seven Stories"], "answer_start": [384, 384, 384]}, "prediction_text": "Seven Stories"}
{"answers": {"text": ["On the Night of the Fire", "On the Night of the Fire", "On the Night of the Fire (1939),"], "answer_start": [78, 78, 78]}, "prediction_text": "On the Night of the Fire (1939)"}
{"answers": {"text": ["Get Carter", "Get Carter", "Get Carter"], "answer_start": [294, 294, 294]}, "prediction_text": "Stormy Monday"}
{"answers": {"text": ["gangster", "noir thriller", "gangster film,"], "answer_start": [478, 507, 478]}, "prediction_text": "Noir thriller."}
{"answers": {"text": ["Mike Figgis", "Mike Figgis", "Mike Figgis"], "answer_start": [548, 548, 548]}, "prediction_text": "Mike Figgis"}
{"answers": {"text": ["Sting", "Sting", "Sting"], "answer_start": [608, 608, 608]}, "prediction_text": "Tommy Lee Jones"}
{"answers": {"text": ["Gosforth Park", "Gosforth Park", "Gosforth Park"], "answer_start": [39, 39, 39]}, "prediction_text": "Gosforth Park"}
{"answers": {"text": ["the Newcastle Eagles", "the Newcastle Eagles", "Newcastle Eagles"], "answer_start": [79, 79, 83]}, "prediction_text": "Newcastle Eagles"}
{"answers": {"text": ["Newcastle Diamonds", "Newcastle Diamonds", "Newcastle Diamonds"], "answer_start": [322, 322, 322]}, "prediction_text": "Newcastle Diamonds"}
{"answers": {"text": ["Brough Park", "Brough Park", "at Brough Park in Byker"], "answer_start": [354, 354, 351]}, "prediction_text": "Brough Park"}
{"answers": {"text": ["Blaydon Race", "Blaydon Race", "Blaydon Race"], "answer_start": [727, 727, 727]}, "prediction_text": "Blaydon Race"}
{"answers": {"text": ["6 miles", "6", "6 miles"], "answer_start": [57, 57, 57]}, "prediction_text": "6 miles (9.7 km)"}
{"answers": {"text": ["Metro Light Rail system", "Metro Light Rail", "via the Metro Light Rail system"], "answer_start": [251, 251, 243]}, "prediction_text": "Metro Light Rail system."}
{"answers": {"text": ["20 minutes", "20 minutes", "20 minutes"], "answer_start": [336, 336, 336]}, "prediction_text": "20 minutes."}
{"answers": {"text": ["over five million", "over five million", "over five million passengers"], "answer_start": [368, 368, 368]}, "prediction_text": "5 million passengers per year."}
{"answers": {"text": ["over 90", "over 90", "over 90 destinations"], "answer_start": [579, 579, 579]}, "prediction_text": "90"}
{"answers": {"text": ["Victorian architecture", "Victorian architecture", "the Victorian architecture"], "answer_start": [115, 115, 111]}, "prediction_text": "Victorian architecture."}
{"answers": {"text": ["six", "six", "six"], "answer_start": [225, 225, 225]}, "prediction_text": "Six."}
{"answers": {"text": ["Victoria", "Victoria", "Queen Victoria"], "answer_start": [298, 298, 292]}, "prediction_text": "Queen Victoria"}
{"answers": {"text": ["Robert Stephenson.", "Robert Stephenson", "John Dobson"], "answer_start": [519, 519, 464]}, "prediction_text": "Robert Stephenson"}
{"answers": {"text": ["Manors", "Manors", "Manors"], "answer_start": [754, 754, 754]}, "prediction_text": "Manors"}
{"answers": {"text": ["half-hourly", "half-hourly", "half-hourly"], "answer_start": [51, 51, 51]}, "prediction_text": "Half-hourly."}
{"answers": {"text": ["about three", "three", "about three hours"], "answer_start": [130, 136, 130]}, "prediction_text": "3 hours."}
{"answers": {"text": ["Edinburgh", "Edinburgh", "Edinburgh"], "answer_start": [293, 293, 293]}, "prediction_text": "Edinburgh"}
{"answers": {"text": ["CrossCountry", "CrossCountry", "CrossCountry"], "answer_start": [377, 377, 377]}, "prediction_text": "CrossCountry trains."}
{"answers": {"text": ["Northern Rail", "Northern Rail", "Northern Rail"], "answer_start": [537, 537, 537]}, "prediction_text": "Northern Rail"}
{"answers": {"text": ["Tyne and Wear Metro", "the Tyne and Wear Metro", "the Tyne and Wear Metro"], "answer_start": [26, 22, 22]}, "prediction_text": "Tyne and Wear Metro."}
{"answers": {"text": ["five", "five", "five phases"], "answer_start": [142, 142, 142]}, "prediction_text": "Five."}
{"answers": {"text": ["deep-level", "deep-level", "deep-level tunnels"], "answer_start": [369, 369, 369]}, "prediction_text": "Deep-level tunnels."}
{"answers": {"text": ["A bridge", "A bridge", "A bridge"], "answer_start": [431, 431, 431]}, "prediction_text": "Bridge across the Tyne."}
{"answers": {"text": ["over 37 million", "over 37 million", "over 37 million passengers"], "answer_start": [609, 609, 609]}, "prediction_text": "37 million passengers a year."}
{"answers": {"text": ["Metro: All Change.'", "Metro: All Change", "Metro: All Change"], "answer_start": [90, 90, 90]}, "prediction_text": "Metro: All Change"}
{"answers": {"text": ["smart ticketing", "smart ticketing", "smart ticketing."], "answer_start": [237, 237, 237]}, "prediction_text": "Smart ticketing."}
{"answers": {"text": ["tracks, signalling and overhead wires", "tracks, signalling and overhead wires", "tracks, signalling and overhead wires"], "answer_start": [439, 439, 439]}, "prediction_text": "Tracks, signalling, and overhead wires."}
{"answers": {"text": ["an entirely new fleet of trains", "new fleet of trains", "an entirely new fleet of trains"], "answer_start": [549, 561, 549]}, "prediction_text": "Trams."}
{"answers": {"text": ["trams", "trams", "trams"], "answer_start": [865, 865, 865]}, "prediction_text": "Trams"}
{"answers": {"text": ["the A1", "A1", "the A1 (Gateshead Newcastle Western Bypass)"], "answer_start": [32, 36, 32]}, "prediction_text": "A1"}
{"answers": {"text": ["the A696", "A69", "A69"], "answer_start": [241, 211, 211]}, "prediction_text": "A69"}
{"answers": {"text": ["the old \"Great North Road\"", "Great North Road", "Great North Road"], "answer_start": [380, 389, 389]}, "prediction_text": "Great North Road"}
{"answers": {"text": ["the roads", "the roads", "roads"], "answer_start": [707, 707, 711]}, "prediction_text": "A1"}
{"answers": {"text": ["the capacity of the Tyne Tunnel", "capacity of the Tyne Tunnel", "the capacity of the Tyne Tunnel"], "answer_start": [906, 910, 906]}, "prediction_text": "The capacity of the Tyne Tunnel."}
{"answers": {"text": ["3", "3", "3"], "answer_start": [10, 10, 10]}, "prediction_text": "3"}
{"answers": {"text": ["two", "two", "two"], "answer_start": [133, 133, 133]}, "prediction_text": "2"}
{"answers": {"text": ["Stagecoach", "Stagecoach", "Stagecoach"], "answer_start": [532, 532, 532]}, "prediction_text": "Arriva"}
{"answers": {"text": ["the Tyne and Wear Passenger Transport Executive.", "the Tyne and Wear Passenger Transport Executive", "Passenger Transport Executive"], "answer_start": [888, 888, 906]}, "prediction_text": "Tyne and Wear Passenger Transport Executive"}
{"answers": {"text": ["Go-Ahead", "Go-Ahead", "Go-Ahead"], "answer_start": [368, 368, 368]}, "prediction_text": "Stagecoach"}
{"answers": {"text": ["1998", "1998", "1998"], "answer_start": [109, 109, 109]}, "prediction_text": "1998"}
{"answers": {"text": ["highlighting the usage of cycling", "cycling", "highlighting the usage of cycling"], "answer_start": [193, 219, 193]}, "prediction_text": "Cycling."}
{"answers": {"text": ["healthy", "healthy", "healthy living"], "answer_start": [283, 283, 283]}, "prediction_text": "Healthy living."}
{"answers": {"text": ["one way", "one way", "one way streets"], "answer_start": [508, 508, 508]}, "prediction_text": "Busier streets."}
{"answers": {"text": ["national networks", "national networks", "to national networks"], "answer_start": [810, 810, 807]}, "prediction_text": "National cycling networks."}
{"answers": {"text": ["Danish DFDS Seaways", "Danish DFDS Seaways", "DFDS Seaways"], "answer_start": [63, 63, 70]}, "prediction_text": "DFDS Seaways"}
{"answers": {"text": ["end of October 2006", "the end of October 2006", "the end of October 2006"], "answer_start": [187, 183, 183]}, "prediction_text": "October 2006"}
{"answers": {"text": ["high fuel prices and new competition from low-cost air services", "high fuel prices and new competition", "high fuel prices and new competition"], "answer_start": [227, 227, 227]}, "prediction_text": "high fuel prices and new competition from low-cost air services."}
{"answers": {"text": ["late 2008", "late 2008", "late 2008"], "answer_start": [371, 371, 371]}, "prediction_text": "Late 2008."}
{"answers": {"text": ["Thomson", "Thomson", "Thomson cruise lines"], "answer_start": [401, 401, 401]}, "prediction_text": "Thomson Cruise Lines"}
{"answers": {"text": ["eleven", "eleven", "eleven"], "answer_start": [10, 10, 10]}, "prediction_text": "Eleven."}
{"answers": {"text": ["seven", "seven", "seven"], "answer_start": [49, 49, 49]}, "prediction_text": "Seven."}
{"answers": {"text": ["the Royal Grammar School", "the Royal Grammar School", "the Royal Grammar School"], "answer_start": [423, 423, 423]}, "prediction_text": "Royal Grammar School"}
{"answers": {"text": ["Newcastle College", "Newcastle College", "Newcastle College"], "answer_start": [688, 688, 688]}, "prediction_text": "Newcastle College"}
{"answers": {"text": ["Catholic", "Catholic", "Catholic"], "answer_start": [928, 928, 928]}, "prediction_text": "Catholic"}
{"answers": {"text": ["two", "two", "two"], "answer_start": [13, 13, 13]}, "prediction_text": "2"}
{"answers": {"text": ["Newcastle University", "Newcastle University", "Newcastle University"], "answer_start": [81, 81, 81]}, "prediction_text": "Newcastle University"}
{"answers": {"text": ["Sunday Times University of the Year award", "Sunday Times University of the Year", "Sunday Times University of the Year award"], "answer_start": [386, 386, 386]}, "prediction_text": "Sunday Times University of the Year award."}
{"answers": {"text": ["polytechnics became new universities", "polytechnics became new universities", "polytechnics became new universities"], "answer_start": [628, 628, 628]}, "prediction_text": "Northumbria University became the University of Northumbria."}
{"answers": {"text": ["Northumbria University", "Northumbria University", "Northumbria University"], "answer_start": [666, 666, 666]}, "prediction_text": "Northumbria University"}
{"answers": {"text": ["three", "three", "three"], "answer_start": [14, 14, 14]}, "prediction_text": "3"}
{"answers": {"text": ["1474", "1474", "1474"], "answer_start": [93, 93, 93]}, "prediction_text": "1474"}
{"answers": {"text": ["Coptic", "Coptic", "Coptic Cathedral"], "answer_start": [170, 170, 170]}, "prediction_text": "St. Mary's"}
{"answers": {"text": ["Thomas", "the Church of St Thomas", "the Church of St Thomas the Martyr"], "answer_start": [392, 375, 375]}, "prediction_text": "St Thomas the Martyr"}
{"answers": {"text": ["parish churches", "parish churches", "as parish churches"], "answer_start": [248, 248, 245]}, "prediction_text": "Parish churches."}
{"answers": {"text": ["The Parish Church of St Andrew", "The Parish Church of St Andrew", "The Parish Church of St Andrew is"], "answer_start": [0, 0, 0]}, "prediction_text": "St Andrew"}
{"answers": {"text": ["1726", "1726", "1726"], "answer_start": [223, 223, 223]}, "prediction_text": "1726"}
{"answers": {"text": ["the main porch", "the main porch", "the main porch"], "answer_start": [205, 205, 205]}, "prediction_text": "The main porch."}
{"answers": {"text": ["ancient churchyards", "ancient churchyards", "ancient churchyards"], "answer_start": [568, 568, 568]}, "prediction_text": "The last of the ancient churchyards."}
{"answers": {"text": ["The church tower", "The church tower", "The church tower"], "answer_start": [706, 706, 706]}, "prediction_text": "The church tower."}
{"answers": {"text": ["City Road", "City Road", "City Road"], "answer_start": [27, 27, 27]}, "prediction_text": "City Road"}
{"answers": {"text": ["a new facility", "The Watermark business park", "The Watermark business park"], "answer_start": [109, 127, 127]}, "prediction_text": "The Watermark business park."}
{"answers": {"text": ["The entrance to studio 5", "The entrance to studio 5", "Road complex"], "answer_start": [193, 193, 230]}, "prediction_text": "The Tube"}
{"answers": {"text": ["result of its colouring", "its colouring", "its colouring,"], "answer_start": [432, 442, 442]}, "prediction_text": "Colouring."}
{"answers": {"text": ["BBC Radio Newcastle", "BBC Radio Newcastle", "BBC Radio Newcastle"], "answer_start": [599, 599, 599]}, "prediction_text": "BBC Radio Newcastle"}
{"answers": {"text": ["NE1fm", "NE1fm", "NE1fm"], "answer_start": [0, 0, 0]}, "prediction_text": "NE1fm"}
{"answers": {"text": ["Newcastle Student Radio", "Newcastle Student Radio", "Newcastle Student Radio"], "answer_start": [88, 88, 88]}, "prediction_text": "Newcastle Student Radio"}
{"answers": {"text": ["since 1951", "1951", "since 1951"], "answer_start": [364, 370, 364]}, "prediction_text": "1951"}
{"answers": {"text": ["Radio Lollipop", "Radio Lollipop", "Radio Lollipop"], "answer_start": [434, 434, 434]}, "prediction_text": "Radio Tyneside"}
{"answers": {"text": ["Newcastle University's student's union building", "Newcastle University", "Newcastle University's student's union building"], "answer_start": [187, 187, 187]}, "prediction_text": "Newcastle University's student union building."}
{"answers": {"text": ["1770", "1770", "1770"], "answer_start": [137, 137, 137]}, "prediction_text": "1770"}
{"answers": {"text": ["Archbishop of Westminster", "Archbishop of Westminster", "Archbishop of Westminster"], "answer_start": [155, 155, 155]}, "prediction_text": "Archbishop of Westminster"}
{"answers": {"text": ["George Stephenson", "George Stephenson", "George Stephenson,"], "answer_start": [552, 552, 552]}, "prediction_text": "Isaac Lowthian Bell"}
{"answers": {"text": ["the incandescent light bulb", "the steam turbine", "the steam turbine"], "answer_start": [693, 642, 642]}, "prediction_text": "The incandescent light bulb."}
{"answers": {"text": ["Thailand", "Thailand", "Thailand"], "answer_start": [960, 960, 960]}, "prediction_text": "Thailand"}
{"answers": {"text": ["Rutherford Grammar School", "Rutherford Grammar School", "Rutherford Grammar School,"], "answer_start": [160, 160, 160]}, "prediction_text": "Rutherford Grammar School"}
{"answers": {"text": ["international footballers", "international footballers", "international footballers"], "answer_start": [255, 255, 255]}, "prediction_text": "Footballers"}
{"answers": {"text": ["Nobel Prize", "Nobel Prize", "Nobel Prize"], "answer_start": [383, 383, 383]}, "prediction_text": "Nobel Prize"}
{"answers": {"text": ["keyed Northumbrian smallpipes", "keyed Northumbrian smallpipes", "inventor of keyed Northumbrian smallpipes"], "answer_start": [508, 508, 496]}, "prediction_text": "Keyed Northumbrian smallpipes."}
{"answers": {"text": ["Newcastle", "Newcastle", "Newcastle."], "answer_start": [94, 327, 327]}, "prediction_text": "Newcastle"}
{"answers": {"text": ["The V&A is located in the Brompton district of the Royal Borough of Kensington and Chelsea", "London", "London", "in the Brompton district of the Royal Borough of Kensington and Chelsea"], "answer_start": [265, 63, 63, 284]}, "prediction_text": "London"}
{"answers": {"text": ["a permanent collection of over 4.5 million objects.", "over 4.5 million objects", "over 4.5 million", "4.5 million"], "answer_start": [140, 166, 166, 171]}, "prediction_text": "4.5 million"}
{"answers": {"text": ["It was founded in 1852", "1852", "1852", "1852"], "answer_start": [192, 210, 210, 210]}, "prediction_text": "1852"}
{"answers": {"text": ["named after Queen Victoria and Prince Albert", "Queen Victoria and Prince Albert", "Queen Victoria and Prince Albert", "Queen Victoria and Prince Albert"], "answer_start": [219, 231, 231, 231]}, "prediction_text": "Queen Victoria and Prince Albert"}
{"answers": {"text": ["Royal Borough of Kensington and Chelsea", "Brompton district of the Royal Borough of Kensington and Chelsea", "Brompton district of the Royal Borough of Kensington and Chelsea,", "Royal"], "answer_start": [316, 291, 291, 316]}, "prediction_text": "Kensington and Chelsea"}
{"answers": {"text": ["1852", "1852", "1852", "1852"], "answer_start": [210, 210, 210, 210]}, "prediction_text": "1852"}
{"answers": {"text": ["Queen Victoria and Prince Albert", "Queen Victoria and Prince Albert", "Queen Victoria and Prince Albert", "Queen Victoria and Prince Albert"], "answer_start": [231, 231, 231, 231]}, "prediction_text": "Queen Victoria and Prince Albert."}
{"answers": {"text": ["Department for Culture, Media and Sport", "Department for Culture, Media and Sport.", "the Department for Culture, Media and Sport.", "the Department for Culture, Media and Sport"], "answer_start": [691, 691, 687, 687]}, "prediction_text": "Department for Culture, Media and Sport"}
{"answers": {"text": ["2001", "2001", "2001", "2001"], "answer_start": [812, 812, 812, 812]}, "prediction_text": "2001"}
{"answers": {"text": ["12.5", "12.5 acres", "12.5 acres", "12.5"], "answer_start": [15, 15, 15, 15]}, "prediction_text": "12.5 acres."}
{"answers": {"text": ["145", "145", "145 galleries", "145"], "answer_start": [42, 42, 42, 42]}, "prediction_text": "145"}
{"answers": {"text": ["5,000", "5,000", "5,000 years", "5,000"], "answer_start": [78, 78, 78, 78]}, "prediction_text": "5,000 years."}
{"answers": {"text": ["Europe, North America, Asia and North Africa", "Europe, North America, Asia and North Africa", "Europe, North America, Asia and North Africa", "Europe, North America, Asia and North Africa"], "answer_start": [158, 158, 158, 158]}, "prediction_text": "Europe, North America, Asia, North Africa."}
{"answers": {"text": ["post-classical sculpture", "post-classical sculpture", "post-classical sculpture", "post-classical"], "answer_start": [484, 484, 484, 484]}, "prediction_text": "Post-classical sculpture."}
{"answers": {"text": ["Great Exhibition of 1851", "Great Exhibition of 1851", "Great Exhibition"], "answer_start": [31, 31, 31]}, "prediction_text": "Great Exhibition of 1851"}
{"answers": {"text": ["Henry Cole", "Henry Cole,", "Henry Cole"], "answer_start": [68, 68, 68]}, "prediction_text": "Henry Cole"}
{"answers": {"text": ["Museum of Manufactures", "Museum of Manufactures", "Museum of Manufactures"], "answer_start": [165, 165, 165]}, "prediction_text": "Museum of Manufactures"}
{"answers": {"text": ["Somerset House", "Somerset House", "Somerset House"], "answer_start": [278, 278, 278]}, "prediction_text": "Somerset House"}
{"answers": {"text": ["Gottfried Semper", "Gottfried Semper", "Gottfried Semper"], "answer_start": [619, 619, 619]}, "prediction_text": "Gottfried Semper"}
{"answers": {"text": ["Queen Victoria", "Queen Victoria", "Queen Victoria"], "answer_start": [24, 24, 24]}, "prediction_text": "Queen Victoria"}
{"answers": {"text": ["22 June 1857", "22 June 1857", "22 June 1857"], "answer_start": [46, 46, 46]}, "prediction_text": "22 June 1857"}
{"answers": {"text": ["George Wallis", "George Wallis", "George Wallis"], "answer_start": [599, 599, 599]}, "prediction_text": "George Wallis"}
{"answers": {"text": ["late night openings", "late night openings", "late night openings"], "answer_start": [83, 83, 83]}, "prediction_text": "Late night openings."}
{"answers": {"text": ["1949", "1949", "1949"], "answer_start": [1029, 1029, 1029]}, "prediction_text": "1949"}
{"answers": {"text": ["between September and November 1946", "between September and November 1946,", "between September and November 1946"], "answer_start": [96, 96, 96]}, "prediction_text": "September and November 1946"}
{"answers": {"text": ["nearly a million and a half", "nearly a million and a half visitors", "nearly a million and a half"], "answer_start": [144, 144, 144]}, "prediction_text": "Nearly a million and a half."}
{"answers": {"text": ["Festival of Britain (1951)", "Council of Industrial Design", "the Council of Industrial Design"], "answer_start": [442, 208, 204]}, "prediction_text": "Council of Industrial Design"}
{"answers": {"text": ["Festival of Britain", "Festival of Britain", "the Festival of Britain"], "answer_start": [442, 442, 438]}, "prediction_text": "Festival of Britain (1951)"}
{"answers": {"text": ["1948", "1948", "1948"], "answer_start": [473, 473, 473]}, "prediction_text": "1948"}
{"answers": {"text": ["a rock concert", "present a rock concert", "a rock concert"], "answer_start": [119, 111, 119]}, "prediction_text": "Rock concert."}
{"answers": {"text": ["Gryphon", "Gryphon", "Gryphon"], "answer_start": [218, 218, 218]}, "prediction_text": "Gryphon"}
{"answers": {"text": ["Roy Strong", "Roy Strong", "Roy Strong"], "answer_start": [463, 463, 463]}, "prediction_text": "Roy Strong"}
{"answers": {"text": ["mediaeval music", "explored the lineage of mediaeval music and instrumentation and related how those contributed to contemporary music 500 years later", "mediaeval"], "answer_start": [255, 231, 255]}, "prediction_text": "British progressive folk-rock band Gryphon."}
{"answers": {"text": ["Dundee", "Dundee", "Dundee"], "answer_start": [179, 179, 179]}, "prediction_text": "Dundee"}
{"answers": {"text": ["\u00a376 million", "\u00a376 million", "\u00a376 million"], "answer_start": [315, 315, 315]}, "prediction_text": "\u00a376 million"}
{"answers": {"text": ["on the city's waterfront", "on the city's waterfront", "on the city's waterfront"], "answer_start": [421, 421, 421]}, "prediction_text": "Dundee City Council"}
{"answers": {"text": ["fashion, architecture, product design, graphic arts and photography", "fashion, architecture, product design, graphic arts and photography", "fashion, architecture, product design, graphic arts and photography"], "answer_start": [474, 474, 474]}, "prediction_text": "Fashion, architecture, product design, graphic arts, photography."}
{"answers": {"text": ["within five years", "within five years"], "answer_start": [576, 576]}, "prediction_text": "Five years."}
{"answers": {"text": ["Brompton Park House", "Brompton Park House", "Brompton Park House"], "answer_start": [302, 302, 302]}, "prediction_text": "South Kensington."}
{"answers": {"text": ["Sheepshanks Gallery", "Sheepshanks Gallery", "1857"], "answer_start": [601, 601, 624]}, "prediction_text": "Sheepshanks Gallery."}
{"answers": {"text": ["Captain Francis Fowke", "Captain Francis Fowke", "Captain Francis Fowke"], "answer_start": [697, 697, 697]}, "prediction_text": "Captain Francis Fowke"}
{"answers": {"text": ["Secretariat Wing", "June 1862", "June 1862"], "answer_start": [1257, 1089, 1089]}, "prediction_text": "1862"}
{"answers": {"text": ["offices and board room", "houses the offices and board room etc. and is not open to the public", "the offices and board room etc."], "answer_start": [1310, 1299, 1306]}, "prediction_text": "The Secretariat Wing houses the offices and board room etc. and is not open to the public."}
{"answers": {"text": ["Oriental Courts", "Owen Jones", "Owen Jones"], "answer_start": [536, 509, 509]}, "prediction_text": "Owen Jones"}
{"answers": {"text": ["Italian Renaissance", "Italian Renaissance", "Italian Renaissance"], "answer_start": [1369, 1369, 1369]}, "prediction_text": "Italian Renaissance."}
{"answers": {"text": ["James Gamble & Reuben Townroe", "James Gamble & Reuben Townroe", "James Gamble & Reuben Townroe"], "answer_start": [1541, 1541, 1541]}, "prediction_text": "James Gamble & Reuben Townroe."}
{"answers": {"text": ["Isaac Newton", "Isaac Newton (astronomy", "Isaac Newton"], "answer_start": [1626, 1626, 1626]}, "prediction_text": "Isaac Newton"}
{"answers": {"text": ["Titian", "Michelangelo (sculpture)", "Michelangelo"], "answer_start": [1727, 1701, 1701]}, "prediction_text": "James Gamble & Reuben Townroe"}
{"answers": {"text": ["Philip Webb and William Morris", "Philip Webb and William Morris", "Philip Webb and William Morris"], "answer_start": [129, 129, 129]}, "prediction_text": "Philip Webb and William Morris."}
{"answers": {"text": ["Edward Burne-Jones", "Edward Burne-Jones", "Edward Burne-Jones"], "answer_start": [452, 452, 452]}, "prediction_text": "Edward Burne-Jones"}
{"answers": {"text": ["James Gamble", "James Gamble,", "James Gamble"], "answer_start": [547, 547, 547]}, "prediction_text": "James Gamble"}
{"answers": {"text": ["Alfred Stevens", "Alfred Stevens", "Alfred Stevens"], "answer_start": [804, 804, 804]}, "prediction_text": "Alfred Stevens"}
{"answers": {"text": ["Sir Edward Poynter", "Sir Edward Poynter", "Sir Edward Poynter"], "answer_start": [941, 941, 941]}, "prediction_text": "Sir Edward Poynter"}
{"answers": {"text": ["Henry Young Darracott Scott", "Colonel (later Major General) Henry Young Darracott Scott,", "Colonel (later Major General) Henry Young Darracott Scott"], "answer_start": [132, 102, 102]}, "prediction_text": "Colonel (later Major General) Henry Young Darracott Scott."}
{"answers": {"text": ["School for Naval Architects", "the science schools", "School for Naval Architects"], "answer_start": [250, 293, 250]}, "prediction_text": "School for Naval Architects"}
{"answers": {"text": ["Cadeby stone", "Cadeby stone", "Cadeby"], "answer_start": [467, 467, 467]}, "prediction_text": "Cadeby stone"}
{"answers": {"text": ["prints and architectural drawings", "(prints, drawings, paintings and photographs)", "prints and architectural drawings"], "answer_start": [604, 649, 604]}, "prediction_text": "Prints and architectural drawings."}
{"answers": {"text": ["2008", "2008", "2008"], "answer_start": [838, 838, 838]}, "prediction_text": "2008"}
{"answers": {"text": ["sgraffito", "sgraffito", "sgraffito"], "answer_start": [179, 179, 179]}, "prediction_text": "sgraffito"}
{"answers": {"text": ["Starkie Gardner", "Starkie Gardner", "Starkie Gardner"], "answer_start": [348, 348, 348]}, "prediction_text": "Starkie Gardner"}
{"answers": {"text": ["southeast of the garden", "the southeast of the garden (the site of the \"Brompton Boilers\"),", "southeast of the garden"], "answer_start": [466, 462, 466]}, "prediction_text": "Southeast of the garden."}
{"answers": {"text": ["Art Library", "the south side of the garden", "south side of the garden"], "answer_start": [755, 808, 812]}, "prediction_text": "South side of the garden."}
{"answers": {"text": ["Reuben Townroe", "Reuben Townroe", "Reuben Townroe"], "answer_start": [912, 912, 912]}, "prediction_text": "Reuben Townroe"}
{"answers": {"text": ["Aston Webb", "Aston Webb", "Aston Webb"], "answer_start": [128, 128, 128]}, "prediction_text": "Aston Webb"}
{"answers": {"text": ["red brick and Portland stone", "red brick and Portland stone", "red brick and Portland stone"], "answer_start": [28, 28, 28]}, "prediction_text": "Red brick and Portland stone."}
{"answers": {"text": ["720 feet", "720 feet", "720 feet"], "answer_start": [68, 68, 68]}, "prediction_text": "720 feet (220 m)"}
{"answers": {"text": ["a statue of fame", "statue of fame", "an open work crown surmounted by a statue of fame"], "answer_start": [636, 638, 603]}, "prediction_text": "Statue of fame."}
{"answers": {"text": ["top row of windows", "top row of windows", "the top row of windows"], "answer_start": [851, 851, 847]}, "prediction_text": "The top row of windows."}
{"answers": {"text": ["Alfred Drury", "Alfred Drury.", "Alfred Drury"], "answer_start": [144, 144, 144]}, "prediction_text": "Alfred Drury"}
{"answers": {"text": ["four", "four levels", "four"], "answer_start": [181, 181, 181]}, "prediction_text": "Four."}
{"answers": {"text": ["Alfred Drury", "Webb", "Webb"], "answer_start": [144, 231, 231]}, "prediction_text": "Alfred Drury"}
{"answers": {"text": ["marble", "marble", "marble"], "answer_start": [414, 414, 414]}, "prediction_text": "Marble"}
{"answers": {"text": ["Queen Victoria", "Prince Albert", "Queen Victoria"], "answer_start": [69, 0, 69]}, "prediction_text": "Queen Victoria"}
{"answers": {"text": ["Art Library", "new storage space for books in the Art Library", "new storage space for books in the Art Library"], "answer_start": [241, 206, 206]}, "prediction_text": "Art Library"}
{"answers": {"text": ["Henry Cole wing", "Henry Cole wing", "Henry Cole wing"], "answer_start": [706, 706, 706]}, "prediction_text": "Henry Cole wing."}
{"answers": {"text": ["a new entrance building", "new entrance building", "a new entrance building"], "answer_start": [931, 933, 931]}, "prediction_text": "The Spiral."}
{"answers": {"text": ["Christopher Hay and Douglas Coyne", "Christopher Hay and Douglas Coyne", "Christopher Hay and Douglas Coyne"], "answer_start": [1164, 1164, 1164]}, "prediction_text": "Christopher Hay and Douglas Coyne."}
{"answers": {"text": ["the Spiral", "the Spiral,", "the Spiral"], "answer_start": [1032, 1032, 1032]}, "prediction_text": "The Spiral."}
{"answers": {"text": ["main silverware gallery", "the main glass galleries and the main silverware gallery", "silverware"], "answer_start": [130, 97, 135]}, "prediction_text": "Indian, Japanese, Chinese, iron work, main glass galleries, main silverware gallery."}
{"answers": {"text": ["mosaic floors", "the mosaic floors in the sculpture gallery", "mosaic floors"], "answer_start": [414, 410, 414]}, "prediction_text": "The mosaic floors."}
{"answers": {"text": ["FuturePlan", "FuturePlan", "FuturePlan"], "answer_start": [708, 708, 708]}, "prediction_text": "FuturePlan"}
{"answers": {"text": ["South Kensington", "South Kensington", "South Kensington"], "answer_start": [1097, 1097, 1097]}, "prediction_text": "South Kensington."}
{"answers": {"text": ["McInnes Usher McKnight Architects", "Gareth Hoskins was responsible for contemporary and architecture, Softroom, Islamic Middle East and the Members' Room, McInnes Usher McKnight Architects", "McInnes Usher McKnight Architects"], "answer_start": [1812, 1693, 1812]}, "prediction_text": "Gareth Hoskins"}
{"answers": {"text": ["Kim Wilkie", "Kim Wilkie", "Kim Wilkie"], "answer_start": [37, 37, 37]}, "prediction_text": "Kim Wilkie"}
{"answers": {"text": ["John Madejski Garden", "the John Madejski Garden", "John Madejski Garden"], "answer_start": [66, 62, 66]}, "prediction_text": "John Madejski Garden"}
{"answers": {"text": ["elliptical", "elliptical", "elliptical"], "answer_start": [198, 198, 198]}, "prediction_text": "Elliptical."}
{"answers": {"text": ["receptions, gatherings or exhibition purposes", "receptions, gatherings or exhibition purposes", "receptions, gatherings or exhibition purposes"], "answer_start": [306, 306, 306]}, "prediction_text": "receptions, gatherings, exhibition purposes."}
{"answers": {"text": ["American Sweetgum", "orange and lemon trees", "American Sweetgum"], "answer_start": [685, 796, 685]}, "prediction_text": "American Sweetgum."}
{"answers": {"text": ["2004", "2004", "2004"], "answer_start": [3, 3, 3]}, "prediction_text": "2004"}
{"answers": {"text": ["Royal Institute of British Architects", "Royal Institute of British Architects", "Royal Institute of British Architects"], "answer_start": [27, 27, 27]}, "prediction_text": "Royal Institute of British Architects"}
{"answers": {"text": ["over 600,000", "over 600,000", "600,000"], "answer_start": [413, 413, 418]}, "prediction_text": "600,000"}
{"answers": {"text": ["RIBA Drawings and Archives Collection", "RIBA Drawings and Archives Collection", "RIBA Drawings and Archives Collection"], "answer_start": [276, 276, 276]}, "prediction_text": "Drawings and Archives Collection"}
{"answers": {"text": ["over 700,000", "over 700,000", "700,000"], "answer_start": [479, 479, 484]}, "prediction_text": "700,000"}
{"answers": {"text": ["Andrea Palladio", "Andrea Palladio", "Andrea Palladio"], "answer_start": [235, 235, 235]}, "prediction_text": "Andrea Palladio"}
{"answers": {"text": ["Zaha Hadid", "Andrea Palladio", "Zaha Hadid"], "answer_start": [990, 235, 990]}, "prediction_text": "Zaha Hadid"}
{"answers": {"text": ["over 330", "over 330", "over 330"], "answer_start": [214, 214, 214]}, "prediction_text": "330"}
{"answers": {"text": ["Sir Christopher Wren", "Sir Christopher Wren", "Sir Christopher Wren"], "answer_start": [482, 482, 482]}, "prediction_text": "Andrea Palladio"}
{"answers": {"text": ["Sir Edwin Lutyens", "Zaha Hadid", "Sir Edwin Lutyens"], "answer_start": [852, 990, 852]}, "prediction_text": "Andrea Palladio"}
{"answers": {"text": ["Bishopsgate", "Bishopsgate", "Bishopsgate"], "answer_start": [159, 159, 159]}, "prediction_text": "Bishopsgate"}
{"answers": {"text": ["Great Fire of London", "Great Fire of London", "Great Fire of London"], "answer_start": [248, 248, 248]}, "prediction_text": "Great Fire of London"}
{"answers": {"text": ["c1600", "c1600", "c1600"], "answer_start": [148, 148, 148]}, "prediction_text": "1600"}
{"answers": {"text": ["Montal", "chateau of Montal", "Montal"], "answer_start": [482, 471, 482]}, "prediction_text": "Montal"}
{"answers": {"text": ["Alhambra", "Alhambra", "Alhambra"], "answer_start": [782, 782, 782]}, "prediction_text": "Alhambra"}
{"answers": {"text": ["over 19,000", "over 19,000", "over 19,000"], "answer_start": [14, 14, 14]}, "prediction_text": "19,000"}
{"answers": {"text": ["2006", "2006", "2006"], "answer_start": [184, 184, 184]}, "prediction_text": "2006"}
{"answers": {"text": ["Ardabil Carpet", "the Ardabil Carpet", "Ardabil Carpet"], "answer_start": [266, 262, 266]}, "prediction_text": "The Ardabil Carpet."}
{"answers": {"text": ["Spain", "Spain, North Africa, the Middle East, Central Asia and Afghanistan", "Spain"], "answer_start": [362, 362, 362]}, "prediction_text": "Spain"}
{"answers": {"text": ["1909", "1909", "1909"], "answer_start": [1016, 1016, 1016]}, "prediction_text": "1909"}
{"answers": {"text": ["nearly 60,000", "nearly 60,000", "nearly 60,000"], "answer_start": [123, 123, 123]}, "prediction_text": "60,000"}
{"answers": {"text": ["about 10,000", "10,000", "10,000"], "answer_start": [156, 162, 162]}, "prediction_text": "60,000"}
{"answers": {"text": ["6000", "6000", "6000"], "answer_start": [182, 182, 182]}, "prediction_text": "6000"}
{"answers": {"text": ["1991", "1991", "1991"], "answer_start": [292, 292, 292]}, "prediction_text": "1991"}
{"answers": {"text": ["Jawaharlal Nehru", "Jawaharlal Nehru", "Jawaharlal Nehru"], "answer_start": [242, 242, 242]}, "prediction_text": "Jawaharlal Nehru"}
{"answers": {"text": ["more than 70,000", "more than 70,000", "70,000"], "answer_start": [36, 36, 46]}, "prediction_text": "70,000"}
{"answers": {"text": ["China, Japan and Korea", "China, Japan and Korea", "East Asia: China, Japan and Korea"], "answer_start": [99, 99, 88]}, "prediction_text": "China, Japan, and Korea."}
{"answers": {"text": ["The T. T. Tsui Gallery", "T. T. Tsui Gallery of Chinese art", "The T. T. Tsui Gallery of Chinese art"], "answer_start": [123, 127, 123]}, "prediction_text": "T. T. Tsui Gallery of Chinese art."}
{"answers": {"text": ["1991", "1991", "1991"], "answer_start": [171, 171, 171]}, "prediction_text": "1991"}
{"answers": {"text": ["Ming and Qing", "Ming and Qing", "Ming and Qing"], "answer_start": [381, 381, 381]}, "prediction_text": "Ming and Qing."}
{"answers": {"text": ["Toshiba", "Toshiba", "Toshiba"], "answer_start": [4, 4, 4]}, "prediction_text": "Toshiba"}
{"answers": {"text": ["1986", "1986", "1986"], "answer_start": [55, 55, 55]}, "prediction_text": "1986"}
{"answers": {"text": ["13th", "13th-century", "13th"], "answer_start": [156, 156, 156]}, "prediction_text": "13th century."}
{"answers": {"text": ["from 1550 to 1900", "1550 to 1900", "1550 to 1900"], "answer_start": [91, 96, 96]}, "prediction_text": "1550 to 1900."}
{"answers": {"text": ["bronze", "bronze", "bronze"], "answer_start": [703, 703, 703]}, "prediction_text": "Bronze."}
{"answers": {"text": ["from the 14th to the 19th century", "14th to the 19th century", "14th to the 19th century"], "answer_start": [353, 362, 362]}, "prediction_text": "14th to 19th centuries."}
{"answers": {"text": ["Sri Lanka", "Thailand", "Thailand"], "answer_start": [560, 519, 519]}, "prediction_text": "Sri Lanka"}
{"answers": {"text": ["Hindu and Buddhist sculptures", "Refined Hindu and Buddhist sculptures", "betel-nut cutters, ivory combs and bronze palanquin hooks"], "answer_start": [720, 712, 804]}, "prediction_text": "Bronze, ivory, gold, silver, stone, terracotta, and wood."}
{"answers": {"text": ["mother-of-pearl", "mother-of-pearl", "mother-of-pearl"], "answer_start": [197, 197, 197]}, "prediction_text": "mother-of-pearl."}
{"answers": {"text": ["ivory", "ivory", "ivory"], "answer_start": [823, 823, 823]}, "prediction_text": "Bronze."}
{"answers": {"text": ["Leonardo da Vinci", "Leonardo da Vinci", "Leonardo da Vinci"], "answer_start": [72, 72, 72]}, "prediction_text": "Leonardo da Vinci"}
{"answers": {"text": ["Forster I, Forster II, and Forster III", "Forster I, Forster II, and Forster III,", "Forster I, Forster II, and Forster III"], "answer_start": [161, 161, 161]}, "prediction_text": "Forster I, Forster II, Forster III."}
{"answers": {"text": ["over 14,000", "over 14,000", "over 14,000"], "answer_start": [601, 601, 601]}, "prediction_text": "14,000."}
{"answers": {"text": ["1869", "1869", "1869"], "answer_start": [636, 636, 636]}, "prediction_text": "1869"}
{"answers": {"text": ["1876", "1876", "1876"], "answer_start": [504, 504, 504]}, "prediction_text": "1876"}
{"answers": {"text": ["Charles Dickens", "Charles Dickens", "Charles Dickens"], "answer_start": [58, 58, 58]}, "prediction_text": "Charles Dickens and Beatrix Potter."}
{"answers": {"text": ["Beatrix Potter", "Beatrix Potter", "Beatrix Potter"], "answer_start": [78, 78, 78]}, "prediction_text": "Charles Dickens"}
{"answers": {"text": ["from the 12th to 16th", "12th to 16th", "12th to 16th"], "answer_start": [140, 149, 149]}, "prediction_text": "12th to 16th centuries."}
{"answers": {"text": ["the trial and rehabilitation of Joan of Arc", "trial and rehabilitation of Joan of Arc", "the trial and rehabilitation of Joan of Arc"], "answer_start": [432, 436, 432]}, "prediction_text": "Joan of Arc."}
{"answers": {"text": ["Lucas Horenbout", "Lucas Horenbout", "Lucas Horenbout"], "answer_start": [380, 380, 380]}, "prediction_text": "Lucas Horenbout"}
{"answers": {"text": ["Word and Image Department", "Word and Image Department", "Word and Image Department"], "answer_start": [38, 38, 38]}, "prediction_text": "Word and Image Department"}
{"answers": {"text": ["MODES", "MODES", "MODES"], "answer_start": [235, 235, 235]}, "prediction_text": "MODES cataloging system."}
{"answers": {"text": ["Encoded Archival Description", "Encoded Archival Description (EAD", "Encoded Archival Description (EAD)"], "answer_start": [431, 431, 431]}, "prediction_text": "Encoded Archival Description (EAD)"}
{"answers": {"text": ["newly accessioned into the collection", "newly accessioned", "newly accessioned into the collection"], "answer_start": [579, 579, 579]}, "prediction_text": "Archival material."}
{"answers": {"text": ["Search the Collections", "Search the Collections,", "Search the Collections"], "answer_start": [736, 736, 736]}, "prediction_text": "Search the Collections"}
{"answers": {"text": ["2007", "2007", "2007"], "answer_start": [202, 202, 202]}, "prediction_text": "2007"}
{"answers": {"text": ["Factory Project", "Factory Project", "the Factory Project"], "answer_start": [883, 257, 253]}, "prediction_text": "Factory Project"}
{"answers": {"text": ["Andy Warhol", "Andy Warhol", "Andy Warhol"], "answer_start": [286, 286, 286]}, "prediction_text": "Andy Warhol"}
{"answers": {"text": ["15,000", "15,000", "15,000"], "answer_start": [696, 696, 696]}, "prediction_text": "15,000"}
{"answers": {"text": ["to catalog everything", "catalog everything", "to catalog everything"], "answer_start": [902, 905, 902]}, "prediction_text": "Cataloging."}
{"answers": {"text": ["British patrons", "British patrons", "British patrons"], "answer_start": [150, 150, 150]}, "prediction_text": "British patrons."}
{"answers": {"text": ["Asia", "Asia", "Asia"], "answer_start": [191, 191, 191]}, "prediction_text": "Asia"}
{"answers": {"text": ["Gian Lorenzo Bernini", "Gian Lorenzo Bernini", "Gian Lorenzo Bernini"], "answer_start": [311, 311, 311]}, "prediction_text": "Gian Lorenzo Bernini"}
{"answers": {"text": ["Horace Walpole", "Horace Walpole", "Horace Walpole"], "answer_start": [640, 640, 640]}, "prediction_text": "Horace Walpole"}
{"answers": {"text": ["porcelain, cloth and wallpaper", "porcelain, cloth and wallpaper", "porcelain, cloth and wallpaper"], "answer_start": [207, 207, 207]}, "prediction_text": "Porcelain, cloth, wallpaper."}
{"answers": {"text": ["increase in tea drinking", "increase in tea drinking", "increase in tea drinking"], "answer_start": [470, 470, 470]}, "prediction_text": "European styles."}
{"answers": {"text": ["increasing emphasis on entertainment and leisure", "printed books and the work of European artists and craftsmen employed in Britain", "entertainment and leisure"], "answer_start": [403, 140, 426]}, "prediction_text": "Entertainment and leisure."}
{"answers": {"text": ["John Ruskin", "John Ruskin", "John Ruskin"], "answer_start": [1218, 1218, 1218]}, "prediction_text": "John Ruskin"}
{"answers": {"text": ["the growth of mass production", "the growth of mass production", "mass production"], "answer_start": [667, 667, 681]}, "prediction_text": "Mass production."}
{"answers": {"text": ["Arts and Crafts", "Gothic Revival", "Arts and Crafts"], "answer_start": [1250, 1009, 1250]}, "prediction_text": "Arts and Crafts"}
{"answers": {"text": ["Trajan's Column", "sculpture wing", "Trajan's Column"], "answer_start": [264, 71, 264]}, "prediction_text": "Trajan's Column"}
{"answers": {"text": ["cut in half", "cut in half", "cut in half"], "answer_start": [281, 281, 281]}, "prediction_text": "Cut in half."}
{"answers": {"text": ["David", "David.", "David"], "answer_start": [477, 477, 477]}, "prediction_text": "David"}
{"answers": {"text": ["sculptures, friezes and tombs", "sculptures, friezes and tombs", "sculptures, friezes and tombs"], "answer_start": [180, 180, 180]}, "prediction_text": "sculptures, friezes, tombs."}
{"answers": {"text": ["in a glass case", "glass case", "in a glass case"], "answer_start": [649, 654, 649]}, "prediction_text": "In a glass case."}
{"answers": {"text": ["1731", "1731"], "answer_start": [197, 197]}, "prediction_text": "1731"}
{"answers": {"text": ["Frederick II the Great", "Frederick II the Great", "Frederick II the Great"], "answer_start": [258, 258, 258]}, "prediction_text": "Frederick II the Great"}
{"answers": {"text": ["1762", "1762", "1762"], "answer_start": [250, 250, 250]}, "prediction_text": "1762"}
{"answers": {"text": ["1909", "1909", "1909"], "answer_start": [742, 742, 742]}, "prediction_text": "1909"}
{"answers": {"text": ["Chinese and Japanese ceramics", "Chinese and Japanese ceramics.", "Chinese and Japanese ceramics"], "answer_start": [785, 785, 785]}, "prediction_text": "Chinese and Japanese ceramics."}
{"answers": {"text": ["Josiah Wedgwood, William De Morgan and Bernard Leach", "Josiah Wedgwood, William De Morgan and Bernard Leach", "Josiah Wedgwood, William De Morgan and Bernard Leach"], "answer_start": [29, 29, 29]}, "prediction_text": "Josiah Wedgwood, William De Morgan, Bernard Leach."}
{"answers": {"text": ["Britain and Holland", "Britain and Holland", "Britain and Holland"], "answer_start": [215, 215, 215]}, "prediction_text": "Britain and Holland."}
{"answers": {"text": ["ceramic stoves", "a series of elaborately ornamented ceramic stoves", "a series of elaborately ornamented ceramic stoves"], "answer_start": [488, 453, 453]}, "prediction_text": "Bernard Palissy"}
{"answers": {"text": ["from the 16th and 17th centuries", "16th and 17th centuries,", "16th and 17th centuries"], "answer_start": [503, 512, 512]}, "prediction_text": "16th and 17th centuries."}
{"answers": {"text": ["Germany and Switzerland", "Germany and Switzerland", "Germany and Switzerland"], "answer_start": [545, 545, 545]}, "prediction_text": "Germany and Switzerland."}
{"answers": {"text": ["4000", "4000 years", "4000"], "answer_start": [28, 28, 28]}, "prediction_text": "4000 years."}
{"answers": {"text": ["over 6000", "over 6000", "over 6000"], "answer_start": [64, 64, 64]}, "prediction_text": "6000"}
{"answers": {"text": ["Ancient Egypt", "Ancient Egypt", "Ancient Egypt"], "answer_start": [173, 173, 173]}, "prediction_text": "Ancient Egypt."}
{"answers": {"text": ["Ren\u00e9 Lalique", "Ren\u00e9 Lalique", "Ren\u00e9 Lalique"], "answer_start": [459, 459, 459]}, "prediction_text": "Ren\u00e9 Lalique"}
{"answers": {"text": ["Louis Comfort Tiffany and \u00c9mile Gall\u00e9", "Louis Comfort Tiffany and \u00c9mile Gall\u00e9", "Louis Comfort Tiffany and \u00c9mile Gall\u00e9"], "answer_start": [363, 363, 363]}, "prediction_text": "Louis Comfort Tiffany and \u00c9mile Gall\u00e9."}
{"answers": {"text": ["1994", "1994", "1994"], "answer_start": [35, 35, 35]}, "prediction_text": "1994"}
{"answers": {"text": ["Danny Lane", "Danny Lane,", "Danny Lane"], "answer_start": [109, 109, 109]}, "prediction_text": "Danny Lane"}
{"answers": {"text": ["2004", "2004", "2004"], "answer_start": [171, 171, 171]}, "prediction_text": "2004"}
{"answers": {"text": ["Dale Chihuly", "Dale Chihuly", "Dale Chihuly"], "answer_start": [793, 793, 793]}, "prediction_text": "Dale Chihuly"}
{"answers": {"text": ["13th", "13th-century", "13th"], "answer_start": [552, 552, 552]}, "prediction_text": "13th century"}
{"answers": {"text": ["over 10,000", "over 10,000", "10,000"], "answer_start": [36, 36, 41]}, "prediction_text": "10,000"}
{"answers": {"text": ["2,000", "2,000", "2,000"], "answer_start": [60, 60, 60]}, "prediction_text": "2,000"}
{"answers": {"text": ["D\u00fcrer", "D\u00fcrer", "D\u00fcrer"], "answer_start": [104, 104, 104]}, "prediction_text": "John Martin"}
{"answers": {"text": ["Rembrandt", "Rembrandt", "Rembrandt"], "answer_start": [165, 165, 165]}, "prediction_text": "Rembrandt"}
{"answers": {"text": ["Jean Auguste Dominique Ingres", "Jean Auguste Dominique Ingres,", "Jean Auguste Dominique Ingres"], "answer_start": [326, 326, 326]}, "prediction_text": "Bernardo Buontalenti"}
{"answers": {"text": ["over 14,000", "over 14,000 outfits", "over 14,000", "over 14,000"], "answer_start": [72, 72, 72, 72]}, "prediction_text": "1,442"}
{"answers": {"text": ["Word and Image department", "Word and Image department.", "Word and Image", "Word and Image"], "answer_start": [237, 237, 237, 237]}, "prediction_text": "Word and Image department."}
{"answers": {"text": ["Because everyday clothing from previous eras has not generally survived", "Because everyday clothing from previous eras has not generally survived", "everyday clothing from previous eras has not generally survived", "everyday clothing from previous eras has not generally survived"], "answer_start": [264, 264, 272, 272]}, "prediction_text": "Fashion."}
{"answers": {"text": ["1913", "1913", "1913", "1913"], "answer_start": [470, 470, 470, 470]}, "prediction_text": "1913"}
{"answers": {"text": ["Harrods", "Harrods", "Harrods", "Harrods"], "answer_start": [577, 577, 577, 577]}, "prediction_text": "Harrods"}
{"answers": {"text": ["2002", "2002", "2002"], "answer_start": [3, 3, 3]}, "prediction_text": "2002"}
{"answers": {"text": ["Vivienne Westwood", "Coco Chanel", "Vivienne Westwood"], "answer_start": [59, 146, 59]}, "prediction_text": "Vivienne Westwood"}
{"answers": {"text": ["178", "178", "178"], "answer_start": [55, 55, 55]}, "prediction_text": "178"}
{"answers": {"text": ["Costiff", "Vivienne Westwood", "Costiff"], "answer_start": [33, 59, 33]}, "prediction_text": "Costiff collection"}
{"answers": {"text": ["modern", "modern fashion", "modern"], "answer_start": [463, 463, 463]}, "prediction_text": "Modern fashion."}
{"answers": {"text": ["Italian and French Renaissance", "Italian and French Renaissance objects", "Renaissance"], "answer_start": [27, 27, 46]}, "prediction_text": "1859-1865"}
{"answers": {"text": ["between 1859 and 1865", "1859 and 1865", "between 1859 and 1865"], "answer_start": [79, 87, 79]}, "prediction_text": "1859-1865"}
{"answers": {"text": ["French 18th-century art and furnishings", "art and furnishings", "art and furnishings"], "answer_start": [161, 181, 181]}, "prediction_text": "French 18th-century art and furnishings."}
{"answers": {"text": ["1882", "1882", "1882"], "answer_start": [227, 227, 227]}, "prediction_text": "1882"}
{"answers": {"text": ["\u00a3250,000", "\u00a3250,000", "\u00a3250,000"], "answer_start": [248, 248, 248]}, "prediction_text": "\u00a3250,000."}
{"answers": {"text": ["1580", "1580", "1580"], "answer_start": [49, 49, 49]}, "prediction_text": "1580"}
{"answers": {"text": ["Hans Vredeman de Vries", "Hans Vredeman de Vries", "Hans Vredeman de Vries"], "answer_start": [92, 92, 92]}, "prediction_text": "Hans Vredeman de Vries"}
{"answers": {"text": ["c1750", "c1750", "c1750"], "answer_start": [232, 232, 232]}, "prediction_text": "1750"}
{"answers": {"text": ["Germany", "Germany", "Germany"], "answer_start": [243, 243, 243]}, "prediction_text": "Germany"}
{"answers": {"text": ["Charles and Ray Eames", "Charles and Ray Eames", "Charles and Ray Eames"], "answer_start": [815, 815, 815]}, "prediction_text": "Ernest Gimson, Edward William Godwin, Charles Voysey, Adolf Loos, and Otto Wagner."}
{"answers": {"text": ["over 6000", "over 6000", "over 6000"], "answer_start": [37, 37, 37]}, "prediction_text": "6000"}
{"answers": {"text": ["Ancient Egypt", "Ancient Egypt", "Ancient Egypt"], "answer_start": [166, 166, 166]}, "prediction_text": "Ancient Egypt"}
{"answers": {"text": ["1869", "1869"], "answer_start": [937, 937]}, "prediction_text": "1869"}
{"answers": {"text": ["154", "1869,", "154"], "answer_start": [914, 937, 914]}, "prediction_text": "154 gems."}
{"answers": {"text": ["William and Judith Bollinger", "William and Judith Bollinger", "William and Judith Bollinger"], "answer_start": [1189, 1189, 1189]}, "prediction_text": "William and Judith Bollinger"}
{"answers": {"text": ["secular and sacred", "secular and sacred covering both Christian (Roman Catholic, Anglican and Greek Orthodox) and Jewish liturgical vessels and items", "secular and sacred"], "answer_start": [132, 132, 132]}, "prediction_text": "Christian, Jewish, secular."}
{"answers": {"text": ["1496\u201397", "1496\u201397", "1496\u201397"], "answer_start": [537, 537, 537]}, "prediction_text": "1496-97"}
{"answers": {"text": ["8", "8 tonnes", "nearly 8"], "answer_start": [1003, 1003, 996]}, "prediction_text": "8 tonnes."}
{"answers": {"text": ["Sir George Gilbert Scott", "Sir George Gilbert Scott", "Sir George Gilbert Scott"], "answer_start": [1062, 1062, 1062]}, "prediction_text": "Sir George Gilbert Scott"}
{"answers": {"text": ["over 10,000", "over 10,000", "over 10,000"], "answer_start": [10, 10, 10]}, "prediction_text": "10,000"}
{"answers": {"text": ["c1110", "c1110", "c1110"], "answer_start": [93, 93, 93]}, "prediction_text": "c1110"}
{"answers": {"text": ["gilt bronze", "gilt bronze", "gilt bronze"], "answer_start": [110, 110, 110]}, "prediction_text": "Gilt bronze"}
{"answers": {"text": ["St Thomas Becket", "St Thomas Becket,", "St Thomas Becket"], "answer_start": [338, 338, 338]}, "prediction_text": "St Thomas Becket"}
{"answers": {"text": ["c1180", "c1180", "c1180"], "answer_start": [311, 311, 311]}, "prediction_text": "c1180"}
{"answers": {"text": ["gilt copper", "gilt copper", "gilt copper"], "answer_start": [366, 366, 366]}, "prediction_text": "Gilt copper"}
{"answers": {"text": ["over 5,100", "over 5,100", "over 5,100"], "answer_start": [122, 122, 122]}, "prediction_text": "5,100"}
{"answers": {"text": ["Bryan Davies", "Bryan Davies", "Bryan Davies"], "answer_start": [258, 258, 258]}, "prediction_text": "Bryan Davies"}
{"answers": {"text": ["Horniman Museum", "The Horniman", "Horniman"], "answer_start": [551, 696, 551]}, "prediction_text": "Horniman Museum"}
{"answers": {"text": ["35", "35", "35"], "answer_start": [801, 801, 801]}, "prediction_text": "35"}
{"answers": {"text": ["2010", "2010,", "2010"], "answer_start": [51, 51, 51]}, "prediction_text": "2010"}
{"answers": {"text": ["1130", "1130"], "answer_start": [30, 30]}, "prediction_text": "1130"}
{"answers": {"text": ["650", "650"], "answer_start": [47, 47]}, "prediction_text": "650"}
{"answers": {"text": ["6800", "6800"], "answer_start": [75, 75]}, "prediction_text": "650"}
{"answers": {"text": ["Queen Elizabeth II", "Queen Elizabeth II"], "answer_start": [232, 232]}, "prediction_text": "Her Majesty the Queen Elizabeth II"}
{"answers": {"text": ["Andr\u00e9s Marzal De Sax", "Andr\u00e9s Marzal De Sax"], "answer_start": [774, 774]}, "prediction_text": "Andr\u00e9s Marzal De Sax"}
{"answers": {"text": ["1857", "1857", "1857"], "answer_start": [3, 3, 3]}, "prediction_text": "1857"}
{"answers": {"text": ["233", "233", "233"], "answer_start": [33, 33, 33]}, "prediction_text": "233 paintings."}
{"answers": {"text": ["forming a 'A National Gallery of British Art'", "forming a 'A National Gallery of British Art',", "forming a 'A National Gallery of British Art'"], "answer_start": [157, 157, 157]}, "prediction_text": "To form a 'National Gallery of British Art'."}
{"answers": {"text": ["The Hay Wain", "The Hay Wain.", "The Hay Wain"], "answer_start": [698, 698, 698]}, "prediction_text": "The Hay Wain."}
{"answers": {"text": ["British", "British", "British"], "answer_start": [71, 71, 71]}, "prediction_text": "British artists."}
{"answers": {"text": ["continental art 1600\u20131800", "of continental art 1600\u20131800", "galleries of continental art"], "answer_start": [263, 260, 250]}, "prediction_text": "Continental art 1600\u20131800."}
{"answers": {"text": ["Madame de Pompadour", "Madame de Pompadour", "Madame de Pompadour"], "answer_start": [431, 431, 431]}, "prediction_text": "Fran\u00e7ois Clouet"}
{"answers": {"text": ["Carlo Crivelli's Virgin and Child", "Carlo Crivelli's Virgin and Child)", "Carlo Crivelli's Virgin and Child"], "answer_start": [136, 136, 136]}, "prediction_text": "Carlo Crivelli's Virgin and Child."}
{"answers": {"text": ["Fran\u00e7ois, Duc d'Alen\u00e7on", "Duc d'Alen\u00e7on", "Duc d'Alen\u00e7on"], "answer_start": [316, 326, 326]}, "prediction_text": "Fran\u00e7ois Clouet"}
{"answers": {"text": ["Eadweard Muybridge", "Eadweard Muybridge's", "Eadweard Muybridge"], "answer_start": [47, 47, 47]}, "prediction_text": "Eadweard Muybridge"}
{"answers": {"text": ["1887", "1887", "1887"], "answer_start": [104, 104, 104]}, "prediction_text": "1887"}
{"answers": {"text": ["781", "781", "781"], "answer_start": [127, 127, 127]}, "prediction_text": "781"}
{"answers": {"text": ["animals and humans performimg various actions", "images of different animals and humans performimg various actions", "different animals and humans performimg various actions"], "answer_start": [233, 213, 223]}, "prediction_text": "Images of different animals and humans performing various actions."}
{"answers": {"text": ["James Lafayette", "James Lafayette's", "James Lafayette"], "answer_start": [396, 396, 396]}, "prediction_text": "James Lafayette"}
{"answers": {"text": ["post-classical European", "post-classical European sculpture", "post-classical European"], "answer_start": [73, 73, 73]}, "prediction_text": "400 AD to 1914."}
{"answers": {"text": ["22,000", "22,000 objects", "22,000"], "answer_start": [145, 145, 145]}, "prediction_text": "22,000"}
{"answers": {"text": ["from about 400 AD to 1914", "400 AD to 1914", "400 AD to 1914"], "answer_start": [200, 211, 211]}, "prediction_text": "400 AD to 1914."}
{"answers": {"text": ["All", "All uses", "All"], "answer_start": [438, 438, 438]}, "prediction_text": "tomb, memorial, allegorical, religious, mythical, statues for gardens."}
{"answers": {"text": ["National Galleries of Scotland", "National Galleries of Scotland", "National Galleries of Scotland"], "answer_start": [224, 224, 224]}, "prediction_text": "National Galleries of Scotland."}
{"answers": {"text": ["Neptune and Triton", "Neptune and Triton", "Neptune and Triton"], "answer_start": [932, 932, 932]}, "prediction_text": "Neptune and Triton."}
{"answers": {"text": ["Chancel Chapel", "Chancel Chapel", "Chancel Chapel"], "answer_start": [1469, 1469, 1469]}, "prediction_text": "Chancel Chapel from Santa Chiara Florence."}
{"answers": {"text": ["Giuliano da Sangallo", "Giuliano da Sangallo", "Santa Chiara Florence"], "answer_start": [1540, 1540, 1489]}, "prediction_text": "Giuliano da Sangallo"}
{"answers": {"text": ["1493\u20131500", "1493\u20131500,", "1493\u20131500"], "answer_start": [1517, 1517, 1517]}, "prediction_text": "Santa Chiara Florence."}
{"answers": {"text": ["more than 20", "20 works", "20"], "answer_start": [24, 34, 34]}, "prediction_text": "20"}
{"answers": {"text": ["the sculptor", "by the sculptor", "Rodin"], "answer_start": [183, 180, 0]}, "prediction_text": "Britain"}
{"answers": {"text": ["1914", "1914", "1914"], "answer_start": [199, 199, 199]}, "prediction_text": "1914"}
{"answers": {"text": ["World War I", "World War I,", "World War I"], "answer_start": [258, 258, 258]}, "prediction_text": "World War I"}
{"answers": {"text": ["St John the Baptist", "St John the Baptist", "St John the Baptist"], "answer_start": [294, 294, 294]}, "prediction_text": "St John the Baptist"}
{"answers": {"text": ["George Frampton", "Agostino Carlini", "George Frampton"], "answer_start": [445, 249, 445]}, "prediction_text": "Nicholas Stone"}
{"answers": {"text": ["Thomas Brock", "Thomas Brock", "Thomas Brock"], "answer_start": [415, 415, 415]}, "prediction_text": "Nicholas Stone"}
{"answers": {"text": ["Sir Francis Chantrey", "Sir Francis Chantrey,", "Sir Francis Chantrey"], "answer_start": [328, 328, 328]}, "prediction_text": "Nicholas Stone"}
{"answers": {"text": ["Europeans who were based in Britain", "British and Europeans who were based in Britain", "both British and Europeans"], "answer_start": [27, 15, 10]}, "prediction_text": "Nicholas Stone, Caius Gabriel Cibber, Grinling Gibbons, John Michael Rysbrack, Louis-Fran\u00e7ois Roubiliac, Peter Scheemakers, Sir Henry Cheere, Agostino Carlini, Thomas Banks, Joseph Nollekens, Joseph Wilton, John Flaxman, Sir Francis Chantrey, John Gibson, Edward Hodges Baily, Lord Leighton, Alfred Stevens, Thomas Brock, Alfred Gilbert, George Frampton, and Eric Gill."}
{"answers": {"text": ["Dorothy and Michael Hintze", "Dorothy and Michael Hintze", "Dorothy and Michael Hintze"], "answer_start": [24, 24, 24]}, "prediction_text": "Dorothy and Michael Hintze"}
{"answers": {"text": ["1950", "1600 to 1950", "1600 to 1950"], "answer_start": [149, 364, 364]}, "prediction_text": "1950"}
{"answers": {"text": ["by theme", "by theme, tomb sculpture, portraiture, garden sculpture and mythology", "by theme, tomb sculpture, portraiture, garden sculpture and mythology"], "answer_start": [614, 614, 614]}, "prediction_text": "By theme, tomb sculpture, portraiture, garden sculpture, and mythology."}
{"answers": {"text": ["Henry Moore and Jacob Epstein", "Henry Moore and Jacob Epstein", "Henry Moore and Jacob Epstein"], "answer_start": [233, 233, 233]}, "prediction_text": "Henry Moore and Jacob Epstein."}
{"answers": {"text": ["Tate Britain", "Tate Britain", "Tate Britain"], "answer_start": [207, 207, 207]}, "prediction_text": "Tate Britain"}
{"answers": {"text": ["more than 53,000", "53,000", "53,000"], "answer_start": [39, 49, 49]}, "prediction_text": "53,000"}
{"answers": {"text": ["all populated continents", "western European", "all populated continents"], "answer_start": [97, 73, 97]}, "prediction_text": "Europe, Asia, Africa."}
{"answers": {"text": ["from the 1st century AD to the present", "1st century AD to the present,", "1st century AD to the present"], "answer_start": [146, 155, 155]}, "prediction_text": "1st century AD to present."}
{"answers": {"text": ["western Europe", "all populated continents", "western European"], "answer_start": [73, 97, 73]}, "prediction_text": "Europe"}
{"answers": {"text": ["by technique", "technique", "technique"], "answer_start": [356, 359, 359]}, "prediction_text": "Techniques."}
{"answers": {"text": ["Cloth of St Gereon", "Cloth of St Gereon", "a fragment of the Cloth of St Gereon"], "answer_start": [51, 51, 33]}, "prediction_text": "Cloth of St Gereon"}
{"answers": {"text": ["15th", "15th-century", "15th"], "answer_start": [200, 200, 200]}, "prediction_text": "15th century."}
{"answers": {"text": ["the Netherlands", "Netherlands", "the Netherlands"], "answer_start": [234, 238, 234]}, "prediction_text": "Netherlands"}
{"answers": {"text": ["hunting of various animals", "hunting of various animals", "the hunting of various animals"], "answer_start": [265, 265, 261]}, "prediction_text": "Hunting of various animals."}
{"answers": {"text": ["John Vanderbank's workshop", "John Vanderbank's", "John Vanderbank"], "answer_start": [553, 553, 553]}, "prediction_text": "Sheldon & Mortlake"}
{"answers": {"text": ["late 14th-century", "late 14th-century", "late 14th-century"], "answer_start": [65, 65, 65]}, "prediction_text": "14th century."}
{"answers": {"text": ["William Morris", "William Morris", "William Morris"], "answer_start": [217, 217, 217]}, "prediction_text": "Marion Dorn"}
{"answers": {"text": ["1887", "1887", "1887"], "answer_start": [318, 318, 318]}, "prediction_text": "1887"}
{"answers": {"text": ["Marion Dorn", "Marion Dorn", "Marion Dorn"], "answer_start": [450, 450, 450]}, "prediction_text": "Serge Chermayeff"}
{"answers": {"text": ["Serge Chermayeff", "Serge Chermayeff", "Serge Chermayeff"], "answer_start": [511, 511, 511]}, "prediction_text": "Serge Chermayeff"}
{"answers": {"text": ["Theatre Museum", "Theatre Museum", "the Theatre Museum"], "answer_start": [54, 54, 50]}, "prediction_text": "Theatre Museum"}
{"answers": {"text": ["2009", "2009", "2009"], "answer_start": [86, 86, 86]}, "prediction_text": "2009"}
{"answers": {"text": ["material about live performance", "UK's biggest national collection of material about live performance in the UK", "material about live performance in the UK since Shakespeare's day"], "answer_start": [242, 206, 242]}, "prediction_text": "UK's biggest national collection of material about live performance in the UK."}
{"answers": {"text": ["Shakespeare", "Shakespeare's", "Shakespeare"], "answer_start": [290, 290, 290]}, "prediction_text": "Shakespeare"}
{"answers": {"text": ["research, exhibitions and other shows", "research, exhibitions and other shows", "research, exhibitions and other shows"], "answer_start": [153, 153, 153]}, "prediction_text": "Research, exhibitions, other shows."}
{"answers": {"text": ["Conservation", "Conservation", "Conservation"], "answer_start": [0, 0, 0]}, "prediction_text": "Conservation"}
{"answers": {"text": ["temperature and light", "temperature and light", "temperature and light"], "answer_start": [572, 572, 572]}, "prediction_text": "Temperature and light."}
{"answers": {"text": ["interventive", "interventive", "interventive"], "answer_start": [690, 690, 690]}, "prediction_text": "Interventive conservation."}
{"answers": {"text": ["V&A Museum of Childhood", "V&A Museum of Childhood"], "answer_start": [134, 134]}, "prediction_text": "V&A Museum of Childhood."}
{"answers": {"text": ["preventive", "preventive", "preventive"], "answer_start": [269, 269, 269]}, "prediction_text": "Preventive conservation."}
{"answers": {"text": ["The Walt Disney Company", "Disney\u2013ABC Television Group", "Disney\u2013ABC Television Group"], "answer_start": [236, 157, 157]}, "prediction_text": "Disney\u2013ABC Television Group"}
{"answers": {"text": ["1957", "1957", "1957"], "answer_start": [75, 75, 75]}, "prediction_text": "1957"}
{"answers": {"text": ["Manhattan", "Manhattan", "Manhattan"], "answer_start": [391, 391, 391]}, "prediction_text": "Manhattan"}
{"answers": {"text": ["Columbus Avenue and West 66th Street", "Columbus Avenue and West 66th Street", "Columbus Avenue and West 66th Street"], "answer_start": [351, 351, 351]}, "prediction_text": "Columbus Avenue and West 66th Street."}
{"answers": {"text": ["Disney Media Networks", "Disney Media Networks", "Disney Media Networks"], "answer_start": [202, 202, 202]}, "prediction_text": "Disney Media Networks division"}
{"answers": {"text": ["October 12, 1943", "October 12, 1943", "October 12, 1943"], "answer_start": [27, 27, 27]}, "prediction_text": "1943"}
{"answers": {"text": ["radio network", "radio", "radio network"], "answer_start": [49, 49, 49]}, "prediction_text": "Radio network"}
{"answers": {"text": ["1948", "1948", "1948"], "answer_start": [222, 222, 222]}, "prediction_text": "1948"}
{"answers": {"text": ["ESPN", "ESPN", "ESPN"], "answer_start": [672, 672, 672]}, "prediction_text": "ESPN"}
{"answers": {"text": ["Capital Cities Communications", "Capital Cities Communications", "Capital Cities Communications"], "answer_start": [711, 711, 711]}, "prediction_text": "Capital Cities Communications"}
{"answers": {"text": ["232", "232", "over 232"], "answer_start": [61, 61, 56]}, "prediction_text": "232"}
{"answers": {"text": ["Citadel Broadcasting", "Citadel Broadcasting", "Citadel Broadcasting"], "answer_start": [768, 768, 768]}, "prediction_text": "Citadel Broadcasting"}
{"answers": {"text": ["eight", "eight", "eight"], "answer_start": [27, 27, 27]}, "prediction_text": "8"}
{"answers": {"text": ["Canadian Radio-television and Telecommunications Commission", "Canadian Radio-television and Telecommunications Commission", "Canadian Radio-television and Telecommunications Commission"], "answer_start": [449, 449, 449]}, "prediction_text": "Canadian Radio-television and Telecommunications Commission (CRTC)"}
{"answers": {"text": ["Citadel Broadcasting", "Citadel Broadcasting", "Citadel Broadcasting"], "answer_start": [768, 768, 768]}, "prediction_text": "Citadel Broadcasting"}
{"answers": {"text": ["Radio Corporation of America", "Radio Corporation of America (RCA)", "Radio Corporation of America"], "answer_start": [238, 238, 238]}, "prediction_text": "Radio Corporation of America (RCA)"}
{"answers": {"text": ["NBC Blue and NBC Red", "NBC Blue and NBC Red", "NBC Blue and NBC Red"], "answer_start": [355, 355, 355]}, "prediction_text": "NBC Blue and NBC Red."}
{"answers": {"text": ["major cities", "major cities", "major cities"], "answer_start": [545, 545, 545]}, "prediction_text": "Major cities."}
{"answers": {"text": ["drama series", "drama series", "drama series"], "answer_start": [571, 571, 571]}, "prediction_text": "Drama series."}
{"answers": {"text": ["NBC Blue", "NBC Blue Network", "NBC Blue Network"], "answer_start": [381, 381, 381]}, "prediction_text": "NBC Blue Network"}
{"answers": {"text": ["Mutual", "Mutual", "Mutual filed"], "answer_start": [9, 9, 9]}, "prediction_text": "Mutual Broadcasting Corporation (Mutual)"}
{"answers": {"text": ["1938", "1938", "1938"], "answer_start": [210, 210, 210]}, "prediction_text": "1940"}
{"answers": {"text": ["1940", "1940", "1940"], "answer_start": [368, 368, 368]}, "prediction_text": "1940"}
{"answers": {"text": ["NBC Red Network", "NBC Red Network", "NBC Red Network"], "answer_start": [471, 471, 471]}, "prediction_text": "NBC Red Network"}
{"answers": {"text": ["NBC Blue", "NBC Blue", "NBC Blue"], "answer_start": [581, 581, 581]}, "prediction_text": "NBC Blue"}
{"answers": {"text": ["Mark Woods", "Mark Woods", "Mark Woods"], "answer_start": [124, 124, 124]}, "prediction_text": "Mark Woods"}
{"answers": {"text": ["NBC Blue Network", "NBC Blue Network", "NBC Blue Network"], "answer_start": [154, 154, 154]}, "prediction_text": "NBC Blue"}
{"answers": {"text": ["Dillon, Read & Co.", "Dillon, Read & Co", "Dillon, Read & Co."], "answer_start": [867, 867, 867]}, "prediction_text": "Dillon, Read & Co."}
{"answers": {"text": ["David Sarnoff", "David Sarnoff", "David Sarnoff"], "answer_start": [1051, 1051, 1051]}, "prediction_text": "David Sarnoff"}
{"answers": {"text": ["$7.5 million", "$7.5 million", "$7.5 million"], "answer_start": [959, 959, 959]}, "prediction_text": "$7.5 million."}
{"answers": {"text": ["Life Savers candy", "Life Savers", "Life Savers"], "answer_start": [32, 32, 32]}, "prediction_text": "Life Savers candy."}
{"answers": {"text": ["October 12, 1943", "October 12, 1943", "October 12, 1943"], "answer_start": [371, 371, 371]}, "prediction_text": "October 12, 1943."}
{"answers": {"text": ["George B. Storer", "George B. Storer", "1944"], "answer_start": [592, 592, 612]}, "prediction_text": "George B. Storer"}
{"answers": {"text": ["president and CEO", "president and CEO", "president"], "answer_start": [733, 733, 733]}, "prediction_text": "Vice-chairman"}
{"answers": {"text": ["June 30, 1951", "1951", "June 30, 1951"], "answer_start": [872, 881, 872]}, "prediction_text": "June 30, 1951."}
{"answers": {"text": ["Magnetophon tape recorder", "Magnetophon", "Magnetophon tape recorder"], "answer_start": [693, 693, 693]}, "prediction_text": "Magnetophon"}
{"answers": {"text": ["Paul Whiteman", "Paul Whiteman", "Paul Whiteman"], "answer_start": [153, 153, 153]}, "prediction_text": "Paul Whiteman"}
{"answers": {"text": ["ABC", "ABC", "NBC Blue"], "answer_start": [0, 0, 68]}, "prediction_text": "ABC"}
{"answers": {"text": ["Bing Crosby", "Bing Crosby", "Bing Crosby"], "answer_start": [960, 960, 960]}, "prediction_text": "Bing Crosby"}
{"answers": {"text": ["public service", "public service", "public service"], "answer_start": [93, 93, 93]}, "prediction_text": "public service"}
{"answers": {"text": ["$155 million", "$155 million", "$155 million"], "answer_start": [402, 402, 402]}, "prediction_text": "$155 million."}
{"answers": {"text": ["ABC1", "ABC1", "ABC1"], "answer_start": [717, 717, 717]}, "prediction_text": "ABC1"}
{"answers": {"text": ["September 8, 2007", "September 8, 2007", "September 8, 2007"], "answer_start": [803, 803, 803]}, "prediction_text": "October 2007"}
{"answers": {"text": ["ABC International", "ABC International", "ABC International"], "answer_start": [1006, 1006, 1006]}, "prediction_text": "Disney International"}
{"answers": {"text": ["United States", "United States", "United States"], "answer_start": [494, 494, 494]}, "prediction_text": "United Kingdom"}
{"answers": {"text": ["1959", "1959", "1959"], "answer_start": [130, 130, 130]}, "prediction_text": "1959"}
{"answers": {"text": ["satellite television", "The arrival of satellite television", "satellite television"], "answer_start": [273, 258, 273]}, "prediction_text": "Satellite television."}
{"answers": {"text": ["Japan and Latin America", "Japan and Latin America", "Japan and Latin America"], "answer_start": [591, 591, 591]}, "prediction_text": "Japan and Latin America."}
{"answers": {"text": ["legislation to limit foreign ownership of broadcasting properties", "legislation to limit foreign ownership of broadcasting properties", "governments also wanted to increase their independence and strengthen legislation"], "answer_start": [430, 430, 360]}, "prediction_text": "Japan and Latin America."}
{"answers": {"text": ["coronation of Queen Elizabeth II", "the coronation of Queen Elizabeth II", "the coronation of Queen Elizabeth II"], "answer_start": [364, 360, 360]}, "prediction_text": "Coronation of Queen Elizabeth II."}
{"answers": {"text": ["Beirut", "Beirut", "Beirut"], "answer_start": [1030, 1030, 1030]}, "prediction_text": "Japan"}
{"answers": {"text": ["Mainichi Broadcasting System", "Mainichi Broadcasting System", "Mainichi Broadcasting System"], "answer_start": [897, 897, 897]}, "prediction_text": "Mainichi Broadcasting System"}
{"answers": {"text": ["flight delays", "technical problems and flight delays", "technical problems and flight delays"], "answer_start": [517, 494, 494]}, "prediction_text": "Technical problems and flight delays."}
{"answers": {"text": ["technical problems", "technical problems and flight delays", "technical problems and flight delays"], "answer_start": [494, 494, 494]}, "prediction_text": "Technical problems and flight delays."}
{"answers": {"text": ["Peanuts", "Peanuts", "Peanuts"], "answer_start": [367, 367, 367]}, "prediction_text": "Peanuts television specials."}
{"answers": {"text": ["Emmy Awards", "Emmy Awards", "Emmy Awards"], "answer_start": [64, 64, 64]}, "prediction_text": "Academy Awards"}
{"answers": {"text": ["1965", "1965", "1965"], "answer_start": [476, 476, 476]}, "prediction_text": "1965"}
{"answers": {"text": ["the Academy Awards", "Academy Awards", "Academy Awards, Emmy Awards"], "answer_start": [44, 48, 48]}, "prediction_text": "Academy Awards"}
{"answers": {"text": ["It's the Great Pumpkin", "It's the Great Pumpkin, Charlie Brown", "It's the Great Pumpkin"], "answer_start": [621, 621, 621]}, "prediction_text": "A Charlie Brown Halloween"}
{"answers": {"text": ["1974", "1974"], "answer_start": [6, 6]}, "prediction_text": "1974"}
{"answers": {"text": ["Ryan Seacrest", "Ryan Seacrest", "Ryan Seacrest"], "answer_start": [158, 158, 158]}, "prediction_text": "Ryan Seacrest"}
{"answers": {"text": ["1954", "1954", "1954"], "answer_start": [452, 452, 452]}, "prediction_text": "1954"}
{"answers": {"text": ["Times Square", "Times Square", "New Year"], "answer_start": [350, 350, 75]}, "prediction_text": "Dick Clark's."}
{"answers": {"text": ["TLC", "TLC", "ABC"], "answer_start": [538, 538, 754]}, "prediction_text": "TLC"}
{"answers": {"text": ["General Hospital", "General Hospital", "General Hospital"], "answer_start": [95, 95, 95]}, "prediction_text": "General Hospital"}
{"answers": {"text": ["1975", "1975", "1975"], "answer_start": [337, 337, 337]}, "prediction_text": "1975"}
{"answers": {"text": ["The Edge of Night", "The Edge of Night", "The Edge of Night"], "answer_start": [707, 707, 707]}, "prediction_text": "The Edge of Night"}
{"answers": {"text": ["The View and The Chew", "The View and The Chew", "The View and The Chew"], "answer_start": [53, 53, 53]}, "prediction_text": "The View and The Chew."}
{"answers": {"text": ["1963", "1963", "1963"], "answer_start": [243, 243, 243]}, "prediction_text": "1968"}
{"answers": {"text": ["X Games", "X Games", "X Games"], "answer_start": [1302, 1302, 1302]}, "prediction_text": "X Games."}
{"answers": {"text": ["2006", "2006", "2006"], "answer_start": [469, 469, 469]}, "prediction_text": "2006"}
{"answers": {"text": ["12:00 to 6:00 p.m. Eastern Time", "late afternoons", "Saturday Night Football"], "answer_start": [80, 530, 234]}, "prediction_text": "12:00 to 6:00 p.m. Eastern Time (9:00 a.m. to 3:00 p.m. Pacific) and, during college football season, during prime time on Saturday nights as part of the Saturday Night Football package."}
{"answers": {"text": ["NBA", "NBA", "NBA"], "answer_start": [1357, 1412, 1412]}, "prediction_text": "NBA"}
{"answers": {"text": ["The Open Championship golf and The Wimbledon tennis tournaments", "The Open Championship golf and The Wimbledon", "The Wimbledon"], "answer_start": [1138, 1138, 1169]}, "prediction_text": "The Open Championship golf and The Wimbledon tennis tournaments."}
{"answers": {"text": ["Frank Marx", "Frank Marx", "Frank Marx"], "answer_start": [442, 442, 442]}, "prediction_text": "Frank Marx"}
{"answers": {"text": ["channels 2 through 6", "2 through 6", "low-band VHF"], "answer_start": [556, 565, 513]}, "prediction_text": "2 through 6."}
{"answers": {"text": ["1947", "1947", "1947"], "answer_start": [157, 157, 157]}, "prediction_text": "1947"}
{"answers": {"text": ["VHF channel 7", "channel 7", "channel 7"], "answer_start": [424, 428, 428]}, "prediction_text": "VHF channel 7"}
{"answers": {"text": ["108", "108", "108"], "answer_start": [329, 329, 329]}, "prediction_text": "108"}
{"answers": {"text": ["two", "two", "two"], "answer_start": [429, 429, 429]}, "prediction_text": "2"}
{"answers": {"text": ["DuMont Television Network", "DuMont Television Network", "DuMont Television Network"], "answer_start": [253, 253, 253]}, "prediction_text": "DuMont Television Network"}
{"answers": {"text": ["CBS and NBC", "CBS and NBC", "CBS and NBC"], "answer_start": [125, 125, 125]}, "prediction_text": "CBS and NBC."}
{"answers": {"text": ["U.S. Supreme Court", "U.S. Supreme Court", "United Paramount Theatres (UPT)"], "answer_start": [93, 93, 43]}, "prediction_text": "U.S. Supreme Court"}
{"answers": {"text": ["Paramount Pictures", "Paramount Pictures", "Paramount Pictures"], "answer_start": [168, 168, 168]}, "prediction_text": "Paramount Pictures"}
{"answers": {"text": ["nine", "nine", "nine"], "answer_start": [285, 285, 285]}, "prediction_text": "Nine full-time affiliates."}
{"answers": {"text": ["CBS", "CBS", "CBS"], "answer_start": [572, 572, 572]}, "prediction_text": "CBS"}
{"answers": {"text": ["Prudential Insurance Company of America", "Prudential Insurance Company of America", "Prudential Insurance Company of America."], "answer_start": [778, 778, 778]}, "prediction_text": "Prudential Insurance Company of America"}
{"answers": {"text": ["Leonard Goldenson", "Leonard Goldenson", "Leonard Goldenson"], "answer_start": [0, 0, 0]}, "prediction_text": "Leonard Goldenson"}
{"answers": {"text": ["William S. Paley", "William S. Paley", "William S. Paley"], "answer_start": [212, 212, 212]}, "prediction_text": "William S. Paley"}
{"answers": {"text": ["June 6, 1951", "June 6, 1951", "June 6, 1951"], "answer_start": [560, 560, 560]}, "prediction_text": "June 6, 1951."}
{"answers": {"text": ["1952", "1952", "1952"], "answer_start": [3, 3, 3]}, "prediction_text": "1952"}
{"answers": {"text": ["February 9, 1953", "February 9, 1953", "February 9, 1953"], "answer_start": [355, 355, 355]}, "prediction_text": "February 9, 1953"}
{"answers": {"text": ["American Broadcasting-Paramount Theatres, Inc", "American Broadcasting-Paramount Theatres, Inc.", "American Broadcasting-Paramount Theatres, Inc"], "answer_start": [479, 479, 479]}, "prediction_text": "American Broadcasting-Paramount Theatres Inc."}
{"answers": {"text": ["the Paramount Building", "Paramount Building at 1501 Broadway in Manhattan", "Paramount Building"], "answer_start": [547, 551, 551]}, "prediction_text": "Paramount Building at 1501 Broadway in Manhattan."}
{"answers": {"text": ["August 10, 1948", "August 10, 1948", "August 10, 1948"], "answer_start": [123, 123, 123]}, "prediction_text": "August 10."}
{"answers": {"text": ["October 1948", "October 1948", "October 1948"], "answer_start": [407, 407, 407]}, "prediction_text": "October 1948"}
{"answers": {"text": ["Mount Wilson", "Mount Wilson", "August 1"], "answer_start": [1028, 1028, 1127]}, "prediction_text": "Mount Wilson"}
{"answers": {"text": ["The Prospect Studios", "The Prospect Studios", "The Prospect Studios"], "answer_start": [975, 975, 975]}, "prediction_text": "The Prospect Studios"}
{"answers": {"text": ["September 30, 1960", "September 30, 1960", "September 30, 1960"], "answer_start": [212, 212, 212]}, "prediction_text": "September 30, 1960"}
{"answers": {"text": ["1960s", "1960s", "1960s"], "answer_start": [4, 4, 4]}, "prediction_text": "1960s"}
{"answers": {"text": ["William Hanna and Joseph Barbera", "William Hanna and Joseph Barbera", "William Hanna and Joseph Barbera"], "answer_start": [336, 336, 336]}, "prediction_text": "William Hanna and Joseph Barbera"}
{"answers": {"text": ["1960s", "1960s"], "answer_start": [4, 4]}, "prediction_text": "September 30, 1960"}
{"answers": {"text": ["1959", "1959", "1959"], "answer_start": [3, 3, 3]}, "prediction_text": "1959"}
{"answers": {"text": ["NBC", "NBC", "NBC"], "answer_start": [298, 298, 298]}, "prediction_text": "NBC"}
{"answers": {"text": ["1961", "1961", "1961"], "answer_start": [644, 644, 262]}, "prediction_text": "1961"}
{"answers": {"text": ["1985", "1985", "1985"], "answer_start": [707, 707, 707]}, "prediction_text": "1961"}
{"answers": {"text": ["circle logo", "circle logo", "circle logo"], "answer_start": [74, 74, 74]}, "prediction_text": "The dot."}
{"answers": {"text": ["Troika Design Group", "Troika Design Group", "Troika Design Group"], "answer_start": [319, 319, 319]}, "prediction_text": "Troika Design Group"}
{"answers": {"text": ["black-and-yellow", "black-and-yellow", "black-and-yellow"], "answer_start": [409, 409, 409]}, "prediction_text": "Black and yellow."}
{"answers": {"text": ["the dot", "\"the dot\"", "the dot"], "answer_start": [100, 99, 100]}, "prediction_text": "The dot"}
{"answers": {"text": ["Pittard Sullivan", "Pittard Sullivan", "Pittard Sullivan"], "answer_start": [78, 78, 78]}, "prediction_text": "Pittard Sullivan"}
{"answers": {"text": ["2015", "2015", "2015"], "answer_start": [935, 935, 935]}, "prediction_text": "2002"}
{"answers": {"text": ["\"We Love TV\" image campaign", "We Love TV", "We Love TV"], "answer_start": [464, 465, 465]}, "prediction_text": "We Love TV campaign"}
{"answers": {"text": ["ABC on Demand to the beginning of the ABC show", "ABC on Demand", "ABC on Demand"], "answer_start": [1284, 1284, 1284]}, "prediction_text": "ABC on Demand."}
{"answers": {"text": ["1993\u201394 season", "1993\u201394", "1993\u201394"], "answer_start": [462, 462, 462]}, "prediction_text": "1993-94"}
{"answers": {"text": ["1995\u201396 season", "1995\u201396", "2011\u201312"], "answer_start": [645, 645, 774]}, "prediction_text": "1995-96 season."}
{"answers": {"text": ["1983", "1983", "1983"], "answer_start": [3, 3, 3]}, "prediction_text": "1983"}
{"answers": {"text": ["That Special Feeling", "That Special Feeling", "That Special Feeling"], "answer_start": [164, 164, 164]}, "prediction_text": "That Special Feeling"}
{"answers": {"text": ["1977", "1977", "1977"], "answer_start": [302, 302, 302]}, "prediction_text": "1977"}
{"answers": {"text": ["black background", "black", "black"], "answer_start": [347, 347, 347]}, "prediction_text": "Black"}
{"answers": {"text": ["glossy gold", "gold", "white, blue, pink, rainbow neon and glittering"], "answer_start": [393, 400, 191]}, "prediction_text": "Glossy gold."}
{"answers": {"text": ["Paul Rand", "Paul Rand", "Paul Rand"], "answer_start": [26, 26, 26]}, "prediction_text": "Paul Rand"}
{"answers": {"text": ["Bauhaus typeface", "Bauhaus typeface", "Bauhaus"], "answer_start": [293, 293, 293]}, "prediction_text": "Bayer (1920s)"}
{"answers": {"text": ["Herbert Bayer", "Herbert Bayer", "Herbert Bayer"], "answer_start": [322, 322, 322]}, "prediction_text": "Herbert Bayer"}
{"answers": {"text": ["1963\u201364 season", "1963\u201364 season", "1962"], "answer_start": [233, 233, 3]}, "prediction_text": "1963"}
{"answers": {"text": ["ABC Radio", "ABC Radio", "ABC Radio"], "answer_start": [92, 92, 92]}, "prediction_text": "Clear Channel Communications"}
{"answers": {"text": ["October 19, 2005", "October 19, 2005", "October 19, 2005"], "answer_start": [327, 327, 327]}, "prediction_text": "October 19, 2005"}
{"answers": {"text": ["six divisions", "six", "six"], "answer_start": [395, 395, 395]}, "prediction_text": "6"}
{"answers": {"text": ["2004", "2004", "2004"], "answer_start": [3, 3, 3]}, "prediction_text": "2004"}
{"answers": {"text": ["Grey's Anatomy", "Grey's Anatomy", "Grey's Anatomy"], "answer_start": [397, 397, 397]}, "prediction_text": "Desperate Housewives"}
{"answers": {"text": ["Anne Sweeney", "Anne Sweeney", "Anne Sweeney"], "answer_start": [644, 644, 644]}, "prediction_text": "Anne Sweeney"}
{"answers": {"text": ["NASCAR", "NASCAR", "ESPN"], "answer_start": [936, 936, 878]}, "prediction_text": "NASCAR"}
{"answers": {"text": ["2002", "2002", "2002"], "answer_start": [63, 63, 63]}, "prediction_text": "2002"}
{"answers": {"text": ["Michael Eisner", "Michael Eisner", "Michael Eisner"], "answer_start": [103, 103, 103]}, "prediction_text": "Michael Eisner"}
{"answers": {"text": ["The Bachelor", "The Bachelor", "The Bachelor"], "answer_start": [420, 420, 420]}, "prediction_text": "The Bachelor"}
{"answers": {"text": ["The Bachelorette", "The Bachelorette", "The Bachelorette"], "answer_start": [496, 496, 496]}, "prediction_text": "The Bachelorette"}
{"answers": {"text": ["Time Warner Cable", "Time Warner Cable", "Time Warner Cable"], "answer_start": [63, 63, 63]}, "prediction_text": "Time Warner Cable"}
{"answers": {"text": ["ABC", "ABC", "ABC"], "answer_start": [548, 548, 548]}, "prediction_text": "The FCC."}
{"answers": {"text": ["ABC", "ABC", "ABC"], "answer_start": [641, 641, 641]}, "prediction_text": "NBC"}
{"answers": {"text": ["afternoon of May 2.", "afternoon of May 2", "December 31, 1999"], "answer_start": [621, 621, 374]}, "prediction_text": "May 2."}
{"answers": {"text": ["2000", "2000", "2000"], "answer_start": [375, 375, 375]}, "prediction_text": "2000"}
{"answers": {"text": ["The WB", "The WB", "WB"], "answer_start": [524, 524, 528]}, "prediction_text": "The WB"}
{"answers": {"text": ["CBS", "CBS", "CBS"], "answer_start": [648, 648, 648]}, "prediction_text": "CBS"}
{"answers": {"text": ["August 1999", "August 1999", "August 1999"], "answer_start": [3, 3, 3]}, "prediction_text": "August 1999"}
{"answers": {"text": ["Regis Philbin", "Regis Philbin", "Regis Philbin"], "answer_start": [181, 181, 181]}, "prediction_text": "Regis Philbin"}
{"answers": {"text": ["Buena Vista Television", "Buena Vista Television", "Buena Vista Television"], "answer_start": [679, 679, 679]}, "prediction_text": "Buena Vista Television"}
{"answers": {"text": ["Meredith Vieira", "Meredith Vieira", "Meredith Vieira"], "answer_start": [787, 787, 787]}, "prediction_text": "Meredith Vieira"}
{"answers": {"text": ["July 31, 1995", "July 31, 1995", "On July 31, 1995"], "answer_start": [3, 3, 0]}, "prediction_text": "July 31, 1995"}
{"answers": {"text": ["ABC Inc.", "ABC Inc.", "ABC Inc"], "answer_start": [344, 344, 344]}, "prediction_text": "ABC Inc."}
{"answers": {"text": ["Knight Ridder", "Knight Ridder", "Knight Ridder"], "answer_start": [943, 943, 943]}, "prediction_text": "Knight Ridder"}
{"answers": {"text": ["Robert Iger", "Robert Iger", "Robert Iger"], "answer_start": [1029, 1029, 1029]}, "prediction_text": "Robert Iger"}
{"answers": {"text": ["Sports Night", "Sports Night", "Sports Night"], "answer_start": [1407, 1407, 1407]}, "prediction_text": "Sports Night"}
{"answers": {"text": ["1965\u201366 season", "1965\u201366", "1965\u201366"], "answer_start": [21, 21, 21]}, "prediction_text": "1965-66"}
{"answers": {"text": ["third place", "third", "third"], "answer_start": [147, 147, 147]}, "prediction_text": "Third place."}
{"answers": {"text": ["Beating the Odds: The Untold Story Behind the Rise of ABC", "\"Beating the Odds: The Untold Story Behind the Rise of ABC\"", "Beating the Odds: The Untold Story Behind the Rise of ABC"], "answer_start": [414, 413, 414]}, "prediction_text": "Beating the Odds: The Untold Story Behind the Rise of ABC."}
{"answers": {"text": ["May 1, 1953", "May 1, 1953", "May 1, 1953"], "answer_start": [3, 3, 3]}, "prediction_text": "May 1, 1953"}
{"answers": {"text": ["7 West 66th Street", "7 West 66th Street", "7 West 66th Street"], "answer_start": [190, 190, 190]}, "prediction_text": "7 West 66th Street."}
{"answers": {"text": ["Baltimore", "Baltimore", "Baltimore"], "answer_start": [421, 421, 421]}, "prediction_text": "Baltimore"}
{"answers": {"text": ["Robert Kintner", "Robert Kintner", "Robert Kintner"], "answer_start": [99, 99, 99]}, "prediction_text": "Robert Kintner"}
{"answers": {"text": ["DuMont Television Network", "DuMont Television Network", "DuMont Television Network"], "answer_start": [497, 497, 497]}, "prediction_text": "DuMont Television Network"}
{"answers": {"text": ["ABC-DuMont", "ABC-DuMont", "ABC-DuMont"], "answer_start": [628, 628, 628]}, "prediction_text": "ABC-DuMont"}
{"answers": {"text": ["$5 million in cash", "$5 million in cash", "$5 million"], "answer_start": [687, 687, 687]}, "prediction_text": "$5 million."}
{"answers": {"text": ["Paramount Pictures", "Paramount Pictures", "Paramount Pictures"], "answer_start": [231, 231, 231]}, "prediction_text": "Paramount Pictures"}
{"answers": {"text": ["The Lone Ranger", "The Lone Ranger", "The Lone Ranger"], "answer_start": [542, 542, 542]}, "prediction_text": "The Lone Ranger"}
{"answers": {"text": ["The Adventures of Ozzie and Harriet", "The Adventures of Ozzie and Harriet", "The Adventures of Ozzie and Harriet"], "answer_start": [609, 609, 609]}, "prediction_text": "The Adventures of Ozzie and Harriet."}
{"answers": {"text": ["Cheyenne", "Cheyenne", "Cheyenne"], "answer_start": [297, 297, 297]}, "prediction_text": "Sugarfoot"}
{"answers": {"text": ["Sugarfoot", "Sugarfoot", "Sugarfoot"], "answer_start": [348, 348, 348]}, "prediction_text": "Maverick"}
{"answers": {"text": ["Walt Disney", "Walt Disney", "Walt Disney"], "answer_start": [519, 519, 519]}, "prediction_text": "Walt Disney"}
{"answers": {"text": ["Warner Bros. Presents", "wheel series Warner Bros. Presents", "wheel series"], "answer_start": [160, 147, 147]}, "prediction_text": "Wheel series."}
{"answers": {"text": ["Roy", "Roy", "Roy"], "answer_start": [28, 28, 28]}, "prediction_text": "Roy"}
{"answers": {"text": ["$500,000", "$500,000", "$500,000"], "answer_start": [220, 220, 220]}, "prediction_text": "$500,000"}
{"answers": {"text": ["1954", "1954", "1954"], "answer_start": [342, 342, 342]}, "prediction_text": "1954"}
{"answers": {"text": ["Disneyland", "Disneyland", "Disneyland"], "answer_start": [448, 448, 448]}, "prediction_text": "Disneyland"}
{"answers": {"text": ["Allen Shaw", "Allen Shaw", "Allen Shaw"], "answer_start": [109, 109, 109]}, "prediction_text": "Allen Shaw"}
{"answers": {"text": ["Harold L. Neal", "Harold L. Neal", "Harold L. Neal"], "answer_start": [207, 207, 207]}, "prediction_text": "Harold L. Neal"}
{"answers": {"text": ["LOVE Radio", "LOVE Radio", "LOVE Radio"], "answer_start": [331, 331, 331]}, "prediction_text": "LOVE Radio"}
{"answers": {"text": ["seven", "seven", "seven"], "answer_start": [418, 418, 418]}, "prediction_text": "7"}
{"answers": {"text": ["1969", "1969", "September 1969"], "answer_start": [37, 37, 27]}, "prediction_text": "1969"}
{"answers": {"text": ["Duel", "Duel", "Duel"], "answer_start": [419, 419, 419]}, "prediction_text": "Duel"}
{"answers": {"text": ["1971", "1971", "1971"], "answer_start": [409, 409, 409]}, "prediction_text": "1971"}
{"answers": {"text": ["$400,000\u2013$450,000", "$400,000\u2013$450,000", "$400,000\u2013$450,000"], "answer_start": [468, 468, 468]}, "prediction_text": "$400,000\u2013$450,000."}
{"answers": {"text": ["early 1970s", "1970s", "1970s"], "answer_start": [7, 13, 13]}, "prediction_text": "1970s"}
{"answers": {"text": ["ABC", "ABC", "ABC"], "answer_start": [112, 112, 112]}, "prediction_text": "CBS"}
{"answers": {"text": ["behavioral and demographic data", "behavioral and demographic", "behavioral and demographic"], "answer_start": [220, 220, 220]}, "prediction_text": "Behavioral and demographic data."}
{"answers": {"text": ["Monday Night Football", "Monday Night Football", "Monday Night Football"], "answer_start": [21, 21, 21]}, "prediction_text": "Monday Night Football"}
{"answers": {"text": ["2006", "2006", "2006"], "answer_start": [209, 209, 209]}, "prediction_text": "2006"}
{"answers": {"text": ["ESPN", "ESPN", "NBC"], "answer_start": [329, 329, 258]}, "prediction_text": "ESPN"}
{"answers": {"text": ["15%\u201316%", "15%\u201316%", "15%\u201316%"], "answer_start": [474, 474, 474]}, "prediction_text": "15%\u201316%."}
{"answers": {"text": ["1970", "1970", "1970"], "answer_start": [3, 3, 3]}, "prediction_text": "1970"}
{"answers": {"text": ["1972", "1972", "1972"], "answer_start": [265, 265, 265]}, "prediction_text": "1972"}
{"answers": {"text": ["Worldvision Enterprises", "Worldvision Enterprises", "Worldvision Enterprises"], "answer_start": [381, 381, 381]}, "prediction_text": "ABC Circle Films"}
{"answers": {"text": ["cigarette advertising from all television and radio networks", "cigarette advertising", "1970"], "answer_start": [84, 84, 9]}, "prediction_text": "cigarette advertising"}
{"answers": {"text": ["January 2, 1971", "January 2, 1971", "January 2, 1971"], "answer_start": [184, 184, 184]}, "prediction_text": "January 2, 1971."}
{"answers": {"text": ["Henry Plitt", "Henry Plitt", "Henry Plitt"], "answer_start": [319, 319, 319]}, "prediction_text": "Henry Plitt"}
{"answers": {"text": ["Elton Rule", "Elton Rule", "Elton Rule"], "answer_start": [361, 361, 361]}, "prediction_text": "Elton Rule"}
{"answers": {"text": ["1966", "1966", "1966"], "answer_start": [54, 54, 54]}, "prediction_text": "1966"}
{"answers": {"text": ["Happy Days", "Happy Days", "Happy Days"], "answer_start": [164, 164, 164]}, "prediction_text": "Happy Days"}
{"answers": {"text": ["youth-oriented programming", "developing youth-oriented programming", "developing youth-oriented programming"], "answer_start": [341, 330, 330]}, "prediction_text": "Program development manager"}
{"answers": {"text": ["Paramount Pictures", "Paramount Pictures", "Paramount Pictures"], "answer_start": [750, 750, 750]}, "prediction_text": "Disney"}
{"answers": {"text": ["Fred Pierce", "Fred Pierce"], "answer_start": [23, 23]}, "prediction_text": "Fred Pierce"}
{"answers": {"text": ["Fred Silverman", "Fred Silverman", "Fred Silverman"], "answer_start": [95, 95, 95]}, "prediction_text": "Fred Pierce"}
{"answers": {"text": ["S.W.A.T", "S.W.A.T.", "S.W.A.T"], "answer_start": [341, 341, 341]}, "prediction_text": "S.W.A.T."}
{"answers": {"text": ["November 3, 1975", "November 3, 1975", "1973"], "answer_start": [1407, 1407, 644]}, "prediction_text": "November 3, 1975."}
{"answers": {"text": ["president of NBC's entertainment division", "president of NBC's entertainment division", "president of NBC"], "answer_start": [352, 352, 352]}, "prediction_text": "President of NBC's entertainment division."}
{"answers": {"text": ["Laverne & Shirley", "Laverne & Shirley", "Laverne & Shirley"], "answer_start": [471, 471, 471]}, "prediction_text": "Laverne & Shirley"}
{"answers": {"text": ["jiggle TV", "\"jiggle TV\"", "\"jiggle TV\""], "answer_start": [661, 660, 660]}, "prediction_text": "Jiggle TV"}
{"answers": {"text": ["Alex Haley", "Alex Haley", "Alex Haley"], "answer_start": [134, 134, 134]}, "prediction_text": "Alex Haley"}
{"answers": {"text": ["Aaron Spelling", "Aaron Spelling", "Aaron Spelling"], "answer_start": [262, 262, 262]}, "prediction_text": "Aaron Spelling"}
{"answers": {"text": ["nine seasons", "nine", "nine"], "answer_start": [491, 491, 491]}, "prediction_text": "Nine."}
{"answers": {"text": ["1976\u201377 season", "1976\u201377", "1976\u201377"], "answer_start": [770, 770, 770]}, "prediction_text": "1976-77"}
{"answers": {"text": ["Soap", "Soap", "Soap"], "answer_start": [829, 829, 829]}, "prediction_text": "Soap"}
{"answers": {"text": ["Roone Arledge", "Roone Arledge", "Roone Arledge"], "answer_start": [126, 126, 126]}, "prediction_text": "Roone Arledge"}
{"answers": {"text": ["ABC Sports", "ABC Sports", "ABC Sports"], "answer_start": [214, 214, 214]}, "prediction_text": "ABC Sports"}
{"answers": {"text": ["7 Lincoln Square", "7 Lincoln Square", "7 Lincoln Square"], "answer_start": [534, 534, 534]}, "prediction_text": "7 Lincoln Square"}
{"answers": {"text": ["June 1979", "June 1979", "1979"], "answer_start": [762, 762, 767]}, "prediction_text": "June 1979"}
{"answers": {"text": ["June 1978", "June 1978", "June 1978"], "answer_start": [3, 3, 3]}, "prediction_text": "June 1978"}
{"answers": {"text": ["Hugh Downs", "Hugh Downs", "Hugh Downs"], "answer_start": [291, 291, 291]}, "prediction_text": "Hugh Downs"}
{"answers": {"text": ["Barbara Walters", "Barbara Walters", "Barbara Walters"], "answer_start": [377, 377, 377]}, "prediction_text": "Barbara Walters"}
{"answers": {"text": ["MCA Inc.", "MCA Inc.", "MCA Inc"], "answer_start": [448, 448, 448]}, "prediction_text": "MCA Inc."}
{"answers": {"text": ["ABC Cable News", "ABC Cable News", "ABC Cable News"], "answer_start": [73, 73, 73]}, "prediction_text": "ABC Cable News"}
{"answers": {"text": ["ABC News Now", "ABC News Now", "ABC News Now"], "answer_start": [267, 267, 267]}, "prediction_text": "ABC News Now"}
{"answers": {"text": ["WJRT-TV", "WJRT-TV", "WJRT-TV"], "answer_start": [423, 423, 423]}, "prediction_text": "WJRT-TV"}
{"answers": {"text": ["WTVG", "WTVG", "WTVG"], "answer_start": [435, 435, 435]}, "prediction_text": "WJRT-TV"}
{"answers": {"text": ["Writers Guild of America", "The Writers Guild of America", "Writers Guild of America"], "answer_start": [4, 0, 4]}, "prediction_text": "Writers Guild of America (WGA)"}
{"answers": {"text": ["Duel", "Duel", "Duel"], "answer_start": [607, 607, 607]}, "prediction_text": "Duel"}
{"answers": {"text": ["Caris & Co.", "Caris & Co.", "Caris & Co."], "answer_start": [996, 996, 996]}, "prediction_text": "Caris & Co."}
{"answers": {"text": ["ABC Entertainment", "ABC Entertainment", "ABC Studios"], "answer_start": [50, 50, 72]}, "prediction_text": "ABC Entertainment Group"}
{"answers": {"text": ["ABC Entertainment Group", "ABC Entertainment Group", "Entertainment Group"], "answer_start": [105, 105, 109]}, "prediction_text": "ABC Entertainment Group"}
{"answers": {"text": ["Citadel Media", "Citadel Media", "Citadel Media"], "answer_start": [387, 387, 387]}, "prediction_text": "Citadel Media"}
{"answers": {"text": ["iTunes", "iTunes", "iTunes"], "answer_start": [640, 640, 640]}, "prediction_text": "iTunes"}
{"answers": {"text": ["2010", "2010", "2010"], "answer_start": [62, 62, 62]}, "prediction_text": "2010"}
{"answers": {"text": ["2004", "2004", "2004"], "answer_start": [172, 172, 172]}, "prediction_text": "2004"}
{"answers": {"text": ["Fridays", "Fridays", "Fridays"], "answer_start": [267, 267, 267]}, "prediction_text": "Fridays at the start of its fourth season."}
{"answers": {"text": ["Wednesdays", "Wednesdays", "Wednesdays"], "answer_start": [383, 383, 383]}, "prediction_text": "Wednesdays."}
{"answers": {"text": ["1970", "1970", "1970"], "answer_start": [42, 42, 42]}, "prediction_text": "1970"}
{"answers": {"text": ["Worldvision Enterprises", "Worldvision Enterprises", "Worldvision Enterprises"], "answer_start": [87, 87, 87]}, "prediction_text": "Worldvision Enterprises"}
{"answers": {"text": ["ABC Circle Films", "ABC Circle Films", "ABC Circle Films"], "answer_start": [145, 145, 145]}, "prediction_text": "ABC Circle Films"}
{"answers": {"text": ["Turner Broadcasting System", "Turner Broadcasting System", "Turner Broadcasting System"], "answer_start": [738, 738, 738]}, "prediction_text": "Turner Broadcasting System"}
{"answers": {"text": ["Disney\u2013ABC Domestic Television", "Disney\u2013ABC Domestic Television", "Disney\u2013ABC Domestic Television"], "answer_start": [343, 343, 343]}, "prediction_text": "Disney\u2013ABC Domestic Television"}
{"answers": {"text": ["Buena Vista Television", "Buena Vista Television", "Buena Vista Television"], "answer_start": [393, 393, 393]}, "prediction_text": "Buena Vista Television"}
{"answers": {"text": ["Buena Vista International Television", "Buena Vista International Television", "Buena Vista International Television"], "answer_start": [520, 520, 520]}, "prediction_text": "Buena Vista International Television"}
{"answers": {"text": ["Selznick library", "Selznick", "Selznick library"], "answer_start": [47, 47, 47]}, "prediction_text": "Selmur Productions catalog"}
{"answers": {"text": ["WABC-TV and WPVI-TV", "WABC-TV and WPVI-TV", "WABC-TV and WPVI-TV"], "answer_start": [73, 73, 73]}, "prediction_text": "WABC-TV and WPVI-TV."}
{"answers": {"text": ["eight", "eight", "eight"], "answer_start": [187, 187, 187]}, "prediction_text": "8"}
{"answers": {"text": ["235 additional television stations", "235", "235"], "answer_start": [274, 274, 274]}, "prediction_text": "235"}
{"answers": {"text": ["96.26%", "96.26%", "96.26%"], "answer_start": [539, 539, 539]}, "prediction_text": "96.26%"}
{"answers": {"text": ["1946", "1946", "1943"], "answer_start": [114, 114, 68]}, "prediction_text": "1946"}
{"answers": {"text": ["the seal of the Federal Communications Commission", "1953", "\"ABC\" enclosed in a circular shield surmounted by the bald eagle"], "answer_start": [367, 318, 435]}, "prediction_text": "The Federal Communications Commission seal."}
{"answers": {"text": ["1957", "1957", "1957"], "answer_start": [504, 504, 504]}, "prediction_text": "1957"}
{"answers": {"text": ["2011", "2011", "2011"], "answer_start": [9, 9, 9]}, "prediction_text": "2011"}
{"answers": {"text": ["Extreme Makeover: Home Edition", "Extreme Makeover: Home Edition", "Extreme Makeover: Home Edition"], "answer_start": [42, 42, 42]}, "prediction_text": "One Life to Live"}
{"answers": {"text": ["HD", "HD", "HD"], "answer_start": [235, 235, 235]}, "prediction_text": "HD"}
{"answers": {"text": ["Litton's Weekend Aventure", "Litton's Weekend Aventure", "Litton's Weekend Aventure"], "answer_start": [666, 666, 666]}, "prediction_text": "Litton's Weekend Aventure"}
{"answers": {"text": ["720p high definition", "720p high definition", "720p"], "answer_start": [36, 36, 36]}, "prediction_text": "720p high definition."}
{"answers": {"text": ["1080i HD", "1080i HD", "1080i HD"], "answer_start": [247, 247, 247]}, "prediction_text": "1080i HD"}
{"answers": {"text": ["11", "11", "11"], "answer_start": [263, 263, 263]}, "prediction_text": "11"}
{"answers": {"text": ["720p high definition", "720p high definition", "480i"], "answer_start": [36, 36, 336]}, "prediction_text": "720p high definition."}
{"answers": {"text": ["Body of Proof", "Body of Proof", "Body of Proof"], "answer_start": [215, 215, 215]}, "prediction_text": "Body of Proof"}
{"answers": {"text": ["Happy Endings", "Happy Endings", "Happy Endings"], "answer_start": [409, 409, 409]}, "prediction_text": "Happy Endings"}
{"answers": {"text": ["NBC", "NBC", "NBC"], "answer_start": [719, 719, 719]}, "prediction_text": "NBC"}
{"answers": {"text": ["V", "V", "Brothers & Sisters"], "answer_start": [567, 567, 479]}, "prediction_text": "Brothers & Sisters"}
{"answers": {"text": ["All My Children and One Life to Live", "All My Children and One Life to Live", "All My Children and One Life to Live"], "answer_start": [61, 61, 61]}, "prediction_text": "All My Children and One Life to Live."}
{"answers": {"text": ["Prospect Park", "Prospect Park", "Prospect Park"], "answer_start": [213, 213, 213]}, "prediction_text": "Prospect Park"}
{"answers": {"text": ["Hulu", "Hulu", "Hulu"], "answer_start": [266, 266, 266]}, "prediction_text": "Hulu"}
{"answers": {"text": ["The Revolution", "The Revolution", "The Revolution"], "answer_start": [619, 619, 619]}, "prediction_text": "The Revolution"}
{"answers": {"text": ["18\u201349 demographic", "18\u201349", "18\u201349"], "answer_start": [780, 780, 780]}, "prediction_text": "18-49."}
{"answers": {"text": ["2004", "eight years", "2004"], "answer_start": [487, 389, 487]}, "prediction_text": "2004"}
{"answers": {"text": ["CBS", "CBS", "CBS"], "answer_start": [554, 554, 554]}, "prediction_text": "CBS"}
{"answers": {"text": ["Agents of S.H.I.E.L.D.", "Agents of S.H.I.E.L.D"], "answer_start": [90, 90]}, "prediction_text": "The Goldbergs"}
{"answers": {"text": ["The Neighbors", "The Neighbors", "Agents of S.H.I.E.L.D"], "answer_start": [214, 214, 90]}, "prediction_text": "The Neighbors"}
{"answers": {"text": ["The Middle and Modern Family", "The Middle and Modern Family", "The Middle and Modern Family"], "answer_start": [193, 193, 193]}, "prediction_text": "Castle and Modern Family."}
{"answers": {"text": ["Dragon's Den", "Dragon's Den", "Dragon's Den"], "answer_start": [312, 312, 312]}, "prediction_text": "Dragon's Den"}
{"answers": {"text": ["Sundays", "Sundays", "Sundays"], "answer_start": [380, 380, 380]}, "prediction_text": "Sundays."}
{"answers": {"text": ["Tim Allen", "Tim Allen", "Tim Allen"], "answer_start": [617, 617, 617]}, "prediction_text": "Tim Allen"}
{"answers": {"text": ["Daniel Burke", "Daniel Burke", "Daniel Burke"], "answer_start": [0, 0, 0]}, "prediction_text": "Thomas Murphy"}
{"answers": {"text": ["Thomas Murphy", "Thomas Murphy", "Thomas Murphy"], "answer_start": [69, 69, 69]}, "prediction_text": "Thomas Murphy"}
{"answers": {"text": ["NYPD Blue", "NYPD Blue", "NYPD Blue"], "answer_start": [178, 178, 178]}, "prediction_text": "NYPD Blue"}
{"answers": {"text": ["Steven Bochco", "Steven Bochco", "Steven Bochco"], "answer_start": [221, 221, 221]}, "prediction_text": "Steven Bochco"}
{"answers": {"text": ["ten seasons", "ten", "ten"], "answer_start": [346, 346, 346]}, "prediction_text": "Ten seasons."}
{"answers": {"text": ["1993", "1993", "1993"], "answer_start": [3, 3, 3]}, "prediction_text": "1993"}
{"answers": {"text": ["DIC Entertainment", "DIC Entertainment", "DIC Entertainment"], "answer_start": [226, 226, 226]}, "prediction_text": "DIC Entertainment"}
{"answers": {"text": ["Time Warner Cable", "Time Warner Cable", "Time Warner Cable"], "answer_start": [278, 278, 278]}, "prediction_text": "Time Warner Cable"}
{"answers": {"text": ["23.63% of American households", "23.63% of American households", "23.63%"], "answer_start": [444, 444, 444]}, "prediction_text": "23.63%"}
{"answers": {"text": ["WLS", "WLS", "WLS"], "answer_start": [316, 316, 316]}, "prediction_text": "WLS"}
{"answers": {"text": ["May 9, 1960", "May 9, 1960", "May 9, 1960"], "answer_start": [448, 448, 448]}, "prediction_text": "May 9, 1960"}
{"answers": {"text": ["John Bassett", "John Bassett", "John Bassett"], "answer_start": [555, 555, 555]}, "prediction_text": "John Bassett"}
{"answers": {"text": ["CFTO-TV", "CFTO-TV", "CFTO-TV"], "answer_start": [730, 730, 730]}, "prediction_text": "CFTO-TV"}
{"answers": {"text": ["Wide World of Sports", "Wide World of Sports", "Wide World of Sports"], "answer_start": [214, 214, 214]}, "prediction_text": "Wide World of Sports"}
{"answers": {"text": ["Edgar Scherick", "Edgar Scherick", "Edgar Scherick"], "answer_start": [267, 267, 267]}, "prediction_text": "Edgar Scherick"}
{"answers": {"text": ["Roone Arledge", "Roone Arledge", "Roone Arledge"], "answer_start": [348, 348, 348]}, "prediction_text": "Roone Arledge"}
{"answers": {"text": ["Sports Programs, Inc.", "Sports Programs, Inc.", "Sports Programs, Inc"], "answer_start": [434, 434, 434]}, "prediction_text": "Sports Programs, Inc."}
{"answers": {"text": ["American Broadcasting Companies", "American Broadcasting Companies", "American Broadcasting Companies"], "answer_start": [92, 92, 92]}, "prediction_text": "American Broadcasting Companies"}
{"answers": {"text": ["The Dating Game", "The Dating Game", "The Dating Game"], "answer_start": [309, 309, 309]}, "prediction_text": "The Dating Game"}
{"answers": {"text": ["The Newlywed Game", "The Newlywed Game", "The Newlywed Game"], "answer_start": [551, 551, 551]}, "prediction_text": "The Dating Game"}
{"answers": {"text": ["1330 Avenue of the Americas in Manhattan", "1330 Avenue of the Americas in Manhattan", "1330 Avenue of the Americas in Manhattan"], "answer_start": [841, 841, 841]}, "prediction_text": "1330 Avenue of the Americas"}
{"answers": {"text": ["90%", "90%", "90%"], "answer_start": [128, 128, 128]}, "prediction_text": "90%"}
{"answers": {"text": ["Dynasty", "Dynasty", "Dynasty"], "answer_start": [260, 260, 260]}, "prediction_text": "Dynasty"}
{"answers": {"text": ["Mork & Mindy", "Mork & Mindy", "Mork & Mindy"], "answer_start": [689, 689, 689]}, "prediction_text": "Charlie's Angels"}
{"answers": {"text": ["Alpha Repertory Television Service (ARTS)", "Alpha Repertory Television Service (ARTS)", "Alpha Repertory Television Service (ARTS)"], "answer_start": [771, 771, 771]}, "prediction_text": "ARTS"}
{"answers": {"text": ["Infinity Broadcasting Corporation", "Infinity Broadcasting Corporation", "Infinity Broadcasting Corporation"], "answer_start": [30, 30, 30]}, "prediction_text": "Infinity Broadcasting Corporation"}
{"answers": {"text": ["Getty Oil", "Getty Oil", "Getty Oil's"], "answer_start": [260, 260, 260]}, "prediction_text": "Getty Oil"}
{"answers": {"text": ["The Entertainment Channel", "The Entertainment Channel", "The Entertainment Channel"], "answer_start": [653, 653, 653]}, "prediction_text": "The Entertainment Channel."}
{"answers": {"text": ["Arts & Entertainment Television (A&E)", "Arts & Entertainment Television (A&E)", "Arts & Entertainment Television (A&E)"], "answer_start": [715, 715, 715]}, "prediction_text": "Arts & Entertainment Television (A&E)"}
{"answers": {"text": ["Daniel B. Burke", "Daniel B. Burke", "Daniel B. Burke"], "answer_start": [65, 65, 65]}, "prediction_text": "Daniel B. Burke"}
{"answers": {"text": ["chairman and CEO", "chairman and CEO", "chairman"], "answer_start": [103, 103, 103]}, "prediction_text": "Chairman and CEO."}
{"answers": {"text": ["$465 million", "$465 million", "$465 million"], "answer_start": [161, 161, 161]}, "prediction_text": "$465 million"}
{"answers": {"text": ["America's Funniest Home Videos", "America's Funniest Home Videos", "America's Funniest Home Videos"], "answer_start": [282, 282, 282]}, "prediction_text": "America's Funniest Home Videos."}
{"answers": {"text": ["Home Improvement", "Home Improvement", "Home Improvement"], "answer_start": [667, 667, 667]}, "prediction_text": "Home Improvement"}
{"answers": {"text": ["General Hospital", "General Hospital", "General Hospital"], "answer_start": [314, 314, 314]}, "prediction_text": "General Hospital"}
{"answers": {"text": ["The View and The Chew", "The View and The Chew", "The View and The Chew"], "answer_start": [273, 273, 273]}, "prediction_text": "The View and The Chew."}
{"answers": {"text": ["7:00 to 9:00 a.m. weekdays", "7:00 to 9:00 a.m", "7:00 to 9:00 a.m."], "answer_start": [388, 388, 388]}, "prediction_text": "7:00 to 9:00 a.m. weekdays."}
{"answers": {"text": ["Jimmy Kimmel", "Jimmy Kimmel", "Jimmy Kimmel"], "answer_start": [840, 840, 840]}, "prediction_text": "Jimmy Kimmel Live!"}
{"answers": {"text": ["New Jersey, Rhode Island and Delaware", "New Jersey, Rhode Island and Delaware", "New Jersey, Rhode Island and Delaware"], "answer_start": [11, 11, 11]}, "prediction_text": "New Jersey, Rhode Island, and Delaware."}
{"answers": {"text": ["WBMA-LD", "WBMA-LD", "(WBMA-LD"], "answer_start": [489, 489, 488]}, "prediction_text": "WBMA-LD"}
{"answers": {"text": ["WBND-LD", "WBND-LD", "WBND-LD"], "answer_start": [545, 545, 545]}, "prediction_text": "WMDT"}
{"answers": {"text": ["WLQP-LP", "WLQP-LP", "WLQP-LP"], "answer_start": [511, 511, 511]}, "prediction_text": "WLQP-LP"}
{"answers": {"text": ["ABC Circle Films", "ABC Circle Films", "ABC Circle Films"], "answer_start": [222, 222, 222]}, "prediction_text": "ABC Circle Films"}
{"answers": {"text": ["ABC Studios", "ABC Studios", "ABC Studios"], "answer_start": [342, 342, 342]}, "prediction_text": "ABC Studios"}
{"answers": {"text": ["ABC Television Center", "ABC Television Center", "The Prospect Studios"], "answer_start": [437, 437, 464]}, "prediction_text": "The ABC Television Center."}
{"answers": {"text": ["ABC Television Center, East", "ABC Television Center, East", "ABC Television Center"], "answer_start": [589, 589, 589]}, "prediction_text": "ABC Television Center"}
{"answers": {"text": ["Times Square Studios", "Times Square Studios", "Times Square Studios"], "answer_start": [18, 18, 18]}, "prediction_text": "Good Morning America"}
{"answers": {"text": ["Good Morning America and Nightline", "Good Morning America and Nightline", "Good Morning America and Nightline"], "answer_start": [153, 153, 153]}, "prediction_text": "Good Morning America and Nightline."}
{"answers": {"text": ["Peter Jennings", "Peter Jennings", "Peter Jennings"], "answer_start": [468, 468, 468]}, "prediction_text": "Peter Jennings Way"}
{"answers": {"text": ["World News Tonight", "World News Tonight", "World News Tonight"], "answer_start": [574, 574, 574]}, "prediction_text": "World News Tonight"}
{"answers": {"text": ["ABC on Demand", "ABC on Demand", "ABC on Demand"], "answer_start": [140, 140, 140]}, "prediction_text": "ABC on Demand"}
{"answers": {"text": ["Hulu", "Hulu", "Hulu"], "answer_start": [266, 266, 266]}, "prediction_text": "Hulu"}
{"answers": {"text": ["July 6, 2009", "July 6, 2009", "July 6, 2009"], "answer_start": [518, 518, 518]}, "prediction_text": "July 6, 2009."}
{"answers": {"text": ["27% ownership stake", "27%", "27%"], "answer_start": [688, 688, 688]}, "prediction_text": "27%"}
{"answers": {"text": ["the day after their original broadcast", "the day after their original broadcast", "day after"], "answer_start": [112, 112, 116]}, "prediction_text": "WATCH ABC, Hulu, and ABC on Demand."}
{"answers": {"text": ["eight", "eight", "eight"], "answer_start": [487, 487, 487]}, "prediction_text": "eight days."}
{"answers": {"text": ["fast forwarding of accessed content", "fast forwarding", "fast forwarding"], "answer_start": [282, 282, 282]}, "prediction_text": "Fast forwarding."}
{"answers": {"text": ["January 7, 2014", "January 7, 2014", "January 7, 2014"], "answer_start": [378, 378, 378]}, "prediction_text": "January 7, 2014."}
{"answers": {"text": ["LoyalKaspar", "LoyalKaspar", "LoyalKaspar"], "answer_start": [317, 317, 317]}, "prediction_text": "LoyalKaspar"}
{"answers": {"text": ["four variants", "four", "four"], "answer_start": [566, 566, 566]}, "prediction_text": "Four."}
{"answers": {"text": ["ABC Modern", "ABC Modern", "ABC Modern"], "answer_start": [1009, 1009, 1009]}, "prediction_text": "\"ABC Modern\" (which was inspired by the logotype)"}
{"answers": {"text": ["ESPN", "ESPN on ABC", "ESPN"], "answer_start": [893, 893, 893]}, "prediction_text": "ESPN"}
{"answers": {"text": ["14", "14", "14"], "answer_start": [264, 264, 264]}, "prediction_text": "14"}
{"answers": {"text": ["74", "74", "71"], "answer_start": [302, 302, 355]}, "prediction_text": "74"}
{"answers": {"text": ["All-Channel Receiver Act", "All-Channel Receiver Act", "All-Channel Receiver Act"], "answer_start": [665, 665, 665]}, "prediction_text": "All-Channel Receiver Act (1961)"}
{"answers": {"text": ["UHF tuning", "UHF", "UHF"], "answer_start": [745, 745, 745]}, "prediction_text": "UHF tuning."}
{"answers": {"text": ["Youngstown", "WKST-TV in Youngstown", "Youngstown"], "answer_start": [221, 210, 221]}, "prediction_text": "Youngstown"}
{"answers": {"text": ["five times lower viewership", "five times", "five times"], "answer_start": [477, 477, 477]}, "prediction_text": "Five times lower."}
{"answers": {"text": ["WTRF-TV", "WTRF-TV", "WTRF-TV"], "answer_start": [966, 966, 966]}, "prediction_text": "WTRF-TV"}
{"answers": {"text": ["1980s", "1980s or even the advent of digital television in the 2000s", "1980s or even the advent of digital television in the 2000s"], "answer_start": [877, 877, 877]}, "prediction_text": "1960s"}
{"answers": {"text": ["Walt Disney Presents", "Walt Disney Presents", "Walt Disney Presents"], "answer_start": [67, 67, 67]}, "prediction_text": "Walt Disney Presents"}
{"answers": {"text": ["Desilu Productions", "Desilu Productions", "Desilu Productions"], "answer_start": [446, 446, 446]}, "prediction_text": "Desilu Productions"}
{"answers": {"text": ["its use of violence", "its use of violence", "violence"], "answer_start": [567, 567, 578]}, "prediction_text": "Violence."}
{"answers": {"text": ["April 1959", "April 1959", "April 1959"], "answer_start": [687, 687, 687]}, "prediction_text": "April 1959"}
{"answers": {"text": ["ABC Sunday Night Movie", "ABC Sunday Night Movie", "ABC Sunday Night Movie"], "answer_start": [230, 230, 230]}, "prediction_text": "Sunday Night Movie"}
{"answers": {"text": ["$15.5 million", "$15.5", "$15.5 million"], "answer_start": [540, 540, 540]}, "prediction_text": "$15.5 million"}
{"answers": {"text": ["Hanna-Barbera", "Hanna-Barbera", "Hanna-Barbera"], "answer_start": [694, 694, 694]}, "prediction_text": "Hanna-Barbera"}
{"answers": {"text": ["The Jetsons", "The Jetsons", "The Jetsons"], "answer_start": [709, 709, 709]}, "prediction_text": "The Flintstones."}
{"answers": {"text": ["April 1, 1963", "April 1, 1963", "April 1, 1963"], "answer_start": [833, 833, 833]}, "prediction_text": "1963"}
{"answers": {"text": ["ITT", "ITT", "ITT"], "answer_start": [64, 64, 64]}, "prediction_text": "ITT"}
{"answers": {"text": ["Donald F. Turner", "Donald F. Turner", "Donald F. Turner"], "answer_start": [234, 234, 234]}, "prediction_text": "Donald F. Turner"}
{"answers": {"text": ["Department of Justice", "Department of Justice", "Department of Justice"], "answer_start": [683, 683, 683]}, "prediction_text": "Department of Justice"}
{"answers": {"text": ["January 1, 1968", "January 1, 1968", "January 1, 1968"], "answer_start": [827, 827, 827]}, "prediction_text": "January 1, 1968."}
{"answers": {"text": ["Capital Cities Communications", "Capital Cities Communications", "Capital Cities Communications"], "answer_start": [63, 63, 63]}, "prediction_text": "Capital Cities Communications"}
{"answers": {"text": ["$3.5 billion", "$3.5 billion and $118 for each of ABC's shares as well as a guarantee of 10% (or $3) for a total of $121 per share", "$3.5 billion"], "answer_start": [362, 362, 362]}, "prediction_text": "$3.5 billion and $118 for each of ABC's shares."}
{"answers": {"text": ["Warren Buffett", "Warren Buffett", "Warren Buffett"], "answer_start": [868, 868, 868]}, "prediction_text": "Warren Buffett"}
{"answers": {"text": ["E. W. Scripps Company", "E. W. Scripps Company", "E. W. Scripps Company"], "answer_start": [1375, 1375, 1375]}, "prediction_text": "E. W. Scripps Company"}
{"answers": {"text": ["12 television stations", "12", "12"], "answer_start": [1245, 1245, 1245]}, "prediction_text": "12"}
{"answers": {"text": ["September 5, 1985", "September 5, 1985", "September 5, 1985"], "answer_start": [71, 71, 71]}, "prediction_text": "September 5, 1985."}
{"answers": {"text": ["Capital Cities/ABC, Inc.", "Capital Cities/ABC, Inc.", "Capital Cities/ABC, Inc."], "answer_start": [205, 205, 205]}, "prediction_text": "Capital Cities/ABC, Inc."}
{"answers": {"text": ["president of ABC's broadcasting division", "president of ABC's broadcasting division", "president"], "answer_start": [603, 603, 603]}, "prediction_text": "President of ABC Broadcasting."}
{"answers": {"text": ["Michael P. Millardi", "Michael P. Millardi", "Michael P. Millardi"], "answer_start": [645, 645, 645]}, "prediction_text": "Michael P. Millardi"}
{"answers": {"text": ["Roone Arledge", "Roone Arledge", "Roone Arledge"], "answer_start": [960, 960, 960]}, "prediction_text": "Roone Arledge"}
{"answers": {"text": ["Laverne & Shirley", "Laverne & Shirley", "Laverne & Shirley"], "answer_start": [116, 116, 116]}, "prediction_text": "Laverne & Shirley"}
{"answers": {"text": ["Three's Company", "Three's Company", "Three's Company"], "answer_start": [172, 172, 172]}, "prediction_text": "The Love Boat"}
{"answers": {"text": ["NBC", "NBC", "NBC"], "answer_start": [348, 348, 348]}, "prediction_text": "NBC"}
{"answers": {"text": ["The Love Boat", "The Love Boat", "The Love Boat"], "answer_start": [269, 269, 269]}, "prediction_text": "The Love Boat"}
{"answers": {"text": ["comedies and family-oriented series", "comedies and family-oriented", "comedies and family-oriented"], "answer_start": [543, 543, 543]}, "prediction_text": "Family-oriented series."}
{"answers": {"text": ["the \"TGIF\" block", "TGIF", "TGIF"], "answer_start": [170, 175, 175]}, "prediction_text": "TGIF"}
{"answers": {"text": ["Thank Goodness It's Funny", "Thank Goodness It's Funny", "Thank Goodness It's Funny"], "answer_start": [227, 227, 227]}, "prediction_text": "Thank Goodness It's Funny"}
{"answers": {"text": ["Miller-Boyett Productions", "Miller-Boyett Productions", "Miller-Boyett Productions"], "answer_start": [329, 329, 329]}, "prediction_text": "Miller-Boyett Productions"}
{"answers": {"text": ["Warner Bros.", "Warner Bros.", "Warner Bros"], "answer_start": [358, 358, 358]}, "prediction_text": "Warner Bros."}
{"answers": {"text": ["seven radio stations", "seven", "seven"], "answer_start": [117, 117, 117]}, "prediction_text": "seven."}
{"answers": {"text": ["Charly", "Charly", "Charly"], "answer_start": [457, 457, 457]}, "prediction_text": "Charly"}
{"answers": {"text": ["Ralph Nelson", "Ralph Nelson", "ABC Pictures"], "answer_start": [435, 435, 347]}, "prediction_text": "Ralph Nelson"}
{"answers": {"text": ["1985", "1985", "1985"], "answer_start": [535, 535, 535]}, "prediction_text": "1985"}
{"answers": {"text": ["Redwood City, California", "Redwood City, California", "July 1968"], "answer_start": [751, 751, 639]}, "prediction_text": "Redwood City, California"}
{"answers": {"text": ["westerns and detective series", "westerns and detective series", "westerns and detective series"], "answer_start": [212, 212, 212]}, "prediction_text": "Westerns and detective series."}
{"answers": {"text": ["500%", "almost 500%", "500%"], "answer_start": [261, 254, 261]}, "prediction_text": "500%"}
{"answers": {"text": ["between 10% and 18%", "between 10% and 18%", "10% and 18%"], "answer_start": [363, 363, 371]}, "prediction_text": "10-18%."}
{"answers": {"text": ["Ollie Treiz", "Ollie Treiz", "Ollie Treiz"], "answer_start": [512, 512, 512]}, "prediction_text": "Ollie Treiz"}
{"answers": {"text": ["Dick Clark", "Dick Clark", "Dick Clark"], "answer_start": [907, 907, 907]}, "prediction_text": "Dick Clark"}
{"answers": {"text": ["counterprogramming", "counterprogramming against its competitors", "\"philosophy of counterprogramming against its competitors\""], "answer_start": [74, 74, 59]}, "prediction_text": "Counterprogramming."}
{"answers": {"text": ["Zorro", "Zorro", "Zorro"], "answer_start": [353, 353, 353]}, "prediction_text": "Zorro"}
{"answers": {"text": ["Life", "Life", "Life"], "answer_start": [644, 644, 644]}, "prediction_text": "Life"}
{"answers": {"text": ["detective shows", "detective", "detective"], "answer_start": [457, 457, 457]}, "prediction_text": "Detective shows."}
{"answers": {"text": ["WATCH ABC", "WATCH ABC", "WATCH ABC"], "answer_start": [27, 27, 27]}, "prediction_text": "WATCH ABC"}
{"answers": {"text": ["New York City O&O WABC-TV and Philadelphia O&O WPVI-TV", "New York City O&O WABC-TV and Philadelphia O&O WPVI-TV", "WABC-TV and Philadelphia O&O WPVI-TV"], "answer_start": [715, 715, 733]}, "prediction_text": "Hearst Television."}
{"answers": {"text": ["Hearst Television", "Hearst Television", "Hearst Television"], "answer_start": [993, 993, 993]}, "prediction_text": "Hearst Television"}
{"answers": {"text": ["WatchESPN", "WatchESPN", "WatchESPN"], "answer_start": [457, 457, 457]}, "prediction_text": "WatchESPN"}
{"answers": {"text": ["Sinclair Broadcast Group", "Sinclair Broadcast Group", "Sinclair Broadcast Group"], "answer_start": [4, 4, 4]}, "prediction_text": "Sinclair Broadcasting Group"}
{"answers": {"text": ["WABM-DT2/WDBB-DT2 in the Birmingham market", "WABM-DT2/WDBB-DT2", "Sinclair"], "answer_start": [253, 253, 186]}, "prediction_text": "WABM-DT2/WDBB-DT2"}
{"answers": {"text": ["E. W. Scripps Company", "The E. W. Scripps Company", "E. W. Scripps Company"], "answer_start": [533, 529, 533]}, "prediction_text": "Sinclair Broadcasting Group"}
{"answers": {"text": ["28 ABC affiliates and two additional subchannel-only affiliates", "28 ABC affiliates and two additional subchannel-only affiliates", "28 ABC affiliates and two additional subchannel-only"], "answer_start": [121, 121, 121]}, "prediction_text": "28"}
{"answers": {"text": ["15", "15"], "answer_start": [636, 636]}, "prediction_text": "15"}
{"answers": {"text": ["Start Here", "\"Start Here\"", "Start Here"], "answer_start": [97, 96, 97]}, "prediction_text": "Start Here"}
{"answers": {"text": ["Troika", "Troika", "Troika"], "answer_start": [128, 128, 128]}, "prediction_text": "Troika"}
{"answers": {"text": ["the entertainment division", "entertainment", "entertainment"], "answer_start": [708, 712, 712]}, "prediction_text": "Entertainment Division"}
{"answers": {"text": ["ABC News", "ABC News", "ABC News"], "answer_start": [769, 769, 769]}, "prediction_text": "Entertainment Division"}
{"answers": {"text": ["WFTS-TV and WWSB", "WFTS-TV and WWSB", "WFTS-TV and WWSB"], "answer_start": [138, 138, 138]}, "prediction_text": "WFTS-TV and WWSB"}
{"answers": {"text": ["KMBC-TV and KQTV", "KMBC-TV and KQTV", "KMBC-TV and KQTV"], "answer_start": [262, 262, 262]}, "prediction_text": "KQTV and KQTV."}
{"answers": {"text": ["WZZM and WOTV", "WZZM and WOTV", "WZZM and WOTV"], "answer_start": [184, 184, 184]}, "prediction_text": "WZZM and WOTV."}
{"answers": {"text": ["WTSP", "WTSP", "WTSP"], "answer_start": [613, 613, 613]}, "prediction_text": "WTSP"}
{"answers": {"text": ["the Mongol Empire", "Mongol Empire", "Mongol Empire"], "answer_start": [89, 93, 93]}, "prediction_text": "Mongol Empire"}
{"answers": {"text": ["many of the nomadic tribes of Northeast Asia", "nomadic tribes of Northeast Asia", "many of the nomadic tribes of Northeast Asia."], "answer_start": [28, 40, 28]}, "prediction_text": "Many of the nomadic tribes of Northeast Asia."}
{"answers": {"text": ["Khwarezmian and Xia controlled lands", "Khwarezmian and Xia controlled lands", "Qara Khitai, Caucasus, Khwarezmid Empire, Western Xia and Jin dynasties"], "answer_start": [450, 450, 267]}, "prediction_text": "Khwarezmian and Xia controlled lands."}
{"answers": {"text": ["a substantial portion of Central Asia and China", "substantial portion of Central Asia and China", "a substantial portion of Central Asia and China"], "answer_start": [539, 541, 539]}, "prediction_text": "Central Asia and China."}
{"answers": {"text": ["the Qara Khitai, Caucasus, Khwarezmid Empire, Western Xia and Jin dynasties", "Qara Khitai, Caucasus, Khwarezmid Empire, Western Xia and Jin", "Khwarezmian and Xia controlled lands"], "answer_start": [263, 267, 450]}, "prediction_text": "Western Xia, Jin dynasty."}
{"answers": {"text": ["\u00d6gedei Khan", "\u00d6gedei Khan", "\u00d6gedei Khan"], "answer_start": [38, 38, 38]}, "prediction_text": "\u00d6gedei Khan"}
{"answers": {"text": ["1227", "1227", "1227"], "answer_start": [143, 143, 143]}, "prediction_text": "1227"}
{"answers": {"text": ["Western Xia", "Western Xia", "Western Xia"], "answer_start": [168, 168, 168]}, "prediction_text": "Western Xia"}
{"answers": {"text": ["his sons and grandsons", "sons and grandsons", "his sons and grandsons."], "answer_start": [108, 112, 108]}, "prediction_text": "His sons and grandsons."}
{"answers": {"text": ["somewhere in Mongolia at an unknown location", "Mongolia", "unmarked grave somewhere in Mongolia"], "answer_start": [216, 229, 201]}, "prediction_text": "Mongolia"}
{"answers": {"text": ["Del\u00fc\u00fcn Boldog", "Del\u00fc\u00fcn Boldog", "Del\u00fc\u00fcn Boldog,"], "answer_start": [37, 37, 37]}, "prediction_text": "Del\u00fc\u00fcn Boldog."}
{"answers": {"text": ["Yes\u00fcgei, a Khamag Mongol's major chief of the Kiyad", "Yes\u00fcgei", "Yes\u00fcgei"], "answer_start": [405, 405, 405]}, "prediction_text": "Yes\u00fcgei"}
{"answers": {"text": ["1162", "1162", "1162"], "answer_start": [29, 29, 29]}, "prediction_text": "1162"}
{"answers": {"text": ["a Tatar chieftain, Tem\u00fcjin-\u00fcge, whom his father had just captured", "Tem\u00fcjin-\u00fcge", "Tatar chieftain, Tem\u00fcjin-\u00fcge"], "answer_start": [605, 624, 607]}, "prediction_text": "Tem\u00fcjin-\u00fcge"}
{"answers": {"text": ["Tem\u00fclen", "Tem\u00fclen", "Tem\u00fclen"], "answer_start": [82, 82, 82]}, "prediction_text": "Tem\u00fclen"}
{"answers": {"text": ["Hasar, Hachiun, and Tem\u00fcge", "Hasar, Hachiun, and Tem\u00fcge", "Hasar, Hachiun, and Tem\u00fcge"], "answer_start": [33, 33, 33]}, "prediction_text": "Hasar, Hachiun, Tem\u00fcge."}
{"answers": {"text": ["B\u00f6rte", "B\u00f6rte", "B\u00f6rte"], "answer_start": [349, 349, 349]}, "prediction_text": "B\u00f6rte"}
{"answers": {"text": ["Khongirad", "Khongirad", "Khongirad"], "answer_start": [386, 386, 386]}, "prediction_text": "Khongirad"}
{"answers": {"text": ["Dai Setsen", "Dai Setsen", "Dai Setsen"], "answer_start": [437, 437, 437]}, "prediction_text": "Dai Setsen"}
{"answers": {"text": ["Begter", "Begter", "Begter"], "answer_start": [189, 189, 189]}, "prediction_text": "Khasar"}
{"answers": {"text": ["Hoelun", "Hoelun", "Temujin's mother"], "answer_start": [320, 320, 303]}, "prediction_text": "Hoelun"}
{"answers": {"text": ["Tem\u00fcjin and his brother Khasar", "Tem\u00fcjin and his brother Khasar", "Tem\u00fcjin and his brother Khasar"], "answer_start": [484, 484, 484]}, "prediction_text": "Khasar"}
{"answers": {"text": ["during one hunting excursion", "one hunting excursion", "during one hunting excursion"], "answer_start": [450, 457, 450]}, "prediction_text": "During one hunting excursion."}
{"answers": {"text": ["the Tayichi'ud", "Tayichi'ud", "his father's former allies, the Tayichi'ud"], "answer_start": [109, 113, 81]}, "prediction_text": "Father's former allies."}
{"answers": {"text": ["with a cangue, a sort of portable stocks", "cangue", "cangue, a sort of portable stocks"], "answer_start": [169, 176, 176]}, "prediction_text": "By his father's former allies."}
{"answers": {"text": ["Chilaun", "Chilaun", "the father of Chilaun"], "answer_start": [268, 268, 254]}, "prediction_text": "Jelme and Bo'orchu."}
{"answers": {"text": ["Jelme and Bo'orchu", "Jelme and Bo'orchu", "Jelme and Bo'orchu"], "answer_start": [467, 467, 467]}, "prediction_text": "Jelme and Bo'orchu."}
{"answers": {"text": ["a river crevice", "a river crevice", "a river crevice."], "answer_start": [404, 404, 404]}, "prediction_text": "A river crevice."}
{"answers": {"text": ["arranged marriages", "arranged marriages", "arranged marriages"], "answer_start": [89, 89, 89]}, "prediction_text": "Tem\u00fcjin's mother Hoelun."}
{"answers": {"text": ["Tem\u00fcjin's mother Hoelun", "Hoelun", "Tem\u00fcjin's mother Hoelun"], "answer_start": [457, 474, 457]}, "prediction_text": "Hoelun"}
{"answers": {"text": ["the Chinese dynasties to the south", "Chinese dynasties", "Chinese dynasties to the south"], "answer_start": [421, 425, 425]}, "prediction_text": "Chinese dynasties to the south."}
{"answers": {"text": ["the need for alliances", "need for alliances", "need for alliances."], "answer_start": [566, 570, 570]}, "prediction_text": "The need for alliances."}
{"answers": {"text": ["the Onggirat", "Onggirat", "Onggirat tribe"], "answer_start": [63, 67, 67]}, "prediction_text": "Onggirat"}
{"answers": {"text": ["the Merkits", "Merkits", "the Merkits"], "answer_start": [227, 231, 227]}, "prediction_text": "Merkits"}
{"answers": {"text": ["Jamukha, and his protector, Toghrul Khan of the Keraite tribe", "Jamukha, and his protector, Toghrul Khan", "Jamukha, and his protector, Toghrul Khan"], "answer_start": [342, 342, 342]}, "prediction_text": "Jamukha"}
{"answers": {"text": ["Jochi", "Jochi", "Jochi"], "answer_start": [430, 430, 430]}, "prediction_text": "Jochi"}
{"answers": {"text": ["1185", "1185", "(1185"], "answer_start": [437, 437, 436]}, "prediction_text": "1226"}
{"answers": {"text": ["three", "three", "three"], "answer_start": [10, 10, 10]}, "prediction_text": "3 sons."}
{"answers": {"text": ["Chagatai", "Chagatai", "\u00d6gedei"], "answer_start": [27, 27, 49]}, "prediction_text": "Chagatai"}
{"answers": {"text": ["1241", "1241", "1241"], "answer_start": [42, 42, 42]}, "prediction_text": "1241"}
{"answers": {"text": ["Tolui", "Tolui", "Tolui"], "answer_start": [73, 73, 73]}, "prediction_text": "\u00d6gedei"}
{"answers": {"text": ["six", "six", "six"], "answer_start": [283, 283, 283]}, "prediction_text": "Six."}
{"answers": {"text": ["sworn brother or blood brother", "sworn brother", "sworn brother or blood brother)"], "answer_start": [130, 130, 130]}, "prediction_text": "Sworn brother or blood brother."}
{"answers": {"text": ["Toghrul", "Toghrul", "Toghrul"], "answer_start": [162, 162, 162]}, "prediction_text": "Toghrul"}
{"answers": {"text": ["the Keraites", "Keraites", "Keraites"], "answer_start": [187, 191, 191]}, "prediction_text": "Jadaran"}
{"answers": {"text": ["20,000", "20,000", "20,000"], "answer_start": [468, 468, 468]}, "prediction_text": "20,000"}
{"answers": {"text": ["Jamukha", "Jamukha", "Jamukha"], "answer_start": [555, 555, 555]}, "prediction_text": "Jamukha"}
{"answers": {"text": ["the traditional Mongolian aristocracy", "traditional Mongolian aristocracy", "the traditional Mongolian aristocracy"], "answer_start": [132, 136, 132]}, "prediction_text": "Traditional Mongolian aristocracy."}
{"answers": {"text": ["Kokochu", "Kokochu", "Kokochu"], "answer_start": [357, 357, 357]}, "prediction_text": "Kokochu"}
{"answers": {"text": ["1186", "1186", "1186"], "answer_start": [462, 462, 462]}, "prediction_text": "1186"}
{"answers": {"text": ["Battle of Dalan Balzhut", "Battle of Dalan Balzhut", "1187"], "answer_start": [819, 819, 611]}, "prediction_text": "Battle of Dalan Balzhut."}
{"answers": {"text": ["Qara Khitai", "Qara Khitai", "Qara Khitai."], "answer_start": [1084, 1084, 1084]}, "prediction_text": "Qara Khitai"}
{"answers": {"text": ["the Yassa code", "Yassa code", "Yassa code"], "answer_start": [70, 74, 74]}, "prediction_text": "Yassa code"}
{"answers": {"text": ["wealth from future possible war spoils", "wealth", "wealth"], "answer_start": [126, 126, 126]}, "prediction_text": "wealth."}
{"answers": {"text": ["orphans from the conquered tribe", "orphans", "orphans from the conquered tribe"], "answer_start": [395, 395, 395]}, "prediction_text": "Orphans."}
{"answers": {"text": ["his protection", "political innovations", "protection and integrated its members into his own tribe."], "answer_start": [297, 466, 301]}, "prediction_text": "wealth."}
{"answers": {"text": ["Jochi", "Jochi", "Jochi,"], "answer_start": [112, 112, 112]}, "prediction_text": "Jochi"}
{"answers": {"text": ["Jamukha", "Jamukha", "Jamukha,"], "answer_start": [295, 295, 295]}, "prediction_text": "Jamukha"}
{"answers": {"text": ["Jamukha", "Jamukha", "Jamukha"], "answer_start": [485, 485, 485]}, "prediction_text": "Jamukha"}
{"answers": {"text": ["the Keraite", "Keraite", "Keraite tribe"], "answer_start": [590, 594, 594]}, "prediction_text": "Keraite tribe"}
{"answers": {"text": ["the Naimans", "Naimans", "the Naimans"], "answer_start": [38, 42, 38]}, "prediction_text": "Subutai"}
{"answers": {"text": ["1201", "1201", "1201"], "answer_start": [213, 213, 213]}, "prediction_text": "1201"}
{"answers": {"text": ["universal ruler", "universal ruler", "universal ruler"], "answer_start": [261, 261, 261]}, "prediction_text": "Universal ruler."}
{"answers": {"text": ["Subutai", "Subutai", "Subutai"], "answer_start": [528, 528, 528]}, "prediction_text": "Subutai"}
{"answers": {"text": ["1206", "1206", "1206"], "answer_start": [657, 657, 657]}, "prediction_text": "1206"}
{"answers": {"text": ["his friendship", "friendship", "friendship"], "answer_start": [55, 59, 59]}, "prediction_text": "Friendship."}
{"answers": {"text": ["he did not want disloyal men in his army", "did not want disloyal men in his army", "he did not want disloyal men in his army"], "answer_start": [178, 181, 178]}, "prediction_text": "To break the back."}
{"answers": {"text": ["a noble death", "a noble death", "a noble death."], "answer_start": [340, 340, 340]}, "prediction_text": "A noble death."}
{"answers": {"text": ["breaking the back", "breaking the back", "breaking the back"], "answer_start": [420, 420, 420]}, "prediction_text": "Noble death."}
{"answers": {"text": ["the Chinese", "Chinese", "the Chinese"], "answer_start": [698, 702, 698]}, "prediction_text": "Chinese"}
{"answers": {"text": ["Jamukha", "Jamukha", "tribes led by Jamukha"], "answer_start": [826, 826, 812]}, "prediction_text": "Jamukha"}
{"answers": {"text": ["Khasar", "Khasar", "Khasar"], "answer_start": [382, 382, 382]}, "prediction_text": "Khasar"}
{"answers": {"text": ["Yam route systems", "understanding the motivations of his rivals"], "answer_start": [562, 472]}, "prediction_text": "Yam route systems."}
{"answers": {"text": ["Wang Khan", "Wang Khan", "Wang Khan"], "answer_start": [207, 207, 207]}, "prediction_text": "Wang Khan"}
{"answers": {"text": ["1206", "1206", "1206"], "answer_start": [16, 16, 16]}, "prediction_text": "1206"}
{"answers": {"text": ["Khuruldai", "Khuruldai", "Khuruldai"], "answer_start": [254, 254, 254]}, "prediction_text": "Khuruldai"}
{"answers": {"text": ["Khagan", "Khagan", "Khagan"], "answer_start": [404, 404, 404]}, "prediction_text": "Khagan"}
{"answers": {"text": ["\u00d6gedei", "\u00d6gedei", "\u00d6gedei"], "answer_start": [491, 491, 491]}, "prediction_text": "\u00d6gedei"}
{"answers": {"text": ["a council of Mongol chiefs", "a council of Mongol chiefs", "a council of Mongol chiefs"], "answer_start": [265, 265, 265]}, "prediction_text": "\u00d6gedei"}
{"answers": {"text": ["the Jin dynasty", "Jin dynasty.", "Jin dynasty"], "answer_start": [82, 86, 86]}, "prediction_text": "Jin dynasty"}
{"answers": {"text": ["Ming-Tan", "Ming-Tan", "Ming-Tan"], "answer_start": [261, 261, 261]}, "prediction_text": "Ming-Tan"}
{"answers": {"text": ["1215", "1215", "1215"], "answer_start": [489, 489, 489]}, "prediction_text": "1215"}
{"answers": {"text": ["Kaifeng", "Kaifeng", "Kaifeng,"], "answer_start": [644, 644, 644]}, "prediction_text": "Kaifeng"}
{"answers": {"text": ["\u00d6gedei Khan", "\u00d6gedei Khan", "\u00d6gedei Khan"], "answer_start": [803, 803, 803]}, "prediction_text": "\u00d6gedei Khan"}
{"answers": {"text": ["Kuchlug", "Kuchlug", "Kuchlug"], "answer_start": [0, 0, 0]}, "prediction_text": "Genghis Khan"}
{"answers": {"text": ["the Liao dynasty", "Liao", "Liao dynasty"], "answer_start": [241, 245, 245]}, "prediction_text": "Liao dynasty"}
{"answers": {"text": ["20,000", "20,000", "20,000"], "answer_start": [537, 537, 537]}, "prediction_text": "Two tumen."}
{"answers": {"text": ["Jebe", "Jebe", "Jebe"], "answer_start": [598, 598, 598]}, "prediction_text": "Jebe"}
{"answers": {"text": ["The Arrow", "The Arrow", "The Arrow"], "answer_start": [614, 614, 614]}, "prediction_text": "The Arrow"}
{"answers": {"text": ["inciting internal revolt", "inciting internal revolt", "inciting internal revolt among Kuchlug's supporters"], "answer_start": [93, 93, 93]}, "prediction_text": "Inciting internal revolt."}
{"answers": {"text": ["west of Kashgar", "west of Kashgar", "west of Kashgar"], "answer_start": [247, 247, 247]}, "prediction_text": "Kashgar"}
{"answers": {"text": ["Lake Balkhash", "Lake Balkhash", "Lake Balkhash,"], "answer_start": [443, 443, 443]}, "prediction_text": "Lake Balkhash."}
{"answers": {"text": ["Khwarezmid Empire", "Khwarezmid", "Khwarezmid Empire"], "answer_start": [489, 489, 489]}, "prediction_text": "Lake Balkhash"}
{"answers": {"text": ["a Muslim state", "Muslim", "Muslim"], "answer_start": [509, 511, 511]}, "prediction_text": "Muslim state"}
{"answers": {"text": ["Shah Ala ad-Din Muhammad", "Shah Ala ad-Din Muhammad", "Shah Ala ad-Din Muhammad"], "answer_start": [67, 67, 67]}, "prediction_text": "Shah Ala ad-Din Muhammad"}
{"answers": {"text": ["Inalchuq", "Inalchuq", "Inalchuq"], "answer_start": [300, 300, 300]}, "prediction_text": "Inalchuq"}
{"answers": {"text": ["the Muslim", "Muslim", "the Muslim"], "answer_start": [845, 742, 845]}, "prediction_text": "Muslim"}
{"answers": {"text": ["100,000", "100,000", "100,000 soldiers"], "answer_start": [1079, 1079, 1079]}, "prediction_text": "100,000 soldiers."}
{"answers": {"text": ["the Silk Road", "Silk Road", "Silk Road"], "answer_start": [186, 190, 190]}, "prediction_text": "Silk Road"}
{"answers": {"text": ["Tien Shan", "Tien Shan", "Tien Shan"], "answer_start": [70, 70, 70]}, "prediction_text": "Tien Shan Mountains"}
{"answers": {"text": ["three", "three", "three groups"], "answer_start": [261, 261, 261]}, "prediction_text": "Three."}
{"answers": {"text": ["the southeast", "southeast", "the southeast part of Khwarzemia"], "answer_start": [397, 401, 397]}, "prediction_text": "Southeast part of Khwarzemia."}
{"answers": {"text": ["Tolui", "Tolui", "Genghis Khan and Tolui"], "answer_start": [536, 536, 519]}, "prediction_text": "Tolui"}
{"answers": {"text": ["Samarkand", "Samarkand", "Samarkand"], "answer_start": [483, 483, 483]}, "prediction_text": "Northeast."}
{"answers": {"text": ["fragmentation", "fragmentation", "fragmentation"], "answer_start": [156, 156, 156]}, "prediction_text": "The Shah's army."}
{"answers": {"text": ["Otrar", "Otrar", "Otrar"], "answer_start": [432, 432, 432]}, "prediction_text": "Otrar"}
{"answers": {"text": ["silver", "silver", "molten silver"], "answer_start": [627, 627, 620]}, "prediction_text": "Silver"}
{"answers": {"text": ["fled", "fled", "fled"], "answer_start": [726, 726, 726]}, "prediction_text": "fled."}
{"answers": {"text": ["Subutai and Jebe", "Subutai and Jebe", "Subutai and Jebe"], "answer_start": [775, 775, 775]}, "prediction_text": "Subutai and Jebe."}
{"answers": {"text": ["Samarkand", "Samarkand", "Samarkand"], "answer_start": [82, 82, 82]}, "prediction_text": "Samarkand"}
{"answers": {"text": ["Bukhara", "Bukhara", "Bukhara"], "answer_start": [123, 123, 123]}, "prediction_text": "Bukhara"}
{"answers": {"text": ["a river", "a river", "a river"], "answer_start": [443, 443, 443]}, "prediction_text": "River."}
{"answers": {"text": ["captured enemies", "captured enemies", "captured enemies"], "answer_start": [37, 37, 37]}, "prediction_text": "captured enemies"}
{"answers": {"text": ["reneged", "reneged", "executed every soldier"], "answer_start": [220, 220, 255]}, "prediction_text": "Executed every soldier that had taken arms against him at Samarkand."}
{"answers": {"text": ["pyramids of severed heads", "pyramids of severed heads", "pyramids of severed heads"], "answer_start": [442, 442, 442]}, "prediction_text": "severed heads."}
{"answers": {"text": ["opened the gates", "opened the gates to the Mongols", "opened the gates to the Mongols"], "answer_start": [140, 140, 140]}, "prediction_text": "Opened the gates to the Mongols."}
{"answers": {"text": ["a unit of Turkish defenders", "a unit of Turkish defenders", "Turkish defenders"], "answer_start": [180, 180, 190]}, "prediction_text": "Turkish defenders."}
{"answers": {"text": ["artisans and craftsmen", "artisans and craftsmen", "artisans and craftsmen"], "answer_start": [299, 299, 299]}, "prediction_text": "Artisans and craftsmen."}
{"answers": {"text": ["the flail of God", "the flail of God", "the flail of God,"], "answer_start": [687, 687, 687]}, "prediction_text": "Genghis Khan described himself as the flail of God."}
{"answers": {"text": ["young men who had not fought", "young men who had not fought", "young men who had not fought"], "answer_start": [350, 350, 350]}, "prediction_text": "Young men."}
{"answers": {"text": ["1220", "1220", "1220,"], "answer_start": [46, 46, 46]}, "prediction_text": "1220"}
{"answers": {"text": ["Subutai", "Subutai", "Subutai"], "answer_start": [167, 167, 167]}, "prediction_text": "Subutai"}
{"answers": {"text": ["near the Black Sea", "Black Sea", "Russia"], "answer_start": [601, 610, 399]}, "prediction_text": "Armenia and Azerbaijan."}
{"answers": {"text": ["Kalka River", "Kalka River", "Kalka River"], "answer_start": [996, 996, 996]}, "prediction_text": "Armenia and Azerbaijan."}
{"answers": {"text": ["Mstislav the Bold of Halych and Mstislav III of Kiev", "Mstislav the Bold of Halych and Mstislav III of Kiev", "Mstislav the Bold of Halych and Mstislav III of Kiev"], "answer_start": [761, 761, 761]}, "prediction_text": "Mstislav III of Kiev."}
{"answers": {"text": ["Batu", "Batu", "Genghis Khan's grandson Batu"], "answer_start": [883, 883, 859]}, "prediction_text": "Batu"}
{"answers": {"text": ["the Golden Horde", "Golden Horde", "the Golden Horde"], "answer_start": [892, 896, 892]}, "prediction_text": "Batu and the Golden Horde"}
{"answers": {"text": ["Subutai and Jebe", "Subutai and Jebe", "Subutai and Jebe"], "answer_start": [302, 302, 302]}, "prediction_text": "Batu and the Golden Horde."}
{"answers": {"text": ["1225", "1225", "1225"], "answer_start": [681, 681, 681]}, "prediction_text": "1225"}
{"answers": {"text": ["on the road back to Samarkand", "on the road back to Samarkand", "the road back to Samarkand"], "answer_start": [234, 234, 237]}, "prediction_text": "Samarkand"}
{"answers": {"text": ["1226", "1226", "1226"], "answer_start": [3, 3, 3]}, "prediction_text": "1226"}
{"answers": {"text": ["autumn", "autumn", "autumn"], "answer_start": [209, 209, 209]}, "prediction_text": "Autumn"}
{"answers": {"text": ["the Mongols", "Mongols", "the Mongols"], "answer_start": [297, 301, 297]}, "prediction_text": "Genghis Khan"}
{"answers": {"text": ["the Yellow River", "Yellow River", "Yellow River"], "answer_start": [432, 436, 436]}, "prediction_text": "Yellow River"}
{"answers": {"text": ["a line of five stars arranged in the sky", "a line of five stars", "a line of five stars arranged in the sky"], "answer_start": [550, 550, 550]}, "prediction_text": "Five stars arranged in the sky."}
{"answers": {"text": ["Ning Hia", "Ning Hia", "Ning Hia"], "answer_start": [74, 74, 74]}, "prediction_text": "Ning Hia"}
{"answers": {"text": ["Ma Jianlong", "Ma Jianlong", "Ma Jianlong"], "answer_start": [241, 241, 241]}, "prediction_text": "Ma Jianlong"}
{"answers": {"text": ["arrows", "arrows", "arrows"], "answer_start": [417, 417, 417]}, "prediction_text": "Arrows."}
{"answers": {"text": ["Liupanshan", "Liupanshan", "Liupanshan"], "answer_start": [482, 482, 482]}, "prediction_text": "Liupanshan (Qingshui County, Gansu Province)"}
{"answers": {"text": ["executed", "executed", "executed"], "answer_start": [778, 778, 778]}, "prediction_text": "Executed."}
{"answers": {"text": ["Jochi", "Jochi", "Jochi"], "answer_start": [250, 250, 187]}, "prediction_text": "Jochi"}
{"answers": {"text": ["Chagatai", "Chagatai", "Chagatai"], "answer_start": [498, 383, 383]}, "prediction_text": "\u00d6gedei"}
{"answers": {"text": ["invasion of the Khwarezmid Empire", "invasion of the Khwarezmid Empire", "invasion of the Khwarezmid Empire"], "answer_start": [447, 447, 447]}, "prediction_text": "Khwarezmid Empire invasion."}
{"answers": {"text": ["\u00d6gedei", "\u00d6gedei", "\u00d6gedei"], "answer_start": [670, 670, 670]}, "prediction_text": "\u00d6gedei"}
{"answers": {"text": ["Chagatai and Jochi", "Chagatai and Jochi", "Chagatai and Jochi)"], "answer_start": [78, 78, 78]}, "prediction_text": "Chagatai and Jochi."}
{"answers": {"text": ["Chagatai", "Chagatai", "Chagatai"], "answer_start": [310, 78, 310]}, "prediction_text": "Tolui"}
{"answers": {"text": ["Tolui", "Tolui", "Tolui,"], "answer_start": [486, 486, 486]}, "prediction_text": "Tolui."}
{"answers": {"text": ["\u00d6gedei", "\u00d6gedei", "\u00d6gedei"], "answer_start": [855, 855, 855]}, "prediction_text": "\u00d6gedei"}
{"answers": {"text": ["1226", "1226", "1226"], "answer_start": [14, 14, 14]}, "prediction_text": "1226"}
{"answers": {"text": ["Khorasan", "Khorasan", "Khorasan"], "answer_start": [326, 326, 326]}, "prediction_text": "Khorasan."}
{"answers": {"text": ["Urgench", "Urgench", "Urgench"], "answer_start": [443, 443, 483]}, "prediction_text": "Urgench."}
{"answers": {"text": ["Sultan Muhammad", "Sultan Muhammad", "Sultan Muhammad"], "answer_start": [811, 811, 811]}, "prediction_text": "Sultan Muhammad."}
{"answers": {"text": ["Sultan Muhammad was already dead in 1223", "Sultan Muhammad was already dead in 1223", "Sultan Muhammad was already dead in 1223,"], "answer_start": [1033, 1033, 1033]}, "prediction_text": "Juzjani suggests that the disagreement arose from a quarrel between Jochi and his brothers in the siege of Urgench."}
{"answers": {"text": ["Yinchuan", "Yinchuan", "Yinchuan"], "answer_start": [35, 35, 35]}, "prediction_text": "Western Xia"}
{"answers": {"text": ["hunting", "hunting", "hunting"], "answer_start": [380, 380, 380]}, "prediction_text": "Hunting and dying."}
{"answers": {"text": ["arrow", "arrow", "arrow wound"], "answer_start": [615, 615, 615]}, "prediction_text": "Arrow wound."}
{"answers": {"text": ["Western Xia", "Western Xia", "Western Xia"], "answer_start": [720, 720, 720]}, "prediction_text": "Western Xia"}
{"answers": {"text": ["Oirads", "Oirads", "Oirads"], "answer_start": [983, 983, 983]}, "prediction_text": "Oirads"}
{"answers": {"text": ["without markings", "buried without markings", "buried without markings"], "answer_start": [56, 49, 49]}, "prediction_text": "Markings."}
{"answers": {"text": ["Khentii Aimag", "Khentii Aimag", "Khentii Aimag"], "answer_start": [198, 198, 198]}, "prediction_text": "Khentii Aimag"}
{"answers": {"text": ["Onon River", "Onon", "Onon River"], "answer_start": [267, 267, 267]}, "prediction_text": "Onon River"}
{"answers": {"text": ["The Genghis Khan Mausoleum", "Genghis Khan Mausoleum", "The Genghis Khan Mausoleum"], "answer_start": [473, 477, 473]}, "prediction_text": "The Genghis Khan Mausoleum."}
{"answers": {"text": ["Edsen Khoroo", "Edsen Khoroo", "Edsen Khoroo"], "answer_start": [112, 112, 112]}, "prediction_text": "Edsen Khoroo"}
{"answers": {"text": ["Dongshan Dafo Dian", "Dongshan Dafo Dian", "the Dongshan Dafo Dian"], "answer_start": [290, 290, 286]}, "prediction_text": "Kumbum Monastery"}
{"answers": {"text": ["Kumbum Monastery or Ta'er Shi near Xining", "Kumbum Monastery", "Tibetan monastery of Kumbum Monastery"], "answer_start": [479, 479, 458]}, "prediction_text": "Yan'an."}
{"answers": {"text": ["1954", "1954", "1954,"], "answer_start": [572, 572, 572]}, "prediction_text": "1949"}
{"answers": {"text": ["Red Guards", "Red Guards", "1968"], "answer_start": [754, 754, 717]}, "prediction_text": "Red Guards"}
{"answers": {"text": ["October 6, 2004", "October 6, 2004", "October 6, 2004"], "answer_start": [3, 3, 3]}, "prediction_text": "October 6, 2004"}
{"answers": {"text": ["a river", "river", "river"], "answer_start": [244, 246, 246]}, "prediction_text": "River"}
{"answers": {"text": ["Sumerian King Gilgamesh of Uruk and Atilla the Hun", "King Gilgamesh of Uruk and Atilla the Hun", "Gilgamesh of Uruk and Atilla the Hun"], "answer_start": [344, 353, 358]}, "prediction_text": "Gilgamesh and Atilla."}
{"answers": {"text": ["horses", "horses", "horses"], "answer_start": [457, 457, 457]}, "prediction_text": "horses, trees."}
{"answers": {"text": ["Genghis Khan", "Genghis Khan", "Genghis Khan"], "answer_start": [93, 93, 93]}, "prediction_text": "Genghis Khan"}
{"answers": {"text": ["Yassa", "Yassa", "Yassa"], "answer_start": [75, 75, 75]}, "prediction_text": "Yassa"}
{"answers": {"text": ["meritocracy", "meritocracy", "meritocracy"], "answer_start": [250, 250, 250]}, "prediction_text": "Meritocracy."}
{"answers": {"text": ["Genghis Khan and his family", "Genghis Khan and his family", "Genghis Khan and his family"], "answer_start": [293, 293, 293]}, "prediction_text": "Genghis Khan and his family."}
{"answers": {"text": ["Muhammad Khan", "Muhammad Khan", "Muhammad Khan"], "answer_start": [666, 666, 666]}, "prediction_text": "Genghis Khan and his family."}
{"answers": {"text": ["tax exemptions", "tax exemptions", "tax exemptions"], "answer_start": [11, 11, 11]}, "prediction_text": "Tax exemptions."}
{"answers": {"text": ["Ong Khan", "Ong Khan", "Ong Khan"], "answer_start": [315, 315, 315]}, "prediction_text": "Ong Khan"}
{"answers": {"text": ["a personal concept", "tolerance", "a personal concept, and not subject to law or interference"], "answer_start": [196, 468, 196]}, "prediction_text": "Religious tolerance."}
{"answers": {"text": ["Shamanist, Buddhist or Christian", "Shamanist, Buddhist or Christian", "Shamanist, Buddhist or Christian"], "answer_start": [424, 424, 424]}, "prediction_text": "Shamanist, Buddhist, Christian."}
{"answers": {"text": ["T\u00f6regene Khatun", "T\u00f6regene Khatun", "T\u00f6regene Khatun"], "answer_start": [435, 435, 435]}, "prediction_text": "T\u00f6regene Khatun"}
{"answers": {"text": ["the Pax Mongolica (Mongol Peace)", "Pax Mongolica", "Pax Mongolica (Mongol Peace)"], "answer_start": [620, 624, 624]}, "prediction_text": "Pax Mongolica (Mongol Peace)"}
{"answers": {"text": ["the Chinese", "Chinese", "Chinese"], "answer_start": [337, 341, 341]}, "prediction_text": "Chinese"}
{"answers": {"text": ["legal equality of all individuals, including women", "legal equality of all individuals", "Great Yassa"], "answer_start": [167, 167, 123]}, "prediction_text": "A civil state under the Great Yassa."}
{"answers": {"text": ["Chu'Tsai", "Chu'Tsai", "Chu'Tsai,"], "answer_start": [307, 307, 307]}, "prediction_text": "Chu'Tsai"}
{"answers": {"text": ["they were nomads", "nomads", "nomads and thus had no experience governing cities"], "answer_start": [190, 200, 200]}, "prediction_text": "Nomads."}
{"answers": {"text": ["Jin", "Jin", "Jin dynasty"], "answer_start": [417, 417, 391]}, "prediction_text": "Jin"}
{"answers": {"text": ["Khitan rulers", "Khitan rulers", "Khitan rulers,"], "answer_start": [516, 516, 516]}, "prediction_text": "Khitan rulers."}
{"answers": {"text": ["his generals", "Muqali, Jebe and Subutai", "his generals"], "answer_start": [35, 57, 35]}, "prediction_text": "Muqali, Jebe, Subutai."}
{"answers": {"text": ["Karakorum", "Karakorum", "Karakorum"], "answer_start": [329, 329, 329]}, "prediction_text": "Karakorum"}
{"answers": {"text": ["Muqali", "Muqali", "Muqali,"], "answer_start": [340, 340, 340]}, "prediction_text": "Muqali"}
{"answers": {"text": ["Subutai and Jebe", "Subutai and Jebe", "Subutai and Jebe"], "answer_start": [486, 486, 486]}, "prediction_text": "Muqali and Jebe."}
{"answers": {"text": ["unwavering loyalty", "unwavering loyalty", "unwavering loyalty"], "answer_start": [751, 751, 751]}, "prediction_text": "Unwavering loyalty."}
{"answers": {"text": ["rivers", "rivers", "rivers"], "answer_start": [122, 122, 122]}, "prediction_text": "Rivers."}
{"answers": {"text": ["Muslim and Chinese", "Muslim and Chinese", "Muslim and Chinese"], "answer_start": [295, 295, 295]}, "prediction_text": "Muslim and Chinese."}
{"answers": {"text": ["feigned retreat", "feigned retreat", "feigned retreat"], "answer_start": [463, 463, 463]}, "prediction_text": "feigned retreat to break enemy formations."}
{"answers": {"text": ["driving them in front of the army", "driving them in front of the army", "driving them in front of the army"], "answer_start": [157, 157, 157]}, "prediction_text": "Used to be used to lure small enemy groups away from the larger group and defended position for ambush and counterattack."}
{"answers": {"text": ["Sea of Japan", "Sea of Japan", "Caspian Sea to the Sea of Japan"], "answer_start": [191, 191, 172]}, "prediction_text": "Caspian Sea"}
{"answers": {"text": ["Caspian Sea", "Caspian Sea", "Caspian Sea to the Sea of Japan"], "answer_start": [172, 172, 172]}, "prediction_text": "Sea of Japan"}
{"answers": {"text": ["\u00d6gedei Khan", "\u00d6gedei Khan", "\u00d6gedei Khan"], "answer_start": [320, 320, 320]}, "prediction_text": "\u00d6gedei Khan"}
{"answers": {"text": ["1279", "1279", "1279"], "answer_start": [570, 570, 570]}, "prediction_text": "1279"}
{"answers": {"text": ["the Silk Road", "Silk Road", "Silk Road"], "answer_start": [39, 43, 43]}, "prediction_text": "Silk Road"}
{"answers": {"text": ["Turkey", "Turkey", "Turkey,"], "answer_start": [423, 423, 423]}, "prediction_text": "Turkey"}
{"answers": {"text": ["tolerant", "tolerant", "tolerant of religions"], "answer_start": [342, 342, 342]}, "prediction_text": "Tolerated religions."}
{"answers": {"text": ["increased", "expanding the horizons", "increased communication and trade"], "answer_start": [108, 187, 108]}, "prediction_text": "Increased communication and trade."}
{"answers": {"text": ["1990s", "1990s", "1990s"], "answer_start": [13, 13, 13]}, "prediction_text": "1990s"}
{"answers": {"text": ["uniting warring tribes", "uniting warring tribes", "uniting warring tribes"], "answer_start": [322, 322, 322]}, "prediction_text": "Uniting warring tribes."}
{"answers": {"text": ["Genghis Khan's children", "Genghis Khan's children", "father of the Mongols"], "answer_start": [467, 467, 521]}, "prediction_text": "They refer to him as \"father of the Mongols\" and \"father of the nation\"."}
{"answers": {"text": ["his brutality", "butchery", "his butchery"], "answer_start": [632, 775, 771]}, "prediction_text": "His butchery."}
{"answers": {"text": ["unfairly biased", "unfairly biased", "his positive role is underrated"], "answer_start": [725, 725, 806]}, "prediction_text": "unfairly biased against Genghis Khan."}
{"answers": {"text": ["t\u00f6gr\u00f6g", "t\u00f6gr\u00f6g", "t\u00f6gr\u00f6g"], "answer_start": [286, 286, 286]}, "prediction_text": "T\u00f6gr\u00f6g (\u20ae)"}
{"answers": {"text": ["Genghis Khan", "Genghis Khan's"], "answer_start": [19, 19]}, "prediction_text": "500, 1,000, 5,000, 10,000, 20,000."}
{"answers": {"text": ["Chinggis Khaan International Airport", "Chinggis Khaan International Airport", "Chinggis Khaan International Airport"], "answer_start": [360, 360, 360]}, "prediction_text": "Chinggis Khaan International Airport"}
{"answers": {"text": ["to avoid trivialization", "avoid trivialization", "avoid trivialization."], "answer_start": [571, 574, 574]}, "prediction_text": "To avoid trivialization."}
{"answers": {"text": ["Ulaanbaatar", "Ulaanbaatar", "Ulaanbaatar"], "answer_start": [474, 474, 474]}, "prediction_text": "Ulaanbaatar"}
{"answers": {"text": ["Ikh Zasag", "Ikh Zasag (", "Ikh Zasag"], "answer_start": [484, 484, 484]}, "prediction_text": "Ikh Zasag law."}
{"answers": {"text": ["corruption and bribery", "corruption and bribery", "corruption and bribery"], "answer_start": [661, 661, 661]}, "prediction_text": "Illegal matters related to corruption and bribery."}
{"answers": {"text": ["Tsakhiagiin Elbegdorj", "Tsakhiagiin Elbegdorj", "Elbegdorj"], "answer_start": [719, 719, 1203]}, "prediction_text": "Tsakhiagiin Elbegdorj"}
{"answers": {"text": ["traditional Mongolian script", "traditional Mongolian script", "traditional Mongolian script"], "answer_start": [431, 431, 431]}, "prediction_text": "The Ikh Zasag law."}
{"answers": {"text": ["Inner Mongolia region", "Inner Mongolia region", "Inner Mongolia region"], "answer_start": [118, 118, 118]}, "prediction_text": "Inner Mongolia."}
{"answers": {"text": ["5 million", "around 5 million", "around 5 million"], "answer_start": [283, 276, 276]}, "prediction_text": "5 million."}
{"answers": {"text": ["Kublai Khan", "Kublai Khan", "Kublai Khan"], "answer_start": [397, 397, 397]}, "prediction_text": "Kublai Khan"}
{"answers": {"text": ["Yuan", "Yuan", "Yuan dynasty"], "answer_start": [453, 453, 657]}, "prediction_text": "Yuan dynasty"}
{"answers": {"text": ["grandson", "grandson", "grandson"], "answer_start": [388, 388, 388]}, "prediction_text": "Grandson."}
{"answers": {"text": ["Iran", "Iran", "Iran"], "answer_start": [40, 40, 40]}, "prediction_text": "Iran"}
{"answers": {"text": ["three-fourths", "three-fourths", "up to three-fourths of the population"], "answer_start": [293, 293, 287]}, "prediction_text": "Three-fourths."}
{"answers": {"text": ["10 to 15 million", "10 to 15 million people", "10 to 15 million people"], "answer_start": [358, 358, 358]}, "prediction_text": "10 to 15 million."}
{"answers": {"text": ["Hulagu Khan", "Hulagu Khan", "Hulagu Khan"], "answer_start": [181, 181, 181]}, "prediction_text": "Hulagu Khan"}
{"answers": {"text": ["the Mamluks of Egypt", "Mamluks", "the Mamluks of Egypt"], "answer_start": [286, 290, 286]}, "prediction_text": "Egypt."}
{"answers": {"text": ["Ghazan Khan", "Ghazan Khan", "Ghazan Khan"], "answer_start": [332, 332, 332]}, "prediction_text": "Ghazan Khan"}
{"answers": {"text": ["1237", "1237", "1237"], "answer_start": [593, 593, 593]}, "prediction_text": "1237."}
{"answers": {"text": ["Novgorod and Pskov", "Novgorod and Pskov", "Novgorod and Pskov"], "answer_start": [811, 811, 811]}, "prediction_text": "Novgorod and Pskov."}
{"answers": {"text": ["Mughal emperors", "Mughal emperors", "Mughal emperors"], "answer_start": [20, 20, 20]}, "prediction_text": "Timur"}
{"answers": {"text": ["Timur", "Timur", "Timur"], "answer_start": [431, 92, 92]}, "prediction_text": "Genghis Khan"}
{"answers": {"text": ["Nishapur", "Nishapur", "Nishapur"], "answer_start": [312, 312, 312]}, "prediction_text": "Nishapur"}
{"answers": {"text": ["tenggis", "tenggis", "tenggis"], "answer_start": [95, 95, 95]}, "prediction_text": "tenggis"}
{"answers": {"text": ["Lake Baikal", "Lake Baikal", "Lake Baikal"], "answer_start": [153, 153, 153]}, "prediction_text": "Lake Baikal"}
{"answers": {"text": ["\"right\", \"just\", or \"true\"", "right", "right\", \"just\", or \"true\","], "answer_start": [375, 376, 376]}, "prediction_text": "Right."}
{"answers": {"text": ["Zh\u00e8ng", "Zh\u00e8ng", "Zh\u00e8ng"], "answer_start": [348, 348, 348]}, "prediction_text": "Genghis"}
{"answers": {"text": ["Chinggis", "Chinggis", "\"Jenggis"], "answer_start": [627, 627, 470]}, "prediction_text": "Zh\u00e8ng (Chinese: \u6b63)"}
{"answers": {"text": ["Chinggis Khaan", "Chinggis Khaan", "Chinggis Khaan", "Chinggis Khaan"], "answer_start": [95, 95, 95, 95]}, "prediction_text": "Chinggis Khan"}
{"answers": {"text": ["Cengiz Han", "Cengiz Han", "Cengiz Han", "Cengiz Han, \u00c7ingiz Xan, \u00c7ingiz Han, Chingizxon, \u00c7\u0131\u00f1\u011f\u0131z Xan, Chengez Khan, Chinggis Khan, Chinggis Xaan, Chingis Khan, Jenghis Khan, Chinggis Qan, Djingis Kahn"], "answer_start": [198, 198, 198, 198]}, "prediction_text": "Cengiz Han."}
{"answers": {"text": ["Ti\u011bm\u00f9zh\u0113n", "Ti\u011bm\u00f9zh\u0113n", "Ti\u011bm\u00f9zh\u0113n", "Ti\u011bm\u00f9zh\u0113n"], "answer_start": [516, 516, 516, 516]}, "prediction_text": "Ti\u011bm\u00f9zh\u0113n."}
{"answers": {"text": ["Chinghiz, Chinghis, and Chingiz", "Chinghiz, Chinghis, and Chingiz", "Chinghiz, Chinghis, and Chingiz", "Chinghiz, Chinghis, and Chingiz"], "answer_start": [119, 119, 119, 119]}, "prediction_text": "Chingiz Khan"}
{"answers": {"text": ["Ch\u00e9ngj\u00eds\u012b H\u00e1n", "Ch\u00e9ngj\u00eds\u012b H\u00e1n", "Ch\u00e9ngj\u00eds\u012b H\u00e1n", "Ch\u00e9ngj\u00eds\u012b H\u00e1n,"], "answer_start": [175, 175, 175, 175]}, "prediction_text": "Ti\u011bm\u00f9zh\u0113n."}
{"answers": {"text": ["its root word pharma", "pharma", "pharma", "pharma"], "answer_start": [34, 48, 48, 48]}, "prediction_text": "pharma"}
{"answers": {"text": ["ingredients for medicines, sold tobacco and patent medicines", "ingredients for medicines, sold tobacco and patent medicines", "ingredients for medicines, sold tobacco and patent medicines", "medicines"], "answer_start": [483, 483, 483, 499]}, "prediction_text": "Tobacco and patent medicines."}
{"answers": {"text": ["sorcery or even poison", "sorcery or even poison", "sorcery", "sorcery"], "answer_start": [163, 163, 163, 163]}, "prediction_text": "Sorcery or even poison."}
{"answers": {"text": ["outdated or only approproriate if herbal remedies were on offer to a large extent", "outdated", "outdated", "outdated"], "answer_start": [767, 767, 767, 767]}, "prediction_text": "outdated or only approproriate."}
{"answers": {"text": ["many other herbs not listed", "many other herbs", "other herbs not listed", "other herbs"], "answer_start": [872, 872, 877, 877]}, "prediction_text": "Tobacco and patent medicines."}
{"answers": {"text": ["healthcare professionals", "healthcare", "healthcare professionals with specialised education"], "answer_start": [16, 16, 16]}, "prediction_text": "Healthcare professionals"}
{"answers": {"text": ["optimal health outcomes", "optimal", "optimal health outcomes"], "answer_start": [117, 117, 117]}, "prediction_text": "Optimal health outcomes."}
{"answers": {"text": ["optimisation of a drug treatment for an individual", "optimisation of a drug treatment", "optimisation of a drug treatment for an individual"], "answer_start": [475, 475, 475]}, "prediction_text": "Optimal health outcomes for their patients."}
{"answers": {"text": ["small-business proprietors", "small-business proprietors", "small-business proprietors"], "answer_start": [222, 222, 222]}, "prediction_text": "Small-business proprietors."}
{"answers": {"text": ["specialised education and training", "specialised education and training", "specialised education and training"], "answer_start": [46, 46, 46]}, "prediction_text": "Medicines."}
{"answers": {"text": ["other senior pharmacy technicians", "senior pharmacy technicians", "other senior pharmacy technicians"], "answer_start": [220, 226, 220]}, "prediction_text": "Senior pharmacy technicians."}
{"answers": {"text": ["the General Pharmaceutical Council (GPhC) register", "General Pharmaceutical Council (GPhC)", "General Pharmaceutical Council (GPhC) register"], "answer_start": [640, 644, 644]}, "prediction_text": "General Pharmaceutical Council (GPhC)"}
{"answers": {"text": ["regulates the practice of pharmacists and pharmacy technicians", "governing body for pharmacy health care professionals", "regulates the practice of pharmacists and pharmacy technicians"], "answer_start": [778, 708, 778]}, "prediction_text": "Regulate the practice of pharmacists and pharmacy technicians."}
{"answers": {"text": ["health care professional", "pharmacy health care", "health care professional"], "answer_start": [48, 727, 48]}, "prediction_text": "Health care professional"}
{"answers": {"text": ["manage the pharmacy department and specialised areas in pharmacy practice", "manage the pharmacy department and specialised areas", "manage the pharmacy department"], "answer_start": [342, 342, 342]}, "prediction_text": "Supervision, management, specialised areas."}
{"answers": {"text": ["writing a five volume book in his native Greek", "writing a five volume book", "five volume book in his native Greek"], "answer_start": [220, 220, 230]}, "prediction_text": "Writing a five volume book."}
{"answers": {"text": ["De Materia Medica", "De Materia Medica", "De Materia Medica"], "answer_start": [331, 331, 331]}, "prediction_text": "De Materia Medica"}
{"answers": {"text": ["materia medica", "materia medica", "materia medica"], "answer_start": [533, 533, 533]}, "prediction_text": "materia medica"}
{"answers": {"text": ["Diocles of Carystus", "Diocles of Carystus", "Diocles of Carystus"], "answer_start": [19, 19, 19]}, "prediction_text": "Diocles of Carystus"}
{"answers": {"text": ["many middle eastern scientists", "middle eastern scientists", "many middle eastern scientists"], "answer_start": [445, 450, 445]}, "prediction_text": "Middle Eastern scientists."}
{"answers": {"text": ["highly respected", "highly respected", "highly respected"], "answer_start": [160, 160, 160]}, "prediction_text": "Highly respected."}
{"answers": {"text": ["the Taih\u014d Code (701) and re-stated in the Y\u014dr\u014d Code (718)", "Taih\u014d Code (701) and re-stated in the Y\u014dr\u014d Code", "Taih\u014d Code (701) and re-stated in the Y\u014dr\u014d Code"], "answer_start": [239, 243, 243]}, "prediction_text": "Taih\u014d Code (701) and Y\u014dr\u014d Code (718)"}
{"answers": {"text": ["the pre-Heian Imperial court", "Taih\u014d Code", "pre-Heian Imperial court"], "answer_start": [318, 243, 322]}, "prediction_text": "The Taih\u014d Code (701) and the Y\u014dr\u014d Code (718)"}
{"answers": {"text": ["status superior to all others in health-related fields such as physicians and acupuncturists", "Ranked positions", "status superior to all others in health-related fields"], "answer_start": [553, 298, 553]}, "prediction_text": "Ranked."}
{"answers": {"text": ["ranked above", "ranked above", "ranked above the two personal physicians of the Emperor"], "answer_start": [698, 698, 698]}, "prediction_text": "Ranked above."}
{"answers": {"text": ["botany and chemistry", "botany and chemistry", "advances made in the Middle East in botany and chemistry"], "answer_start": [40, 40, 4]}, "prediction_text": "Botany and chemistry."}
{"answers": {"text": ["Muhammad ibn Zakar\u012bya R\u0101zi", "Muhammad ibn Zakar\u012bya R\u0101zi", "Muhammad ibn Zakar\u012bya R\u0101zi"], "answer_start": [131, 131, 131]}, "prediction_text": "Muhammad ibn Zakar\u012bya R\u0101zi (Rhazes)"}
{"answers": {"text": ["Abu al-Qasim al-Zahrawi", "Abu al-Qasim al-Zahrawi", "Abu al-Qasim al-Zahrawi"], "answer_start": [249, 249, 249]}, "prediction_text": "Abu al-Qasim al-Zahrawi (Abulcasis) (936\u20131013)"}
{"answers": {"text": ["Al-Muwaffaq", "Al-Muwaffaq", "Al-Muwaffaq"], "answer_start": [1570, 1570, 1570]}, "prediction_text": "Peter of Abano."}
{"answers": {"text": ["sodium carbonate and potassium carbonate", "sodium carbonate and potassium carbonate", "sodium carbonate and potassium carbonate"], "answer_start": [1838, 1838, 1838]}, "prediction_text": "sodium carbonate and potassium carbonate."}
{"answers": {"text": ["1317", "1317", "1317"], "answer_start": [125, 125, 125]}, "prediction_text": "1317"}
{"answers": {"text": ["Church of Santa Maria Novella in Florence, Italy", "Church of Santa Maria Novella in Florence, Italy", "Florence, Italy"], "answer_start": [264, 264, 297]}, "prediction_text": "Florence, Italy"}
{"answers": {"text": ["museum", "a museum", "museum"], "answer_start": [447, 445, 447]}, "prediction_text": "Albarellos."}
{"answers": {"text": ["albarellos from the 16th and 17th centuries, old prescription books and antique drugs", "albarellos from the 16th and 17th centuries, old prescription books and antique drugs", "old prescription books and antique drugs"], "answer_start": [495, 495, 540]}, "prediction_text": "Albarellos."}
{"answers": {"text": ["1221", "1221", "1221"], "answer_start": [252, 252, 252]}, "prediction_text": "1221"}
{"answers": {"text": ["pharmacy legislation", "pharmacy legislation", "pharmacy legislation"], "answer_start": [48, 48, 48]}, "prediction_text": "Pharmacy legislation."}
{"answers": {"text": ["within the dispensary compounding/dispensing medications", "communicating with patients", "compounding/dispensing medications"], "answer_start": [224, 403, 246]}, "prediction_text": "Storage conditions, compulsory texts, equipment, etc."}
{"answers": {"text": ["automation", "automation", "automation"], "answer_start": [481, 481, 481]}, "prediction_text": "Automation"}
{"answers": {"text": ["patients' prescriptions and patient safety issues", "patients' prescriptions and patient safety issues", "dealing with patients' prescriptions and patient safety issues"], "answer_start": [538, 538, 525]}, "prediction_text": "Patients' prescriptions and patient safety issues."}
{"answers": {"text": ["storage conditions, compulsory texts, equipment, etc.", "storage conditions, compulsory texts, equipment, etc.", "storage conditions, compulsory texts, equipment"], "answer_start": [92, 92, 92]}, "prediction_text": "Storage conditions."}
{"answers": {"text": ["a pharmacy practice residency", "pharmacy practice residency", "pharmacy practice residency"], "answer_start": [322, 324, 324]}, "prediction_text": "Hospitals."}
{"answers": {"text": ["various disciplines of pharmacy", "various disciplines of pharmacy", "various disciplines of pharmacy"], "answer_start": [509, 509, 509]}, "prediction_text": "Hematology/oncology, HIV/AIDS, infectious disease, critical care, emergency medicine, toxicology, nuclear pharmacy, pain management, psychiatry, anti-coagulation clinics, herbal medicine, neurology/epilepsy management, pediatrics, neonatal pharmacists."}
{"answers": {"text": ["effectiveness of treatment regimens", "effectiveness of treatment regimens", "effectiveness of treatment regimens"], "answer_start": [73, 73, 73]}, "prediction_text": "Medications."}
{"answers": {"text": ["pharmacists practicing in hospitals", "pharmacists practicing in hospitals", "clinical pharmacists"], "answer_start": [223, 223, 459]}, "prediction_text": "Clinical pharmacists."}
{"answers": {"text": ["within the premises of the hospital", "premises of the hospital", "within the premises of the hospital"], "answer_start": [39, 50, 39]}, "prediction_text": "Hospital pharmacies."}
{"answers": {"text": ["unit-dose, or a single dose of medicine", "unit-dose", "unit-dose, or a single dose of medicine"], "answer_start": [260, 260, 260]}, "prediction_text": "Unit-dose."}
{"answers": {"text": ["high risk preparations and some other compounding functions", "high risk preparations and some other compounding functions", "high risk preparations and some other compounding functions"], "answer_start": [663, 663, 663]}, "prediction_text": "High risk preparations and other compounding functions."}
{"answers": {"text": ["The high cost of medications and drug-related technology", "high cost of medications", "high cost of medications and drug-related technology"], "answer_start": [767, 771, 771]}, "prediction_text": "The high cost of medications and drug-related technology."}
{"answers": {"text": ["Hospital pharmacies usually stock a larger range of medications, including more specialized medications", "more specialized medications", "stock a larger range of medications, including more specialized medications"], "answer_start": [76, 151, 104]}, "prediction_text": "A larger range of medications, including more specialized medications, and more specialized medications."}
{"answers": {"text": ["optimizes the use of medication and promotes health, wellness, and disease prevention", "optimizes the use of medication and promotes health, wellness, and disease prevention", "direct patient care services that optimizes the use of medication and promotes health, wellness, and disease prevention"], "answer_start": [54, 54, 20]}, "prediction_text": "Optimize the use of medication and promote health, wellness, and disease prevention."}
{"answers": {"text": ["inside hospitals and clinics", "inside hospitals and clinics", "the clinical pharmacy movement initially began inside hospitals and clinics"], "answer_start": [260, 260, 213]}, "prediction_text": "Hospitals and clinics."}
{"answers": {"text": ["physicians and other healthcare professionals", "physicians", "physicians and other healthcare professionals"], "answer_start": [334, 334, 334]}, "prediction_text": "Physicians and other healthcare professionals."}
{"answers": {"text": ["patient care rounds drug product selection", "interdisciplinary approach", "patient care rounds drug product selection"], "answer_start": [535, 465, 535]}, "prediction_text": "Patient care rounds drug product selection."}
{"answers": {"text": ["all health care settings", "drug product selection", "all health care settings"], "answer_start": [183, 555, 183]}, "prediction_text": "Hospitals and clinics."}
{"answers": {"text": ["creating a comprehensive drug therapy plan for patient-specific problems", "identifying goals of therapy", "creating a comprehensive drug therapy plan for patient-specific problems"], "answer_start": [40, 114, 40]}, "prediction_text": "Creating a drug therapy plan."}
{"answers": {"text": ["an evaluation of the appropriateness of the drug therapy", "an evaluation of the appropriateness of the drug therapy", "evaluation of the appropriateness of the drug therapy"], "answer_start": [274, 274, 277]}, "prediction_text": "The pharmacist must evaluate the appropriateness of the drug therapy (e.g., drug choice, dose, route, frequency, and duration of therapy) and its efficacy."}
{"answers": {"text": ["drug choice, dose, route, frequency, and duration of therapy", "drug choice, dose, route, frequency, and duration of therapy", "drug choice, dose, route, frequency, and duration of therapy"], "answer_start": [338, 338, 338]}, "prediction_text": "drug choice, dose, route, frequency, duration, and patient drug allergies."}
{"answers": {"text": ["potential drug interactions, adverse drug reactions", "potential drug interactions", "potential drug interactions, adverse drug reactions, and assess patient drug allergies"], "answer_start": [455, 455, 455]}, "prediction_text": "Adverse drug reactions, drug interactions, drug allergies."}
{"answers": {"text": ["full independent prescribing authority", "full independent prescribing authority", "full independent prescribing authority"], "answer_start": [132, 132, 132]}, "prediction_text": "Full independent prescribing authority."}
{"answers": {"text": ["North Carolina and New Mexico", "North Carolina and New Mexico", "North Carolina and New Mexico"], "answer_start": [192, 192, 192]}, "prediction_text": "North Carolina and New Mexico."}
{"answers": {"text": ["2011", "2011", "2011"], "answer_start": [316, 316, 316]}, "prediction_text": "2011"}
{"answers": {"text": ["Board Certified Ambulatory Care Pharmacist", "Board Certified Ambulatory Care Pharmacist", "Board Certified Ambulatory Care Pharmacist"], "answer_start": [555, 555, 555]}, "prediction_text": "BCACP"}
{"answers": {"text": ["the VA, the Indian Health Service, and NIH", "the VA, the Indian Health Service, and NIH", "VA, the Indian Health Service, and NIH"], "answer_start": [50, 50, 54]}, "prediction_text": "VA, Indian Health Service, NIH."}
{"answers": {"text": ["medication regimen review", "medication regimen review", "medication regimen review"], "answer_start": [45, 45, 45]}, "prediction_text": "Medication regimen review (i.e. \"cognitive services\")"}
{"answers": {"text": ["nursing homes", "nursing homes", "nursing homes"], "answer_start": [181, 181, 181]}, "prediction_text": "Nursing homes."}
{"answers": {"text": ["Omnicare, Kindred Healthcare and PharMerica", "Omnicare, Kindred Healthcare and PharMerica", "Omnicare, Kindred Healthcare and PharMerica"], "answer_start": [463, 463, 463]}, "prediction_text": "Omnicare, Kindred Healthcare, PharMerica."}
{"answers": {"text": ["because many elderly people are now taking numerous medications but continue to live outside of institutional settings", "many elderly people are now taking numerous medications but continue to live outside of institutional settings", "many elderly people are now taking numerous medications but continue to live outside of institutional settings"], "answer_start": [621, 629, 629]}, "prediction_text": "Elderly people."}
{"answers": {"text": ["employ consultant pharmacists and/or provide consulting services", "employ consultant pharmacists", "employ consultant pharmacists and/or provide consulting services"], "answer_start": [767, 767, 767]}, "prediction_text": "Employ consultant pharmacists."}
{"answers": {"text": ["about the year 2000", "2000", "2000"], "answer_start": [6, 21, 21]}, "prediction_text": "2000"}
{"answers": {"text": ["brick-and-mortar community pharmacies that serve consumers online and those that walk in their door", "brick-and-mortar community pharmacies", "brick-and-mortar community pharmacies"], "answer_start": [213, 213, 213]}, "prediction_text": "Community pharmacies."}
{"answers": {"text": ["online pharmacies", "online pharmacies", "online pharmacies"], "answer_start": [626, 626, 626]}, "prediction_text": "Online pharmacies."}
{"answers": {"text": ["another customer might overhear about the drugs that they take", "homebound", "more convenient and private method"], "answer_start": [527, 715, 439]}, "prediction_text": "Convenience."}
{"answers": {"text": ["the method by which the medications are requested and received", "method by which the medications are requested and received", "the method by which the medications are requested and received"], "answer_start": [340, 344, 340]}, "prediction_text": "Method by which the medications are requested and received."}
{"answers": {"text": ["to avoid the \"inconvenience\" of visiting a doctor or to obtain medications which their doctors were unwilling to prescribe", "avoid the \"inconvenience\" of visiting a doctor", "avoid the \"inconvenience\" of visiting a doctor or to obtain medications which their doctors were unwilling to prescribe"], "answer_start": [220, 223, 223]}, "prediction_text": "To avoid the \"inconvenience\" of visiting a doctor or to obtain medications which their doctors were unwilling to prescribe."}
{"answers": {"text": ["those who feel that only doctors can reliably assess contraindications, risk/benefit ratios, and an individual's overall suitability for use of a medication.", "by those who feel that only doctors can reliably assess contraindications, risk/benefit ratios, and an individual's overall suitability for use of a medication", "those who feel that only doctors can reliably assess contraindications, risk/benefit ratios, and an individual's overall suitability for use of a medication"], "answer_start": [427, 424, 427]}, "prediction_text": "Patients."}
{"answers": {"text": ["dispensing substandard products", "potentially dangerous", "reports of such pharmacies dispensing substandard products"], "answer_start": [633, 390, 606]}, "prediction_text": "Convenience."}
{"answers": {"text": ["sell prescription drugs without requiring a prescription", "sell prescription drugs without requiring a prescription", "some Internet pharmacies sell prescription drugs without requiring a prescription"], "answer_start": [114, 114, 89]}, "prediction_text": "Selling prescription drugs without requiring a prescription."}
{"answers": {"text": ["sell prescription drugs and require a valid prescription", "sell prescription drugs", "sell prescription drugs and require a valid prescription"], "answer_start": [31, 31, 31]}, "prediction_text": "Sell prescription drugs."}
{"answers": {"text": ["the ease with which people, youth in particular, can obtain controlled substances", "ease with which people, youth in particular, can obtain controlled substances", "the ease with which people, youth in particular, can obtain controlled substances"], "answer_start": [50, 54, 50]}, "prediction_text": "Ease with which people, youth in particular, can obtain controlled substances (e.g., Vicodin, generically known as hydrocodone) via the Internet without a prescription issued by a doctor/practitioner who has an established doctor-patient relationship."}
{"answers": {"text": ["it must be issued for a legitimate medical purpose by a licensed practitioner acting in the course of legitimate doctor-patient relationship", "it must be issued for a legitimate medical purpose by a licensed practitioner acting in the course of legitimate doctor-patient relationship", "it must be issued for a legitimate medical purpose by a licensed practitioner acting in the course of legitimate doctor-patient relationship"], "answer_start": [576, 576, 576]}, "prediction_text": "Legitimate medical purpose, licensed practitioner acting in course of legitimate doctor-patient relationship."}
{"answers": {"text": ["the ease with which people, youth in particular, can obtain controlled substances", "ease with which people, youth in particular, can obtain controlled substances", "the ease with which people, youth in particular, can obtain controlled substances"], "answer_start": [50, 54, 50]}, "prediction_text": "Ease with which people, youth in particular, can obtain controlled substances (e.g., Vicodin, generically known as hydrocodone) via the Internet without a prescription issued by a doctor/practitioner who has an established doctor-patient relationship."}
{"answers": {"text": ["it must be issued for a legitimate medical purpose by a licensed practitioner acting in the course of legitimate doctor-patient relationship", "it must be issued for a legitimate medical purpose by a licensed practitioner acting in the course of legitimate doctor-patient relationship", "it must be issued for a legitimate medical purpose by a licensed practitioner acting in the course of legitimate doctor-patient relationship"], "answer_start": [576, 576, 576]}, "prediction_text": "Legitimate medical purpose, licensed practitioner acting in course of legitimate doctor-patient relationship."}
{"answers": {"text": ["to ensure that the prescription is valid", "ensure that the prescription is valid", "The filling pharmacy has a corresponding responsibility to ensure that the prescription is valid"], "answer_start": [774, 777, 718]}, "prediction_text": "To ensure that the prescription is valid."}
{"answers": {"text": ["individual state laws", "individual state laws", "Often, individual state laws outline what defines a valid patient-doctor relationship"], "answer_start": [823, 823, 816]}, "prediction_text": "State laws."}
{"answers": {"text": ["Vicodin, generically known as hydrocodone", "Vicodin", "Vicodin"], "answer_start": [139, 139, 139]}, "prediction_text": "Vicodin"}
{"answers": {"text": ["to reduce consumer costs", "reduce consumer costs", "in order to reduce consumer costs"], "answer_start": [125, 128, 116]}, "prediction_text": "To reduce consumer costs."}
{"answers": {"text": ["Canada", "Canada", "Canada"], "answer_start": [88, 88, 88]}, "prediction_text": "Canada"}
{"answers": {"text": ["international drug suppliers, rather than consumers", "international drug suppliers", "international drug suppliers"], "answer_start": [322, 322, 322]}, "prediction_text": "International drug suppliers."}
{"answers": {"text": ["There is no known case", "no known case", "no"], "answer_start": [375, 384, 384]}, "prediction_text": "No."}
{"answers": {"text": ["to legalize importation of medications from Canada and other countries", "legalize importation of medications", "legalize importation of medications from Canada and other countries"], "answer_start": [44, 47, 47]}, "prediction_text": "Legalize importation of medications from Canada and other countries."}
{"answers": {"text": ["pharmacy practice science and applied information science", "pharmacy practice science and applied information science", "pharmacy practice science and applied information science"], "answer_start": [43, 43, 43]}, "prediction_text": "Pharmacy practice science and applied information science."}
{"answers": {"text": ["information technology departments or for healthcare information technology vendor companies", "information technology departments or for healthcare information technology vendor companies", "information technology departments or for healthcare information technology vendor companies"], "answer_start": [198, 198, 198]}, "prediction_text": "Information technology departments, healthcare information technology vendor companies."}
{"answers": {"text": ["major national and international patient information projects and health system interoperability goals", "major national and international patient information projects", "major national and international patient information projects"], "answer_start": [395, 395, 395]}, "prediction_text": "Major national and international patient information projects and health system interoperability goals."}
{"answers": {"text": ["medication management system development, deployment and optimization", "medication management system development, deployment and optimization", "medication management system development, deployment and optimization"], "answer_start": [554, 554, 554]}, "prediction_text": "Pharmacy practice science, applied information science, information technology departments, healthcare information technology vendor companies."}
{"answers": {"text": ["quickly", "growing quickly", "quickly"], "answer_start": [366, 358, 366]}, "prediction_text": "Rapidly."}
{"answers": {"text": ["specialty pharmacies", "specialty pharmacies", "specialty pharmacies"], "answer_start": [485, 485, 485]}, "prediction_text": "Specialty pharmacies"}
{"answers": {"text": ["19", "19", "19 of 28 newly FDA approved medications"], "answer_start": [739, 739, 739]}, "prediction_text": "19"}
{"answers": {"text": ["cancer, hepatitis, and rheumatoid arthritis", "chronic and complex disease states", "chronic and complex disease states such as cancer, hepatitis, and rheumatoid arthritis"], "answer_start": [149, 106, 106]}, "prediction_text": "Cancer, hepatitis, rheumatoid arthritis."}
{"answers": {"text": ["novel medications that need to be properly stored, administered, carefully monitored, and clinically managed", "novel medications", "novel medications that need to be properly stored, administered, carefully monitored, and clinically managed"], "answer_start": [337, 337, 337]}, "prediction_text": "Novel medications."}
{"answers": {"text": ["lab monitoring, adherence counseling, and assist patients with cost-containment strategies needed to obtain their expensive specialty drugs", "lab monitoring, adherence counseling, and assist patients with cost-containment strategies", "lab monitoring, adherence counseling, and assist patients with cost-containment strategies needed to obtain their expensive specialty drugs"], "answer_start": [519, 519, 519]}, "prediction_text": "Lab monitoring, adherence counseling, and assist patients with cost-containment strategies."}
{"answers": {"text": ["separately from physicians", "separately from physicians", "separately from physicians"], "answer_start": [77, 77, 77]}, "prediction_text": "Separate from physicians."}
{"answers": {"text": ["only pharmacists", "only pharmacists", "only pharmacists may supply scheduled pharmaceuticals to the public"], "answer_start": [151, 151, 151]}, "prediction_text": "Pharmacists."}
{"answers": {"text": ["the American Medical Association (AMA)", "American Medical Association", "American Medical Association (AMA)"], "answer_start": [334, 338, 338]}, "prediction_text": "American Medical Association (AMA)"}
{"answers": {"text": ["7 to 10 percent", "7 to 10 percent", "7 to 10 percent of American physicians"], "answer_start": [590, 590, 590]}, "prediction_text": "7 to 10 percent."}
{"answers": {"text": ["form business partnerships with physicians or give them \"kickback\" payments", "form business partnerships with physicians", "pharmacists cannot form business partnerships with physicians or give them \"kickback\" payments"], "answer_start": [248, 248, 229]}, "prediction_text": "Form business partnerships with physicians."}
{"answers": {"text": ["Austria", "Austria", "Austria"], "answer_start": [435, 435, 435]}, "prediction_text": "Austria"}
{"answers": {"text": ["In some rural areas in the United Kingdom", "rural areas in the United Kingdom", "prescribe and dispense prescription-only medicines to their patients from within their practices"], "answer_start": [0, 8, 99]}, "prediction_text": "Within their practices."}
{"answers": {"text": ["1.6 kilometres", "1.6 kilometres", "1.6 kilometres"], "answer_start": [337, 337, 337]}, "prediction_text": "1.6 kilometers."}
{"answers": {"text": ["more than 4 kilometers", "4 kilometers", "more than 4 kilometers"], "answer_start": [493, 503, 493]}, "prediction_text": "1.6 kilometers."}
{"answers": {"text": ["the high risk of a conflict of interest and/or the avoidance of absolute powers", "high risk of a conflict of interest", "high risk of a conflict of interest and/or the avoidance of absolute powers"], "answer_start": [36, 40, 40]}, "prediction_text": "To avoid a conflict of interest and/or the avoidance of absolute powers."}
{"answers": {"text": ["because he or she can then sell more medications to the patient", "sell more medications to the patient", "sell more medications to the patient"], "answer_start": [259, 286, 286]}, "prediction_text": "To sell more medications."}
{"answers": {"text": ["the checks and balances system of the U.S. and many other governments.", "checks and balances system of the U.S. and many other governments", "similarity to the checks and balances system of the U.S. and many other governments"], "answer_start": [544, 548, 530]}, "prediction_text": "U.S. checks and balances system."}
{"answers": {"text": ["exaggerating their seriousness", "avoiding the unnecessary use of medication that may have side-effects", "because he or she can then sell more medications to the patient"], "answer_start": [227, 433, 259]}, "prediction_text": "By selling more medications to the patient."}
{"answers": {"text": ["in obtaining cost-effective medication and avoiding the unnecessary use of medication that may have side-effects", "obtaining cost-effective medication", "the physician has a financial self-interest in \"diagnosing\" as many conditions as possible"], "answer_start": [390, 393, 128]}, "prediction_text": "The doctor's self-interest is at odds with the patient's self-interest in obtaining cost-effective medication and avoiding the unnecessary use of medication that may have side-effects."}
{"answers": {"text": ["expected to become more integral within the health care system", "patient care skills", "pharmacists are expected to become more integral within the health care system"], "answer_start": [39, 211, 23]}, "prediction_text": "More integral within the health care system."}
{"answers": {"text": ["increasingly expected to be compensated for their patient care skills", "patient care skills", "pharmacists are increasingly expected to be compensated for their patient care skills"], "answer_start": [161, 211, 145]}, "prediction_text": "The responsibilities of pharmacists are believed to be taking on more in the future."}
{"answers": {"text": ["clinical services that pharmacists can provide for their patients", "clinical services that pharmacists can provide for their patients", "the clinical services that pharmacists can provide for their patients"], "answer_start": [296, 296, 292]}, "prediction_text": "Clinical services."}
{"answers": {"text": ["thorough analysis of all medication (prescription, non-prescription, and herbals) currently being taken by an individual", "thorough analysis of all medication (prescription, non-prescription, and herbals) currently being taken by an individual.", "the thorough analysis of all medication (prescription, non-prescription, and herbals) currently being taken by an individual"], "answer_start": [389, 389, 385]}, "prediction_text": "thorough analysis of all medication (prescription, non-prescription, and herbals) currently being taken by an individual."}
{"answers": {"text": ["a reconciliation of medication and patient education resulting in increased patient health outcomes and decreased costs to the health care system", "increased patient health outcomes and decreased costs", "a reconciliation of medication and patient education resulting in increased patient health outcomes and decreased costs to the health care system"], "answer_start": [525, 591, 525]}, "prediction_text": "Increased patient health outcomes and decreased costs to the health care system."}
{"answers": {"text": ["Alberta and British Columbia", "Alberta and British Columbia", "Alberta and British Columbia"], "answer_start": [279, 279, 279]}, "prediction_text": "Alberta and British Columbia."}
{"answers": {"text": ["the Australian Government", "Australian Government", "the Australian Government"], "answer_start": [117, 121, 117]}, "prediction_text": "Australian Government"}
{"answers": {"text": ["medicine use reviews", "medicine use reviews", "medicine use reviews"], "answer_start": [630, 630, 630]}, "prediction_text": "Medicine use reviews."}
{"answers": {"text": ["pharmaceutical care or clinical pharmacy", "pharmaceutical care or clinical pharmacy", "pharmaceutical care or clinical pharmacy"], "answer_start": [965, 965, 965]}, "prediction_text": "Pharmaceutical care or clinical pharmacy."}
{"answers": {"text": ["Doctor of Pharmacy (Pharm. D.)", "Doctor of Pharmacy", "the Doctor of Pharmacy (Pharm. D.) degree"], "answer_start": [1079, 1079, 1075]}, "prediction_text": "Pharm. D."}
{"answers": {"text": ["the mortar and pestle and the \u211e (recipere) character", "mortar and pestle and the \u211e (recipere) character", "the mortar and pestle and the \u211e (recipere) character"], "answer_start": [89, 93, 89]}, "prediction_text": "Mortar and pestle."}
{"answers": {"text": ["The show globe", "show globe", "show globe"], "answer_start": [189, 193, 193]}, "prediction_text": "Globe"}
{"answers": {"text": ["the Netherlands", "the Netherlands", "the Netherlands"], "answer_start": [347, 347, 347]}, "prediction_text": "France"}
{"answers": {"text": ["Germany and Austria", "Germany and Austria", "Germany and Austria"], "answer_start": [648, 648, 648]}, "prediction_text": "Germany and Austria."}
{"answers": {"text": ["France, Argentina, the United Kingdom, Belgium, Ireland, Italy, Spain, and India", "France, Argentina, the United Kingdom, Belgium, Ireland, Italy, Spain, and India", "France, Argentina, the United Kingdom, Belgium, Ireland, Italy, Spain, and India"], "answer_start": [487, 487, 487]}, "prediction_text": "France, Argentina, United Kingdom, Belgium, Ireland, Italy, Spain, and India."}
{"answers": {"text": ["a system of many biological structures and processes within an organism that protects against disease", "system of many biological structures and processes", "a system of many biological structures and processes within an organism", "a system of many biological structures and processes within an organism"], "answer_start": [21, 23, 21, 21]}, "prediction_text": "The immune system."}
{"answers": {"text": ["a wide variety of agents, known as pathogens, from viruses to parasitic worms", "disease", "disease", "against disease"], "answer_start": [175, 115, 115, 107]}, "prediction_text": "Disease."}
{"answers": {"text": ["the innate immune system versus the adaptive immune system", "innate immune system versus the adaptive immune system", "humoral immunity versus cell-mediated immunity", "innate immune system versus the adaptive immune system"], "answer_start": [393, 397, 456, 397]}, "prediction_text": "Innate immune system versus adaptive immune system."}
{"answers": {"text": ["the neuroimmune system", "neuroimmune system", "neuroimmune system", "neuroimmune system"], "answer_start": [651, 655, 655, 655]}, "prediction_text": "Blood-brain barrier"}
{"answers": {"text": ["biological structures and processes within an organism", "system of many biological structures and processes", "a system of many biological structures and processes within an organism", "a system of many biological structures and processes within an organism"], "answer_start": [38, 23, 21, 21]}, "prediction_text": "The immune system."}
{"answers": {"text": ["pathogens, from viruses to parasitic worms", "disease", "disease", "against disease"], "answer_start": [210, 115, 115, 107]}, "prediction_text": "Disease."}
{"answers": {"text": ["innate immune system versus the adaptive immune system", "innate immune system versus the adaptive immune system", "humoral immunity versus cell-mediated immunity", "innate immune system versus the adaptive immune system"], "answer_start": [397, 397, 456, 397]}, "prediction_text": "Innate immune system versus adaptive immune system."}
{"answers": {"text": ["disease", "disease", "disease", "disease"], "answer_start": [115, 115, 115, 115]}, "prediction_text": "Disease."}
{"answers": {"text": ["pathogens", "pathogens", "pathogens", "pathogens"], "answer_start": [210, 210, 210, 210]}, "prediction_text": "Viruses, parasites, and other pathogens."}
{"answers": {"text": ["neuroimmune system", "neuroimmune", "neuroimmune", "neuroimmune system"], "answer_start": [655, 655, 655, 655]}, "prediction_text": "Blood-brain barrier"}
{"answers": {"text": ["blood\u2013brain barrier, blood\u2013cerebrospinal fluid barrier", "blood\u2013brain barrier, blood\u2013cerebrospinal fluid barrier, and similar fluid\u2013brain barriers", "blood\u2013brain barrier, blood\u2013cerebrospinal fluid barrier, and similar fluid\u2013brain barriers", "fluid\u2013brain barriers"], "answer_start": [519, 519, 519, 587]}, "prediction_text": "Blood\u2013brain barrier."}
{"answers": {"text": ["pathogens", "pathogens", "pathogens", "pathogens"], "answer_start": [210, 210, 210, 210]}, "prediction_text": "pathogens"}
{"answers": {"text": ["innate immune system versus the adaptive immune system", "innate immune system versus the adaptive immune system", "innate immune system versus the adaptive immune system", "innate immune system versus the adaptive immune system"], "answer_start": [397, 397, 397, 397]}, "prediction_text": "Innate immune system versus adaptive immune system."}
{"answers": {"text": ["humoral immunity versus cell-mediated immunity", "humoral immunity versus cell-mediated immunity", "humoral immunity versus cell-mediated immunity", "humoral immunity versus cell-mediated immunity"], "answer_start": [456, 456, 456, 456]}, "prediction_text": "Humoral immunity versus cell-mediated immunity."}
{"answers": {"text": ["neuroimmune system", "neuroimmune", "neuroimmune system", "neuroimmune system"], "answer_start": [655, 655, 655, 655]}, "prediction_text": "The immune system of the brained."}
{"answers": {"text": ["Pathogens can rapidly evolve and adapt", "rapidly evolve and adapt", "rapidly evolve and adapt"], "answer_start": [0, 14, 14]}, "prediction_text": "evolve and adapt."}
{"answers": {"text": ["enzymes that protect against bacteriophage infections", "rudimentary", "rudimentary"], "answer_start": [306, 264, 264]}, "prediction_text": "Phagocytosis."}
{"answers": {"text": ["eukaryotes", "eukaryotes", "bacteria"], "answer_start": [410, 410, 245]}, "prediction_text": "Plants and invertebrates."}
{"answers": {"text": ["creates immunological memory", "immunological memory", "creates immunological memory"], "answer_start": [811, 819, 811]}, "prediction_text": "Enhances response to subsequent encounters with that same pathogen."}
{"answers": {"text": ["bacteriophage", "bacteriophage", "bacteriophage infections"], "answer_start": [335, 335, 335]}, "prediction_text": "bacteriophage infections."}
{"answers": {"text": ["defensins", "defensins", "defensins"], "answer_start": [564, 564, 564]}, "prediction_text": "Defensins."}
{"answers": {"text": ["vaccination", "vaccination", "vaccination"], "answer_start": [1022, 1022, 1022]}, "prediction_text": "Vaccination."}
{"answers": {"text": ["Adaptive (or acquired) immunity", "Adaptive (or acquired) immunity", "Adaptive (or acquired) immunity"], "answer_start": [779, 779, 779]}, "prediction_text": "Adaptive immunity."}
{"answers": {"text": ["autoimmune diseases, inflammatory diseases and cancer", "autoimmune diseases", "autoimmune diseases"], "answer_start": [45, 45, 45]}, "prediction_text": "Autoimmune diseases, inflammatory diseases, and cancer."}
{"answers": {"text": ["when the immune system is less active than normal", "when the immune system is less active than normal", "when the immune system is less active than normal"], "answer_start": [124, 124, 124]}, "prediction_text": "When the immune system is less active than normal."}
{"answers": {"text": ["recurring and life-threatening infections", "recurring and life-threatening infections.", "genetic disease"], "answer_start": [188, 188, 289]}, "prediction_text": "recurrent and life-threatening infections."}
{"answers": {"text": ["genetic disease", "genetic disease", "HIV/AIDS"], "answer_start": [289, 289, 375]}, "prediction_text": "Genetic disease."}
{"answers": {"text": ["rheumatoid arthritis", "Hashimoto's thyroiditis", "Hashimoto's thyroiditis"], "answer_start": [616, 591, 591]}, "prediction_text": "Hashimoto's thyroiditis."}
{"answers": {"text": ["Immunodeficiency", "Immunodeficiency", "Immunodeficiency occurs"], "answer_start": [100, 100, 100]}, "prediction_text": "Recurring and life-threatening infections."}
{"answers": {"text": ["autoimmunity", "autoimmunity", "autoimmunity"], "answer_start": [442, 442, 442]}, "prediction_text": "Autoimmunity"}
{"answers": {"text": ["Immunology", "Immunology", "Immunology"], "answer_start": [698, 698, 698]}, "prediction_text": "Immunology"}
{"answers": {"text": ["HIV/AIDS", "HIV/AIDS", "HIV/AIDS"], "answer_start": [375, 375, 375]}, "prediction_text": "HIV/AIDS"}
{"answers": {"text": ["plague of Athens in 430 BC", "plague of Athens in 430 BC", "during the plague of Athens in 430 BC"], "answer_start": [227, 227, 216]}, "prediction_text": "Thucydides"}
{"answers": {"text": ["scorpion", "scorpion", "scorpion venom"], "answer_start": [483, 483, 483]}, "prediction_text": "Scorpion venom."}
{"answers": {"text": ["Louis Pasteur", "Louis Pasteur", "Louis Pasteur"], "answer_start": [638, 638, 638]}, "prediction_text": "Louis Pasteur"}
{"answers": {"text": ["Walter Reed", "Walter Reed", "Walter Reed."], "answer_start": [1095, 1095, 1095]}, "prediction_text": "Walter Reed"}
{"answers": {"text": ["Robert Koch", "Robert Koch", "Robert Koch"], "answer_start": [850, 850, 850]}, "prediction_text": "Louis Pasteur"}
{"answers": {"text": ["microorganisms", "microorganisms", "microorganisms"], "answer_start": [930, 930, 930]}, "prediction_text": "Viruses."}
{"answers": {"text": ["yellow fever virus", "yellow fever", "yellow fever virus"], "answer_start": [1073, 1073, 1073]}, "prediction_text": "Yellow fever virus."}
{"answers": {"text": ["Athens in 430 BC", "430 BC", "430 BC."], "answer_start": [237, 247, 247]}, "prediction_text": "430 BC"}
{"answers": {"text": ["immunological memory", "immunological memory", "immunological memory"], "answer_start": [764, 764, 764]}, "prediction_text": "Adaptive immune system"}
{"answers": {"text": ["the innate immune system", "innate immune system", "the innate immune system"], "answer_start": [250, 254, 250]}, "prediction_text": "Innate immune system."}
{"answers": {"text": ["the adaptive immune system", "adaptive immune system", "the adaptive immune system"], "answer_start": [487, 491, 487]}, "prediction_text": "The adaptive immune system."}
{"answers": {"text": ["Innate immune systems", "Innate", "Innate immune systems"], "answer_start": [325, 325, 325]}, "prediction_text": "Innate immune systems."}
{"answers": {"text": ["adaptive immune system", "adaptive", "the adaptive immune system"], "answer_start": [491, 491, 487]}, "prediction_text": "Adaptive immune system."}
{"answers": {"text": ["immunological memory", "immunological memory", "immunological memory"], "answer_start": [764, 764, 764]}, "prediction_text": "Immunological memory."}
{"answers": {"text": ["physical barriers", "physical barriers", "physical barriers"], "answer_start": [118, 118, 118]}, "prediction_text": "Physical barriers."}
{"answers": {"text": ["self and non-self", "self and non-self", "self and non-self molecules"], "answer_start": [100, 100, 100]}, "prediction_text": "self and non-self molecules."}
{"answers": {"text": ["self molecules", "self", "self molecules"], "answer_start": [144, 144, 144]}, "prediction_text": "Body."}
{"answers": {"text": ["non-self molecules", "non-self", "non-self molecules"], "answer_start": [286, 286, 286]}, "prediction_text": "antigens"}
{"answers": {"text": ["antigens", "antigens", "antigens"], "answer_start": [391, 391, 391]}, "prediction_text": "Antigens"}
{"answers": {"text": ["specific immune receptors", "receptors", "specific immune receptors"], "answer_start": [475, 491, 475]}, "prediction_text": "specific immune receptors"}
{"answers": {"text": ["pattern recognition receptors", "receptors", "cells"], "answer_start": [198, 218, 75]}, "prediction_text": "Pattern recognition receptors."}
{"answers": {"text": ["innate immune system", "innate immune system", "The innate immune"], "answer_start": [656, 656, 652]}, "prediction_text": "Innate immune system"}
{"answers": {"text": ["microorganisms", "microorganisms", "microorganisms"], "answer_start": [297, 297, 297]}, "prediction_text": "Microorganisms"}
{"answers": {"text": ["non-specific", "non-specific", "non-specific"], "answer_start": [507, 507, 507]}, "prediction_text": "non-specific"}
{"answers": {"text": ["exoskeleton", "exoskeleton", "exoskeleton"], "answer_start": [145, 145, 145]}, "prediction_text": "exoskeleton"}
{"answers": {"text": ["The waxy cuticle", "waxy cuticle", "waxy cuticle"], "answer_start": [108, 112, 112]}, "prediction_text": "waxy cuticle"}
{"answers": {"text": ["coughing and sneezing", "coughing and sneezing", "coughing and sneezing"], "answer_start": [515, 515, 515]}, "prediction_text": "coughing and sneezing"}
{"answers": {"text": ["mucus", "mucus", "mucus"], "answer_start": [695, 695, 695]}, "prediction_text": "Mucus."}
{"answers": {"text": ["tears", "tears", "tears"], "answer_start": [637, 637, 637]}, "prediction_text": "tears and urine"}
{"answers": {"text": ["\u03b2-defensins", "\u03b2-defensins", "\u03b2-defensins"], "answer_start": [124, 124, 124]}, "prediction_text": "\u03b2-defensins"}
{"answers": {"text": ["lysozyme and phospholipase A2", "lysozyme and phospholipase A2", "lysozyme"], "answer_start": [153, 153, 153]}, "prediction_text": "Lysozyme and phospholipase A2."}
{"answers": {"text": ["defensins and zinc", "defensins and zinc", "defensins"], "answer_start": [364, 364, 364]}, "prediction_text": "Defensins and zinc."}
{"answers": {"text": ["gastric acid and proteases", "gastric acid and proteases", "gastric acid"], "answer_start": [418, 418, 418]}, "prediction_text": "Zinc and defensins."}
{"answers": {"text": ["menarche", "menarche", "menarche"], "answer_start": [299, 299, 299]}, "prediction_text": "Menarche."}
{"answers": {"text": ["commensal flora", "commensal flora", "commensal flora"], "answer_start": [54, 54, 54]}, "prediction_text": "commensal flora"}
{"answers": {"text": ["fungi", "fungi", "fungi"], "answer_start": [430, 430, 430]}, "prediction_text": "Fungi"}
{"answers": {"text": ["lactobacilli", "lactobacilli", "lactobacilli"], "answer_start": [656, 656, 656]}, "prediction_text": "Lactobacilli."}
{"answers": {"text": ["pH or available iron", "pH or available iron", "balance of microbial populations"], "answer_start": [233, 233, 733]}, "prediction_text": "pH or available iron."}
{"answers": {"text": ["Inflammation", "Inflammation", "Inflammation"], "answer_start": [0, 0, 0]}, "prediction_text": "Inflammation"}
{"answers": {"text": ["increased blood flow into tissue", "increased blood flow into tissue", "increased blood flow into tissue"], "answer_start": [166, 166, 166]}, "prediction_text": "Increased blood flow into tissue."}
{"answers": {"text": ["eicosanoids and cytokines", "eicosanoids and cytokines", "eicosanoids"], "answer_start": [228, 228, 228]}, "prediction_text": "Eicosanoids and cytokines."}
{"answers": {"text": ["prostaglandins", "prostaglandins", "prostaglandins"], "answer_start": [324, 324, 324]}, "prediction_text": "Prostaglandins."}
{"answers": {"text": ["interleukins", "interleukins", "interleukins"], "answer_start": [517, 517, 517]}, "prediction_text": "Interleukins."}
{"answers": {"text": ["phagocytes", "phagocytes", "phagocytes"], "answer_start": [92, 92, 92]}, "prediction_text": "Phagocytes"}
{"answers": {"text": ["cytokines", "cytokines", "cytokines"], "answer_start": [254, 254, 254]}, "prediction_text": "cytokines"}
{"answers": {"text": ["phagosome", "phagosome", "phagosome"], "answer_start": [371, 371, 371]}, "prediction_text": "Phagosome"}
{"answers": {"text": ["phagolysosome", "phagolysosome", "phagolysosome"], "answer_start": [601, 456, 456]}, "prediction_text": "Phagolysosome."}
{"answers": {"text": ["acquiring nutrients", "acquiring nutrients", "acquiring nutrients"], "answer_start": [651, 651, 651]}, "prediction_text": "acquiring nutrients"}
{"answers": {"text": ["Neutrophils and macrophages", "Neutrophils and macrophages", "Neutrophils and macrophages"], "answer_start": [0, 0, 0]}, "prediction_text": "Neutrophils and macrophages."}
{"answers": {"text": ["Neutrophils", "Neutrophils", "Neutrophils"], "answer_start": [109, 109, 109]}, "prediction_text": "Neutrophils"}
{"answers": {"text": ["50% to 60%", "50% to 60%", "50% to 60%"], "answer_start": [226, 226, 226]}, "prediction_text": "50% to 60%"}
{"answers": {"text": ["chemotaxis", "chemotaxis", "chemotaxis"], "answer_start": [435, 435, 435]}, "prediction_text": "chemotaxis"}
{"answers": {"text": ["interleukin 1", "interleukin 1", "interleukin 1"], "answer_start": [684, 684, 684]}, "prediction_text": "Interleukin 1."}
{"answers": {"text": ["Leukocytes", "Leukocytes", "Leukocytes"], "answer_start": [0, 0, 0]}, "prediction_text": "Leukocytes (white blood cells)"}
{"answers": {"text": ["Leukocytes (white blood cells)", "white blood cells", "white blood cells"], "answer_start": [0, 12, 12]}, "prediction_text": "Phagocytes, mast cells, eosinophils, basophils, and natural killer cells."}
{"answers": {"text": ["adaptive immune system", "adaptive", "adaptive immune system."], "answer_start": [500, 500, 500]}, "prediction_text": "Adaptive immune system."}
{"answers": {"text": ["macrophages, neutrophils, and dendritic cells", "macrophages, neutrophils, and dendritic cells", "macrophages, neutrophils, and dendritic cells"], "answer_start": [175, 175, 175]}, "prediction_text": "Macrophages, neutrophils, and dendritic cells."}
{"answers": {"text": ["Dendritic cells", "Dendritic cells", "Dendritic cells"], "answer_start": [0, 0, 0]}, "prediction_text": "Dendritic cells (DC)"}
{"answers": {"text": ["neuronal dendrites", "neuronal dendrites", "neuronal dendrites"], "answer_start": [224, 224, 224]}, "prediction_text": "Neuronal dendrites"}
{"answers": {"text": ["T cells", "T cells", "T cells"], "answer_start": [484, 484, 484]}, "prediction_text": "T cells"}
{"answers": {"text": ["T cells", "T cells", "T cells"], "answer_start": [484, 484, 484]}, "prediction_text": "T cells."}
{"answers": {"text": ["missing self", "missing self", "missing self"], "answer_start": [271, 271, 271]}, "prediction_text": "\"missing self\""}
{"answers": {"text": ["Natural killer cells", "Natural killer cells", "Natural killer cells"], "answer_start": [0, 0, 0]}, "prediction_text": "Natural killer cells."}
{"answers": {"text": ["MHC I (major histocompatibility complex)", "MHC I", "MHC I (major histocompatibility complex)"], "answer_start": [360, 360, 360]}, "prediction_text": "MHC I (major histocompatibility complex)"}
{"answers": {"text": ["killer cell immunoglobulin receptors (KIR", "killer cell immunoglobulin", "killer cell immunoglobulin receptors (KIR)"], "answer_start": [993, 993, 993]}, "prediction_text": "KIR."}
{"answers": {"text": ["vertebrates", "vertebrates", "early vertebrates"], "answer_start": [44, 44, 38]}, "prediction_text": "Early vertebrates."}
{"answers": {"text": ["antigen presentation", "antigen presentation", "antigen presentation"], "answer_start": [325, 325, 325]}, "prediction_text": "antigen presentation"}
{"answers": {"text": ["pathogens or pathogen-infected cells", "pathogens", "pathogen-infected cells"], "answer_start": [436, 436, 449]}, "prediction_text": "pathogens or pathogen-infected cells."}
{"answers": {"text": ["killer T cell and the helper T cell", "killer T cell and the helper T cell", "the killer T cell and the helper T cell"], "answer_start": [371, 371, 367]}, "prediction_text": "Killer T cell, helper T cell."}
{"answers": {"text": ["regulatory T cells", "regulatory", "regulatory T cells"], "answer_start": [430, 430, 430]}, "prediction_text": "Regulatory T cells."}
{"answers": {"text": ["Class I MHC molecules", "Class I MHC", "Class I MHC molecules"], "answer_start": [548, 548, 548]}, "prediction_text": "Class I MHC molecules."}
{"answers": {"text": ["Class II MHC molecules", "Class II MHC", "Class II MHC molecules"], "answer_start": [650, 650, 650]}, "prediction_text": "Class II MHC molecules."}
{"answers": {"text": ["\u03b3\u03b4 T cells", "\u03b3\u03b4", "\u03b3\u03b4 T cells"], "answer_start": [806, 806, 806]}, "prediction_text": "\u03b3\u03b4 T cells."}
{"answers": {"text": ["Killer T cells", "Killer T", "Killer T cells"], "answer_start": [0, 0, 0]}, "prediction_text": "Killer T cells."}
{"answers": {"text": ["CD8", "CD8", "CD8"], "answer_start": [465, 465, 465]}, "prediction_text": "CD8"}
{"answers": {"text": ["T cell receptor (TCR)", "T cell receptor", "T cell receptor (TCR)"], "answer_start": [264, 264, 264]}, "prediction_text": "CD8."}
{"answers": {"text": ["granulysin", "granulysin", "granulysin (a protease)"], "answer_start": [797, 797, 797]}, "prediction_text": "Granulysin (a protease)"}
{"answers": {"text": ["perforin", "perforin", "perforin"], "answer_start": [656, 656, 656]}, "prediction_text": "perforin"}
{"answers": {"text": ["CD4 co-receptor", "CD4", "CD4 co-receptor"], "answer_start": [166, 166, 166]}, "prediction_text": "Lck"}
{"answers": {"text": ["around 200\u2013300", "around 200\u2013300", "around 200\u2013300"], "answer_start": [416, 416, 416]}, "prediction_text": "200-300."}
{"answers": {"text": ["a single MHC:antigen molecule", "single", "a single MHC:antigen molecule"], "answer_start": [578, 580, 578]}, "prediction_text": "200-300."}
{"answers": {"text": ["cytokines", "cytokines", "cytokines"], "answer_start": [774, 774, 774]}, "prediction_text": "Cytokines."}
{"answers": {"text": ["CD40 ligand", "CD40", "CD40 ligand"], "answer_start": [1078, 1078, 1078]}, "prediction_text": "CD40 ligand."}
{"answers": {"text": ["helper T cells, cytotoxic T cells and NK cells", "helper T cells, cytotoxic T cells", "helper T cells"], "answer_start": [152, 152, 152]}, "prediction_text": "helper T cells, cytotoxic T cells, and NK cells."}
{"answers": {"text": ["alternative T cell receptor (TCR)", "T cell", "T cell receptor (TCR)"], "answer_start": [44, 56, 56]}, "prediction_text": "TCR."}
{"answers": {"text": ["\u03b3\u03b4 T cells", "\u03b3\u03b4", "\u03b3\u03b4 T cells"], "answer_start": [395, 395, 395]}, "prediction_text": "\u03b3\u03b4 T cells."}
{"answers": {"text": ["receptor diversity", "receptor diversity", "receptor diversity"], "answer_start": [564, 564, 564]}, "prediction_text": "receptor diversity and can also develop a memory phenotype."}
{"answers": {"text": ["V\u03b39/V\u03b42 T cells", "V\u03b39/V\u03b42", "V\u03b39/V\u03b42 T cells"], "answer_start": [822, 822, 822]}, "prediction_text": "V\u03b39/V\u03b42 T cells."}
{"answers": {"text": ["B cell", "B", "A B cell"], "answer_start": [2, 2, 0]}, "prediction_text": "B cell"}
{"answers": {"text": ["proteolysis", "proteolysis", "proteolysis"], "answer_start": [170, 170, 170]}, "prediction_text": "Proteolysis."}
{"answers": {"text": ["lymphokines", "lymphokines", "lymphokines"], "answer_start": [372, 372, 372]}, "prediction_text": "lymphokines"}
{"answers": {"text": ["long-lived memory cells", "long-lived memory cells", "long-lived memory cells"], "answer_start": [94, 94, 94]}, "prediction_text": "Long-lived memory cells."}
{"answers": {"text": ["adaptive", "adaptive", "strong response"], "answer_start": [296, 296, 236]}, "prediction_text": "Adaptive."}
{"answers": {"text": ["passive short-term memory or active long-term memory", "passive short-term memory or active long-term memory", "passive short-term memory or active long-term memory"], "answer_start": [514, 514, 514]}, "prediction_text": "Passive short-term memory, active long-term memory."}
{"answers": {"text": ["specific pathogen", "pathogen", "each specific pathogen"], "answer_start": [190, 199, 185]}, "prediction_text": "Pathogens"}
{"answers": {"text": ["microbes", "microbes", "microbes"], "answer_start": [42, 42, 42]}, "prediction_text": "microbes"}
{"answers": {"text": ["IgG", "IgG", "IgG"], "answer_start": [218, 218, 218]}, "prediction_text": "IgG"}
{"answers": {"text": ["Breast milk or colostrum", "Breast milk", "Breast milk or colostrum"], "answer_start": [412, 412, 412]}, "prediction_text": "Breast milk or colostrum."}
{"answers": {"text": ["passive immunity", "passive", "passive immunity"], "answer_start": [726, 726, 726]}, "prediction_text": "Passive immunity."}
{"answers": {"text": ["immunomodulators", "immunomodulators", "immunomodulators"], "answer_start": [20, 20, 20]}, "prediction_text": "Immunomodulators"}
{"answers": {"text": ["adaptive and innate immune responses", "both adaptive and innate", "adaptive and innate immune responses"], "answer_start": [154, 149, 154]}, "prediction_text": "Adaptive and innate immune responses."}
{"answers": {"text": ["lupus erythematosus", "lupus erythematosus", "lupus erythematosus"], "answer_start": [225, 225, 225]}, "prediction_text": "Lupus erythematosus."}
{"answers": {"text": ["immunosuppressive", "immunosuppressive", "immunosuppressive"], "answer_start": [383, 383, 383]}, "prediction_text": "Immunosuppression."}
{"answers": {"text": ["NFIL3", "NFIL3", "NFIL3"], "answer_start": [243, 243, 243]}, "prediction_text": "NFIL3"}
{"answers": {"text": ["heart disease, chronic pain, and asthma", "heart disease, chronic pain, and asthma", "chronic pain"], "answer_start": [573, 573, 588]}, "prediction_text": "Heart disease, chronic pain, asthma."}
{"answers": {"text": ["sleep deprivation", "sleep", "sleep deprivation"], "answer_start": [20, 20, 20]}, "prediction_text": "Sleep deprivation."}
{"answers": {"text": ["decline in hormone levels with age", "decline in hormone levels", "decline in hormone levels"], "answer_start": [37, 37, 37]}, "prediction_text": "Decreased hormone levels."}
{"answers": {"text": ["vitamin D", "vitamin D", "vitamin D."], "answer_start": [657, 657, 657]}, "prediction_text": "Vitamin D."}
{"answers": {"text": ["hormones", "hormones", "hormones"], "answer_start": [166, 166, 166]}, "prediction_text": "Thyroid hormone activity."}
{"answers": {"text": ["cholecalciferol", "cholecalciferol", "cholecalciferol"], "answer_start": [556, 556, 556]}, "prediction_text": "cholecalciferol"}
{"answers": {"text": ["killer T cells", "killer T cells", "killer T cells"], "answer_start": [88, 88, 88]}, "prediction_text": "Killer T cells."}
{"answers": {"text": ["MHC class I molecules", "MHC class I", "MHC class I molecules"], "answer_start": [404, 404, 404]}, "prediction_text": "MHC class I molecules."}
{"answers": {"text": ["viral antigens", "viral", "viral antigens"], "answer_start": [227, 227, 227]}, "prediction_text": "Viral antigens."}
{"answers": {"text": ["antibodies", "antibodies", "antibodies"], "answer_start": [507, 507, 507]}, "prediction_text": "Antibodies."}
{"answers": {"text": ["phagocytic cells", "phagocytic", "phagocytic cells"], "answer_start": [28, 28, 28]}, "prediction_text": "phagocytic cells"}
{"answers": {"text": ["Pathogen-associated molecular patterns", "Pathogen-associated molecular patterns", "Pathogen-associated molecular patterns or PAMPs"], "answer_start": [222, 222, 222]}, "prediction_text": "Pathogen-associated molecular patterns or PAMPs."}
{"answers": {"text": ["apoptosis", "apoptosis", "rapid apoptosis"], "answer_start": [421, 421, 415]}, "prediction_text": "Apoptosis."}
{"answers": {"text": ["Systemic acquired resistance (SAR)", "Systemic acquired resistance", "Systemic acquired resistance (SAR)"], "answer_start": [497, 497, 497]}, "prediction_text": "SAR"}
{"answers": {"text": ["RNA silencing mechanisms", "RNA silencing mechanisms", "RNA silencing mechanisms"], "answer_start": [653, 653, 653]}, "prediction_text": "RNA silencing mechanisms."}
{"answers": {"text": ["autoimmune disorders", "autoimmune", "autoimmune disorders"], "answer_start": [91, 91, 91]}, "prediction_text": "Autoimmune disorders."}
{"answers": {"text": ["self and non-self", "self and non-self", "self and non-self"], "answer_start": [175, 175, 175]}, "prediction_text": "T cells and antibodies."}
{"answers": {"text": ["thymus and bone marrow", "thymus and bone marrow", "thymus and bone marrow"], "answer_start": [366, 366, 366]}, "prediction_text": "Thymus and bone marrow."}
{"answers": {"text": ["\"self\" peptides", "self", "self\" peptides"], "answer_start": [291, 292, 292]}, "prediction_text": "self-antigens"}
{"answers": {"text": ["Immunodeficiencies", "Immunodeficiencies", "Immunodeficiencies"], "answer_start": [0, 0, 0]}, "prediction_text": "Immunodeficiencies."}
{"answers": {"text": ["the young and the elderly", "the young and the elderly", "young and the elderly"], "answer_start": [174, 174, 178]}, "prediction_text": "Young and elderly."}
{"answers": {"text": ["around 50 years of age", "50", "around 50 years of age"], "answer_start": [247, 254, 247]}, "prediction_text": "50 years of age."}
{"answers": {"text": ["obesity, alcoholism, and drug use", "obesity, alcoholism, and drug use", "obesity, alcoholism, and drug use"], "answer_start": [319, 319, 319]}, "prediction_text": "Obesity, alcoholism, drug use."}
{"answers": {"text": ["malnutrition", "malnutrition", "malnutrition"], "answer_start": [405, 405, 405]}, "prediction_text": "Malnutrition."}
{"answers": {"text": ["vaccination", "vaccination", "vaccination"], "answer_start": [148, 148, 148]}, "prediction_text": "Vaccination."}
{"answers": {"text": ["immunization", "immunization", "immunization"], "answer_start": [207, 207, 207]}, "prediction_text": "Immunization"}
{"answers": {"text": ["an antigen from a pathogen", "antigen", "antigen from a pathogen"], "answer_start": [237, 240, 240]}, "prediction_text": "antigen"}
{"answers": {"text": ["natural specificity of the immune system", "natural specificity", "the natural specificity"], "answer_start": [508, 508, 504]}, "prediction_text": "Natural specificity."}
{"answers": {"text": ["enzymes", "enzymes", "enzymes"], "answer_start": [291, 291, 291]}, "prediction_text": "Enzymes."}
{"answers": {"text": ["type III secretion system", "type III secretion system", "type III secretion system"], "answer_start": [397, 397, 397]}, "prediction_text": "Type III secretion system."}
{"answers": {"text": ["shut down host defenses", "shut down host defenses", "shut down host defenses."], "answer_start": [584, 584, 584]}, "prediction_text": "Shut down host defenses."}
{"answers": {"text": ["elude host immune responses", "elude host immune responses", "ability to elude host immune responses"], "answer_start": [54, 54, 43]}, "prediction_text": "Elude host immune responses."}
{"answers": {"text": ["Frank Burnet", "Frank Burnet", "Frank Burnet"], "answer_start": [18, 18, 18]}, "prediction_text": "Frank Burnet"}
{"answers": {"text": ["pathogens, an allograft", "pathogens, an allograft", "pathogens, an allograft"], "answer_start": [383, 383, 383]}, "prediction_text": "Pathogens, an allograft."}
{"answers": {"text": ["histocompatibility", "histocompatibility", "histocompatibility"], "answer_start": [514, 514, 514]}, "prediction_text": "The complex \"two-signal\" activation of T cells is referred to as \"two-signal\" activation of T cells."}
{"answers": {"text": ["Niels Jerne", "Niels Jerne", "Niels Jerne"], "answer_start": [65, 65, 65]}, "prediction_text": "Niels Jerne"}
{"answers": {"text": ["Glucocorticoids", "Glucocorticoids", "Glucocorticoids"], "answer_start": [79, 79, 79]}, "prediction_text": "Glucocorticoids."}
{"answers": {"text": ["cytotoxic or immunosuppressive drugs", "cytotoxic or immunosuppressive", "Cytotoxic drugs"], "answer_start": [364, 364, 439]}, "prediction_text": "Cytotoxic or immunosuppressive drugs."}
{"answers": {"text": ["methotrexate or azathioprine", "methotrexate or azathioprine", "methotrexate or azathioprine"], "answer_start": [409, 409, 409]}, "prediction_text": "Methotrexate and azathioprine."}
{"answers": {"text": ["cyclosporin", "cyclosporin", "cyclosporin"], "answer_start": [707, 707, 707]}, "prediction_text": "Cyclosporin"}
{"answers": {"text": ["cytotoxic natural killer cells and CTLs (cytotoxic T lymphocytes)", "cytotoxic natural killer cells and CTLs", "cytotoxic natural killer cells and CTLs (cytotoxic T lymphocytes)"], "answer_start": [72, 72, 72]}, "prediction_text": "Cytotoxic natural killer cells and CTLs."}
{"answers": {"text": ["cortisol and catecholamines", "cortisol and catecholamines", "cortisol and catecholamines"], "answer_start": [290, 290, 290]}, "prediction_text": "Cortisol and catecholamines."}
{"answers": {"text": ["melatonin", "melatonin", "melatonin"], "answer_start": [600, 600, 600]}, "prediction_text": "Melatonin"}
{"answers": {"text": ["free radical production", "free radical", "free radical production"], "answer_start": [739, 739, 739]}, "prediction_text": "free radical production."}
{"answers": {"text": ["a vitamin D receptor", "extends a vitamin D receptor", "vitamin D receptor"], "answer_start": [56, 48, 58]}, "prediction_text": "A vitamin D receptor."}
{"answers": {"text": ["calcitriol", "calcitriol", "steroid hormone calcitriol"], "answer_start": [201, 201, 185]}, "prediction_text": "calcitriol"}
{"answers": {"text": ["symbiotic relationship", "symbiotic", "symbiotic relationship"], "answer_start": [228, 228, 228]}, "prediction_text": "T-cells extend a vitamin D receptor."}
{"answers": {"text": ["gene CYP27B1", "CYP27B1", "gene CYP27B1"], "answer_start": [433, 438, 433]}, "prediction_text": "CYP27B1"}
{"answers": {"text": ["dendritic cells, keratinocytes and macrophages", "dendritic cells, keratinocytes and macrophages", "dendritic cells"], "answer_start": [767, 767, 767]}, "prediction_text": "Dendritic cells, keratinocytes, macrophages."}
{"answers": {"text": ["Pattern recognition receptors", "Pattern recognition receptors", "Pattern recognition receptors"], "answer_start": [0, 0, 0]}, "prediction_text": "Antimicrobial peptides."}
{"answers": {"text": ["defensins", "defensins", "defensins"], "answer_start": [151, 151, 151]}, "prediction_text": "Defensins"}
{"answers": {"text": ["phagocytic cells", "phagocytic", "phagocytic cells"], "answer_start": [355, 355, 355]}, "prediction_text": "Phagocytic cells."}
{"answers": {"text": ["RNA interference pathway", "RNA interference", "RNA interference pathway"], "answer_start": [444, 444, 444]}, "prediction_text": "Ribonucleases and the RNA interference pathway."}
{"answers": {"text": ["immunoglobulins and T cell receptors", "immunoglobulins and T cell receptors", "immunoglobulins"], "answer_start": [159, 159, 159]}, "prediction_text": "immunoglobulins and T cell receptors."}
{"answers": {"text": ["the lamprey and hagfish", "lamprey and hagfish", "the lamprey and hagfish"], "answer_start": [340, 344, 340]}, "prediction_text": "lamprey and hagfish."}
{"answers": {"text": ["Variable lymphocyte receptors (VLRs)", "Variable lymphocyte receptors", "Variable lymphocyte receptors (VLRs)"], "answer_start": [421, 421, 421]}, "prediction_text": "VLRs"}
{"answers": {"text": ["adaptive immune system", "adaptive", "the adaptive immune system"], "answer_start": [17, 17, 125]}, "prediction_text": "Adaptive immune system."}
{"answers": {"text": ["lymphocytes", "lymphocytes", "lymphocytes"], "answer_start": [126, 126, 126]}, "prediction_text": "lymphocytes or an antibody-based humoral response."}
{"answers": {"text": ["the restriction modification system", "restriction modification system", "restriction modification system"], "answer_start": [418, 422, 422]}, "prediction_text": "restriction modification system"}
{"answers": {"text": ["bacteriophages", "viral", "bacteriophages"], "answer_start": [505, 481, 505]}, "prediction_text": "Viral pathogens."}
{"answers": {"text": ["CRISPR", "CRISPR sequences", "CRISPR"], "answer_start": [592, 592, 592]}, "prediction_text": "CRISPR sequences."}
{"answers": {"text": ["\"cellular\" and \"humoral\" theories of immunity", "\"cellular\" and \"humoral\"", "\"cellular\" and \"humoral\" theories"], "answer_start": [317, 317, 317]}, "prediction_text": "Cellular and humoral."}
{"answers": {"text": ["Elie Metchnikoff", "Elie Metchnikoff", "Elie Metchnikoff"], "answer_start": [439, 439, 439]}, "prediction_text": "Elie Metchnikoff"}
{"answers": {"text": ["phagocytes", "phagocytes", "phagocytes"], "answer_start": [488, 488, 488]}, "prediction_text": "Phagocytes."}
{"answers": {"text": ["Robert Koch and Emil von Behring", "Robert Koch and Emil von Behring", "Robert Koch and Emil von Behring,"], "answer_start": [613, 613, 613]}, "prediction_text": "Robert Koch and Emil von Behring."}
{"answers": {"text": ["soluble components (molecules)", "soluble components", "soluble components (molecules)"], "answer_start": [689, 689, 689]}, "prediction_text": "Humors."}
{"answers": {"text": ["cancers", "cancers", "cancers"], "answer_start": [65, 65, 65]}, "prediction_text": "Cancer cells."}
{"answers": {"text": ["MHC class I molecules", "MHC class I", "MHC class I molecules"], "answer_start": [117, 117, 117]}, "prediction_text": "MHC class I molecules."}
{"answers": {"text": ["cytokine TGF-\u03b2", "cytokine TGF-\u03b2", "cytokine TGF-\u03b2"], "answer_start": [302, 302, 302]}, "prediction_text": "TGF-\u03b2"}
{"answers": {"text": ["macrophages and lymphocytes", "macrophages and lymphocytes", "macrophages and lymphocytes"], "answer_start": [351, 351, 351]}, "prediction_text": "Macrophages and lymphocytes."}
{"answers": {"text": ["Hypersensitivity", "Hypersensitivity", "Hypersensitivity"], "answer_start": [0, 0, 0]}, "prediction_text": "Hypersensitivity"}
{"answers": {"text": ["four classes (Type I \u2013 IV)", "four", "four classes"], "answer_start": [98, 98, 98]}, "prediction_text": "Four."}
{"answers": {"text": ["Type I", "Type I", "Type I hypersensitivity"], "answer_start": [210, 210, 210]}, "prediction_text": "Type I hypersensitivity."}
{"answers": {"text": ["IgE", "IgE", "IgE"], "answer_start": [396, 396, 396]}, "prediction_text": "IgE"}
{"answers": {"text": ["Type II hypersensitivity", "Type II", "Type II hypersensitivity"], "answer_start": [488, 488, 488]}, "prediction_text": "Type I hypersensitivity."}
{"answers": {"text": ["intracellular pathogenesis", "intracellular pathogenesis", "intracellular pathogenesis"], "answer_start": [135, 135, 135]}, "prediction_text": "Intracellular pathogenesis."}
{"answers": {"text": ["Salmonella", "Salmonella", "Salmonella"], "answer_start": [404, 404, 404]}, "prediction_text": "Salmonella."}
{"answers": {"text": ["Plasmodium falciparum", "Plasmodium falciparum", "Plasmodium falciparum"], "answer_start": [464, 464, 464]}, "prediction_text": "Leishmania spp."}
{"answers": {"text": ["Mycobacterium tuberculosis", "Mycobacterium tuberculosis", "Mycobacterium tuberculosis"], "answer_start": [548, 548, 548]}, "prediction_text": "Mycobacterium tuberculosis."}
{"answers": {"text": ["protein A", "G", "Streptococcus (protein G)"], "answer_start": [1166, 1139, 1116]}, "prediction_text": "Protein A."}
{"answers": {"text": ["antigenic variation", "antigenic variation", "antigenic variation"], "answer_start": [263, 263, 263]}, "prediction_text": "antigenic variation."}
{"answers": {"text": ["HIV", "HIV", "HIV"], "answer_start": [298, 298, 298]}, "prediction_text": "HIV"}
{"answers": {"text": ["Trypanosoma brucei", "Trypanosoma brucei", "Trypanosoma brucei"], "answer_start": [553, 553, 553]}, "prediction_text": "Trypanosoma brucei."}
{"answers": {"text": ["antigens", "antigens", "antigens"], "answer_start": [728, 728, 728]}, "prediction_text": "Host molecules."}
{"answers": {"text": ["immune surveillance", "immune surveillance", "immune surveillance"], "answer_start": [96, 96, 96]}, "prediction_text": "Immune surveillance."}
{"answers": {"text": ["human papillomavirus", "human papillomavirus", "human papillomavirus"], "answer_start": [433, 433, 433]}, "prediction_text": "Human papillomavirus (HPV)"}
{"answers": {"text": ["tyrosinase", "tyrosinase", "tyrosinase"], "answer_start": [641, 641, 641]}, "prediction_text": "Tyrosinase."}
{"answers": {"text": ["melanomas", "melanomas", "melanomas"], "answer_start": [757, 757, 757]}, "prediction_text": "Melanomas."}
{"answers": {"text": ["melanocytes", "melanocytes", "melanocytes"], "answer_start": [725, 725, 725]}, "prediction_text": "melanocytes"}
{"answers": {"text": [">500 Da", ">500 Da", ">500 Da"], "answer_start": [14, 14, 14]}, "prediction_text": "Larger drugs (>500 Da) can provoke a neutralizing immune response."}
{"answers": {"text": ["hydrophilic amino acids", "hydrophilic", "hydrophilic amino acids"], "answer_start": [749, 749, 749]}, "prediction_text": "hydrophilic amino acids."}
{"answers": {"text": ["Immunoproteomics", "Immunoproteomics", "Immunoproteomics"], "answer_start": [1257, 1257, 1257]}, "prediction_text": "Immunoinformatics"}
{"answers": {"text": ["B cells", "B", "B cells"], "answer_start": [1142, 1142, 1142]}, "prediction_text": "B cells."}
{"answers": {"text": ["immunoinformatics", "immunoinformatics", "immunoinformatics"], "answer_start": [1238, 1238, 1238]}, "prediction_text": "immunoinformatics"}
{"answers": {"text": ["leptin, pituitary growth hormone, and prolactin", "leptin, pituitary growth hormone, and prolactin", "leptin"], "answer_start": [404, 404, 404]}, "prediction_text": "Leptin, pituitary growth hormone, and prolactin."}
{"answers": {"text": ["APCs", "APCs", "APCs"], "answer_start": [1069, 1069, 1069]}, "prediction_text": "APCs"}
{"answers": {"text": ["Th1", "one that supports Th1"], "answer_start": [1153, 1135]}, "prediction_text": "Leptin, pituitary growth hormone, and prolactin."}
{"answers": {"text": ["Th1 immune responses", "Th1", "Th1 immune responses"], "answer_start": [1356, 1356, 1356]}, "prediction_text": "Th1 responses."}
{"answers": {"text": ["carbohydrates", "carbohydrates", "carbohydrates"], "answer_start": [152, 152, 152]}, "prediction_text": "carbohydrates"}
{"answers": {"text": ["disrupting their plasma membrane", "disrupting their plasma membrane", "disrupting their plasma membrane."], "answer_start": [929, 929, 929]}, "prediction_text": "disrupt their plasma membrane."}
{"answers": {"text": ["signal amplification", "signal amplification", "signal amplification"], "answer_start": [295, 295, 295]}, "prediction_text": "Complement activation."}
{"answers": {"text": ["catalytic cascade", "catalytic", "catalytic"], "answer_start": [598, 598, 598]}, "prediction_text": "Catalytic cascade."}
{"answers": {"text": ["Civil disobedience", "Civil disobedience", "Civil disobedience", "Civil disobedience", "Civil disobedience"], "answer_start": [130, 130, 130, 130, 130]}, "prediction_text": "Civil disobedience"}
{"answers": {"text": ["apartheid", "fight against apartheid", "the fight against apartheid", "the fight against apartheid", "Singing Revolution to bring independence to the Baltic countries"], "answer_start": [497, 483, 479, 479, 554]}, "prediction_text": "The Rose Revolution."}
{"answers": {"text": ["Singing Revolution", "Singing Revolution", "the Singing Revolution", "Singing Revolution", "Singing Revolution"], "answer_start": [554, 554, 550, 554, 554]}, "prediction_text": "Rose Revolution"}
{"answers": {"text": ["Ukraine", "Ukraine", "Ukraine", "Ukraine", "Ukraine"], "answer_start": [726, 726, 726, 726, 726]}, "prediction_text": "Ukraine"}
{"answers": {"text": ["Georgia", "Georgia", "Georgia", "Georgia", "Georgia"], "answer_start": [684, 684, 684, 684, 684]}, "prediction_text": "Georgia"}
{"answers": {"text": ["Egyptians", "Egyptians", "Egyptians", "Egyptians", "India"], "answer_start": [65, 65, 65, 65, 296]}, "prediction_text": "Egyptians"}
{"answers": {"text": ["the British", "British", "Egyptians against the British occupation in the 1919 Revolution.", "British occupation", "communist governments"], "answer_start": [83, 87, 65, 87, 437]}, "prediction_text": "Egyptians"}
{"answers": {"text": ["nonviolent resistance", "unfair laws", "nonviolent resistance movements", "rebelled against what they deem to be unfair"], "answer_start": [261, 223, 261, 185]}, "prediction_text": "To rebelling against unfair laws."}
{"answers": {"text": ["unfair laws", "unfair laws", "unfair laws.", "unfair laws", "what they deem to be unfair"], "answer_start": [223, 223, 223, 223, 202]}, "prediction_text": "unfair laws"}
{"answers": {"text": ["American Civil Rights Movement", "American Civil Rights Movement", "American Civil Rights Movement", "Civil Rights Movement", "American Civil Rights Movement"], "answer_start": [515, 515, 515, 524, 515]}, "prediction_text": "American Civil Rights Movement"}
{"answers": {"text": ["Antigone", "Antigone", "Antigone", "Antigone", "Antigone"], "answer_start": [73, 73, 73, 73, 73]}, "prediction_text": "Antigone"}
{"answers": {"text": ["former King of Thebes", "Oedipus", "Oedipus", "Oedipus", "King of Thebes"], "answer_start": [126, 149, 149, 149, 133]}, "prediction_text": "Oedipus"}
{"answers": {"text": ["Creon", "Creon", "Creon", "Creon", "Creon"], "answer_start": [165, 165, 165, 165, 165]}, "prediction_text": "Creon"}
{"answers": {"text": ["Oedipus", "Antigone", "Antigone", "Antigone", "Antigone"], "answer_start": [149, 92, 73, 73, 92]}, "prediction_text": "Sophocles"}
{"answers": {"text": ["giving her brother Polynices a proper burial", "trying to stop her from giving her brother Polynices a proper burial", "giving her brother Polynices a proper burial", "giving her brother Polynices a proper burial.", "from giving her brother Polynices a proper burial"], "answer_start": [231, 207, 231, 231, 226]}, "prediction_text": "giving Polynices a proper burial."}
{"answers": {"text": ["Antigone", "Antigone", "Antigone", "Antigone", "Antigone"], "answer_start": [73, 73, 73, 73, 73]}, "prediction_text": "Sophocles' Antigone."}
{"answers": {"text": ["Sophocles", "Sophocles", "Sophocles", "Sophocles", "Sophocles"], "answer_start": [57, 57, 57, 57, 57]}, "prediction_text": "Sophocles"}
{"answers": {"text": ["Creon, the current King of Thebes", "Creon", "Creon, the current King of Thebes", "Creon", "Creon"], "answer_start": [165, 165, 165, 165, 165]}, "prediction_text": "Creon"}
{"answers": {"text": ["giving her brother Polynices a proper burial", "giving her brother Polynices a proper burial", "the current King of Thebes, who is trying to stop her from giving her brother Polynices a proper burial", "Creon, the current King of Thebes, who is trying to stop her from giving her brother Polynices a proper burial", "giving her brother Polynices a proper burial"], "answer_start": [231, 231, 172, 165, 231]}, "prediction_text": "The protest in Antigone was about the death of Polynices."}
{"answers": {"text": ["obey her conscience rather than human law", "She gives a stirring speech", "She gives a stirring speech in which she tells him that she must obey her conscience rather than human law", "a stirring speech", "She gives a stirring speech in which she tells him that she must obey her conscience rather than human law"], "answer_start": [342, 277, 277, 287, 277]}, "prediction_text": "Sophocles' play."}
{"answers": {"text": ["Percy Shelley", "Percy Shelley", "Percy Shelley", "Percy Shelley", "Percy Shelley"], "answer_start": [46, 46, 46, 46, 46]}, "prediction_text": "Percy Shelley"}
{"answers": {"text": ["nonviolent", "political poem", "nonviolent", "nonviolent", "nonviolent"], "answer_start": [349, 70, 349, 349, 349]}, "prediction_text": "Nonviolent protest."}
{"answers": {"text": ["Satyagraha", "Satyagraha", "Satyagraha", "Satyagraha", "Satyagraha"], "answer_start": [519, 498, 498, 519, 498]}, "prediction_text": "Satyagraha"}
{"answers": {"text": ["free India", "free India", "campaign for a free India", "protest and political action", "during the campaign for a free India"], "answer_start": [755, 755, 740, 596, 729]}, "prediction_text": "Satyagraha."}
{"answers": {"text": ["Henry David Thoreau", "Gandhi", "Henry David Thoreau", "Henry David Thoreau", "Gandhi"], "answer_start": [406, 472, 406, 406, 472]}, "prediction_text": "Henry David Thoreau"}
{"answers": {"text": ["Percy Shelley", "Percy Shelley", "Percy Shelley", "Percy Shelley", "Percy Shelley"], "answer_start": [46, 46, 46, 46, 46]}, "prediction_text": "Percy Shelley"}
{"answers": {"text": ["unjust forms of authority", "Peterloo massacre", "unjust forms of authority", "the unjust forms of authority", "Peterloo massacre"], "answer_start": [179, 14, 179, 175, 14]}, "prediction_text": "unjust forms of authority."}
{"answers": {"text": ["principle of nonviolent protest", "nonviolent protest", "nonviolent protest", "nonviolent protest", "nonviolent protest"], "answer_start": [336, 349, 349, 349, 349]}, "prediction_text": "Nonviolent protest."}
{"answers": {"text": ["doctrine of Satyagraha", "Satyagraha", "Satyagraha", "Satyagraha", "Satyagraha"], "answer_start": [486, 519, 519, 519, 498]}, "prediction_text": "Satyagraha"}
{"answers": {"text": ["Gandhi", "Gandhi", "Gandhi", "Gandhi", "Gandhi"], "answer_start": [472, 658, 472, 472, 472]}, "prediction_text": "Gandhi"}
{"answers": {"text": ["muggers, arsonists, draft evaders, campaign hecklers, campus militants, anti-war demonstrators, juvenile delinquents and political assassins", "civil disobedience", "muggers, arsonists, draft evaders, campaign hecklers, campus militants, anti-war demonstrators, juvenile delinquents and political assassins", "muggers, arsonists, draft evaders, campaign hecklers, campus militants, anti-war demonstrators, juvenile delinquents and political assassins", "activities of muggers, arsonists, draft evaders, campaign hecklers, campus militants, anti-war demonstrators, juvenile delinquents and political assassins"], "answer_start": [370, 34, 370, 370, 356]}, "prediction_text": "muggers, arsonists, draft evaders, campaign hecklers, campus militants, anti-war demonstrators, juvenile delinquents, political assassins."}
{"answers": {"text": ["Marshall Cohen", "Marshall Cohen", "Marshall Cohen", "Marshall Cohen", "Marshall Cohen"], "answer_start": [134, 134, 134, 134, 134]}, "prediction_text": "Marshall Cohen"}
{"answers": {"text": ["ambiguity", "debased", "ambiguity", "ambiguity", "ambiguity"], "answer_start": [79, 125, 79, 79, 79]}, "prediction_text": "Ambiguity."}
{"answers": {"text": ["utterly debased", "debased", "everything from bringing a test-case in the federal courts to taking aim at a federal official", "utterly debased"], "answer_start": [117, 125, 186, 117]}, "prediction_text": "debased"}
{"answers": {"text": ["become utterly debased", "utterly debased", "in modern times, become utterly debased", "become utterly debased", "become utterly debased"], "answer_start": [110, 117, 93, 110, 110]}, "prediction_text": "Marshall Cohen."}
{"answers": {"text": ["Marshall Cohen", "Marshall Cohen", "Marshall Cohen", "Marshall Cohen", "President Agnew"], "answer_start": [134, 134, 134, 134, 299]}, "prediction_text": "Marshall Cohen"}
{"answers": {"text": ["code-word describing the activities of muggers, arsonists, draft evaders", "utterly debased", "become utterly debased", "become utterly debased", "become utterly debased"], "answer_start": [331, 117, 110, 110, 110]}, "prediction_text": "Debased."}
{"answers": {"text": ["Vice President Agnew", "Vice President Agnew", "Vice President Agnew", "Vice President Agnew", "Marshall Cohen"], "answer_start": [294, 294, 294, 294, 134]}, "prediction_text": "Marshall Cohen"}
{"answers": {"text": ["ambiguity", "ambiguity", "ambiguity", "ambiguity"], "answer_start": [79, 79, 79, 79]}, "prediction_text": "Ambiguity."}
{"answers": {"text": ["LeGrande", "LeGrande", "LeGrande", "LeGrande", "LeGrande"], "answer_start": [0, 0, 0, 0, 0]}, "prediction_text": "LeGrande"}
{"answers": {"text": ["impossible", "a maze of semantical problems and grammatical niceties", "impossible", "impossible"], "answer_start": [121, 258, 121, 121]}, "prediction_text": "impossible"}
{"answers": {"text": ["lawful protest demonstration, nonviolent civil disobedience, and violent civil disobedience", "lawful protest demonstration, nonviolent civil disobedience, and violent civil disobedience", "lawful protest demonstration, nonviolent civil disobedience, and violent civil disobedience", "lawful protest demonstration, nonviolent civil disobedience, and violent civil disobedience", "lawful protest demonstration, nonviolent civil disobedience, and violent civil disobedience"], "answer_start": [498, 498, 498, 498, 498]}, "prediction_text": "lawful protest demonstration, nonviolent civil disobedience, and violent civil disobedience."}
{"answers": {"text": ["semantical", "semantical", "a maze of semantical problems", "semantical", "semantical problems"], "answer_start": [268, 268, 258, 268, 268]}, "prediction_text": "semantical problems"}
{"answers": {"text": ["specific", "civil disobedience", "specific", "specific"], "answer_start": [360, 571, 360, 360]}, "prediction_text": "The author's words offer only what they intended for them to imply regarding this type of terminology."}
{"answers": {"text": ["LeGrande", "LeGrande", "LeGrande", "LeGrande", "LeGrande"], "answer_start": [0, 0, 0, 0, 0]}, "prediction_text": "LeGrande"}
{"answers": {"text": ["voluminous literature", "voluminous", "voluminous", "voluminous"], "answer_start": [150, 150, 150, 150]}, "prediction_text": "Voluminous."}
{"answers": {"text": ["semantical problems and grammatical niceties", "maze of semantical problems and grammatical niceties", "semantical problems and grammatical niceties", "semantical problems and grammatical niceties"], "answer_start": [268, 260, 268, 268]}, "prediction_text": "grammatical niceties"}
{"answers": {"text": ["nonviolent civil disobedience", "nonviolent civil disobedience", "nonviolent", "nonviolent", "nonviolent"], "answer_start": [528, 528, 528, 528, 528]}, "prediction_text": "Nonviolent civil disobedience."}
{"answers": {"text": ["violent civil disobedience", "violent civil disobedience", "violent", "violent", "violent"], "answer_start": [563, 563, 563, 563, 563]}, "prediction_text": "Nonviolent civil disobedience."}
{"answers": {"text": ["constitutional impasse", "her or his capacity as public official", "capacity as public official", "constitutional impasse"], "answer_start": [127, 454, 465, 127]}, "prediction_text": "Civil disobedience"}
{"answers": {"text": ["citizen's", "head of government", "private citizen", "private citizen", "private"], "answer_start": [57, 268, 505, 505, 505]}, "prediction_text": "Head of government."}
{"answers": {"text": ["to the state and its laws", "relation to the state and its laws", "a citizen's relation to the state and its laws,", "pertaining to a citizen's relation to the state and its laws", "as pertaining to a citizen's relation to the state and its laws"], "answer_start": [76, 67, 55, 41, 38]}, "prediction_text": "According to the definition in the text."}
{"answers": {"text": ["the head of government would be acting in her or his capacity as public official", "two public agencies, especially two equally sovereign branches of government, conflict", "two public agencies, especially two equally sovereign branches of government, conflict", "if the head of government of a country were to refuse to enforce a decision of that country's highest court", "two public agencies, especially two equally sovereign branches of government, conflict"], "answer_start": [412, 159, 159, 261, 159]}, "prediction_text": "When the head of government of a country refuses to enforce a decision of that country's highest court."}
{"answers": {"text": ["Civil disobedience", "Civil disobedience", "Civil disobedience", "Civil disobedience", "Civil disobedience"], "answer_start": [0, 0, 0, 0, 0]}, "prediction_text": "Civil disobedience"}
{"answers": {"text": ["the state and its laws", "a citizen's relation to the state and its laws", "a citizen's relation to the state and its laws", "a citizen's relation to the state and its laws", "a citizen's relation to the state and its laws"], "answer_start": [79, 55, 55, 55, 55]}, "prediction_text": "The relation between the state and its laws."}
{"answers": {"text": ["refuse to enforce a decision", "constitutional impasse", "two public agencies, especially two equally sovereign branches of government, conflict.", "a constitutional impasse in which two public agencies", "constitutional impasse"], "answer_start": [308, 127, 159, 125, 127]}, "prediction_text": "A constitutional impasse."}
{"answers": {"text": ["head of government", "public official", "public official", "public"], "answer_start": [416, 477, 477, 477]}, "prediction_text": "Head of government."}
{"answers": {"text": ["private citizen", "private citizen", "private citizen", "private"], "answer_start": [505, 505, 505, 505]}, "prediction_text": "Citizens"}
{"answers": {"text": ["sovereign branches of government", "public official", "public agencies", "public"], "answer_start": [203, 477, 163, 477]}, "prediction_text": "Citizens"}
{"answers": {"text": ["Thoreau", "Thoreau", "Thoreau", "Thoreau", "Thoreau's"], "answer_start": [40, 489, 40, 40, 40]}, "prediction_text": "Thoreau"}
{"answers": {"text": ["imprisonment", "imprisonment", "imprisonment", "imprisonment", "imprisonment"], "answer_start": [393, 393, 393, 393, 393]}, "prediction_text": "Imprisonment."}
{"answers": {"text": ["not necessarily right", "not necessarily right", "the will of elite politicians", "the will of elite politicians", "not necessarily right"], "answer_start": [1201, 1201, 861, 861, 1201]}, "prediction_text": "abused and perverted."}
{"answers": {"text": ["Resign", "refusal to pay", "Resign", "Resign", "Resign"], "answer_start": [511, 473, 511, 511, 511]}, "prediction_text": "Resign."}
{"answers": {"text": ["elite politicians", "elite politicians", "individuals", "elite politicians", "individuals"], "answer_start": [873, 873, 218, 873, 218]}, "prediction_text": "A confused taxman."}
{"answers": {"text": ["The individual", "individuals", "The individual", "The individual", "individual"], "answer_start": [115, 1112, 115, 115, 119]}, "prediction_text": "The individual."}
{"answers": {"text": ["individuals", "individuals", "an individual", "individuals", "individuals"], "answer_start": [218, 1112, 294, 218, 218]}, "prediction_text": "The individual."}
{"answers": {"text": ["Thoreau", "Thoreau", "Thoreau", "Thoreau", "Thoreau"], "answer_start": [489, 489, 40, 489, 569]}, "prediction_text": "Thoreau"}
{"answers": {"text": ["Resign", "Resign", "Resign", "Resign", "Resign"], "answer_start": [511, 511, 511, 511, 511]}, "prediction_text": "Resign."}
{"answers": {"text": ["not necessarily right", "may be powerful but it is not necessarily right", "The majority may be powerful but it is not necessarily right", "The majority may be powerful but it is not necessarily right", "may be powerful but it is not necessarily right"], "answer_start": [1201, 1175, 1162, 1162, 1175]}, "prediction_text": "The majority may be powerful, but it is not necessarily right."}
{"answers": {"text": ["governmental entities", "against governmental entities", "governmental entities", "governmental entities", "governmental entities"], "answer_start": [91, 83, 91, 91, 91]}, "prediction_text": "Trade unions, banks, private universities."}
{"answers": {"text": ["trade unions, banks, and private universities", "non-governmental agencies", "decisions of non-governmental agencies", "non-governmental agencies", "non-governmental agencies"], "answer_start": [216, 182, 169, 182, 182]}, "prediction_text": "Trade unions, banks, private universities."}
{"answers": {"text": ["legal system", "foreign", "legal system", "international organizations and foreign governments"], "answer_start": [321, 487, 321, 455]}, "prediction_text": "Trade unions."}
{"answers": {"text": ["international organizations and foreign governments", "a larger challenge to the legal system that permits those decisions to be taken", "international organizations and foreign governments", "breaches of law in protest against international organizations and foreign governments", "opposition to the decisions of non-governmental agencies such as trade unions, banks, and private universities"], "answer_start": [455, 295, 455, 420, 151]}, "prediction_text": "trade unions, banks, private universities."}
{"answers": {"text": ["Brownlee", "Brownlee", "Brownlee", "Brownlee", "Brownlee"], "answer_start": [114, 114, 114, 114, 114]}, "prediction_text": "Brownlee"}
{"answers": {"text": ["a larger challenge to the legal system", "challenge to the legal system that permits those decisions to be taken", "it reflects \"a larger challenge to the legal system that permits those decisions to be taken", "a larger challenge to the legal system"], "answer_start": [295, 304, 282, 295]}, "prediction_text": "To protest against international organizations and foreign governments."}
{"answers": {"text": ["only justified against governmental entities", "civil disobedience is only justified against governmental entities", "civil disobedience is only justified against governmental entities.", "civil disobedience is only justified against governmental entities", "that civil disobedience is only justified against governmental entities"], "answer_start": [68, 46, 46, 46, 41]}, "prediction_text": "That civil disobedience is only justified against governmental entities."}
{"answers": {"text": ["universities", "private universities", "private universities", "private universities", "private universities"], "answer_start": [249, 241, 241, 241, 241]}, "prediction_text": "Trade unions."}
{"answers": {"text": ["civil disobedience", "lawbreaking", "civil disobedience", "civil disobedience", "civil disobedience"], "answer_start": [130, 30, 130, 130, 130]}, "prediction_text": "Public civil disobedience."}
{"answers": {"text": ["covert lawbreaking", "lawbreaking", "covert lawbreaking", "covert lawbreaking", "covert lawbreaking"], "answer_start": [346, 353, 346, 346, 346]}, "prediction_text": "covert lawbreaking."}
{"answers": {"text": ["hiding a Jew in their house", "hiding a Jew", "hiding a Jew in their house", "hiding a Jew in their house", "hiding a Jew"], "answer_start": [886, 886, 886, 886, 886]}, "prediction_text": "fabricating evidence or committing perjury."}
{"answers": {"text": ["(Exodus 1: 15-19)", "Book of Exodus", "the Book of Exodus", "Shiphrah and Puah refused a direct order of Pharaoh but misrepresented how they did it", "Book of Exodus,"], "answer_start": [1093, 983, 979, 1005, 983]}, "prediction_text": "Exodus 1: 15-19."}
{"answers": {"text": ["Shiphrah and Puah", "Shiphrah and Puah", "Shiphrah and Puah", "Shiphrah and Puah", "Shiphrah and Puah"], "answer_start": [1005, 1005, 1005, 1005, 1005]}, "prediction_text": "Shiphrah and Puah."}
{"answers": {"text": ["must be publicly announced", "publicly announced", "must be publicly announced", "publicly announced", "civil disobedience"], "answer_start": [80, 88, 80, 88, 130]}, "prediction_text": "Publicly announce non public lawbreaking."}
{"answers": {"text": ["rules that conflict with morality", "rules that conflict with morality", "rules that conflict with morality", "rules that conflict with morality"], "answer_start": [212, 212, 212, 212]}, "prediction_text": "Rules that conflict with morality."}
{"answers": {"text": ["fabricating evidence or committing perjury", "fabricating evidence or committing perjury", "covert lawbreaking", "assisting in fabricating evidence or committing perjury", "covert lawbreaking"], "answer_start": [513, 513, 346, 500, 346]}, "prediction_text": "assisting in fabricating evidence or committing perjury."}
{"answers": {"text": ["the dilemma faced by German citizens", "German citizens when Hitler's secret police demanded to know if they were hiding a Jew in their house", "the dilemma faced by German citizens", "the dilemma faced by German citizens when Hitler's secret police demanded to know if they were hiding a Jew in their house"], "answer_start": [791, 812, 791, 791]}, "prediction_text": "Shiphrah and Puah."}
{"answers": {"text": ["Book of Exodus", "Exodus", "Exodus", "Exodus", "Exodus"], "answer_start": [983, 991, 991, 991, 991]}, "prediction_text": "Exodus 1: 15-19"}
{"answers": {"text": ["non-violence", "non-violent", "non-violent", "non-violent", "non-violent"], "answer_start": [122, 77, 332, 332, 332]}, "prediction_text": "Carefully chosen and legitimate means."}
{"answers": {"text": ["Black's Law", "Black's Law Dictionary", "Black's Law Dictionary", "Black's Law Dictionary", "Black's Law Dictionary"], "answer_start": [90, 90, 90, 90, 90]}, "prediction_text": "Black's Law Dictionary."}
{"answers": {"text": ["civil rebellion", "rebellion", "civil rebellion are justified by appeal to constitutional defects, rebellion is much more", "rebellion", "rebellion"], "answer_start": [404, 471, 404, 471, 471]}, "prediction_text": "Rebellion."}
{"answers": {"text": ["tolerance", "tolerance of civil disobedience", "tolerance of civil disobedience", "tolerance of civil disobedience", "tolerance"], "answer_start": [846, 846, 846, 846, 846]}, "prediction_text": "tolerance"}
{"answers": {"text": ["violence", "non-violent", "appeal to constitutional defects", "violence", "civil disobedience"], "answer_start": [126, 77, 437, 126, 38]}, "prediction_text": "Non-violent."}
{"answers": {"text": ["non-violent", "non-violent", "non-violent", "non-violent", "non-violence"], "answer_start": [77, 77, 332, 77, 122]}, "prediction_text": "Non-violent."}
{"answers": {"text": ["civil rebellion", "civil rebellion", "civil rebellion", "rebellion", "rebellion"], "answer_start": [404, 639, 639, 471, 471]}, "prediction_text": "Civil rebellion."}
{"answers": {"text": ["destructive", "use of force and violence and refusal to submit to arrest", "rebellion is much more destructive", "rebellion is much more destructive", "force and violence and refusal to submit to arrest"], "answer_start": [494, 702, 471, 471, 709]}, "prediction_text": "Destruction."}
{"answers": {"text": ["help preserve society's tolerance of civil disobedience", "preserve society's tolerance of civil disobedience", "rebellion is much more destructive", "Civil disobedients' refraining from violence is also said to help preserve society's tolerance of civil disobedience", "help preserve society's tolerance"], "answer_start": [822, 827, 471, 761, 822]}, "prediction_text": "To preserve society's tolerance of civil disobedience."}
{"answers": {"text": ["Revolutionary civil disobedience", "Revolutionary civil disobedience", "Revolutionary civil disobedience", "Revolutionary civil disobedience", "Revolutionary civil disobedience"], "answer_start": [298, 298, 298, 298, 298]}, "prediction_text": "Revolution."}
{"answers": {"text": ["Hungarians", "Hungarians", "the Hungarians under Ferenc De\u00e1k", "Hungarians", "Hungarians"], "answer_start": [716, 716, 712, 716, 716]}, "prediction_text": "Hungarians under Ferenc De\u00e1k."}
{"answers": {"text": ["Ferenc De\u00e1k", "Ferenc De\u00e1k", "Ferenc De\u00e1k", "Ferenc De\u00e1k", "Ferenc De\u00e1k"], "answer_start": [733, 733, 733, 733, 733]}, "prediction_text": "Ferenc De\u00e1k"}
{"answers": {"text": ["Gandhi's", "Gandhi", "Gandhi", "Gandhi's"], "answer_start": [616, 616, 616, 616]}, "prediction_text": "Hungarians under Ferenc De\u00e1k."}
{"answers": {"text": ["cultural traditions, social customs, religious beliefs", "revolutionary civil disobedience", "change cultural traditions, social customs, religious beliefs, etc", "cultural traditions, social customs, religious beliefs", "peaceable revolution"], "answer_start": [400, 653, 393, 400, 876]}, "prediction_text": "Cultural traditions, social customs, religious beliefs, etc...revolution doesn't have to be political."}
{"answers": {"text": ["disobedience of laws", "Non-revolutionary civil disobedience", "Non-revolutionary civil disobedience", "Non-revolutionary", "cultural revolution"], "answer_start": [49, 0, 0, 0, 509]}, "prediction_text": "Non-revolutionary civil disobedience."}
{"answers": {"text": ["judged \"wrong\" by an individual conscience", "they are judged \"wrong\" by an individual conscience", "they are judged \"wrong\" by an individual conscience", "to cause their repeal", "they are judged \"wrong\" by an individual conscience"], "answer_start": [99, 90, 90, 203, 90]}, "prediction_text": "To overthrow a government."}
{"answers": {"text": ["render certain laws ineffective", "to render certain laws ineffective, to cause their repeal, or to exert pressure to get one's political wishes on some other issue", "render certain laws ineffective, to cause their repeal", "t to render certain laws ineffective, to cause their repeal, or to exert pressure to get one's political wishes on some other issue", "to render certain laws ineffective,"], "answer_start": [170, 167, 170, 165, 167]}, "prediction_text": "To overthrow a government."}
{"answers": {"text": ["Revolutionary civil disobedience", "Revolutionary civil disobedience", "Revolutionary civil disobedience", "Revolutionary", "Revolutionary"], "answer_start": [298, 298, 298, 298, 298]}, "prediction_text": "Revolution."}
{"answers": {"text": ["Gandhi", "Gandhi's", "Gandhi's", "Gandhi", "Gandhi's"], "answer_start": [616, 616, 616, 616, 616]}, "prediction_text": "Gandhi"}
{"answers": {"text": ["during the Roman Empire", "Roman Empire", "during the Roman Empire", "the Roman Empire", "during the Roman Empire"], "answer_start": [76, 87, 76, 83, 76]}, "prediction_text": "Roman Empire."}
{"answers": {"text": ["gathered in the streets", "gathered in the streets", "gathered in the streets", "gathered in the streets", "gathered in the streets"], "answer_start": [131, 131, 131, 131, 131]}, "prediction_text": "gathered in the streets to prevent the installation of pagan images."}
{"answers": {"text": ["was not covered in any newspapers", "was not covered in any newspapers in the days, weeks and months after it happened.", "his arrest was not covered in any newspapers", "his arrest was not covered in any newspapers in the days, weeks and months after it happened", "his arrest was not covered in any newspapers"], "answer_start": [742, 742, 731, 731, 731]}, "prediction_text": "The tax collector rose to higher political office."}
{"answers": {"text": ["rose to higher political office", "higher political office", "higher political office", "rose to higher political office", "higher political office"], "answer_start": [860, 868, 868, 860, 868]}, "prediction_text": "High political office."}
{"answers": {"text": ["after the end of the Mexican War", "end of the Mexican War", "after the end of the Mexican War.", "after the end of the Mexican War", "after the end of the Mexican War"], "answer_start": [937, 947, 937, 937, 937]}, "prediction_text": "After the end of the Mexican War."}
{"answers": {"text": ["during the Roman Empire", "during the Roman Empire", "during the Roman Empire", "during the Roman Empire", "during the Roman Empire"], "answer_start": [76, 76, 76, 76, 76]}, "prediction_text": "Roman Empire."}
{"answers": {"text": ["prevent the installation of pagan images", "prevent the installation of pagan images in the Temple in Jerusalem", "prevent the installation of pagan images in the Temple in Jerusalem", "to prevent the installation of pagan images in the Temple in Jerusalem", "prevent the installation of pagan images"], "answer_start": [158, 158, 158, 155, 158]}, "prediction_text": "To prevent the installation of pagan images in the Temple in Jerusalem."}
{"answers": {"text": ["refuse to sign bail", "collectively refuse to sign bail until certain demands are met", "some activists who commit civil disobedience as a group collectively refuse to sign bail until certain demands are met", "jail solidarity"], "answer_start": [350, 337, 281, 491]}, "prediction_text": "Refused to sign bail until certain demands are met."}
{"answers": {"text": ["jail solidarity", "solitary civil disobedience", "jail solidarity", "jail solidarity", "jail solidarity"], "answer_start": [491, 560, 491, 491, 491]}, "prediction_text": "Jail solidarity."}
{"answers": {"text": ["until after the end of the Mexican War", "months", "months after it happened", "Thoreau's essay was not published until after the end of the Mexican War", "after the end of the Mexican War"], "answer_start": [931, 799, 799, 897, 937]}, "prediction_text": "Days, weeks, months."}
{"answers": {"text": ["illegal", "illegal acts", "illegal acts", "illegal acts", "illegal acts"], "answer_start": [54, 54, 54, 54, 54]}, "prediction_text": "Illegal acts"}
{"answers": {"text": ["propaganda", "propaganda", "propaganda", "propaganda", "just a harassment"], "answer_start": [696, 696, 696, 696, 338]}, "prediction_text": "Propaganda."}
{"answers": {"text": ["Voice in the Wilderness", "Voice in the Wilderness", "Voice in the Wilderness", "Voice in the Wilderness", "Voice in the Wilderness"], "answer_start": [810, 810, 810, 810, 810]}, "prediction_text": "Voice in the Wilderness"}
{"answers": {"text": ["738 days", "738 days", "738 days", "738 days", "738 days"], "answer_start": [1134, 1134, 1134, 1134, 1134]}, "prediction_text": "738 days."}
{"answers": {"text": ["successfully preventing it from being cut down", "successfully preventing it from being cut down", "successfully preventing it from being cut down", "successfully preventing it from being cut down", "preventing it from being cut down"], "answer_start": [1144, 1144, 1144, 1144, 1157]}, "prediction_text": "Julia Butterfly Hill."}
{"answers": {"text": ["illegal acts", "symbolic illegal protests", "symbolic illegal protests", "a variety of different illegal acts", "illegal"], "answer_start": [54, 631, 631, 31, 54]}, "prediction_text": "Illegal acts."}
{"answers": {"text": ["trespassing at a nuclear-missile installation", "symbolic illegal protests", "trespassing at a nuclear-missile installation", "the proprietors of illegal medical cannabis dispensaries", "trespassing at a nuclear-missile installation"], "answer_start": [269, 631, 269, 749, 269]}, "prediction_text": "Bedau's \"remoteness\" of the connection between the disobedient act and the objectionable law."}
{"answers": {"text": ["entirely symbolic", "symbolic illegal protests", "harassment", "symbolic", "inane"], "answer_start": [622, 631, 345, 631, 397]}, "prediction_text": "symbolic"}
{"answers": {"text": ["social goal", "social goal", "protests toward public policy", "propaganda", "social"], "answer_start": [940, 940, 648, 696, 940]}, "prediction_text": "Social goals."}
{"answers": {"text": ["Julia Butterfly Hill", "Julia Butterfly Hill", "Julia Butterfly Hill", "Julia Butterfly Hill", "Julia Butterfly Hill"], "answer_start": [1033, 1033, 1033, 1033, 1033]}, "prediction_text": "Julia Butterfly Hill"}
{"answers": {"text": ["sending an email to the Lebanon, New Hampshire city councilors", "sending an email", "sending an email to the Lebanon, New Hampshire city councilors", "sending an email", "sending an email to the Lebanon"], "answer_start": [513, 513, 513, 513, 513]}, "prediction_text": "Sending an email to the Lebanon, New Hampshire city councilors."}
{"answers": {"text": ["\"Wise up or die.\"", "Wise up or die", "\"Wise up or die.\"", "Wise up or die", "Wise up or die"], "answer_start": [585, 586, 585, 586, 586]}, "prediction_text": "\"Wise up or die.\""}
{"answers": {"text": ["criminalized behavior", "forbidden speech", "criminalized behavior", "forbidden", "forbidden"], "answer_start": [19, 114, 19, 114, 114]}, "prediction_text": "forbidden speech"}
{"answers": {"text": ["Supreme Court case of FCC v. Pacifica Foundation", "1978 Supreme Court case of FCC v. Pacifica Foundation", "Supreme Court case of FCC v. Pacifica Foundation", "FCC v. Pacifica Foundation", "the 1978 Supreme Court case of FCC v. Pacifica Foundation"], "answer_start": [265, 260, 265, 287, 256]}, "prediction_text": "FCC v. Pacifica Foundation."}
{"answers": {"text": ["1978", "1978", "1978", "1978", "1978"], "answer_start": [260, 260, 260, 260, 260]}, "prediction_text": "1978"}
{"answers": {"text": ["pure speech", "forbidden speech", "pure speech", "engaging in the forbidden speech"], "answer_start": [44, 114, 44, 98]}, "prediction_text": "Threatening government officials."}
{"answers": {"text": ["broadcasting", "engaging in the forbidden speech", "broadcasting", "broadcasting"], "answer_start": [159, 98, 159, 159]}, "prediction_text": "Engaging in the forbidden speech."}
{"answers": {"text": ["Threatening government officials", "forbidden speech", "engaging in the forbidden speech", "speech", "Threatening government officials"], "answer_start": [315, 114, 98, 49, 315]}, "prediction_text": "Threatening government officials."}
{"answers": {"text": ["sending an email", "email", "Supreme Court case", "broadcasting"], "answer_start": [513, 524, 265, 159]}, "prediction_text": "Threatening government officials."}
{"answers": {"text": ["system to function", "system to function", "for a system to function", "for a system to function", "for a system to function"], "answer_start": [179, 179, 173, 173, 173]}, "prediction_text": "Functioning."}
{"answers": {"text": ["by padlocking the gates", "padlocking the gates", "padlocking the gates", "padlocking the gates", "padlocking the gates"], "answer_start": [566, 569, 569, 569, 569]}, "prediction_text": "padlocking gates"}
{"answers": {"text": ["using sickles to deflate one of the large domes covering two satellite dishes", "using sickles to deflate one of the large domes covering two satellite dishes", "using sickles to deflate one of the large domes covering two satellite dishes", "using sickles to deflate one of the large domes covering two satellite dishes", "using sickles to deflate one of the large domes"], "answer_start": [594, 594, 594, 594, 594]}, "prediction_text": "GCSB Waihopai"}
{"answers": {"text": ["limited coercion", "civil disobedience", "employ limited coercion", "limited coercion", "employ limited coercion"], "answer_start": [443, 14, 436, 443, 436]}, "prediction_text": "They employ limited coercion."}
{"answers": {"text": ["coercive", "civil disobedience", "coercive", "coercive", "civil disobedience"], "answer_start": [237, 14, 237, 237, 14]}, "prediction_text": "Coercive."}
{"answers": {"text": ["refusals to pay taxes", "civil disobedience", "refusals to pay taxes", "refusals to pay taxes", "refusals to pay taxes"], "answer_start": [60, 14, 60, 60, 60]}, "prediction_text": "Illegal boycotts."}
{"answers": {"text": ["coercion", "make it more difficult for a system to function", "make it more difficult for a system to function", "make it more difficult for a system to function"], "answer_start": [328, 150, 150, 150]}, "prediction_text": "Coercion."}
{"answers": {"text": ["engage in moral dialogue", "get their issue onto the table", "get their issue onto the table", ", make it more difficult for a system to function"], "answer_start": [367, 472, 472, 148]}, "prediction_text": "To get their issue onto the table."}
{"answers": {"text": ["padlocking the gates", "illegal boycotts, refusals to pay taxes, draft dodging, distributed denial-of-service attacks, and sit-ins", "boycotts", "illegal boycotts", "sit-ins"], "answer_start": [569, 42, 50, 42, 141]}, "prediction_text": "Padlock gates and use sickles."}
{"answers": {"text": ["criminal investigations", "grant a consent search", "grant a consent search", "criminal investigations", "Many of the same decisions and principles that apply in other criminal investigations", "criminal"], "answer_start": [62, 200, 200, 62, 0, 62]}, "prediction_text": "Criminal investigations."}
{"answers": {"text": ["not to grant a consent search", "not to talk to police officers", "not to talk to police officers", "not to talk", "resist responding to investigators' questions", "grant a consent search"], "answer_start": [193, 255, 255, 255, 558, 200]}, "prediction_text": "Talk to them."}
{"answers": {"text": ["suspect's talking to criminal investigators", "responding to investigators' questions", "responding to investigators' questions", "grant a consent search of his property,", "talking to criminal investigators", "responding to investigators' questions"], "answer_start": [398, 565, 565, 200, 408, 565]}, "prediction_text": "talking to authorities."}
{"answers": {"text": ["lack of understanding of the legal ramifications,", "a suspect's talking to criminal investigators", "lack of understanding", "lack of understanding of the legal ramifications, or due to a fear of seeming rude."], "answer_start": [624, 396, 624, 624]}, "prediction_text": "Lack of understanding, fear of seeming rude, and lack of understanding of the legal ramifications."}
{"answers": {"text": ["use the arrest as an opportunity", "civil disobedience", "civil disobedience", "civil disobedience", "civil disobedience", "to make an impression"], "answer_start": [746, 112, 112, 112, 112, 779]}, "prediction_text": "To make an impression."}
{"answers": {"text": ["accept punishment", "accept punishment", "punishment", "punishment", "punishment"], "answer_start": [58, 58, 65, 65, 65]}, "prediction_text": "The need to acknowledge the legitimacy of the social contract."}
{"answers": {"text": ["validity of the social contract", "their belief in the validity of the social contract", "because of their belief in the validity of the social contract", "belief in the validity of the social contract", "their belief in the validity of the social contract"], "answer_start": [107, 87, 76, 93, 87]}, "prediction_text": "To bind all to obey the laws that a government meeting certain standards of legitimacy has established."}
{"answers": {"text": ["legitimacy of a particular law", "law", "laws", "the legitimacy of any government"], "answer_start": [464, 491, 178, 582]}, "prediction_text": "The legitimacy of the social contract."}
{"answers": {"text": ["anarchists", "anarchists", "anarchists", "anarchists", "anarchists"], "answer_start": [553, 553, 553, 553, 553]}, "prediction_text": "Anarchists"}
{"answers": {"text": ["does not infringe the rights of others", "don't believe in the legitimacy of any government", "a violation of criminal law that does not infringe the rights of others", "see no need to accept punishment for a violation of criminal law that does not infringe the rights of others", "a violation of criminal law that does not infringe the rights of others."], "answer_start": [700, 565, 667, 630, 667]}, "prediction_text": "They don't believe in the legitimacy of any government."}
{"answers": {"text": ["whether or not to plead guilty", "whether or not to plead guilty", "whether or not to plead guilty", "whether or not to plead guilty", "whether or not to plead guilty.", "whether or not to plead guilty."], "answer_start": [48, 48, 48, 48, 48, 48]}, "prediction_text": "Whether or not to plead guilty."}
{"answers": {"text": ["submit to the punishment prescribed by law", "to submit to the punishment prescribed by law", "submit to the punishment", "to submit to the punishment prescribed by law", "to submit to the punishment prescribed by law", "to submit to the punishment prescribed by law"], "answer_start": [173, 170, 173, 170, 170, 170]}, "prediction_text": "To plead guilty."}
{"answers": {"text": ["I feel I did the right thing by violating this particular law", "I feel I did the right thing by violating this particular law", "proud of it", "proud of it", "have violated some specific laws, but I am guilty of doing no w", "have violated some specific laws, but I am guilty of doing no w"], "answer_start": [638, 638, 625, 625, 838, 838]}, "prediction_text": "To avoid the punishment prescribed by law."}
{"answers": {"text": ["Guilt implies wrong-doing", "Guilt implies wrong-doing", "guilty of doing no wrong", "Guilt implies wrong-doing", "I may have violated some specific laws, but I am guilty of doing no wrong", "I may have violated some specific laws, but I am guilty of doing no wrong"], "answer_start": [776, 776, 881, 776, 832, 832]}, "prediction_text": "Guilt."}
{"answers": {"text": ["creative plea", "no contest", "creative plea", "creative plea", "creative plea", "creative plea"], "answer_start": [1177, 948, 1177, 1177, 1177, 1177]}, "prediction_text": "Creative plea."}
{"answers": {"text": ["Camp Mercury nuclear test site", "Camp Mercury nuclear test site", "Camp Mercury nuclear test site", "Camp Mercury nuclear test site", "Camp Mercury nuclear test site"], "answer_start": [85, 85, 85, 85, 85]}, "prediction_text": "Camp Mercury."}
{"answers": {"text": ["tempted to enter the test site", "protesters attempted to enter the test site", "attempted to enter the test site", "13 of the protesters attempted to enter the test site", "protest"], "answer_start": [163, 150, 161, 140, 54]}, "prediction_text": "Attempted entry."}
{"answers": {"text": ["arrested", "arrest", "were immediately arrested", "one at a time they stepped across the \"line\" and were immediately arrested", "put on a bus and taken to the Nye County seat of Tonopah, Nevada, and arraigned for trial before the local Justice of the Peace"], "answer_start": [326, 218, 309, 260, 346]}, "prediction_text": "Arrested."}
{"answers": {"text": ["nolo contendere", "nolo contendere", "nolo contendere", "nolo contendere", "nolo contendere"], "answer_start": [616, 616, 616, 616, 616]}, "prediction_text": "Nolo contendere."}
{"answers": {"text": ["suspended sentences", "suspended sentences", "suspended", "suspended", "suspended"], "answer_start": [759, 759, 759, 759, 759]}, "prediction_text": "Suspended sentences."}
{"answers": {"text": ["a way of continuing their protest", "continuing their protest", "a way of continuing their protest", "a way of continuing their protest", "a way of continuing their protest"], "answer_start": [86, 95, 86, 86, 86]}, "prediction_text": "To continue their protest."}
{"answers": {"text": ["reminding their countrymen of injustice", "reminding their countrymen of injustice", "reminding their countrymen of injustice", "reminding their countrymen of injustice", "reminding their countrymen of injustice"], "answer_start": [133, 133, 133, 133, 133]}, "prediction_text": "Reminding people of injustice."}
{"answers": {"text": ["protest should be maintained all the way", "spirit of protest", "the spirit of protest should be maintained all the way", "the spirit of protest should be maintained all the way", "spirit of protest"], "answer_start": [327, 317, 313, 313, 317]}, "prediction_text": "The spirit of protest."}
{"answers": {"text": ["accept jail penitently", "is to switch suddenly to a spirit of subservience", "To accept jail penitently as an accession to 'the rules' is to switch suddenly to a spirit of subservience", "To accept jail penitently as an accession to 'the rules' is to switch suddenly to a spirit of subservience"], "answer_start": [431, 485, 428, 428]}, "prediction_text": "To continue their protest."}
{"answers": {"text": ["plea bargain", "plead guilty to one misdemeanor count and receive no jail time", "plea bargain", "plea bargain", "plea bargain"], "answer_start": [37, 161, 37, 37, 37]}, "prediction_text": "Plea bargain."}
{"answers": {"text": ["no jail time", "plead guilty to one misdemeanor count and receive no jail time", "receive no jail time", "no jail time", "receive no jail time"], "answer_start": [211, 161, 203, 211, 203]}, "prediction_text": "To secure a plea bargain for everyone."}
{"answers": {"text": ["solidarity tactics", "solidarity tactics", "solidarity", "solidarity", "solidarity"], "answer_start": [285, 285, 285, 285, 285]}, "prediction_text": "solidarity tactics"}
{"answers": {"text": ["blind plea", "blind plea", "blind plea", "blind plea", "blind plea"], "answer_start": [391, 391, 391, 391, 391]}, "prediction_text": "Blind plea."}
{"answers": {"text": ["Mohandas Gandhi", "Mohandas Gandhi", "Mohandas Gandhi", "Mohandas Gandhi", "Gan"], "answer_start": [456, 456, 456, 456, 465]}, "prediction_text": "Mohandas Gandhi"}
{"answers": {"text": ["defiant speech", "defiant speech", "allocution", "defiant speech", "defiant speech", "defiant speech"], "answer_start": [52, 52, 109, 52, 52, 52]}, "prediction_text": "They make a defiant speech."}
{"answers": {"text": ["explaining their actions", "explaining their actions", "make a defiant speech, or a speech explaining their actions,", "explaining their actions", "explaining their actions", "explaining their actions"], "answer_start": [80, 80, 45, 80, 80, 80]}, "prediction_text": "To make a defiant speech."}
{"answers": {"text": ["lack of remorse", "lack of remorse", "the judge increased her sentence", "statement suggested a lack of remorse"], "answer_start": [554, 554, 381, 532]}, "prediction_text": "Moves the individual from responsibility for their actions."}
{"answers": {"text": ["likelihood of repeating", "likelihood of repeating her illegal actions", "a lack of remorse", "lack of remorse", "lack of remorse"], "answer_start": [634, 634, 552, 554, 554]}, "prediction_text": "To avoid responsibility for their actions."}
{"answers": {"text": ["mistreatment from government officials", "mistreatment", "mistreatment from government officials", "sentence", "mistreatment", "mistreatment"], "answer_start": [758, 758, 758, 405, 758, 758]}, "prediction_text": "Mistreatment from government officials."}
{"answers": {"text": ["acquittal and avoid imprisonment", "win an acquittal and avoid imprisonment or a fine", "to use the proceedings as a forum", "win an acquittal", "to win an acquittal and avoid imprisonment or a fine"], "answer_start": [121, 114, 168, 114, 111]}, "prediction_text": "Win acquittal."}
{"answers": {"text": ["use the proceedings as a forum", "use the proceedings as a forum to inform the jury and the public of the political circumstances", "win an acquittal and avoid imprisonment", "use the proceedings as a forum to inform the jury and the public of the political circumstances", "use the proceedings as a forum to inform the jury and the public of the political circumstances surrounding the case"], "answer_start": [171, 171, 114, 171, 171]}, "prediction_text": "Avoid imprisonment."}
{"answers": {"text": ["inform the jury and the public of the political circumstances", "plead not guilty", "plead not guilty", "inform the jury and the public", "plead not guilty"], "answer_start": [205, 40, 40, 205, 40]}, "prediction_text": "Seek jury nullification."}
{"answers": {"text": ["Vietnam War", "Vietnam War", "Vietnam War", "Vietnam War", "Vietnam War"], "answer_start": [486, 486, 486, 486, 486]}, "prediction_text": "Vietnam War"}
{"answers": {"text": ["jury nullification", "jury nullification", "jury nullification", "nullification", "nullification"], "answer_start": [1068, 763, 1068, 768, 768]}, "prediction_text": "Jury nullification."}
{"answers": {"text": ["general disobedience", "undermine the law by encouraging general disobedience which is neither conscientious nor of social benefit", "great amount of it", "general disobedience", "conscientious lawbreakers"], "answer_start": [117, 84, 59, 117, 203]}, "prediction_text": "General disobedience."}
{"answers": {"text": ["neither conscientious nor of social benefit", "neither conscientious nor of social benefit", "any great amount of it would undermine the law", "conscientious lawbreakers"], "answer_start": [147, 147, 55, 203]}, "prediction_text": "To avoid general disobedience."}
{"answers": {"text": ["breaking the law for self-gratification", "breaking the law for self-gratification", "it would undermine the law by encouraging general disobedience", "breaking the law for self-gratification"], "answer_start": [476, 476, 75, 476]}, "prediction_text": "General disobedience."}
{"answers": {"text": ["not being a civil disobedient", "viewed as not being a civil disobedient", "conscientious lawbreakers must be punished", "it would undermine the law by encouraging general disobedience", "constitute civil disobedience"], "answer_start": [880, 870, 203, 75, 416]}, "prediction_text": "To avoid attribution."}
{"answers": {"text": ["avoiding attribution", "viewed as not being a civil disobedient", "it would undermine the law by encouraging general disobedience", "constitute civil disobedience"], "answer_start": [761, 870, 75, 416]}, "prediction_text": "For self-gratification."}
{"answers": {"text": ["Indirect civil disobedience", "Indirect civil disobedience", "Indirect civil disobedience", "Indirect civil disobedience", "Indirect civil disobedience"], "answer_start": [68, 68, 68, 68, 68]}, "prediction_text": "Indirect civil disobedience."}
{"answers": {"text": ["direct civil disobedience", "direct civil disobedience", "direct civil disobedience", "direct civil disobedience", "direct civil disobedience"], "answer_start": [174, 174, 174, 174, 174]}, "prediction_text": "Indirect civil disobedience."}
{"answers": {"text": ["Vietnam War", "Vietnam War", "Vietnam War", "Vietnam War", "Vietnam"], "answer_start": [443, 288, 443, 443, 288]}, "prediction_text": "Vietnam War"}
{"answers": {"text": ["competing harms defense", "necessity defense", "necessity defense", "necessity"], "answer_start": [1035, 506, 506, 506]}, "prediction_text": "Shadow defense."}
{"answers": {"text": ["the leaflets will have to be given to the leafleter's own jury as evidence", "if they arrest fully informed jury leafleters, the leaflets will have to be given to the leafleter's own jury as evidence", "the leaflets will have to be given to the leafleter's own jury as evidence", "the leaflets will have to be given to the leafleter's own jury as evidence", "the leaflets will have to be given to the leafleter's own jury as evidence"], "answer_start": [1377, 1330, 1377, 1377, 1377]}, "prediction_text": "Fully Informed Jury Association activists."}
{"answers": {"text": ["incapacitation", "achieving crime control via incapacitation and deterrence", "achieving crime control via incapacitation and deterrence", "achieving crime control", "achieving crime control"], "answer_start": [79, 51, 51, 51, 51]}, "prediction_text": "Crime control."}
{"answers": {"text": ["would do more harm than good", "whether it would do more harm than good", "it focuses attention on the threat of punishment and not the moral reasons to follow this law", "whether it would do more harm than good", "whether it would do more harm than good."], "answer_start": [553, 542, 322, 542, 542]}, "prediction_text": "Crime control."}
{"answers": {"text": ["the state", "judges", "the state (including the judges)", "the judges", "judges"], "answer_start": [664, 689, 664, 685, 689]}, "prediction_text": "The state."}
{"answers": {"text": ["moral reasons to follow this law", "whether it would do more harm than good", "whether it would do more harm than good", "whether it would do more harm than good", "whether it would do more harm than good"], "answer_start": [383, 542, 542, 542, 542]}, "prediction_text": "deterrence"}
{"answers": {"text": ["Construction", "Construction", "Construction"], "answer_start": [0, 0, 0]}, "prediction_text": "Construction"}
{"answers": {"text": ["manufacturing", "manufacturing", "manufacturing"], "answer_start": [122, 100, 122]}, "prediction_text": "Construction"}
{"answers": {"text": ["six to nine percent", "six to nine percent", "six to nine percent"], "answer_start": [331, 331, 331]}, "prediction_text": "Six to nine percent."}
{"answers": {"text": ["planning,[citation needed] design, and financing", "planning,[citation needed] design, and financing", "planning,[citation needed] design, and financing and continues until the project is built"], "answer_start": [430, 430, 430]}, "prediction_text": "Planning, design, and financing."}
{"answers": {"text": ["a known client", "client", "known client"], "answer_start": [277, 285, 279]}, "prediction_text": "Known client."}
{"answers": {"text": ["An architect", "architect", "architect"], "answer_start": [77, 80, 80]}, "prediction_text": "Architect"}
{"answers": {"text": ["a construction manager, design engineer, construction engineer or project manager", "construction engineer or project manager", "construction manager, design engineer, construction engineer or project manager"], "answer_start": [120, 161, 122]}, "prediction_text": "Construction manager"}
{"answers": {"text": ["effective planning", "effective planning", "effective planning"], "answer_start": [260, 260, 260]}, "prediction_text": "Effective planning."}
{"answers": {"text": ["megaprojects", "megaprojects", "megaprojects"], "answer_start": [703, 703, 703]}, "prediction_text": "megaprojects"}
{"answers": {"text": ["Those involved with the design and execution of the infrastructure", "Those involved with the design and execution of the infrastructure", "Those involved with the design and execution of the infrastructure in question"], "answer_start": [293, 293, 293]}, "prediction_text": "Design and execution of the infrastructure in question."}
{"answers": {"text": ["buildings, infrastructure and industrial", "buildings, infrastructure and industrial", "buildings, infrastructure and industrial"], "answer_start": [53, 53, 53]}, "prediction_text": "Buildings, infrastructure, industrial."}
{"answers": {"text": ["residential and non-residential", "residential and non-residential (commercial/institutional)", "residential and non-residential (commercial/institutional)"], "answer_start": [149, 149, 149]}, "prediction_text": "residential, commercial/institutional, infrastructure, heavy/highway, heavy civil, heavy engineering."}
{"answers": {"text": ["heavy/highway, heavy civil or heavy engineering", "heavy/highway, heavy civil or heavy engineering", "heavy/highway, heavy civil or heavy engineering"], "answer_start": [240, 240, 240]}, "prediction_text": "Heavy/highway"}
{"answers": {"text": ["Infrastructure", "Infrastructure", "Infrastructure"], "answer_start": [209, 209, 209]}, "prediction_text": "Infrastructure"}
{"answers": {"text": ["Industrial", "heavy civil or heavy engineering", "Industrial"], "answer_start": [389, 255, 389]}, "prediction_text": "Industrial"}
{"answers": {"text": ["a trade magazine for the construction industry", "a trade magazine for the construction industry", "trade magazine for the construction industry"], "answer_start": [33, 33, 35]}, "prediction_text": "Engineering News-Record (ENR)"}
{"answers": {"text": ["ENR", "Engineering News-Record (ENR)", "Engineering News-Record (ENR)"], "answer_start": [92, 0, 0]}, "prediction_text": "ENR"}
{"answers": {"text": ["2014", "2014", "2014"], "answer_start": [369, 369, 369]}, "prediction_text": "2014"}
{"answers": {"text": ["transportation, sewer, hazardous waste and water", "data on transportation, sewer, hazardous waste and water", "transportation, sewer, hazardous waste and water"], "answer_start": [652, 644, 652]}, "prediction_text": "Transportation, sewer, hazardous waste, and water."}
{"answers": {"text": ["building construction, heavy and civil engineering construction, and specialty trade contractors", "building construction, heavy and civil engineering construction, and specialty trade contractors", "building construction, heavy and civil engineering construction, and specialty trade contractors"], "answer_start": [287, 287, 287]}, "prediction_text": "Building construction, heavy and civil engineering construction, and specialty trade contractors."}
{"answers": {"text": ["construction service firms (e.g., engineering, architecture) and construction managers", "service firms", "construction service firms (e.g., engineering, architecture) and construction managers"], "answer_start": [415, 428, 415]}, "prediction_text": "Construction service firms (e.g., engineering, architecture) and construction managers (firms engaged in managing construction projects without assuming direct financial responsibility for completion of the construction project)."}
{"answers": {"text": ["The Standard Industrial Classification and the newer North American Industry Classification System", "Standard Industrial Classification", "The Standard Industrial Classification and the newer North American Industry Classification System"], "answer_start": [0, 4, 0]}, "prediction_text": "North American Industry Classification System"}
{"answers": {"text": ["firms engaged in managing construction projects without assuming direct financial responsibility for completion of the construction project", "firms engaged in managing construction projects", "(firms engaged in managing construction projects without assuming direct financial responsibility for completion of the construction project)"], "answer_start": [503, 503, 502]}, "prediction_text": "Firms engaged in managing construction projects without assuming direct financial responsibility for completion of the construction project."}
{"answers": {"text": ["Building construction", "Building construction", "Building construction"], "answer_start": [0, 0, 0]}, "prediction_text": "Building construction."}
{"answers": {"text": ["small renovations", "small renovations", "small renovations"], "answer_start": [151, 151, 151]}, "prediction_text": "small renovations"}
{"answers": {"text": ["the owner of the property", "the owner", "owner of the property"], "answer_start": [234, 234, 238]}, "prediction_text": "Owner"}
{"answers": {"text": ["structural collapse, cost overruns, and/or litigation", "structural collapse, cost overruns, and/or litigation", "structural collapse, cost overruns, and/or litigation"], "answer_start": [545, 545, 545]}, "prediction_text": "Structural collapse, cost overruns, and/or litigation."}
{"answers": {"text": ["make detailed plans and maintain careful oversight", "make detailed plans and maintain careful oversight", "make detailed plans and maintain careful oversight during the project"], "answer_start": [652, 652, 652]}, "prediction_text": "Make detailed plans and maintain careful oversight."}
{"answers": {"text": ["local building authority regulations and codes of practice", "local building authority regulations and codes of practice", "local building authority regulations and codes of practice"], "answer_start": [80, 80, 80]}, "prediction_text": "Local building authority regulations and codes of practice."}
{"answers": {"text": ["Materials readily available in the area", "Materials readily available in the area", "Materials readily available in the area"], "answer_start": [140, 140, 140]}, "prediction_text": "Materials readily available in the area."}
{"answers": {"text": ["a lot of waste", "waste", "waste"], "answer_start": [629, 638, 638]}, "prediction_text": "waste"}
{"answers": {"text": ["Cost of construction", "Cost of construction", "Cost of construction"], "answer_start": [272, 272, 272]}, "prediction_text": "Cost of construction."}
{"answers": {"text": ["3D printing technology", "3D printing technology", "advances in 3D printing technology"], "answer_start": [91, 91, 79]}, "prediction_text": "3D printing technology."}
{"answers": {"text": ["around 20 hours", "around 20 hours", "20 hours"], "answer_start": [349, 349, 356]}, "prediction_text": "20 hours."}
{"answers": {"text": ["Working versions of 3D-printing building technology are already printing", "2014", "2014"], "answer_start": [466, 850, 850]}, "prediction_text": "2014."}
{"answers": {"text": ["2 metres (6 ft 7 in)", "2 metres (6 ft 7 in)", "2 metres (6 ft 7 in)"], "answer_start": [539, 539, 539]}, "prediction_text": "2 meters (6 ft 7 in) per hour."}
{"answers": {"text": ["plan the physical proceedings, and to integrate those proceedings with the other parts", "plan the physical proceedings, and to integrate those proceedings with the other parts", "plan the physical proceedings, and to integrate those proceedings with the other parts"], "answer_start": [148, 148, 148]}, "prediction_text": "Plan the physical proceedings."}
{"answers": {"text": ["designs into reality", "designs into reality", "designs into reality"], "answer_start": [85, 85, 85]}, "prediction_text": "designs into reality"}
{"answers": {"text": ["the property owner", "the property owner", "property owner"], "answer_start": [611, 611, 615]}, "prediction_text": "The property owner."}
{"answers": {"text": ["a quantity surveyor", "quantity surveyor", "quantity surveyor"], "answer_start": [919, 921, 921]}, "prediction_text": "Quantity surveyor."}
{"answers": {"text": ["the most cost efficient bidder", "the most cost efficient bidder", "most cost efficient bidder"], "answer_start": [1011, 1011, 1015]}, "prediction_text": "The most cost efficient bidder."}
{"answers": {"text": ["previously separated specialties", "previously separated specialties", "previously separated specialties, especially among large firms"], "answer_start": [52, 52, 52]}, "prediction_text": "previously separated specialties"}
{"answers": {"text": ["entirely separate companies", "entirely separate companies", "entirely separate companies"], "answer_start": [254, 254, 254]}, "prediction_text": "Separate companies."}
{"answers": {"text": ["\"one-stop shopping\"", "one-stop shopping", "\"one-stop shopping\""], "answer_start": [562, 563, 562]}, "prediction_text": "One-stop shopping."}
{"answers": {"text": ["\"design build\" contract", "design build", "\"design build\" contract"], "answer_start": [657, 658, 657]}, "prediction_text": "Design build contract."}
{"answers": {"text": ["design-build, partnering and construction management", "design-build, partnering and construction management", "design-build, partnering and construction management"], "answer_start": [79, 79, 79]}, "prediction_text": "Design-build, partnering, construction management."}
{"answers": {"text": ["architects, interior designers, engineers and constructors", "architects, interior designers, engineers and constructors", "architects, interior designers, engineers and constructors"], "answer_start": [224, 224, 224]}, "prediction_text": "Architects, interior designers, engineers, and constructors."}
{"answers": {"text": ["establishing relationships with other necessary participants through the design-build process", "establishing relationships with other necessary participants", "establishing relationships with other necessary participants"], "answer_start": [462, 462, 462]}, "prediction_text": "Establishing relationships with other necessary participants."}
{"answers": {"text": ["preventable financial problems", "preventable financial problems", "preventable financial problems"], "answer_start": [38, 38, 38]}, "prediction_text": "preventable financial problems"}
{"answers": {"text": ["when builders ask for too little money to complete the project", "when builders ask for too little money to complete the project", "builders ask for too little money to complete the project"], "answer_start": [87, 87, 92]}, "prediction_text": "Underbid."}
{"answers": {"text": ["when the present amount of funding cannot cover the current costs for labour and materials", "when the present amount of funding cannot cover the current costs for labour and materials", "present amount of funding cannot cover the current costs for labour and materials"], "answer_start": [176, 176, 185]}, "prediction_text": "When the present amount of funding cannot cover the current costs for labour and materials."}
{"answers": {"text": ["Fraud", "Fraud", "Fraud"], "answer_start": [394, 394, 394]}, "prediction_text": "Fraud"}
{"answers": {"text": ["Mortgage bankers, accountants, and cost engineers", "Mortgage bankers, accountants, and cost engineers", "Mortgage bankers, accountants, and cost engineers"], "answer_start": [0, 0, 0]}, "prediction_text": "Mortgage bankers, accountants, and cost engineers."}
{"answers": {"text": ["the mortgage banker", "mortgage banker", "mortgage banker"], "answer_start": [185, 189, 189]}, "prediction_text": "Mortgage banker"}
{"answers": {"text": ["Accountants", "Accountants", "Accountants"], "answer_start": [360, 360, 360]}, "prediction_text": "Accountants."}
{"answers": {"text": ["identified change orders or project changes that increased costs", "identified change orders or project changes that increased costs", "contractor identified change orders or project changes that increased costs"], "answer_start": [676, 676, 665]}, "prediction_text": "Identified change orders or project changes."}
{"answers": {"text": ["Cost engineers and estimators", "Cost engineers and estimators", "Cost engineers and estimators"], "answer_start": [492, 492, 492]}, "prediction_text": "Cost engineers and estimators."}
{"answers": {"text": ["zoning and building code requirements", "zoning and building code requirements", "zoning and building code requirements"], "answer_start": [27, 27, 27]}, "prediction_text": "zoning and building code requirements."}
{"answers": {"text": ["the owner", "owner", "the owner"], "answer_start": [136, 140, 136]}, "prediction_text": "The owner."}
{"answers": {"text": ["the desire to prevent things that are indisputably bad", "desire to prevent things that are indisputably bad", "the desire to prevent things that are indisputably bad"], "answer_start": [212, 216, 212]}, "prediction_text": "Bridge collapses."}
{"answers": {"text": ["things that are a matter of custom or expectation", "things that are a matter of custom or expectation", "things that are a matter of custom or expectation"], "answer_start": [372, 372, 372]}, "prediction_text": "Things that are a matter of custom or expectation."}
{"answers": {"text": ["An attorney", "attorney", "An attorney"], "answer_start": [517, 520, 517]}, "prediction_text": "An attorney."}
{"answers": {"text": ["A construction project", "A construction project", "A construction project"], "answer_start": [0, 0, 0]}, "prediction_text": "Construction project"}
{"answers": {"text": ["A contract", "A contract", "A contract"], "answer_start": [133, 133, 133]}, "prediction_text": "Contracts."}
{"answers": {"text": ["that a delay costs money, and in cases of bottlenecks, the delay can be extremely expensive", "a delay costs money", "that a delay costs money, and in cases of bottlenecks, the delay can be extremely expensive"], "answer_start": [388, 393, 388]}, "prediction_text": "Delays cost money."}
{"answers": {"text": ["that each side is capable of performing the obligations set out", "that each side is capable of performing the obligations set out", "each side is capable of performing the obligations set out"], "answer_start": [528, 528, 533]}, "prediction_text": "clear expectations and clear paths to accomplishing those expectations."}
{"answers": {"text": ["poorly drafted contracts", "poorly drafted contracts", "poorly drafted contracts"], "answer_start": [758, 758, 758]}, "prediction_text": "Poorly drafted contracts."}
{"answers": {"text": ["relationship contracting where the emphasis is on a co-operative relationship", "relationship contracting", "relationship contracting"], "answer_start": [72, 72, 72]}, "prediction_text": "relationship contracting"}
{"answers": {"text": ["Public-Private Partnering", "Public-Private Partnering", "Public-Private Partnering"], "answer_start": [278, 278, 278]}, "prediction_text": "Public-Private Partnership (PPP)"}
{"answers": {"text": ["private finance initiatives (PFIs)", "private finance initiatives (PFIs)", "private finance initiatives (PFIs)"], "answer_start": [315, 315, 315]}, "prediction_text": "Private Finance Initiative (PFIs)"}
{"answers": {"text": ["co-operation", "co-operation", "co-operation"], "answer_start": [454, 454, 454]}, "prediction_text": "co-operation"}
{"answers": {"text": ["the architect or engineer", "the architect or engineer", "the architect or engineer"], "answer_start": [120, 120, 120]}, "prediction_text": "Architect or engineer."}
{"answers": {"text": ["the project coordinator", "project coordinator", "the architect or engineer"], "answer_start": [154, 158, 120]}, "prediction_text": "Architect or engineer."}
{"answers": {"text": ["the architect's client and the main contractor", "the architect's client and the main contractor", "architect's client and the main contractor"], "answer_start": [418, 418, 422]}, "prediction_text": "Architect and main contractor."}
{"answers": {"text": ["the main contractor", "the main contractor", "main contractor"], "answer_start": [445, 527, 531]}, "prediction_text": "The main contractor."}
{"answers": {"text": ["the building is ready to occupy.", "the building is ready to occupy", "building is ready to occupy"], "answer_start": [578, 578, 582]}, "prediction_text": "The building is ready to occupy."}
{"answers": {"text": ["The owner", "The owner", "The owner"], "answer_start": [0, 0, 0]}, "prediction_text": "The owner."}
{"answers": {"text": ["D&B contractors", "D&B contractors", "Several D&B contractors"], "answer_start": [112, 112, 104]}, "prediction_text": "D&B contractors"}
{"answers": {"text": ["The owner", "The owner", "The owner"], "answer_start": [189, 189, 189]}, "prediction_text": "The owner."}
{"answers": {"text": ["a consortium of several contractors", "a consortium of several contractors", "a consortium of several contractors"], "answer_start": [318, 318, 318]}, "prediction_text": "D&B contractors"}
{"answers": {"text": ["they design phase 2", "they design phase 2", "they design phase 2"], "answer_start": [475, 475, 475]}, "prediction_text": "They design phase 2."}
{"answers": {"text": ["contractors", "contractors", "contractors"], "answer_start": [34, 34, 34]}, "prediction_text": "Contractors"}
{"answers": {"text": ["damage", "the likelihood of damage", "the likelihood of damage to the existing electrical, water, sewage, phone, and cable facilities"], "answer_start": [237, 219, 219]}, "prediction_text": "damage to the existing electrical, water, sewage, phone, and cable facilities."}
{"answers": {"text": ["electrical, water, sewage, phone, and cable facilities", "electrical, water, sewage, phone, and cable", "electrical, water, sewage, phone, and cable facilities"], "answer_start": [260, 260, 260]}, "prediction_text": "Electrical, water, sewage, phone, and cable."}
{"answers": {"text": ["the municipal building inspector", "municipal building inspector", "the municipal building inspector"], "answer_start": [419, 423, 419]}, "prediction_text": "Municipal building inspector"}
{"answers": {"text": ["an occupancy permit", "occupancy permit", "an occupancy permit"], "answer_start": [648, 651, 648]}, "prediction_text": "Occupancy permit."}
{"answers": {"text": ["$960 billion", "$960 billion", "$960 billion"], "answer_start": [54, 54, 54]}, "prediction_text": "$960 billion"}
{"answers": {"text": ["$680 billion", "$680 billion", "$680 billion"], "answer_start": [148, 148, 148]}, "prediction_text": "$680 billion"}
{"answers": {"text": ["667,000 firms", "667,000 firms", "667,000 firms"], "answer_start": [288, 288, 288]}, "prediction_text": "667,000"}
{"answers": {"text": ["fewer than 10 employees", "fewer than 10", "fewer than 10"], "answer_start": [434, 434, 434]}, "prediction_text": "Fewer than 10 employees."}
{"answers": {"text": ["828,000", "828,000 women", "828,000"], "answer_start": [604, 604, 604]}, "prediction_text": "828,000"}
{"answers": {"text": ["\u00a342,090", "\u00a342,090", "\u00a342,090"], "answer_start": [484, 484, 484]}, "prediction_text": "\u00a342,090"}
{"answers": {"text": ["\u00a326,719", "\u00a326,719", "\u00a326,719"], "answer_start": [505, 505, 505]}, "prediction_text": "\u00a326,719"}
{"answers": {"text": ["US/Canada", "US/Canada", "US/Canada"], "answer_start": [792, 792, 792]}, "prediction_text": "US/Canada"}
{"answers": {"text": ["Construction", "Construction", "Construction"], "answer_start": [0, 0, 0]}, "prediction_text": "Construction"}
{"answers": {"text": ["Falls", "Falls", "Falls"], "answer_start": [319, 319, 319]}, "prediction_text": "Falls"}
{"answers": {"text": ["electrocution, transportation accidents, and trench cave-ins", "electrocution, transportation accidents, and trench cave-ins", "electrocution, transportation accidents, and trench cave-ins"], "answer_start": [689, 689, 689]}, "prediction_text": "Electrocution, transportation accidents, trench cave-ins."}
{"answers": {"text": ["Proper safety equipment such as harnesses and guardrails and procedures such as securing ladders and inspecting scaffolding", "Proper safety equipment", "Proper safety equipment"], "answer_start": [419, 419, 419]}, "prediction_text": "Proper safety equipment such as harnesses and guardrails."}
{"answers": {"text": ["independent", "independent schools", "independent schools"], "answer_start": [31, 31, 31]}, "prediction_text": "Private schools."}
{"answers": {"text": ["academic", "academic", "academic"], "answer_start": [538, 538, 538]}, "prediction_text": "Academic scholarship."}
{"answers": {"text": ["tuition", "charging their students tuition", "tuition"], "answer_start": [268, 244, 268]}, "prediction_text": "Charging students tuition."}
{"answers": {"text": ["to select their students", "select their students", "select their students"], "answer_start": [181, 184, 184]}, "prediction_text": "The right to select students."}
{"answers": {"text": ["$45,000", "more than $45,000", "$45,000"], "answer_start": [500, 490, 500]}, "prediction_text": "$45,000"}
{"answers": {"text": ["'tuition-free", "tuition-free", "tuition-free"], "answer_start": [464, 465, 465]}, "prediction_text": "Tuition-free schools."}
{"answers": {"text": ["Australia", "Australia", "Australia"], "answer_start": [73, 73, 73]}, "prediction_text": "Australia"}
{"answers": {"text": ["North America", "North America", "North America"], "answer_start": [278, 278, 278]}, "prediction_text": "North America"}
{"answers": {"text": ["lower sixth", "lower sixth", "lower sixth"], "answer_start": [90, 90, 90]}, "prediction_text": "Lower sixth"}
{"answers": {"text": ["upper sixth", "upper sixth", "upper sixth"], "answer_start": [116, 116, 116]}, "prediction_text": "Upper sixth."}
{"answers": {"text": ["prep schools", "prep schools", "prep schools"], "answer_start": [188, 188, 188]}, "prediction_text": "Prep schools"}
{"answers": {"text": ["peer tuitions", "peer tuitions", "peer tuitions and the school's financial endowment"], "answer_start": [405, 405, 405]}, "prediction_text": "Financial endowment."}
{"answers": {"text": ["teachers", "teachers", "best teachers"], "answer_start": [530, 530, 525]}, "prediction_text": "Best teachers."}
{"answers": {"text": ["Roman Catholic", "Roman Catholic"], "answer_start": [515, 515]}, "prediction_text": "Roman Catholic schools."}
{"answers": {"text": ["Orthodox Christians", "Roman Catholic", "Orthodox Christians"], "answer_start": [654, 515, 654]}, "prediction_text": "Orthodox Christians."}
{"answers": {"text": ["religious", "religious", "the usual academic subjects"], "answer_start": [113, 113, 148]}, "prediction_text": "Religious education."}
{"answers": {"text": ["expulsion", "expulsion", "offer a higher quality of education"], "answer_start": [483, 483, 284]}, "prediction_text": "Expulsion."}
{"answers": {"text": ["blazer", "blazer", "blazer"], "answer_start": [688, 688, 688]}, "prediction_text": "Blazer"}
{"answers": {"text": ["more expensive", "more expensive", "more expensive"], "answer_start": [736, 736, 736]}, "prediction_text": "Private schools in Australia are always more expensive."}
{"answers": {"text": ["Presbyterian", "Presbyterian Church", "Presbyterian Church"], "answer_start": [188, 188, 188]}, "prediction_text": "Presbyterian Church"}
{"answers": {"text": ["Catholic", "Catholic", "Catholic"], "answer_start": [493, 493, 493]}, "prediction_text": "Anglican Church"}
{"answers": {"text": ["Sydney", "Sydney", "Sydney"], "answer_start": [680, 680, 680]}, "prediction_text": "Sydney"}
{"answers": {"text": ["girls", "girls", "girls"], "answer_start": [837, 837, 837]}, "prediction_text": "Girls"}
{"answers": {"text": ["7", "7", "7"], "answer_start": [61, 61, 61]}, "prediction_text": "Article 7, Paragraph 4."}
{"answers": {"text": ["second Gleichschaltung", "a second Gleichschaltung", "a second Gleichschaltung"], "answer_start": [289, 287, 287]}, "prediction_text": "Gleichschaltung or similar event."}
{"answers": {"text": ["7.8", "7.8%", "7.8"], "answer_start": [505, 505, 505]}, "prediction_text": "11.1%"}
{"answers": {"text": ["11.1", "11.1%", "11.1"], "answer_start": [616, 616, 616]}, "prediction_text": "11.1%"}
{"answers": {"text": ["0.5", "0.5%", "6.1"], "answer_start": [531, 531, 497]}, "prediction_text": "6.1%"}
{"answers": {"text": ["Sonderungsverbot", "Sonderungsverbot", "Sonderungsverbot"], "answer_start": [633, 633, 633]}, "prediction_text": "Sonderungsverbot"}
{"answers": {"text": ["Ersatzschulen", "Ersatzschulen", "Ersatzschulen"], "answer_start": [0, 0, 0]}, "prediction_text": "Ersatzschulen"}
{"answers": {"text": ["very low", "very low", "very low tuition fees"], "answer_start": [687, 687, 687]}, "prediction_text": "Low."}
{"answers": {"text": ["Erg\u00e4nzungsschulen", "Erg\u00e4nzungsschulen", "Erg\u00e4nzungsschulen"], "answer_start": [0, 0, 0]}, "prediction_text": "Erg\u00e4nzungsschulen"}
{"answers": {"text": ["vocational", "secondary or post-secondary", "vocational schools"], "answer_start": [260, 22, 260]}, "prediction_text": "Private individuals, private organizations, religious groups."}
{"answers": {"text": ["tuition", "tuition", "tuition fees"], "answer_start": [494, 494, 494]}, "prediction_text": "Tuition fees."}
{"answers": {"text": ["religious", "religious groups", "religious"], "answer_start": [145, 145, 145]}, "prediction_text": "Religious groups."}
{"answers": {"text": ["independent", "independent schools", "independent schools"], "answer_start": [37, 37, 37]}, "prediction_text": "Independent school"}
{"answers": {"text": ["CBSE", "CBSE", "CBSE"], "answer_start": [991, 991, 991]}, "prediction_text": "CBSE"}
{"answers": {"text": ["30", "30", "30"], "answer_start": [803, 803, 803]}, "prediction_text": "30"}
{"answers": {"text": ["union government", "the union government", "union government"], "answer_start": [607, 603, 607]}, "prediction_text": "Union government"}
{"answers": {"text": ["societies", "societies", "societies"], "answer_start": [36, 36, 36]}, "prediction_text": "Societies."}
{"answers": {"text": ["India", "India", "India"], "answer_start": [65, 65, 710]}, "prediction_text": "India"}
{"answers": {"text": ["Annual Status of Education Report", "Annual Status of Education Report", "Annual Status of Education Report"], "answer_start": [627, 627, 627]}, "prediction_text": "Annual Status of Education Report (ASER)"}
{"answers": {"text": ["evaluates learning levels in rural India", "evaluates learning levels in rural India", "evaluates learning levels in rural India"], "answer_start": [675, 675, 675]}, "prediction_text": "To evaluate learning levels in rural India."}
{"answers": {"text": ["English", "English", "English"], "answer_start": [925, 925, 925]}, "prediction_text": "English"}
{"answers": {"text": ["scoil phr\u00edobh\u00e1ideach", "scoil phr\u00edobh\u00e1ideach", "scoil phr\u00edobh\u00e1ideach"], "answer_start": [36, 36, 36]}, "prediction_text": "scoil phr\u00edobh\u00e1ideach"}
{"answers": {"text": ["teacher's salaries are paid by the State", "a certain number of teacher's salaries are paid by the State", "certain number of teacher's salaries are paid by the State"], "answer_start": [98, 78, 80]}, "prediction_text": "They are unusual."}
{"answers": {"text": ["\u20ac5,000", "\u20ac5,000", "\u20ac5,000"], "answer_start": [679, 679, 679]}, "prediction_text": "\u20ac5,000."}
{"answers": {"text": ["Society of Jesus", "the Society of Jesus", "Society of Jesus"], "answer_start": [883, 879, 883]}, "prediction_text": "Society of Jesus"}
{"answers": {"text": ["\u20ac25,000 per year", "\u20ac25,000", "\u20ac25,000 per year"], "answer_start": [794, 794, 794]}, "prediction_text": "\u20ac25,000 per year."}
{"answers": {"text": ["1957", "1957", "1957"], "answer_start": [33, 33, 33]}, "prediction_text": "1957"}
{"answers": {"text": ["Chinese", "Chinese", "Chinese"], "answer_start": [497, 497, 497]}, "prediction_text": "English"}
{"answers": {"text": ["English", "English", "English"], "answer_start": [589, 589, 589]}, "prediction_text": "English"}
{"answers": {"text": ["National School", "National Type", "National School"], "answer_start": [135, 271, 135]}, "prediction_text": "National School system"}
{"answers": {"text": ["60", "60", "all"], "answer_start": [618, 618, 65]}, "prediction_text": "60"}
{"answers": {"text": ["aided", "aided", "aided"], "answer_start": [189, 189, 189]}, "prediction_text": "Aided schools."}
{"answers": {"text": ["fully funded by private parties", "fully funded by private parties", "fully funded by private parties"], "answer_start": [240, 240, 240]}, "prediction_text": "They are not funded by government."}
{"answers": {"text": ["Kathmandu", "Kathmandu", "Kathmandu"], "answer_start": [489, 489, 489]}, "prediction_text": "Kathmandu"}
{"answers": {"text": ["English", "English", "English"], "answer_start": [666, 666, 666]}, "prediction_text": "English"}
{"answers": {"text": ["Nepali", "Nepali", "Nepali"], "answer_start": [704, 704, 704]}, "prediction_text": "Nepali"}
{"answers": {"text": ["88", "88", "88"], "answer_start": [28, 28, 28]}, "prediction_text": "88"}
{"answers": {"text": ["28,000", "28,000", "28,000"], "answer_start": [83, 83, 83]}, "prediction_text": "28,000"}
{"answers": {"text": ["3.7", "3.7%", "3.7"], "answer_start": [102, 102, 102]}, "prediction_text": "3.7%"}
{"answers": {"text": ["Catholic", "Catholic", "Catholic"], "answer_start": [853, 853, 853]}, "prediction_text": "Catholic"}
{"answers": {"text": ["Auckland", "Auckland", "Auckland"], "answer_start": [982, 982, 982]}, "prediction_text": "Auckland"}
{"answers": {"text": ["Anglican", "Anglican", "Anglican"], "answer_start": [26, 26, 26]}, "prediction_text": "Anglican"}
{"answers": {"text": ["Wellington", "Wellington", "Wellington"], "answer_start": [211, 211, 211]}, "prediction_text": "Wellington"}
{"answers": {"text": ["Presbyterian", "Presbyterian", "Presbyterian"], "answer_start": [290, 290, 290]}, "prediction_text": "Presbyterian"}
{"answers": {"text": ["Christchurch", "Christchurch", "Christchurch"], "answer_start": [487, 487, 487]}, "prediction_text": "Christchurch"}
{"answers": {"text": ["Society of St Pius X", "the Society of St Pius X", "Catholic schismatic"], "answer_start": [893, 889, 862]}, "prediction_text": "Society of St Pius X"}
{"answers": {"text": ["7.5", "7.5%", "7.5"], "answer_start": [111, 111, 111]}, "prediction_text": "7.5%"}
{"answers": {"text": ["32", "32%", "32"], "answer_start": [139, 139, 139]}, "prediction_text": "32%"}
{"answers": {"text": ["80", "80%", "80"], "answer_start": [177, 177, 177]}, "prediction_text": "80%"}
{"answers": {"text": ["August 1992", "August 1992", "August 1992"], "answer_start": [870, 870, 870]}, "prediction_text": "August 1992"}
{"answers": {"text": ["natural science", "natural science", "English, mathematics and natural science"], "answer_start": [753, 753, 728]}, "prediction_text": "Natural science."}
{"answers": {"text": ["Education Service Contracting", "Education Service Contracting", "Education Service Contracting"], "answer_start": [4, 4, 4]}, "prediction_text": "Tuition Fee Supplement"}
{"answers": {"text": ["Tuition Fee Supplement", "Tuition Fee Supplement", "Tuition Fee Supplement"], "answer_start": [209, 209, 209]}, "prediction_text": "Tuition Fee Supplement"}
{"answers": {"text": ["Private Education Student Financial Assistance", "Private Education Student Financial Assistance", "Private Education Student Financial Assistance"], "answer_start": [376, 376, 376]}, "prediction_text": "Private Education Student Financial Assistance"}
{"answers": {"text": ["South African Schools Act", "South African Schools Act", "South African Schools Act"], "answer_start": [296, 296, 296]}, "prediction_text": "South African Schools Act of 1996"}
{"answers": {"text": ["1996", "1996", "1996"], "answer_start": [325, 325, 325]}, "prediction_text": "1996"}
{"answers": {"text": ["independent", "independent", "independent"], "answer_start": [401, 401, 401]}, "prediction_text": "Independent schools."}
{"answers": {"text": ["traditional private", "traditional private schools", "traditional private schools and schools which are privately governed"], "answer_start": [430, 430, 430]}, "prediction_text": "Traditional private schools."}
{"answers": {"text": ["nineteenth", "nineteenth", "nineteenth century"], "answer_start": [121, 121, 121]}, "prediction_text": "18th century"}
{"answers": {"text": ["government schools formerly reserved for white children", "government schools formerly reserved for white children", "semi-private"], "answer_start": [379, 379, 116]}, "prediction_text": "White government schools."}
{"answers": {"text": ["better", "better", "produce better academic results"], "answer_start": [467, 467, 459]}, "prediction_text": "Better."}
{"answers": {"text": ["higher", "much higher", "much higher"], "answer_start": [813, 808, 808]}, "prediction_text": "Higher."}
{"answers": {"text": ["10", "10%", "10"], "answer_start": [130, 130, 130]}, "prediction_text": "10%"}
{"answers": {"text": ["10,000", "10,000", "nearly 10,000"], "answer_start": [516, 516, 509]}, "prediction_text": "30 schools."}
{"answers": {"text": ["700", "700", "700"], "answer_start": [483, 483, 483]}, "prediction_text": "700"}
{"answers": {"text": ["The Knowledge School", "The Knowledge School", "The Knowledge School"], "answer_start": [408, 408, 408]}, "prediction_text": "The Knowledge School"}
{"answers": {"text": ["voucher", "voucher", "pupils are free to choose a private school"], "answer_start": [253, 253, 11]}, "prediction_text": "School voucher model"}
{"answers": {"text": ["13", "13", "13"], "answer_start": [259, 259, 259]}, "prediction_text": "13 years old."}
{"answers": {"text": ["public", "public", "public"], "answer_start": [281, 281, 281]}, "prediction_text": "Public schools."}
{"answers": {"text": ["9", "9", "9"], "answer_start": [529, 529, 529]}, "prediction_text": "9%"}
{"answers": {"text": ["13", "13", "13"], "answer_start": [630, 630, 630]}, "prediction_text": "13%"}
{"answers": {"text": ["\u00a321,000", "\u00a321,000", "\u00a321,000"], "answer_start": [782, 782, 782]}, "prediction_text": "\u00a327,000+ per year."}
{"answers": {"text": ["Brown v. Board of Education of Topeka", "Brown v. Board of Education of Topeka", "Brown v. Board of Education of Topeka"], "answer_start": [87, 87, 87]}, "prediction_text": "Brown v. Board of Education of Topeka."}
{"answers": {"text": ["segregation academies", "segregation academies"], "answer_start": [584, 584]}, "prediction_text": "Segregation academies"}
{"answers": {"text": ["South", "South", "U.S. South"], "answer_start": [288, 288, 283]}, "prediction_text": "South."}
{"answers": {"text": ["white", "white", "African-American"], "answer_start": [300, 300, 409]}, "prediction_text": "African-American"}
{"answers": {"text": ["African-American", "African-American", "African-American"], "answer_start": [409, 409, 409]}, "prediction_text": "African-American"}
{"answers": {"text": ["endowments", "endowments", "endowments"], "answer_start": [75, 75, 75]}, "prediction_text": "Endowments, scholarship/voucher funds, and donations and grants from religious organizations or private individuals."}
{"answers": {"text": ["First", "First", "First Amendment"], "answer_start": [365, 365, 365]}, "prediction_text": "First Amendment"}
{"answers": {"text": ["Blaine", "Blaine Amendments", "Blaine Amendments"], "answer_start": [401, 401, 401]}, "prediction_text": "Blaine Amendments."}
{"answers": {"text": ["charter", "charter", "charter"], "answer_start": [657, 657, 657]}, "prediction_text": "Charter status."}
{"answers": {"text": ["Massachusetts", "Massachusetts", "Massachusetts"], "answer_start": [141, 141, 141]}, "prediction_text": "Massachusetts"}
{"answers": {"text": ["1852", "1852", "1852"], "answer_start": [158, 158, 158]}, "prediction_text": "1852"}
{"answers": {"text": ["1972", "1972", "1972"], "answer_start": [443, 443, 443]}, "prediction_text": "1972"}
{"answers": {"text": ["268 U.S. 510", "268 U.S. 510", "268 U.S. 510 (1925)"], "answer_start": [480, 480, 480]}, "prediction_text": "Meyer v. Nebraska."}
{"answers": {"text": ["McCrary", "McCrary", "McCrary"], "answer_start": [379, 379, 379]}, "prediction_text": "McCrary"}
{"answers": {"text": ["$40,000", "$40,000", "$40,000"], "answer_start": [95, 95, 95]}, "prediction_text": "$40,000"}
{"answers": {"text": ["$50,000", "$50,000", "$50,000"], "answer_start": [157, 157, 157]}, "prediction_text": "$50,000"}
{"answers": {"text": ["Groton School", "Groton School", "Groton School"], "answer_start": [304, 304, 304]}, "prediction_text": "Groton School"}
{"answers": {"text": ["fundraising", "fundraising drives", "fundraising drives"], "answer_start": [404, 404, 404]}, "prediction_text": "fundraising drives"}
{"answers": {"text": ["John Harvard", "John Harvard", "John Harvard"], "answer_start": [86, 86, 86]}, "prediction_text": "John Harvard"}
{"answers": {"text": ["1977", "1977", "1977"], "answer_start": [1117, 1117, 1117]}, "prediction_text": "1977"}
{"answers": {"text": ["James Bryant Conant", "James Bryant Conant", "James Bryant Conant"], "answer_start": [899, 899, 899]}, "prediction_text": "James Bryant Conant"}
{"answers": {"text": ["Association of American Universities", "Association of American Universities", "Association of American Universities"], "answer_start": [853, 853, 853]}, "prediction_text": "Association of American Universities"}
{"answers": {"text": ["Charles W. Eliot", "Charles W. Eliot", "Charles W. Eliot"], "answer_start": [678, 678, 678]}, "prediction_text": "Charles W. Eliot"}
{"answers": {"text": ["Harvard Library", "Harvard Library", "Harvard"], "answer_start": [263, 263, 263]}, "prediction_text": "Harvard Library"}
{"answers": {"text": ["79 individual libraries", "79", "79"], "answer_start": [357, 357, 357]}, "prediction_text": "79"}
{"answers": {"text": ["18 million volumes", "18 million", "over 18 million"], "answer_start": [391, 391, 386]}, "prediction_text": "18 million."}
{"answers": {"text": ["eight U.S. presidents", "eight", "eight"], "answer_start": [436, 436, 436]}, "prediction_text": "8"}
{"answers": {"text": ["150 Nobel laureates", "150", "150"], "answer_start": [577, 577, 577]}, "prediction_text": "150"}
{"answers": {"text": ["Boston metropolitan area", "Boston", "Boston"], "answer_start": [154, 154, 154]}, "prediction_text": "Boston"}
{"answers": {"text": ["$37.6 billion", "$37.6 billion", "$37.6 billion"], "answer_start": [548, 548, 548]}, "prediction_text": "$37.6 billion"}
{"answers": {"text": ["Charles River", "Charles", "Charles"], "answer_start": [401, 401, 401]}, "prediction_text": "Charles River"}
{"answers": {"text": ["eleven separate academic units", "eleven", "eleven"], "answer_start": [33, 33, 33]}, "prediction_text": "11"}
{"answers": {"text": ["Harvard Yard", "Harvard Yard", "Harvard Yard"], "answer_start": [228, 228, 228]}, "prediction_text": "Harvard Yard"}
{"answers": {"text": ["1636", "1636", "1636"], "answer_start": [22, 22, 22]}, "prediction_text": "1636"}
{"answers": {"text": ["Massachusetts Bay Colony", "Great and General Court of the Massachusetts Bay Colony", "Great and General Court of the Massachusetts Bay Colony"], "answer_start": [73, 42, 42]}, "prediction_text": "Great and General Court of the Massachusetts Bay Colony"}
{"answers": {"text": ["1638", "1638", "1638"], "answer_start": [171, 171, 171]}, "prediction_text": "1638"}
{"answers": {"text": ["1639", "1639", "1639"], "answer_start": [288, 288, 288]}, "prediction_text": "1639"}
{"answers": {"text": ["1650", "1650", "1650"], "answer_start": [547, 547, 547]}, "prediction_text": "1650"}
{"answers": {"text": ["Puritan ministers", "Puritan", "Puritan"], "answer_start": [44, 44, 44]}, "prediction_text": "Puritan"}
{"answers": {"text": ["English university model", "English university", "English university"], "answer_start": [331, 331, 331]}, "prediction_text": "English university model"}
{"answers": {"text": ["It was never affiliated with any particular denomination", "never", "never"], "answer_start": [454, 461, 461]}, "prediction_text": "Congregational and Unitarian."}
{"answers": {"text": ["1804", "1804", "1804"], "answer_start": [377, 377, 377]}, "prediction_text": "1804"}
{"answers": {"text": ["Samuel Webber", "Samuel Webber", "Samuel Webber"], "answer_start": [490, 490, 490]}, "prediction_text": "Samuel Webber"}
{"answers": {"text": ["1805", "1805", "1805"], "answer_start": [468, 468, 468]}, "prediction_text": "1805"}
{"answers": {"text": ["Louis Agassiz", "Louis Agassiz", "Louis Agassiz"], "answer_start": [41, 41, 41]}, "prediction_text": "Louis Agassiz"}
{"answers": {"text": ["intuition", "intuition", "intuition"], "answer_start": [351, 351, 351]}, "prediction_text": "intuition and the assumption that a person can grasp the \"divine plan\" in all phenomena."}
{"answers": {"text": ["Thomas Reid and Dugald Stewart", "Thomas Reid and Dugald Stewart", "Thomas Reid and Dugald Stewart"], "answer_start": [685, 685, 685]}, "prediction_text": "Thomas Reid and Dugald Stewart."}
{"answers": {"text": ["Charles W. Eliot", "Charles W. Eliot", "Charles W. Eliot"], "answer_start": [0, 0, 0]}, "prediction_text": "Charles W. Eliot"}
{"answers": {"text": ["Transcendentalist Unitarian", "Transcendentalist Unitarian", "Transcendentalist Unitarian"], "answer_start": [308, 308, 308]}, "prediction_text": "Transcendentalist Unitarian convictions."}
{"answers": {"text": ["William Ellery Channing and Ralph Waldo Emerson", "William Ellery Channing and Ralph Waldo Emerson", "William Ellery Channing and Ralph Waldo Emerson"], "answer_start": [362, 362, 362]}, "prediction_text": "William Ellery Channing and Ralph Waldo Emerson."}
{"answers": {"text": ["James Bryant Conant", "James Bryant Conant", "James Bryant Conant"], "answer_start": [0, 0, 0]}, "prediction_text": "James Bryant Conant"}
{"answers": {"text": ["identify, recruit", "Conant devised programs", "programs to identify, recruit, and support talented youth"], "answer_start": [279, 252, 267]}, "prediction_text": "Identify, recruit, and support talented youth."}
{"answers": {"text": ["1945", "1945", "1945"], "answer_start": [508, 508, 508]}, "prediction_text": "1945"}
{"answers": {"text": ["about four men attending Harvard College for every woman studying at Radcliffe", "four men attending Harvard College for every woman studying at Radcliffe", "four men attending Harvard College for every woman"], "answer_start": [165, 171, 171]}, "prediction_text": "4:1"}
{"answers": {"text": ["1977", "1977", "1977"], "answer_start": [305, 305, 305]}, "prediction_text": "1977"}
{"answers": {"text": ["the proportion of female undergraduates steadily increased, mirroring a trend throughout higher education in the United States", "steadily increased", "increased"], "answer_start": [311, 351, 360]}, "prediction_text": "Increasing."}
{"answers": {"text": ["3 miles", "3 miles", "about 3 miles"], "answer_start": [87, 87, 81]}, "prediction_text": "3 miles (5 km)"}
{"answers": {"text": ["twelve residential Houses", "twelve", "twelve"], "answer_start": [497, 497, 497]}, "prediction_text": "9"}
{"answers": {"text": ["Charles River", "Charles River", "Charles River"], "answer_start": [582, 582, 582]}, "prediction_text": "South of Harvard Yard."}
{"answers": {"text": ["half a mile northwest of the Yard", "half a mile", "half a mile"], "answer_start": [655, 655, 655]}, "prediction_text": "Half a mile."}
{"answers": {"text": ["Allston", "Allston", "on a 358-acre (145 ha) campus"], "answer_start": [181, 181, 118]}, "prediction_text": "Harvard Stadium"}
{"answers": {"text": ["The John W. Weeks Bridge", "John W. Weeks Bridge", "John W. Weeks Bridge"], "answer_start": [190, 194, 194]}, "prediction_text": "John W. Weeks Bridge"}
{"answers": {"text": ["Longwood Medical and Academic Area", "Longwood Medical and Academic Area", "Longwood Medical and Academic Area"], "answer_start": [438, 438, 438]}, "prediction_text": "Longwood Medical and Academic Area."}
{"answers": {"text": ["approximately fifty percent", "fifty percent", "fifty percent more"], "answer_start": [167, 181, 181]}, "prediction_text": "Fifty percent."}
{"answers": {"text": ["new and enlarged bridges, a shuttle service and/or a tram.", "new and enlarged bridges, a shuttle service and/or a tram", "new and enlarged bridges, a shuttle service and/or a tram"], "answer_start": [313, 313, 313]}, "prediction_text": "Bridges, shuttle service, tram."}
{"answers": {"text": ["enhanced transit infrastructure, possible shuttles open to the public, and park space which will also be publicly accessible.", "enhanced transit infrastructure, possible shuttles open to the public, and park space which will also be publicly accessible", "enhanced transit infrastructure, possible shuttles open to the public, and park space"], "answer_start": [746, 746, 746]}, "prediction_text": "enhanced transit infrastructure, possible shuttles, park space."}
{"answers": {"text": ["2,400", "2,400", "2,400"], "answer_start": [10, 10, 10]}, "prediction_text": "7,200"}
{"answers": {"text": ["7,200", "7,200", "7,200"], "answer_start": [64, 64, 64]}, "prediction_text": "7,200"}
{"answers": {"text": ["14,000", "14,000", "14,000"], "answer_start": [89, 89, 89]}, "prediction_text": "14,000"}
{"answers": {"text": ["1875", "1875", "1875"], "answer_start": [309, 309, 309]}, "prediction_text": "1875"}
{"answers": {"text": ["1858", "1858", "1858"], "answer_start": [409, 409, 409]}, "prediction_text": "1858"}
{"answers": {"text": ["$32 billion", "$32 billion", "$32 billion"], "answer_start": [170, 170, 170]}, "prediction_text": "$32 billion"}
{"answers": {"text": ["30% loss", "$12 billion", "30% loss"], "answer_start": [272, 653, 272]}, "prediction_text": "30%"}
{"answers": {"text": ["Allston Science Complex", "Allston Science Complex", "construction of the $1.2 billion Allston Science Complex"], "answer_start": [798, 798, 765]}, "prediction_text": "Allston Science Complex"}
{"answers": {"text": ["$4.093 million", "$4.093 million", "$4.093 million"], "answer_start": [1046, 1046, 1046]}, "prediction_text": "$4.093 million"}
{"answers": {"text": ["$159 million", "$159 million", "$159 million"], "answer_start": [991, 991, 991]}, "prediction_text": "$159 million"}
{"answers": {"text": ["late 1980s", "late 1980s", "1980s"], "answer_start": [56, 56, 61]}, "prediction_text": "Late 1980s"}
{"answers": {"text": ["South African Vice Consul Duke Kent-Brown.", "Duke Kent-Brown", "Duke Kent-Brown"], "answer_start": [166, 192, 192]}, "prediction_text": "Duke Kent-Brown"}
{"answers": {"text": ["$230 million", "$230 million", "$230 million"], "answer_start": [503, 503, 503]}, "prediction_text": "$230 million."}
{"answers": {"text": ["accepted 5.3% of applicants", "5.3%", "5.3%"], "answer_start": [135, 144, 144]}, "prediction_text": "5.3%"}
{"answers": {"text": ["2007", "2007", "2007"], "answer_start": [324, 324, 324]}, "prediction_text": "2007"}
{"answers": {"text": ["disadvantage low-income and under-represented minority applicants", "believed to disadvantage low-income and under-represented minority applicants", "believed to disadvantage low-income and under-represented minority applicants"], "answer_start": [360, 348, 348]}, "prediction_text": "Disadvantage low-income and under-represented minority applicants applying to selective universities."}
{"answers": {"text": ["2016", "2016", "2016"], "answer_start": [483, 483, 483]}, "prediction_text": "2016"}
{"answers": {"text": ["core curriculum of seven classes", "seven", "seven"], "answer_start": [232, 251, 251]}, "prediction_text": "7 classes."}
{"answers": {"text": ["eight General Education categories", "eight", "eight"], "answer_start": [374, 374, 374]}, "prediction_text": "8"}
{"answers": {"text": ["reliance on teaching fellows", "reliance on teaching fellows", "reliance on teaching fellows"], "answer_start": [915, 915, 915]}, "prediction_text": "Teaching fellows."}
{"answers": {"text": ["beginning in early September and ending in mid-May", "beginning in early September and ending in mid-May", "beginning in early September and ending in mid-May"], "answer_start": [59, 59, 59]}, "prediction_text": "4 terms."}
{"answers": {"text": ["four-course rate average", "four", "four"], "answer_start": [188, 188, 188]}, "prediction_text": "Four courses."}
{"answers": {"text": ["summa cum laude", "summa cum laude", "summa cum laude"], "answer_start": [464, 464, 464]}, "prediction_text": "Phi Beta Kappa"}
{"answers": {"text": ["60%", "60%", "60%"], "answer_start": [1035, 1035, 1035]}, "prediction_text": "60%"}
{"answers": {"text": ["$38,000", "$38,000", "$38,000"], "answer_start": [47, 47, 47]}, "prediction_text": "$38,000"}
{"answers": {"text": ["$57,000", "$57,000", "$57,000"], "answer_start": [91, 91, 91]}, "prediction_text": "$57,000"}
{"answers": {"text": ["nothing for their children to attend, including room and board", "nothing", "nothing"], "answer_start": [156, 156, 156]}, "prediction_text": "$10,000"}
{"answers": {"text": ["$414 million", "$414 million", "$414 million"], "answer_start": [449, 449, 449]}, "prediction_text": "$414 million"}
{"answers": {"text": ["88%", "88%", "88%"], "answer_start": [654, 654, 654]}, "prediction_text": "88%"}
{"answers": {"text": ["Widener Library", "Widener", "Widener Library"], "answer_start": [53, 53, 53]}, "prediction_text": "Widener Library"}
{"answers": {"text": ["Cabot Science Library, Lamont Library, and Widener Library", "Cabot Science Library, Lamont Library, and Widener Library", "Cabot Science Library, Lamont Library, and Widener Library"], "answer_start": [312, 312, 312]}, "prediction_text": "Cabot Science Library, Lamont Library, Widener Library."}
{"answers": {"text": ["Pusey Library", "Pusey Library", "Pusey Library"], "answer_start": [850, 850, 850]}, "prediction_text": "Pusey Library"}
{"answers": {"text": ["18 million volumes", "18 million", "over 18 million"], "answer_start": [143, 143, 138]}, "prediction_text": "18 million."}
{"answers": {"text": ["three museums.", "three", "three"], "answer_start": [99, 99, 99]}, "prediction_text": "3"}
{"answers": {"text": ["Western art from the Middle Ages to the present", "Western art from the Middle Ages to the present", "Western art from the Middle Ages to the present"], "answer_start": [348, 348, 348]}, "prediction_text": "Western art from the Middle Ages to the present."}
{"answers": {"text": ["Peabody Museum of Archaeology and Ethnology", "Peabody Museum of Archaeology and Ethnology", "Peabody Museum of Archaeology and Ethnology"], "answer_start": [802, 802, 802]}, "prediction_text": "Museum of Comparative Zoology."}
{"answers": {"text": ["2003", "2003", "2003"], "answer_start": [158, 158, 158]}, "prediction_text": "2003"}
{"answers": {"text": ["2011", "2011", "2011"], "answer_start": [208, 208, 208]}, "prediction_text": "2011"}
{"answers": {"text": ["second most commonly", "second", "second"], "answer_start": [813, 813, 813]}, "prediction_text": "The Princeton Review."}
{"answers": {"text": ["42", "42", "42"], "answer_start": [32, 32, 32]}, "prediction_text": "42"}
{"answers": {"text": ["Yale University", "Yale", "Yale University"], "answer_start": [138, 138, 138]}, "prediction_text": "Yale University"}
{"answers": {"text": ["every two years when the Harvard and Yale Track and Field teams come together to compete against a combined Oxford University and Cambridge University team", "every two years", "every two years"], "answer_start": [276, 276, 276]}, "prediction_text": "Every two years."}
{"answers": {"text": ["1875", "1875", "1875"], "answer_start": [164, 164, 164]}, "prediction_text": "1903"}
{"answers": {"text": ["1903", "1903", "1903"], "answer_start": [434, 434, 434]}, "prediction_text": "1903"}
{"answers": {"text": ["1906", "1906", "1906"], "answer_start": [1014, 1014, 1014]}, "prediction_text": "1906"}
{"answers": {"text": ["former captain of the Yale football team", "Yale", "Yale"], "answer_start": [758, 780, 780]}, "prediction_text": "Yale"}
{"answers": {"text": ["Lavietes Pavilion", "Lavietes Pavilion", "Lavietes Pavilion"], "answer_start": [53, 53, 53]}, "prediction_text": "Lavietes Pavilion"}
{"answers": {"text": ["Malkin Athletic Center", "Malkin Athletic Center", "Malkin Athletic Center"], "answer_start": [140, 140, 140]}, "prediction_text": "Lavietes Pavilion"}
{"answers": {"text": ["three weight rooms", "three", "three"], "answer_start": [511, 511, 511]}, "prediction_text": "3"}
{"answers": {"text": ["23 years", "23", "23"], "answer_start": [23, 23, 23]}, "prediction_text": "23 years."}
{"answers": {"text": ["Thames River", "Thames River", "Thames River"], "answer_start": [166, 166, 166]}, "prediction_text": "Thames River"}
{"answers": {"text": ["strong rivalry against Cornell", "Cornell", "Cornell"], "answer_start": [412, 435, 435]}, "prediction_text": "Cornell"}
{"answers": {"text": ["2003", "2003", "2003"], "answer_start": [600, 600, 600]}, "prediction_text": "2003"}
{"answers": {"text": ["General Ban Ki-moon", "Ban Ki-moon", "Ban Ki-moon"], "answer_start": [25, 33, 33]}, "prediction_text": "John Hancock"}
{"answers": {"text": ["Juan Manuel Santos", "Juan Manuel Santos", "Juan Manuel Santos"], "answer_start": [297, 297, 297]}, "prediction_text": "John Quincy Adams"}
{"answers": {"text": ["Jos\u00e9 Mar\u00eda Figueres", "Jos\u00e9 Mar\u00eda Figueres", "Jos\u00e9 Mar\u00eda Figueres"], "answer_start": [339, 339, 339]}, "prediction_text": "Jos\u00e9 Mar\u00eda Figueres"}
{"answers": {"text": ["Benjamin Netanyahu", "Benazir Bhutto"], "answer_start": [809, 861]}, "prediction_text": "Benazir Bhutto"}
{"answers": {"text": ["Conan O'Brien", "Conan O'Brien", "Conan O'Brien"], "answer_start": [384, 384, 384]}, "prediction_text": "Conan O'Brien"}
{"answers": {"text": ["Leonard Bernstein", "Leonard Bernstein", "Leonard Bernstein"], "answer_start": [235, 235, 235]}, "prediction_text": "Leonard Bernstein"}
{"answers": {"text": ["Yo Yo Ma", "Yo Yo Ma", "Yo Yo Ma"], "answer_start": [262, 262, 262]}, "prediction_text": "Yo Yo Ma"}
{"answers": {"text": ["W. E. B. Du Bois", "W. E. B. Du Bois", "W. E. B. Du Bois"], "answer_start": [27, 27, 27]}, "prediction_text": "W. E. B. Du Bois"}
{"answers": {"text": ["Shing-Tung Yau", "Shing-Tung Yau", "Shing-Tung Yau"], "answer_start": [641, 641, 641]}, "prediction_text": "Shing-Tung Yau"}
{"answers": {"text": ["Alan Dershowitz and Lawrence Lessig", "Alan Dershowitz and Lawrence Lessig", "Alan Dershowitz and Lawrence Lessig"], "answer_start": [830, 830, 830]}, "prediction_text": "Alan Dershowitz, Lawrence Lessig, Bernard Rands, Joseph Nye, Stanley Hoffmann, Stanley L. Kaplan, Stanley L. Stern, Stanley L. Stern, Stanley L. Stern, Stanley L. Stern, Stanley L. Stern, Stanley L. Stern, Stanley L. Stern, Stanley L. Stern, Stanley L. Stern, Stanley L. Stern, Stanley L. Stern, Stanley L. Stern, Stanley L. Stern, Stanley L. Stern, Stanley L. Stern, Stanley L. Stern, Stanley L. Stern, Stanley L. Stern, Stanley L. Stern, Stanley L. Stern, Stanley L. Stern, Stanley L. Stern, Stanley L. Stern, Stanley L. Stern, Stanley L."}
{"answers": {"text": ["Stephen Greenblatt", "Stephen Greenblatt", "Stephen Greenblatt"], "answer_start": [290, 290, 290]}, "prediction_text": "Stephen Greenblatt"}
{"answers": {"text": ["Jacksonville", "Jacksonville", "Jacksonville"], "answer_start": [0, 0, 0]}, "prediction_text": "Jacksonville"}
{"answers": {"text": ["1,345,596", "1,345,596", "1,345,596"], "answer_start": [608, 608, 608]}, "prediction_text": "1,345,596"}
{"answers": {"text": ["12th", "12th", "12th"], "answer_start": [472, 472, 472]}, "prediction_text": "12th."}
{"answers": {"text": ["Duval", "Duval County", "Duval County"], "answer_start": [164, 164, 164]}, "prediction_text": "Duval County"}
{"answers": {"text": ["1968", "1968", "1968"], "answer_start": [225, 225, 225]}, "prediction_text": "1968"}
{"answers": {"text": ["St. Johns", "St. Johns River", "St. Johns River"], "answer_start": [99, 99, 99]}, "prediction_text": "St. Johns River"}
{"answers": {"text": ["340 miles", "340 miles", "340 miles"], "answer_start": [181, 181, 181]}, "prediction_text": "340 miles (550 km)"}
{"answers": {"text": ["Fort Caroline", "Fort Caroline", "Fort Caroline"], "answer_start": [398, 398, 398]}, "prediction_text": "Fort Caroline"}
{"answers": {"text": ["the Timucua", "Timucua", "the Timucua people"], "answer_start": [329, 333, 329]}, "prediction_text": "Timucua."}
{"answers": {"text": ["Andrew Jackson", "Andrew Jackson", "Andrew Jackson"], "answer_start": [786, 786, 786]}, "prediction_text": "Andrew Jackson"}
{"answers": {"text": ["third largest", "third largest military presence", "third largest"], "answer_start": [344, 344, 344]}, "prediction_text": "3rd"}
{"answers": {"text": ["golf", "golf", "golf"], "answer_start": [619, 619, 619]}, "prediction_text": "Golf"}
{"answers": {"text": ["two", "two", "two"], "answer_start": [248, 152, 152]}, "prediction_text": "2"}
{"answers": {"text": ["\"Jacksonvillians\" or \"Jaxsons\"", "\"Jacksonvillians\"", "\"Jacksonvillians\" or \"Jaxsons\""], "answer_start": [664, 664, 664]}, "prediction_text": "Jaxvillians or Jaxsons."}
{"answers": {"text": ["thousands", "thousands of years", "thousands of years"], "answer_start": [67, 67, 67]}, "prediction_text": "Thousands of years."}
{"answers": {"text": ["a University of North Florida team", "University of North Florida", "University of North Florida"], "answer_start": [170, 172, 172]}, "prediction_text": "University of North Florida team"}
{"answers": {"text": ["Timucua", "Timucua people", "Mocama"], "answer_start": [420, 420, 386]}, "prediction_text": "Timucua"}
{"answers": {"text": ["the historical era", "historical era", "beginning of the historical era"], "answer_start": [334, 338, 321]}, "prediction_text": "The beginning of the historical era."}
{"answers": {"text": ["Ossachite", "Ossachite", "Ossachite"], "answer_start": [668, 668, 668]}, "prediction_text": "Ossachite"}
{"answers": {"text": ["Jean Ribault", "Jean Ribault", "Jean Ribault"], "answer_start": [25, 25, 25]}, "prediction_text": "Jean Ribault"}
{"answers": {"text": ["France", "France", "France"], "answer_start": [235, 235, 235]}, "prediction_text": "France"}
{"answers": {"text": ["Pedro Men\u00e9ndez de Avil\u00e9s", "Pedro Men\u00e9ndez de Avil\u00e9s", "Pedro Men\u00e9ndez de Avil\u00e9s"], "answer_start": [422, 422, 422]}, "prediction_text": "Pedro Men\u00e9ndez de Avil\u00e9s"}
{"answers": {"text": ["San Mateo", "fort San Mateo", "San Mateo"], "answer_start": [731, 726, 731]}, "prediction_text": "San Mateo"}
{"answers": {"text": ["Fort Caroline", "Fort Caroline", "Fort Caroline"], "answer_start": [885, 885, 885]}, "prediction_text": "Fort Caroline"}
{"answers": {"text": ["French and Indian War", "French and Indian War", "French and Indian War"], "answer_start": [53, 53, 53]}, "prediction_text": "The Spanish ceded Florida to Britain in 1763."}
{"answers": {"text": ["constructed the King's Road", "constructed the King's Road", "constructed the King's Road"], "answer_start": [97, 97, 97]}, "prediction_text": "They constructed the King's Road connecting St. Augustine to Georgia."}
{"answers": {"text": ["cattle were brought across the river there.", "cattle were brought across", "cattle were brought across the river there"], "answer_start": [351, 351, 351]}, "prediction_text": "Wacca Pilatka."}
{"answers": {"text": ["Spain", "Spain", "Spain"], "answer_start": [773, 646, 646]}, "prediction_text": "Britain"}
{"answers": {"text": ["February 9, 1832", "1832.", "February 9, 1832"], "answer_start": [1137, 1149, 1137]}, "prediction_text": "February 9, 1832."}
{"answers": {"text": ["Confederate", "Confederate", "the Confederate cause"], "answer_start": [125, 125, 121]}, "prediction_text": "Confederate."}
{"answers": {"text": ["The Skirmish of the Brick Church", "Skirmish of the Brick Church", "Skirmish of the Brick Church"], "answer_start": [359, 363, 363]}, "prediction_text": "Olustee"}
{"answers": {"text": ["Battle of Olustee", "Battle of Olustee", "Battle of Olustee"], "answer_start": [576, 576, 576]}, "prediction_text": "Battle of Cedar Creek."}
{"answers": {"text": ["Warfare and the long occupation", "Warfare and the long occupation", "Warfare and the long occupation"], "answer_start": [828, 828, 828]}, "prediction_text": "Blockade, Union forces, Fort Clinch."}
{"answers": {"text": ["Battle of Cedar Creek", "Battle of Cedar Creek", "1864"], "answer_start": [805, 805, 731]}, "prediction_text": "1864"}
{"answers": {"text": ["Gilded Age", "Reconstruction", "Reconstruction and the Gilded Age"], "answer_start": [30, 7, 7]}, "prediction_text": "During the Gilded Age."}
{"answers": {"text": ["Grover Cleveland", "Grover Cleveland", "President Grover Cleveland"], "answer_start": [198, 198, 188]}, "prediction_text": "Grover Cleveland"}
{"answers": {"text": ["yellow fever outbreaks", "yellow fever outbreaks", "yellow fever outbreaks"], "answer_start": [468, 468, 468]}, "prediction_text": "Yellow fever outbreaks."}
{"answers": {"text": ["extension of the Florida East Coast Railway further south", "Florida East Coast Railway", "the Florida East Coast Railway"], "answer_start": [505, 522, 518]}, "prediction_text": "Extension of the Florida East Coast Railway."}
{"answers": {"text": ["railroad", "railroad", "railroad"], "answer_start": [178, 178, 178]}, "prediction_text": "Railroad"}
{"answers": {"text": ["Spanish moss", "kitchen fire", "Spanish moss"], "answer_start": [92, 78, 92]}, "prediction_text": "Spanish moss."}
{"answers": {"text": ["over 2,000", "2,000 buildings", "over 2,"], "answer_start": [271, 276, 271]}, "prediction_text": "2,000"}
{"answers": {"text": ["declare martial law", "declare martial law", "declare martial law and sent the state militia to maintain order"], "answer_start": [454, 454, 454]}, "prediction_text": "Declared martial law."}
{"answers": {"text": ["Great Fire of 1901", "Great Fire of 1901", "Great Fire of 1901"], "answer_start": [715, 715, 715]}, "prediction_text": "Great Fire of 1901"}
{"answers": {"text": ["New York\u2013based filmmakers", "filmmakers", "New York\u2013based filmmakers"], "answer_start": [14, 29, 14]}, "prediction_text": "New York-based filmmakers."}
{"answers": {"text": ["silent film", "silent film", "silent"], "answer_start": [189, 189, 189]}, "prediction_text": "Silent films."}
{"answers": {"text": ["Winter Film Capital of the World", "Winter Film Capital of the World", "Winter Film Capital of the World"], "answer_start": [262, 262, 262]}, "prediction_text": "Winter Film Capital of the World"}
{"answers": {"text": ["Hollywood", "Hollywood", "the emergence of Hollywood"], "answer_start": [323, 323, 306]}, "prediction_text": "Hollywood"}
{"answers": {"text": ["highways", "construction of highways", "The construction of highways"], "answer_start": [152, 136, 132]}, "prediction_text": "New public building projects."}
{"answers": {"text": ["55.1%", "55.1%", "55.1%"], "answer_start": [812, 812, 812]}, "prediction_text": "55.1%"}
{"answers": {"text": ["\"white flight\"", "white flight", "white flight"], "answer_start": [646, 647, 647]}, "prediction_text": "White flight"}
{"answers": {"text": ["Mayor W. Haydon Burns", "Mayor W. Haydon Burns'", "Mayor W. Haydon Burns"], "answer_start": [384, 384, 384]}, "prediction_text": "Mayor W. Haydon Burns"}
{"answers": {"text": ["World War II", "World War II", "World War II"], "answer_start": [118, 222, 222]}, "prediction_text": "World War II"}
{"answers": {"text": ["Much of the city's tax base dissipated", "tax base dissipated"], "answer_start": [0, 19]}, "prediction_text": "Tax base dissipation."}
{"answers": {"text": ["unincorporated suburbs", "suburbs", "unincorporated suburbs"], "answer_start": [166, 181, 166]}, "prediction_text": "Unincorporated suburbs."}
{"answers": {"text": ["annexing outlying communities", "annexing outlying communities", "annexing outlying communities"], "answer_start": [345, 345, 345]}, "prediction_text": "Annexing outlying communities."}
{"answers": {"text": ["Voters outside the city limits", "Voters outside the city limits", "Voters outside the city limits"], "answer_start": [457, 457, 457]}, "prediction_text": "Voters outside the city limits."}
{"answers": {"text": ["old boy network", "old boy network", "the traditional old boy network"], "answer_start": [137, 137, 121]}, "prediction_text": "Through the traditional old boy network."}
{"answers": {"text": ["11", "11", "11"], "answer_start": [202, 202, 202]}, "prediction_text": "11"}
{"answers": {"text": ["Jacksonville Consolidation", "Jacksonville Consolidation", "Jacksonville Consolidation"], "answer_start": [261, 261, 261]}, "prediction_text": "J. J. Daniel and Claude Yates."}
{"answers": {"text": ["public high schools lost their accreditation", "high schools lost their accreditation", "public high schools lost their accreditation"], "answer_start": [570, 577, 570]}, "prediction_text": "Lower taxes."}
{"answers": {"text": ["voters approved the plan", "governments merged to create the Consolidated City of Jacksonville"], "answer_start": [50, 100]}, "prediction_text": "Voters approved the plan."}
{"answers": {"text": ["Hans Tanzler", "Hans Tanzler", "Hans Tanzler"], "answer_start": [339, 339, 339]}, "prediction_text": "Hans Tanzler"}
{"answers": {"text": ["\"Bold New City of the South\"", "Bold New City of the South", "Consolidated City of Jacksonville"], "answer_start": [428, 429, 133]}, "prediction_text": "Bold New City of the South"}
{"answers": {"text": ["Better Jacksonville Plan", "The Better Jacksonville Plan", "The Better Jacksonville Plan"], "answer_start": [496, 492, 492]}, "prediction_text": "Better Jacksonville Plan"}
{"answers": {"text": ["authorized a half-penny sales tax", "half-penny sales tax", "a half-penny sales tax"], "answer_start": [617, 630, 628]}, "prediction_text": "Half-penny sales tax."}
{"answers": {"text": ["874.3 square miles", "874.3 square miles", "874.3 square miles"], "answer_start": [75, 75, 75]}, "prediction_text": "874.3 square miles (2,264 km2)"}
{"answers": {"text": ["The St. Johns River", "St. Johns River", "St. Johns River"], "answer_start": [519, 523, 523]}, "prediction_text": "Trout River"}
{"answers": {"text": ["The Trout River", "Trout River", "The Trout River"], "answer_start": [557, 561, 557]}, "prediction_text": "Trout River"}
{"answers": {"text": ["13.34%", "13.34%", "13.34%"], "answer_start": [247, 247, 247]}, "prediction_text": "13.34%"}
{"answers": {"text": ["Baldwin", "Baldwin", "Baldwin"], "answer_start": [324, 324, 324]}, "prediction_text": "Baldwin"}
{"answers": {"text": ["tallest building in Downtown Jacksonville", "tallest precast, post-tensioned concrete structure", "tallest building in Downtown Jacksonville"], "answer_start": [4, 505, 4]}, "prediction_text": "Tallest building in Downtown Jacksonville's skyline."}
{"answers": {"text": ["Barnett Center", "Barnett Center", "Barnett Center"], "answer_start": [113, 113, 113]}, "prediction_text": "Barnett Center"}
{"answers": {"text": ["617 ft", "617 ft", "617 ft ("], "answer_start": [148, 148, 148]}, "prediction_text": "617 ft (188 m)"}
{"answers": {"text": ["28", "42", "42"], "answer_start": [440, 176, 176]}, "prediction_text": "28 floors."}
{"answers": {"text": ["its distinctive flared base", "distinctive flared base", "distinctive flared base"], "answer_start": [258, 262, 262]}, "prediction_text": "Flared base."}
{"answers": {"text": ["subtropical", "humid subtropical", "humid subtropical climate"], "answer_start": [86, 80, 80]}, "prediction_text": "Humid subtropical climate (K\u00f6ppen Cfa)"}
{"answers": {"text": ["May through September", "May through September", "the warmest months from May through September"], "answer_start": [256, 256, 232]}, "prediction_text": "May through September."}
{"answers": {"text": ["mild", "mild and sunny.", "mild and sunny"], "answer_start": [125, 459, 459]}, "prediction_text": "mild and sunny"}
{"answers": {"text": ["low latitude", "low latitude", "low latitude"], "answer_start": [358, 358, 358]}, "prediction_text": "Low latitude."}
{"answers": {"text": ["104 \u00b0F", "104 \u00b0F", "104 \u00b0F"], "answer_start": [297, 297, 297]}, "prediction_text": "104 \u00b0F (40 \u00b0C)"}
{"answers": {"text": ["thunderstorms", "thunderstorms", "thunderstorms"], "answer_start": [365, 365, 365]}, "prediction_text": "Thunderstorms erupt."}
{"answers": {"text": ["high humidity", "high humidity", "extremely high humidity"], "answer_start": [520, 520, 510]}, "prediction_text": "High humidity."}
{"answers": {"text": ["July", "July", "July"], "answer_start": [71, 315, 71]}, "prediction_text": "July"}
{"answers": {"text": ["Hurricane Dora", "Hurricane Dora", "Hurricane Dora"], "answer_start": [534, 534, 534]}, "prediction_text": "Hurricane Dora in 1964."}
{"answers": {"text": ["110 mph", "110 mph", "110 mph"], "answer_start": [719, 719, 719]}, "prediction_text": "110 mph (180 km/h)"}
{"answers": {"text": ["Tropical Storm Beryl", "Beryl", "Tropical Storm Beryl"], "answer_start": [1528, 1543, 1528]}, "prediction_text": "Tropical Storm Beryl"}
{"answers": {"text": ["Saffir-Simpson Scale", "Saffir-Simpson", "Saffir-Simpson Scale"], "answer_start": [776, 776, 776]}, "prediction_text": "Saffir-Simpson Scale."}
{"answers": {"text": ["2008", "Fay", "2008"], "answer_start": [837, 859, 837]}, "prediction_text": "2008"}
{"answers": {"text": ["Arab", "Arab", "Arab"], "answer_start": [235, 235, 235]}, "prediction_text": "Arab"}
{"answers": {"text": ["821,784", "821,784", "821,784"], "answer_start": [139, 139, 139]}, "prediction_text": "821,784"}
{"answers": {"text": ["largest", "largest Filipino American community", "largest"], "answer_start": [356, 356, 356]}, "prediction_text": "Tenth."}
{"answers": {"text": ["Filipino", "Filipino community", "Filipino"], "answer_start": [476, 476, 476]}, "prediction_text": "Filipino American community"}
{"answers": {"text": ["29.7%", "29.7%", "29.7%"], "answer_start": [266, 266, 266]}, "prediction_text": "23.9%"}
{"answers": {"text": ["23.9%", "23.9%", "23.9%"], "answer_start": [508, 508, 508]}, "prediction_text": "23.9%"}
{"answers": {"text": ["females", "females", "females"], "answer_start": [688, 688, 688]}, "prediction_text": "Women"}
{"answers": {"text": ["91.3", "91.3", "91.3"], "answer_start": [769, 769, 769]}, "prediction_text": "94.1%"}
{"answers": {"text": ["40%", "40%", "40%", "40"], "answer_start": [145, 145, 145, 145]}, "prediction_text": "40%"}
{"answers": {"text": ["about 3.5 billion people", "3.5 billion", "50%", "3.5 billion"], "answer_start": [557, 563, 523, 563]}, "prediction_text": "3.5 billion."}
{"answers": {"text": ["$759,900", "$759,900", "$759,900"], "answer_start": [1371, 1371, 1371]}, "prediction_text": "$759,900"}
{"answers": {"text": ["the methodology used", "basis of the methodology used", "\"silly argument\" and \"a non-issue . . . a diversion.\"", "basis of the methodology"], "answer_start": [1579, 1570, 2024, 1570]}, "prediction_text": "The Oxfam report."}
{"answers": {"text": ["a diversion", "the basis of the methodology used", "there are more poor people in the United States and Western Europe than in China"], "answer_start": [2064, 1566, 1706]}, "prediction_text": "The Oxfam and Credit Suisse findings are being doubted on the basis of the methodology used."}
{"answers": {"text": ["40%", "40%", "40%", "40"], "answer_start": [145, 145, 145, 145]}, "prediction_text": "40%"}
{"answers": {"text": ["financial assets", "financial assets", "possess more financial assets than the lowest 48 nations combined.", "financial assets"], "answer_start": [235, 235, 222, 235]}, "prediction_text": "More financial assets."}
{"answers": {"text": ["nearly $41 trillion", "$41 trillion", "the \"10 million dollar millionaires\" grew to nearly $41 trillion in 2008", "$41 trillion"], "answer_start": [357, 364, 312, 364]}, "prediction_text": "$41 trillion."}
{"answers": {"text": ["half", "more than half", "own more than half of the global wealth by 2016", "more than half"], "answer_start": [949, 939, 935, 939]}, "prediction_text": "46%"}
{"answers": {"text": ["greater tendency to take on debts", "greater tendency to take on debts", "greater tendency to take on debts"], "answer_start": [1797, 1797, 1797]}, "prediction_text": "A greater tendency to take on debts."}
{"answers": {"text": ["400", "400", "400"], "answer_start": [32, 32, 32]}, "prediction_text": "Half."}
{"answers": {"text": ["New York Times", "New York Times", "New York Times"], "answer_start": [127, 127, 127]}, "prediction_text": "New York Times"}
{"answers": {"text": ["Inherited wealth", "Inherited wealth", "Inherited wealth"], "answer_start": [253, 253, 253]}, "prediction_text": "Head start."}
{"answers": {"text": ["grew up in substantial privilege", "substantial privilege", "substantial privilege"], "answer_start": [489, 500, 500]}, "prediction_text": "Inherited wealth."}
{"answers": {"text": ["wealth", "wealth", "wealth"], "answer_start": [65, 65, 65]}, "prediction_text": "wealth"}
{"answers": {"text": ["richest 1 percent", "richest 1 percent", "richest 1 percent"], "answer_start": [165, 165, 165]}, "prediction_text": "The \"richest 1 percent\" in the United States."}
{"answers": {"text": ["Inherited wealth", "Inherited wealth", "Inherited wealth"], "answer_start": [253, 253, 253]}, "prediction_text": "Inherited wealth."}
{"answers": {"text": ["over 60 percent", "over 60 percent", "over 60 percent"], "answer_start": [435, 435, 435]}, "prediction_text": "60%"}
{"answers": {"text": ["Institute for Policy Studies", "Institute for Policy Studies", "PolitiFact"], "answer_start": [404, 404, 13]}, "prediction_text": "Institute for Policy Studies"}
{"answers": {"text": ["Neoclassical economics", "Neoclassical economics", "Neoclassical economics"], "answer_start": [0, 0, 0]}, "prediction_text": "Neoclassical economics"}
{"answers": {"text": ["differences in value added by labor, capital and land", "differences in value added by labor, capital and land", "differences in value added by different classifications of workers"], "answer_start": [88, 88, 186]}, "prediction_text": "Labor, capital, land."}
{"answers": {"text": ["different classifications of workers", "differences in value added by different classifications of workers", "differences in value added by different classifications of workers"], "answer_start": [216, 186, 186]}, "prediction_text": "Value added by labor."}
{"answers": {"text": ["productivity gap", "productivity gap", "productivity gap"], "answer_start": [467, 467, 467]}, "prediction_text": "Marginal value added."}
{"answers": {"text": ["marginal value added of each economic actor", "differences in value", "marginal value added of each economic actor"], "answer_start": [315, 186, 315]}, "prediction_text": "Marginal value added."}
{"answers": {"text": ["differences in value added by labor, capital and land", "differences in value added by labor, capital and land", "differences in value added by labor, capital and land"], "answer_start": [88, 88, 88]}, "prediction_text": "differences in value added by labor, capital, and land."}
{"answers": {"text": ["value added by different classifications of workers", "value added by labor, capital and land", "value added"], "answer_start": [201, 103, 201]}, "prediction_text": "Value added by labor."}
{"answers": {"text": ["wages and profits", "wages and profits", "wages and profits"], "answer_start": [275, 275, 275]}, "prediction_text": "Wages and profits."}
{"answers": {"text": ["worker, capitalist/business owner, landlord", "worker, capitalist/business owner, landlord", "worker, capitalist/business owner, landlord"], "answer_start": [360, 360, 360]}, "prediction_text": "Workers, capitalists/business owners, landlords."}
{"answers": {"text": ["productivity gap between highly-paid professions and lower-paid professions", "productivity gap", "productivity gap"], "answer_start": [467, 467, 467]}, "prediction_text": "productivity gap"}
{"answers": {"text": ["reduce costs and maximize profits", "substitution of capital equipment for labor", "pressure to reduce costs and maximize profits"], "answer_start": [137, 431, 125]}, "prediction_text": "Competitive pressure."}
{"answers": {"text": ["less workers are required", "raises the productivity of each worker,"], "answer_start": [262, 506]}, "prediction_text": "Stagnant wages."}
{"answers": {"text": ["increasing unemployment", "increasing unemployment", "increasing unemployment"], "answer_start": [321, 321, 321]}, "prediction_text": "unemployment (the \"reserve army of labour\")"}
{"answers": {"text": ["rising levels of property income", "rising levels of property income", "downward pressure on wages"], "answer_start": [629, 629, 399]}, "prediction_text": "Stagnant wages."}
{"answers": {"text": ["labor inputs", "labor inputs (workers)", "labor inputs"], "answer_start": [84, 84, 84]}, "prediction_text": "workers"}
{"answers": {"text": ["reduce costs and maximize profits", "to reduce costs and maximize profits", "raises the productivity of each worker"], "answer_start": [137, 134, 506]}, "prediction_text": "To reduce costs and maximize profits."}
{"answers": {"text": ["substitute capital equipment", "increasingly substitute capital equipment for labor inputs", "pressure to reduce costs and maximize profits"], "answer_start": [51, 38, 125]}, "prediction_text": "substitution of capital equipment for labor."}
{"answers": {"text": ["productivity", "organic composition of capital", "productivity"], "answer_start": [517, 217, 517]}, "prediction_text": "productivity"}
{"answers": {"text": ["stagnant", "stagnant wages", "stagnant"], "answer_start": [585, 585, 585]}, "prediction_text": "Stagnant wages."}
{"answers": {"text": ["workers wages", "workers wages", "wages"], "answer_start": [131, 131, 301]}, "prediction_text": "Wages."}
{"answers": {"text": ["supply and demand", "law of supply and demand", "supply and demand"], "answer_start": [433, 426, 433]}, "prediction_text": "Law of supply and demand."}
{"answers": {"text": ["business is chronically understaffed", "their business is chronically understaffed", "chronically understaffed"], "answer_start": [904, 898, 916]}, "prediction_text": "Understaffed."}
{"answers": {"text": ["offering a higher wage", "by offering a higher wage", "offering a higher wage the best of their labor"], "answer_start": [1000, 997, 1000]}, "prediction_text": "Offer a higher wage."}
{"answers": {"text": ["unfair", "unfair", "unfair"], "answer_start": [834, 834, 834]}, "prediction_text": "Unfair."}
{"answers": {"text": ["the market", "market", "market"], "answer_start": [226, 230, 230]}, "prediction_text": "Professional and labor organizations."}
{"answers": {"text": ["prices", "function of market price of skill", "prices"], "answer_start": [268, 330, 268]}, "prediction_text": "prices for any other good."}
{"answers": {"text": ["wages", "wages", "wages"], "answer_start": [301, 301, 301]}, "prediction_text": "Wages."}
{"answers": {"text": ["markets", "markets", "markets"], "answer_start": [595, 595, 595]}, "prediction_text": "Markets."}
{"answers": {"text": ["unfair", "high levels of inequality", "high levels of inequality"], "answer_start": [834, 772, 772]}, "prediction_text": "High levels of inequality."}
{"answers": {"text": ["Competition amongst workers", "high demand", "competition between employers for employees"], "answer_start": [319, 558, 628]}, "prediction_text": "Competition."}
{"answers": {"text": ["low demand", "high supply", "low wage"], "answer_start": [126, 75, 155]}, "prediction_text": "low wage"}
{"answers": {"text": ["high wages", "high wages", "high wages"], "answer_start": [587, 587, 587]}, "prediction_text": "High wages."}
{"answers": {"text": ["collective bargaining, political influence, or corruption", "collective bargaining, political influence, or corruption", "collective bargaining, political influence, or corruption"], "answer_start": [1149, 1149, 1149]}, "prediction_text": "Collective bargaining."}
{"answers": {"text": ["Professional and labor organizations", "Professional and labor organizations", "Professional and labor organizations"], "answer_start": [970, 970, 970]}, "prediction_text": "Employers"}
{"answers": {"text": ["low wage", "competition", "low wage"], "answer_start": [155, 194, 155]}, "prediction_text": "Low wage."}
{"answers": {"text": ["competition between workers", "competition", "competition"], "answer_start": [194, 194, 194]}, "prediction_text": "Competition between workers."}
{"answers": {"text": ["expendable nature of the worker", "(high supply) competing for a job that few require (low demand)", "the expendable nature of the worker in relation to his or her particular job"], "answer_start": [384, 74, 380]}, "prediction_text": "Expendable nature of the worker."}
{"answers": {"text": ["high", "high wages", "high"], "answer_start": [587, 587, 587]}, "prediction_text": "High wages."}
{"answers": {"text": ["employers", "employers", "employers"], "answer_start": [831, 831, 831]}, "prediction_text": "employers"}
{"answers": {"text": ["entrepreneurship rates", "entrepreneurship rates", "entrepreneurship rates"], "answer_start": [64, 64, 64]}, "prediction_text": "Self-employment."}
{"answers": {"text": ["Necessity-based entrepreneurship", "Necessity-based entrepreneurship", "Necessity-based entrepreneurship"], "answer_start": [203, 203, 203]}, "prediction_text": "Push motivations."}
{"answers": {"text": ["push", "\"push\" motivations", "\"push\""], "answer_start": [305, 304, 304]}, "prediction_text": "Necessity-based."}
{"answers": {"text": ["pull", "\"pull\"", "\"pull\""], "answer_start": [416, 415, 415]}, "prediction_text": "Vocation and more likely to involve the pursue of new products, services, or underserved market needs."}
{"answers": {"text": ["opportunity-based entrepreneurship", "opportunity-based entrepreneurship", "opportunity-based"], "answer_start": [333, 333, 333]}, "prediction_text": "Necessity-based entrepreneurship."}
{"answers": {"text": ["higher economic inequality", "higher economic inequality", "higher economic inequality"], "answer_start": [19, 19, 19]}, "prediction_text": "Necessity-based entrepreneurship."}
{"answers": {"text": ["necessity", "necessity rather than opportunity"], "answer_start": [168, 168]}, "prediction_text": "necessity"}
{"answers": {"text": ["Necessity-based", "Necessity-based entrepreneurship", "Necessity-based"], "answer_start": [203, 203, 203]}, "prediction_text": "Necessity-based entrepreneurship."}
{"answers": {"text": ["achievement-oriented", "achievement-oriented motivations (\"pull\")", "achievement-oriented motivations"], "answer_start": [381, 381, 381]}, "prediction_text": "Achievement-oriented motivations."}
{"answers": {"text": ["positive", "positive", "more positive"], "answer_start": [705, 705, 700]}, "prediction_text": "Positive."}
{"answers": {"text": ["progressive tax", "progressive tax", "progressive"], "answer_start": [103, 103, 103]}, "prediction_text": "Progressive tax."}
{"answers": {"text": ["top tax rate", "progressive tax", "top"], "answer_start": [244, 203, 244]}, "prediction_text": "Tax rate."}
{"answers": {"text": ["social spending", "social spending", "steeper tax progressivity applied to social spending"], "answer_start": [502, 502, 465]}, "prediction_text": "Steeper tax progressivity."}
{"answers": {"text": ["tax system", "progressive tax system", "progressive tax"], "answer_start": [89, 203, 103]}, "prediction_text": "Progressive tax system."}
{"answers": {"text": ["the tax rate", "tax rate", "tax rate"], "answer_start": [137, 141, 141]}, "prediction_text": "The taxable base amount."}
{"answers": {"text": ["level of the top tax rate", "level of the top tax rate", "top tax rate"], "answer_start": [231, 231, 244]}, "prediction_text": "The level of the top tax rate."}
{"answers": {"text": ["steeper tax", "social spending", "steeper tax progressivity"], "answer_start": [465, 502, 465]}, "prediction_text": "Steeper tax progressivity."}
{"answers": {"text": ["the Gini index", "Gini", "Gini"], "answer_start": [671, 613, 613]}, "prediction_text": "Gini index"}
{"answers": {"text": ["access to education", "Education", "education"], "answer_start": [79, 100, 214]}, "prediction_text": "Education."}
{"answers": {"text": ["optional education", "education", "education"], "answer_start": [409, 374, 374]}, "prediction_text": "Education."}
{"answers": {"text": ["lower wages", "lower wages", "lower wages"], "answer_start": [452, 452, 452]}, "prediction_text": "Lower incomes."}
{"answers": {"text": ["poor", "poor", "poor"], "answer_start": [719, 719, 719]}, "prediction_text": "The poor."}
{"answers": {"text": ["savings and investment", "education,", "education"], "answer_start": [578, 374, 418]}, "prediction_text": "Education."}
{"answers": {"text": ["access to education", "access to education", "access to education"], "answer_start": [79, 79, 79]}, "prediction_text": "Education."}
{"answers": {"text": ["high wages", "high wages", "high wages"], "answer_start": [183, 183, 183]}, "prediction_text": "High wages."}
{"answers": {"text": ["lower", "lower wages", "lower wages"], "answer_start": [452, 452, 452]}, "prediction_text": "Lower wages."}
{"answers": {"text": ["lower incomes", "lower incomes", "lower incomes"], "answer_start": [538, 538, 538]}, "prediction_text": "lower incomes"}
{"answers": {"text": ["education", "education", "education"], "answer_start": [614, 614, 614]}, "prediction_text": "Education."}
{"answers": {"text": ["increasing access to education", "increasing access to education", "increasing access to education"], "answer_start": [348, 348, 348]}, "prediction_text": "Education."}
{"answers": {"text": ["$105 billion", "$105 billion", "$105 billion"], "answer_start": [505, 505, 505]}, "prediction_text": "$105 billion."}
{"answers": {"text": ["boom-and-bust cycles", "boom-and-bust cycles", "boom-and-bust cycles"], "answer_start": [244, 244, 244]}, "prediction_text": "The wealth gap."}
{"answers": {"text": ["Standard & Poor", "Standard & Poor", "economists with the Standard & Poor's rating agency"], "answer_start": [29, 29, 9]}, "prediction_text": "S&P"}
{"answers": {"text": ["2014", "2014", "2014"], "answer_start": [3, 3, 3]}, "prediction_text": "2014"}
{"answers": {"text": ["2008-2009", "2008-2009"], "answer_start": [198, 198]}, "prediction_text": "2008-2009 recession"}
{"answers": {"text": ["increasing access to education", "increasing access to education", "increasing access to education"], "answer_start": [348, 348, 348]}, "prediction_text": "Education."}
{"answers": {"text": ["$105 billion", "$105 billion", "$105 billion"], "answer_start": [505, 505, 505]}, "prediction_text": "$105 billion"}
{"answers": {"text": ["boom-and-bust cycles", "boom-and-bust cycles", "boom-and-bust cycles"], "answer_start": [244, 244, 244]}, "prediction_text": "Boom-and-bust cycles."}
{"answers": {"text": ["1910\u20131940", "1910\u20131940", "1910\u20131940"], "answer_start": [52, 52, 52]}, "prediction_text": "1910-1940"}
{"answers": {"text": ["increase", "an increase", "increase"], "answer_start": [76, 73, 76]}, "prediction_text": "Decreased skilled workers."}
{"answers": {"text": ["decrease", "decrease in the price of skilled labor", "decrease"], "answer_start": [120, 120, 120]}, "prediction_text": "Decreased wages."}
{"answers": {"text": ["gender inequality in education", "gender inequality in education", "education"], "answer_start": [792, 792, 733]}, "prediction_text": "Low economic growth."}
{"answers": {"text": ["period of compression", "decrease in wages", "decrease in wages"], "answer_start": [459, 432, 432]}, "prediction_text": "Skilled workers."}
{"answers": {"text": ["from 1910\u20131940", "1910\u20131940", "1910\u20131940"], "answer_start": [47, 52, 52]}, "prediction_text": "1910-1940"}
{"answers": {"text": ["a decrease in the price of skilled labor", "decrease in the price of skilled labor", "decrease in the price of skilled labor"], "answer_start": [118, 120, 120]}, "prediction_text": "Decrease in the price of skilled labor."}
{"answers": {"text": ["designed to equip students with necessary skill sets to be able to perform at work", "designed to equip students with necessary skill sets to be able to perform at work", "designed to equip students with necessary skill sets to be able to perform at work"], "answer_start": [204, 204, 204]}, "prediction_text": "The education during the high school education movement differed from the subsequent high school education."}
{"answers": {"text": ["Education", "Education", "Education"], "answer_start": [545, 545, 545]}, "prediction_text": "Education"}
{"answers": {"text": ["gender inequality in education", "low economic growth", "continued gender inequality in education"], "answer_start": [712, 757, 782]}, "prediction_text": "Gender inequality in education."}
{"answers": {"text": ["unions", "union membership", "unions"], "answer_start": [338, 145, 338]}, "prediction_text": "unions"}
{"answers": {"text": ["continental European countries", "continental European countries", "continental European"], "answer_start": [961, 961, 961]}, "prediction_text": "Continental European countries."}
{"answers": {"text": ["little", "little support", "little"], "answer_start": [717, 717, 717]}, "prediction_text": "Little."}
{"answers": {"text": ["continental European liberalism", "European liberalism", "continental European liberalism"], "answer_start": [299, 311, 299]}, "prediction_text": "Anglo-American liberal policies"}
{"answers": {"text": ["economic inequality", "economic inequality", "economic inequality"], "answer_start": [186, 186, 186]}, "prediction_text": "Decline of union membership."}
{"answers": {"text": ["social exclusion", "social exclusion", "social exclusion"], "answer_start": [459, 459, 459]}, "prediction_text": "Social exclusion."}
{"answers": {"text": ["CEPR", "CEPR", "CEPR"], "answer_start": [44, 44, 44]}, "prediction_text": "CEPR"}
{"answers": {"text": ["little", "little support", "little"], "answer_start": [717, 717, 717]}, "prediction_text": "Little."}
{"answers": {"text": ["lower", "lower level", "lower"], "answer_start": [915, 915, 915]}, "prediction_text": "Lower."}
{"answers": {"text": ["Scandinavia", "Scandinavia", "Scandinavia"], "answer_start": [421, 421, 421]}, "prediction_text": "Scandinavia"}
{"answers": {"text": ["high inequality", "high inequality", "high inequality"], "answer_start": [555, 555, 555]}, "prediction_text": "low inequality"}
{"answers": {"text": ["decline of organized labor", "decline of organized labor", "decline of organized labor"], "answer_start": [77, 77, 77]}, "prediction_text": "Decline of organized labor."}
{"answers": {"text": ["technological changes and globalization", "decline of organized labor", "technological changes and globalization"], "answer_start": [193, 77, 193]}, "prediction_text": "Labor movements."}
{"answers": {"text": ["Sociologist", "Sociologist", "Sociologist"], "answer_start": [0, 0, 0]}, "prediction_text": "Sociologist"}
{"answers": {"text": ["University of Washington", "University of Washington", "University of Washington"], "answer_start": [35, 35, 35]}, "prediction_text": "University of Washington"}
{"answers": {"text": ["decline of organized labor", "decline of organized labor", "decline of organized labor"], "answer_start": [77, 77, 77]}, "prediction_text": "Organized labor."}
{"answers": {"text": ["high", "high rates", "high"], "answer_start": [377, 377, 377]}, "prediction_text": "Low."}
{"answers": {"text": ["weak labor movements", "weak labor movements", "weak labor movements"], "answer_start": [594, 594, 594]}, "prediction_text": "weak labor movements"}
{"answers": {"text": ["reduced wages", "reduced wages", "reduced wages"], "answer_start": [187, 187, 187]}, "prediction_text": "Increased wages."}
{"answers": {"text": ["increased wages", "increased wages", "increased wages"], "answer_start": [289, 289, 289]}, "prediction_text": "Increased wages."}
{"answers": {"text": ["technological innovation", "technological innovation", "technological innovation"], "answer_start": [734, 734, 734]}, "prediction_text": "technological innovation"}
{"answers": {"text": ["machine labor", "machine labor", "machine labor"], "answer_start": [1298, 1298, 1298]}, "prediction_text": "Machine labor."}
{"answers": {"text": ["global", "global", "global"], "answer_start": [58, 58, 58]}, "prediction_text": "Domestic scale."}
{"answers": {"text": ["workers in the poor countries", "low-skilled workers in the poor countries", "poor"], "answer_start": [251, 239, 266]}, "prediction_text": "low-skilled workers"}
{"answers": {"text": ["trade liberalisation", "trade liberalisation", "trade liberalisation"], "answer_start": [350, 350, 350]}, "prediction_text": "Increased trade with poor countries."}
{"answers": {"text": ["minor", "minor", "minor"], "answer_start": [689, 689, 689]}, "prediction_text": "minor"}
{"answers": {"text": ["machine labor", "machine labor", "machine labor"], "answer_start": [1298, 1298, 1298]}, "prediction_text": "Machine labor."}
{"answers": {"text": ["53%", "53%", "53%"], "answer_start": [705, 705, 705]}, "prediction_text": "53%"}
{"answers": {"text": ["-40%", "-40%", "-40%"], "answer_start": [724, 724, 724]}, "prediction_text": "-40%"}
{"answers": {"text": ["less willing to travel or relocate", "women not taking jobs due to marriage or pregnancy", "not taking jobs due to marriage or pregnancy"], "answer_start": [266, 392, 398]}, "prediction_text": "Marriage or pregnancy."}
{"answers": {"text": ["males", "males", "males"], "answer_start": [57, 57, 57]}, "prediction_text": "Women."}
{"answers": {"text": ["Gender", "ranges from 53% in Botswana to -40% in Bahrain", "Gender"], "answer_start": [30, 693, 30]}, "prediction_text": "Gender pay gap."}
{"answers": {"text": ["males in the labor market", "males", "males"], "answer_start": [57, 57, 57]}, "prediction_text": "Males"}
{"answers": {"text": ["women", "women", "women"], "answer_start": [166, 166, 166]}, "prediction_text": "Women"}
{"answers": {"text": ["Thomas Sowell", "Thomas Sowell", "Thomas Sowell"], "answer_start": [302, 302, 302]}, "prediction_text": "Thomas Sowell"}
{"answers": {"text": ["a difference", "difference", "difference in earnings"], "answer_start": [611, 613, 613]}, "prediction_text": "Income gap."}
{"answers": {"text": ["social welfare", "social welfare", "social welfare"], "answer_start": [451, 451, 451]}, "prediction_text": "Social welfare programs."}
{"answers": {"text": ["relatively equal", "relatively equal distributions of wealth", "low"], "answer_start": [189, 189, 158]}, "prediction_text": "Low."}
{"answers": {"text": ["more capital", "more capital", "more capital"], "answer_start": [266, 266, 266]}, "prediction_text": "Capital."}
{"answers": {"text": ["redistribution mechanisms", "redistribution mechanisms such as social welfare programs", "redistribution mechanisms"], "answer_start": [417, 417, 417]}, "prediction_text": "Capital accumulation."}
{"answers": {"text": ["Economist", "Economist", "Economist"], "answer_start": [0, 0, 0]}, "prediction_text": "Economist"}
{"answers": {"text": ["levels of economic inequality", "economic inequality", "economic inequality"], "answer_start": [36, 46, 46]}, "prediction_text": "Equality of wealth."}
{"answers": {"text": ["more capital", "more capital", "more capital"], "answer_start": [266, 266, 266]}, "prediction_text": "Capital."}
{"answers": {"text": ["more wealth", "more wealth and income", "wealth and income"], "answer_start": [329, 329, 334]}, "prediction_text": "More wealth and income."}
{"answers": {"text": ["lower levels of inequality", "lower levels of inequality", "lower levels of inequality"], "answer_start": [514, 514, 514]}, "prediction_text": "More developed countries."}
{"answers": {"text": ["1910 to 1940", "1910 to 1940", "1910 to 1940"], "answer_start": [543, 543, 543]}, "prediction_text": "1910-1940"}
{"answers": {"text": ["1970s", "1970s", "1970s"], "answer_start": [676, 676, 676]}, "prediction_text": "1970s"}
{"answers": {"text": ["service", "service"], "answer_start": [872, 872]}, "prediction_text": "Manufacturing sector."}
{"answers": {"text": ["manufacturing", "manufacturing"], "answer_start": [844, 844]}, "prediction_text": "Manufacturing sector."}
{"answers": {"text": ["Kuznets", "Kuznets", "Kuznets"], "answer_start": [66, 66, 66]}, "prediction_text": "Kuznets"}
{"answers": {"text": ["Kuznets curve", "Kuznets curve", "Kuznets curve"], "answer_start": [178, 178, 178]}, "prediction_text": "Kuznets curve"}
{"answers": {"text": ["very weak", "very weak", "very weak"], "answer_start": [348, 348, 348]}, "prediction_text": "Weak."}
{"answers": {"text": ["eventually decrease", "eventually decrease", "decrease"], "answer_start": [411, 411, 422]}, "prediction_text": "Income inequality will eventually decrease given time."}
{"answers": {"text": ["effect", "in effect", "effect"], "answer_start": [980, 977, 980]}, "prediction_text": "Multiple Kuznets' cycles."}
{"answers": {"text": ["Wealth concentration", "Wealth concentration", "Wealth concentration"], "answer_start": [0, 0, 0]}, "prediction_text": "Wealth concentration."}
{"answers": {"text": ["means to invest", "the means to invest in new sources of creating wealth or to otherwise leverage the accumulation of wealth", "means to invest in new sources of creating wealth"], "answer_start": [263, 259, 263]}, "prediction_text": "Leverage."}
{"answers": {"text": ["greater return of capital", "wealth condensation", "wealth condensation"], "answer_start": [648, 423, 423]}, "prediction_text": "Capital (r)"}
{"answers": {"text": ["larger fortunes", "wealth condensation", "wealth"], "answer_start": [713, 423, 247]}, "prediction_text": "Greater return of capital (r)"}
{"answers": {"text": ["the possession of already-wealthy individuals", "in the possession of already-wealthy individuals or entities", "already-wealthy individuals"], "answer_start": [139, 136, 157]}, "prediction_text": "In the possession of already-wealthy individuals or entities."}
{"answers": {"text": ["those who already hold wealth", "those who already hold wealth", "those who already hold wealth"], "answer_start": [224, 224, 224]}, "prediction_text": "The wealthy."}
{"answers": {"text": ["wealth condensation", "wealth condensation", "wealth condensation"], "answer_start": [423, 423, 423]}, "prediction_text": "wealth condensation"}
{"answers": {"text": ["Thomas Piketty", "Thomas Piketty", "Thomas Piketty"], "answer_start": [521, 521, 521]}, "prediction_text": "Thomas Piketty"}
{"answers": {"text": ["higher returns", "higher returns", "higher returns"], "answer_start": [738, 738, 738]}, "prediction_text": "Higher returns [pp. 384 Table 12.2, U.S. university endowment size vs. real annual rate of return]"}
{"answers": {"text": ["market", "market forces", "market"], "answer_start": [98, 98, 98]}, "prediction_text": "market forces"}
{"answers": {"text": ["Economist", "Economist", "Economist"], "answer_start": [0, 0, 0]}, "prediction_text": "Economist"}
{"answers": {"text": ["rare and desired", "rare and desired skills", "rare and desired skills"], "answer_start": [284, 284, 284]}, "prediction_text": "rare and desired skills"}
{"answers": {"text": ["political power generated by wealth", "political power", "political power"], "answer_start": [588, 588, 588]}, "prediction_text": "Political power."}
{"answers": {"text": ["rent-seeking", "rent-seeking"], "answer_start": [740, 740]}, "prediction_text": "Rent-seeking"}
{"answers": {"text": ["inequality", "inequality", "inequality"], "answer_start": [11, 11, 11]}, "prediction_text": "inequality researchers"}
{"answers": {"text": ["human capital is neglected", "a lower level of economic utility in society", "human capital is neglected"], "answer_start": [270, 130, 270]}, "prediction_text": "Human capital."}
{"answers": {"text": ["life expectancy", "life expectancy", "life expectancy"], "answer_start": [394, 394, 394]}, "prediction_text": "Life expectancy."}
{"answers": {"text": ["inequality", "inequality", "inequality"], "answer_start": [11, 11, 11]}, "prediction_text": "Economic utility in society."}
{"answers": {"text": ["life expectancy is lower", "life expectancy is lower", "lower"], "answer_start": [394, 394, 413]}, "prediction_text": "Lower."}
{"answers": {"text": ["2013", "2013", "2013"], "answer_start": [0, 0, 0]}, "prediction_text": "2013"}
{"answers": {"text": ["rising inequality", "rising inequality", "rising inequality"], "answer_start": [62, 62, 62]}, "prediction_text": "Rising inequality."}
{"answers": {"text": ["negative", "negative effect", "negative"], "answer_start": [262, 262, 262]}, "prediction_text": "Distortions."}
{"answers": {"text": ["Unemployment", "Unemployment", "Unemployment"], "answer_start": [318, 318, 318]}, "prediction_text": "Unrest and conflict."}
{"answers": {"text": ["economic", "economic", "economic"], "answer_start": [737, 737, 737]}, "prediction_text": "Economic growth."}
{"answers": {"text": ["British", "British", "British"], "answer_start": [0, 0, 0]}, "prediction_text": "British"}
{"answers": {"text": ["higher", "higher rates", "higher"], "answer_start": [69, 69, 69]}, "prediction_text": "obesity, mental illness, homicides, teenage births, incarceration, child conflict, drug use, social goods."}
{"answers": {"text": ["lower", "lower rates", "lower"], "answer_start": [211, 211, 211]}, "prediction_text": "Lower."}
{"answers": {"text": ["23", "23", "23"], "answer_start": [451, 451, 451]}, "prediction_text": "23"}
{"answers": {"text": ["equality", "equality", "equality"], "answer_start": [638, 638, 638]}, "prediction_text": "inequality"}
{"answers": {"text": ["better health and longer lives", "better health and longer lives", "better health and longer lives"], "answer_start": [128, 128, 128]}, "prediction_text": "Better health and longer lives."}
{"answers": {"text": ["poorer countries", "poorer countries", "poorer countries"], "answer_start": [222, 222, 222]}, "prediction_text": "Poor countries."}
{"answers": {"text": ["life expectancy", "life expectancy", "life expectancy"], "answer_start": [246, 246, 246]}, "prediction_text": "Life expectancy."}
{"answers": {"text": ["Americans", "Americans", "Americans"], "answer_start": [452, 452, 452]}, "prediction_text": "Americans"}
{"answers": {"text": ["more equally", "more equally", "more equally distributed"], "answer_start": [675, 675, 675]}, "prediction_text": "More equally distributed."}
{"answers": {"text": ["income inequality", "income inequality", "income inequality"], "answer_start": [102, 102, 102]}, "prediction_text": "Income inequality."}
{"answers": {"text": ["authors Richard Wilkinson and Kate Pickett", "Richard Wilkinson and Kate Pickett", "Richard Wilkinson and Kate Pickett"], "answer_start": [190, 198, 198]}, "prediction_text": "Richard Wilkinson and Kate Pickett"}
{"answers": {"text": ["nine", "nine factors", "nine"], "answer_start": [176, 176, 176]}, "prediction_text": "9"}
{"answers": {"text": ["among states in the US with larger income inequalities", "countries with bigger income inequalities", "countries with bigger income inequalities"], "answer_start": [342, 282, 282]}, "prediction_text": "States in the US with larger income inequalities."}
{"answers": {"text": ["greater equality", "greater equality", "greater equality but not per capita income"], "answer_start": [560, 560, 560]}, "prediction_text": "Income inequality."}
{"answers": {"text": ["inequality", "inequality", "inequality"], "answer_start": [53, 53, 53]}, "prediction_text": "inequality"}
{"answers": {"text": ["homicides", "homicides"], "answer_start": [140, 158]}, "prediction_text": "Homicides."}
{"answers": {"text": ["fifty", "over fifty", "fifty"], "answer_start": [258, 253, 258]}, "prediction_text": "Fifty studies."}
{"answers": {"text": ["differences in the amount of inequality", "differences in the amount of inequality", "inequality"], "answer_start": [731, 731, 760]}, "prediction_text": "differences in the amount of inequality in each province or state."}
{"answers": {"text": ["tenfold", "tenfold", "tenfold"], "answer_start": [581, 581, 581]}, "prediction_text": "Tenfold."}
{"answers": {"text": ["the greatest good", "greatest good", "good"], "answer_start": [47, 51, 60]}, "prediction_text": "The greatest number of people."}
{"answers": {"text": ["distributive efficiency", "distributive efficiency", "\"distributive efficiency\""], "answer_start": [266, 266, 265]}, "prediction_text": "distributive efficiency."}
{"answers": {"text": ["a great deal of utility", "a great deal of utility", "basic necessities"], "answer_start": [465, 465, 513]}, "prediction_text": "basic necessities."}
{"answers": {"text": ["decreases", "decreases", "decreases"], "answer_start": [783, 783, 783]}, "prediction_text": "Decreases."}
{"answers": {"text": ["higher aggregate utility", "population-wide satisfaction and happiness", "satisfaction and happiness"], "answer_start": [925, 1053, 1069]}, "prediction_text": "Higher aggregate utility."}
{"answers": {"text": ["consumption", "consumption", "consumption"], "answer_start": [87, 87, 87]}, "prediction_text": "Consumption."}
{"answers": {"text": ["libertarian", "libertarian", "libertarian"], "answer_start": [261, 261, 261]}, "prediction_text": "Libertarian"}
{"answers": {"text": ["2001", "2001", "2001"], "answer_start": [583, 583, 583]}, "prediction_text": "2001"}
{"answers": {"text": ["Thomas B. Edsall", "Thomas B. Edsall", "Thomas B. Edsall"], "answer_start": [687, 687, 687]}, "prediction_text": "Thomas B. Edsall"}
{"answers": {"text": ["journalist", "journalist", "journalist"], "answer_start": [676, 676, 676]}, "prediction_text": "Journalist"}
{"answers": {"text": ["economist", "economist", "economist"], "answer_start": [16, 16, 16]}, "prediction_text": "Economist"}
{"answers": {"text": ["systematic economic inequalities", "systematic economic inequalities", "systematic economic inequalities"], "answer_start": [54, 54, 54]}, "prediction_text": "Systematic economic inequalities."}
{"answers": {"text": ["the Financial crisis of 2007\u201308", "Financial crisis of 2007\u201308", "Financial crisis of 2007\u201308"], "answer_start": [253, 257, 257]}, "prediction_text": "Financial crisis of 2007-08"}
{"answers": {"text": ["easier credit", "easier credit to the lower and middle income earners", "easier credit to the lower and middle income earners"], "answer_start": [420, 420, 420]}, "prediction_text": "Easy credit to lower and middle income earners."}
{"answers": {"text": ["easier credit", "easier credit", "easier credit"], "answer_start": [507, 507, 507]}, "prediction_text": "Unsustainable monetary stimulation."}
{"answers": {"text": ["inequality in wealth and income", "inequality in wealth and income", "wealth and income"], "answer_start": [53, 53, 67]}, "prediction_text": "inequality in wealth and income."}
{"answers": {"text": ["quality of a country's institutions", "quality of a country's institutions and high levels of education", "quality of a country's institutions and high levels of education"], "answer_start": [257, 257, 257]}, "prediction_text": "quality of institutions"}
{"answers": {"text": ["declines", "declines", "declines"], "answer_start": [448, 448, 448]}, "prediction_text": "Declines."}
{"answers": {"text": ["higher GDP growth", "higher GDP growth", "higher GDP growth"], "answer_start": [629, 629, 629]}, "prediction_text": "Higher GDP growth."}
{"answers": {"text": ["The poor and the middle class", "The poor and the middle class", "poor and the middle class"], "answer_start": [648, 648, 652]}, "prediction_text": "The poor and the middle class."}
{"answers": {"text": ["economists", "economists", "economists"], "answer_start": [13, 13, 13]}, "prediction_text": "Economists."}
{"answers": {"text": ["economic growth", "economic growth", "economic growth"], "answer_start": [97, 97, 97]}, "prediction_text": "Economic growth."}
{"answers": {"text": ["subsequent long-run economic growth", "subsequent long-run economic growth", "long-run economic growth"], "answer_start": [204, 204, 215]}, "prediction_text": "subsequent long-run economic growth."}
{"answers": {"text": ["because it is a waste of resources", "waste of resources", "waste of resources"], "answer_start": [279, 295, 295]}, "prediction_text": "waste resources, generates redistributive pressures, drives people to poverty, constrains liquidity limiting labor mobility, erodes self-esteem."}
{"answers": {"text": ["inequality-associated effects", "inequality-associated effects", "inequality"], "answer_start": [622, 622, 622]}, "prediction_text": "redistributive pressures and subsequent distortions."}
{"answers": {"text": ["evidence", "evidence", "prevent growth"], "answer_start": [36, 36, 113]}, "prediction_text": "Evidence that both global inequality and inequality within countries prevent growth."}
{"answers": {"text": ["by limiting aggregate demand", "limiting aggregate demand", "limiting aggregate demand"], "answer_start": [128, 131, 131]}, "prediction_text": "Limits aggregate demand."}
{"answers": {"text": ["Economist", "Economist", "Economist"], "answer_start": [158, 158, 0]}, "prediction_text": "Branko Milanovic and Joseph Stiglitz."}
{"answers": {"text": ["increasing importance of human capital in development", "increasing importance of human capital in development", "increasing importance of human capital"], "answer_start": [393, 393, 393]}, "prediction_text": "Human capital."}
{"answers": {"text": ["widespread education", "widespread education", "widespread education"], "answer_start": [737, 737, 737]}, "prediction_text": "Education."}
{"answers": {"text": ["1993", "1993", "1993"], "answer_start": [3, 3, 3]}, "prediction_text": "1996"}
{"answers": {"text": ["detrimental", "long lasting detrimental effect", "long lasting detrimental"], "answer_start": [114, 101, 101]}, "prediction_text": "lower level of human capital formation"}
{"answers": {"text": ["channels through which inequality may affect economic growth", "channels through which inequality may affect economic growth", "channels through which inequality may affect economic growth"], "answer_start": [223, 223, 223]}, "prediction_text": "channels through which inequality may affect economic growth."}
{"answers": {"text": ["redistributive taxation", "fertility", "fertility"], "answer_start": [602, 494, 494]}, "prediction_text": "Redistributive taxation."}
{"answers": {"text": ["politically and socially unstable", "politically and socially unstable", "politically and socially unstable"], "answer_start": [930, 930, 930]}, "prediction_text": "Politically and socially unstable."}
{"answers": {"text": ["reduce", "reduce growth", "reduce"], "answer_start": [223, 223, 223]}, "prediction_text": "Reduce growth."}
{"answers": {"text": ["encourage", "encourage growth", "encourage"], "answer_start": [270, 270, 270]}, "prediction_text": "Encourage growth in richer countries."}
{"answers": {"text": ["growth and investment", "growth and investment", "growth and investment"], "answer_start": [128, 128, 128]}, "prediction_text": "growth and investment."}
{"answers": {"text": ["Harvard", "Harvard", "Harvard"], "answer_start": [12, 12, 12]}, "prediction_text": "Harvard"}
{"answers": {"text": ["between 1960 and 2000", "between 1960 and 2000", "between 1960 and 2000"], "answer_start": [336, 336, 336]}, "prediction_text": "1960-2000"}
{"answers": {"text": ["Kuznets curve hypothesis", "Kuznets curve hypothesis", "Kuznets curve hypothesis"], "answer_start": [85, 85, 85]}, "prediction_text": "Kuznets curve hypothesis."}
{"answers": {"text": ["first increases", "inequality first increases", "increases"], "answer_start": [167, 156, 173]}, "prediction_text": "Wars and violent economic and political shocks."}
{"answers": {"text": ["Thomas Piketty", "Thomas Piketty", "Thomas Piketty"], "answer_start": [210, 210, 210]}, "prediction_text": "Thomas Piketty"}
{"answers": {"text": ["Economist", "Economist", "Economist"], "answer_start": [200, 200, 200]}, "prediction_text": "Economist"}
{"answers": {"text": ["wars and \"violent economic and political shocks\"", "violent economic and political shocks", "wars and \"violent economic and political shocks\""], "answer_start": [281, 291, 281]}, "prediction_text": "Wars and \"violent economic and political shocks\""}
{"answers": {"text": ["the 1970s", "1970s", "1970s"], "answer_start": [27, 31, 31]}, "prediction_text": "1955"}
{"answers": {"text": ["reduced consumer demand", "reduced consumer demand", "reduced consumer demand"], "answer_start": [253, 253, 253]}, "prediction_text": "reduced consumer demand."}
{"answers": {"text": ["risen with increased income inequality", "risen", "risen"], "answer_start": [328, 328, 328]}, "prediction_text": "Increased."}
{"answers": {"text": ["several years", "several years", "several years"], "answer_start": [603, 603, 603]}, "prediction_text": "Several years."}
{"answers": {"text": ["more equality in the income distribution", "more equality", "equality in the income distribution"], "answer_start": [980, 980, 985]}, "prediction_text": "More equality in the income distribution."}
{"answers": {"text": ["special efforts", "special efforts", "special efforts"], "answer_start": [243, 243, 243]}, "prediction_text": "Special efforts."}
{"answers": {"text": ["existing level of inequality", "existing level of inequality", "existing level of inequality"], "answer_start": [459, 459, 459]}, "prediction_text": "The growth elasticity of poverty depends on the existing level of inequality."}
{"answers": {"text": ["reduction", "halve poverty", "halve poverty"], "answer_start": [726, 616, 616]}, "prediction_text": "60 years."}
{"answers": {"text": ["the United Nations", "United Nations", "United Nations"], "answer_start": [778, 782, 782]}, "prediction_text": "United Nations Secretary General"}
{"answers": {"text": ["reducing poverty", "reducing poverty", "reducing poverty"], "answer_start": [884, 884, 884]}, "prediction_text": "Poverty reduction."}
{"answers": {"text": ["much land and housing", "land and housing", "land and housing"], "answer_start": [38, 43, 43]}, "prediction_text": "Land and housing."}
{"answers": {"text": ["through various associations and other arrangements", "through various associations and other arrangements", "through various associations"], "answer_start": [188, 188, 188]}, "prediction_text": "Associations and other arrangements."}
{"answers": {"text": ["extra-legal", "extra-legal", "extra-legal"], "answer_start": [253, 253, 253]}, "prediction_text": "Extra-legal property."}
{"answers": {"text": ["200", "200", "200"], "answer_start": [383, 383, 383]}, "prediction_text": "200 steps."}
{"answers": {"text": ["government land", "government land", "government land"], "answer_start": [424, 424, 424]}, "prediction_text": "200 steps."}
{"answers": {"text": ["a shortage of affordable housing", "affordable housing", "affordable housing"], "answer_start": [83, 97, 97]}, "prediction_text": "affordable housing"}
{"answers": {"text": ["quality rental units", "quality rental units", "quality rental units"], "answer_start": [236, 236, 236]}, "prediction_text": "Quality rental units."}
{"answers": {"text": ["demand for higher quality housing increased", "demand for higher quality housing increased", "demand for higher quality housing increased"], "answer_start": [274, 274, 274]}, "prediction_text": "Lower income families."}
{"answers": {"text": ["residents willing to pay higher market rate for housing", "gentrification of older neighbourhoods", "new residents willing to pay higher market rate"], "answer_start": [470, 344, 466]}, "prediction_text": "Gentrification."}
{"answers": {"text": ["ad valorem property tax policy", "ad valorem property tax policy", "property tax policy"], "answer_start": [583, 583, 594]}, "prediction_text": "Landlords found new residents willing to pay higher market rate for housing."}
{"answers": {"text": ["by everyone", "by everyone"], "answer_start": [61, 61]}, "prediction_text": "Housing, pensions, education, health care."}
{"answers": {"text": ["their finances", "finances", "finances"], "answer_start": [323, 329, 329]}, "prediction_text": "finances"}
{"answers": {"text": ["aspirational consumption", "aspirational consumption", "aspirational consumption"], "answer_start": [349, 349, 349]}, "prediction_text": "Aspirational consumption."}
{"answers": {"text": ["taking on debt", "taking on debt", "taking on debt"], "answer_start": [555, 555, 555]}, "prediction_text": "Debt"}
{"answers": {"text": ["economic instability", "greater inequality and potential economic instability", "greater inequality and potential economic instability"], "answer_start": [629, 596, 596]}, "prediction_text": "Economic instability."}
{"answers": {"text": ["created", "created", "created", "created"], "answer_start": [69, 69, 69, 69]}, "prediction_text": "created."}
{"answers": {"text": ["emissions per person", "this multiplier", "environmental degradation", "environmental degradation"], "answer_start": [709, 800, 855, 855]}, "prediction_text": "multiplier"}
{"answers": {"text": ["environmental degradation", "this as well", "environmental degradation", "environmental degradation"], "answer_start": [650, 973, 855, 855]}, "prediction_text": "The current high level of population has a large impact on the amount of environmental degradation."}
{"answers": {"text": ["If (as WWF argued), population levels would start to drop to a sustainable level", "If (as WWF argued), population levels would start to drop to a sustainable level", "population levels would start to drop to a sustainable level", "population levels would start to drop to a sustainable level"], "answer_start": [987, 987, 1007, 1007]}, "prediction_text": "By addressing population levels."}
{"answers": {"text": ["private ownership of the means of production", "private ownership of the means of production", "private ownership of the means of production"], "answer_start": [59, 59, 59]}, "prediction_text": "Private ownership of the means of production."}
{"answers": {"text": ["a small portion of the population lives off unearned property income", "small portion of the population lives off unearned property income", "small portion of the population lives off unearned property income"], "answer_start": [153, 155, 155]}, "prediction_text": "Unearned property income."}
{"answers": {"text": ["wage or salary", "wage or salary", "wage or salary"], "answer_start": [401, 401, 401]}, "prediction_text": "Wage or salary."}
{"answers": {"text": ["socially", "socially", "socially"], "answer_start": [509, 509, 509]}, "prediction_text": "Socially owned."}
{"answers": {"text": ["reflective", "reflective", "reflective of individual contributions"], "answer_start": [562, 562, 562]}, "prediction_text": "Wage or salary."}
{"answers": {"text": ["Robert Nozick", "Robert Nozick", "Robert Nozick"], "answer_start": [0, 0, 0]}, "prediction_text": "Robert Nozick"}
{"answers": {"text": ["taxation", "taxation", "taxation"], "answer_start": [91, 91, 91]}, "prediction_text": "taxation"}
{"answers": {"text": ["force", "force"], "answer_start": [184, 184]}, "prediction_text": "forceful taking of property."}
{"answers": {"text": ["forceful taking of property", "forceful taking of property", "forceful taking of property"], "answer_start": [276, 276, 276]}, "prediction_text": "forceful taking of property."}
{"answers": {"text": ["when they improve society as a whole", "when they improve society as a whole", "when they improve society as a whole"], "answer_start": [548, 548, 548]}, "prediction_text": "When they improve society as a whole."}
{"answers": {"text": ["capability deprivation", "capability deprivation", "\u201ccapability deprivation\u201d"], "answer_start": [129, 129, 128]}, "prediction_text": "Capability deprivation."}
{"answers": {"text": ["the end itself", "the end itself", "the end itself"], "answer_start": [300, 300, 300]}, "prediction_text": "an end itself."}
{"answers": {"text": ["to \u201cwid[en] people\u2019s choices and the level of their achieved well-being\u201d", "wid[en] people\u2019s choices and the level of their achieved well-being", "\u201cwid[en] people\u2019s choices and the level of their achieved well-being\u201d"], "answer_start": [328, 332, 331]}, "prediction_text": "Widening people\u2019s choices and the level of their achieved well-being."}
{"answers": {"text": ["through increasing functionings", "increasing functionings (the things a person values doing), capabilities (the freedom to enjoy functionings) and agency (the ability to pursue valued goals)", "through increasing functionings"], "answer_start": [401, 409, 401]}, "prediction_text": "Increase functionings, capabilities, and agency."}
{"answers": {"text": ["the ability to pursue valued goals", "the ability to pursue valued goals"], "answer_start": [530, 530]}, "prediction_text": "The definition of agency as it relates to capabilities is: \"The ability to pursue valued goals.\""}
{"answers": {"text": ["deprived of earning as much", "deprived of earning as much income", "deprived of earning as much income as they would otherwise"], "answer_start": [63, 63, 63]}, "prediction_text": "They are in some way deprived of earning as much income as they would otherwise."}
{"answers": {"text": ["earn as much as a healthy young man", "earn as much as a healthy young man", "earn as much as a healthy young man"], "answer_start": [146, 146, 146]}, "prediction_text": "Earn as much income as a healthy young man."}
{"answers": {"text": ["gender roles and customs", "gender roles and customs", "gender roles and customs"], "answer_start": [183, 183, 183]}, "prediction_text": "gender roles and customs."}
{"answers": {"text": ["for fear of their lives", "fear of their lives", "fear of their lives"], "answer_start": [427, 431, 431]}, "prediction_text": "Fear of their lives."}
{"answers": {"text": ["a better relevant income.", "better relevant income", "a better relevant income"], "answer_start": [871, 873, 871]}, "prediction_text": "A better relevant income."}
{"answers": {"text": ["BBC", "BBC", "BBC"], "answer_start": [77, 77, 77]}, "prediction_text": "BBC"}
{"answers": {"text": ["1963", "1963", "1963"], "answer_start": [87, 87, 87]}, "prediction_text": "1963"}
{"answers": {"text": ["TARDIS", "TARDIS", "TARDIS"], "answer_start": [233, 233, 233]}, "prediction_text": "TARDIS"}
{"answers": {"text": ["a blue British police box", "a blue British police box", "blue British police box"], "answer_start": [304, 304, 306]}, "prediction_text": "Blue British police box"}
{"answers": {"text": ["science-fiction", "science-fiction", "science-fiction"], "answer_start": [24, 24, 24]}, "prediction_text": "British science-fiction television program"}
{"answers": {"text": ["1963 to 1989", "1963 to 1989", "1963 to 1989"], "answer_start": [265, 265, 265]}, "prediction_text": "1963-1989"}
{"answers": {"text": ["Russell T Davies", "2005", "Russell T Davies"], "answer_start": [447, 439, 447]}, "prediction_text": "Russell T Davies"}
{"answers": {"text": ["K-9 and Company", "K-9 and Company", "K-9 and Company"], "answer_start": [908, 908, 908]}, "prediction_text": "K-9 and Company."}
{"answers": {"text": ["BBC Wales", "Russell T Davies", "BBC Wales"], "answer_start": [562, 447, 562]}, "prediction_text": "Russell T Davies"}
{"answers": {"text": ["Christopher Eccleston", "Christopher Eccleston", "Christopher Eccleston"], "answer_start": [630, 630, 630]}, "prediction_text": "Christopher Eccleston"}
{"answers": {"text": ["Twelve", "Twelve", "Twelve"], "answer_start": [0, 0, 0]}, "prediction_text": "Twelve."}
{"answers": {"text": ["Peter Capaldi", "Peter Capaldi", "Peter Capaldi"], "answer_start": [738, 738, 738]}, "prediction_text": "Peter Capaldi"}
{"answers": {"text": ["The Time of the Doctor", "the 2013 Christmas special \"The Time of the Doctor\"", "\"The Time of the Doctor\""], "answer_start": [824, 796, 823]}, "prediction_text": "\"The Time of the Doctor\""}
{"answers": {"text": ["after sustaining an injury", "after sustaining an injury which would be fatal to most other species", "after sustaining an injury"], "answer_start": [433, 433, 433]}, "prediction_text": "After sustaining an injury."}
{"answers": {"text": ["new personality", "new personality", "new personality"], "answer_start": [403, 403, 403]}, "prediction_text": "Personality."}
{"answers": {"text": ["Gallifrey", "Gallifrey", "Gallifrey"], "answer_start": [94, 94, 94]}, "prediction_text": "Gallifrey"}
{"answers": {"text": ["Mark I Type 40 TARDIS", "Mark I Type 40 TARDIS", "Mark I Type 40 TARDIS"], "answer_start": [182, 182, 182]}, "prediction_text": "TARDIS"}
{"answers": {"text": ["Time and Relative Dimension in Space", "Time and Relative Dimension in Space", "Time and Relative Dimension in Space"], "answer_start": [207, 207, 207]}, "prediction_text": "Time and Relative Dimension in Space"}
{"answers": {"text": ["chameleon circuit", "chameleon circuit", "chameleon circuit"], "answer_start": [328, 328, 328]}, "prediction_text": "Chameleon circuit."}
{"answers": {"text": ["due to a malfunction in the chameleon circuit", "malfunction in the chameleon circuit", "a malfunction in the chameleon circuit"], "answer_start": [511, 520, 518]}, "prediction_text": "The chameleon circuit."}
{"answers": {"text": ["rarely", "often", "rarely"], "answer_start": [11, 36, 11]}, "prediction_text": "Rarely."}
{"answers": {"text": ["the Master", "the Master", "the Master"], "answer_start": [651, 651, 651]}, "prediction_text": "Daleks"}
{"answers": {"text": ["regenerate", "regenerate", "regenerate"], "answer_start": [450, 450, 450]}, "prediction_text": "Regenerates."}
{"answers": {"text": ["humans", "companions", "usually humans"], "answer_start": [126, 61, 118]}, "prediction_text": "Humans"}
{"answers": {"text": ["Time Lord", "Time", "Time Lord"], "answer_start": [409, 409, 409]}, "prediction_text": "Time Lord"}
{"answers": {"text": ["23 November 1963", "Saturday, 23 November 1963", "23 November 1963"], "answer_start": [126, 116, 126]}, "prediction_text": "Saturday, 23 November 1963."}
{"answers": {"text": ["The Daleks (a.k.a. The Mutants)", "Daleks", "eponymous aliens"], "answer_start": [1863, 1867, 1922]}, "prediction_text": "Daleks and Thals."}
{"answers": {"text": ["the programme was not permitted to contain any \"bug-eyed monsters\"", "only had the Dalek serial to go", "the programme was not permitted to contain any \"bug-eyed monsters\""], "answer_start": [1270, 1611, 1270]}, "prediction_text": "The Mutants was the only script ready to go."}
{"answers": {"text": ["Terry Nation", "Terry Nation", "Terry Nation"], "answer_start": [964, 964, 964]}, "prediction_text": "Terry Nation"}
{"answers": {"text": ["25 minutes of transmission length", "25 minutes", "25 minutes"], "answer_start": [198, 198, 198]}, "prediction_text": "25 minutes."}
{"answers": {"text": ["26", "26", "26"], "answer_start": [71, 71, 71]}, "prediction_text": "26"}
{"answers": {"text": ["Jonathan Powell", "Jonathan Powell", "Jonathan Powell"], "answer_start": [250, 250, 250]}, "prediction_text": "Jonathan Powell"}
{"answers": {"text": ["Doctor Who: More Than 30 Years in the TARDIS", "Doctor Who: More Than 30 Years in the TARDIS", "Doctor Who: More Than 30 Years in the TARDIS"], "answer_start": [358, 358, 358]}, "prediction_text": "Doctor Who: More Than 30 Years in the TARDIS"}
{"answers": {"text": ["the series would return", "the series would return", "the series would return"], "answer_start": [580, 580, 580]}, "prediction_text": "They affirmed that the series would return."}
{"answers": {"text": ["BBC 1", "BBC 1", "BBC 1"], "answer_start": [96, 96, 96]}, "prediction_text": "BBC 1"}
{"answers": {"text": ["relaunch the show", "relaunch the show", "relaunch the show"], "answer_start": [97, 97, 97]}, "prediction_text": "Relaunch the show."}
{"answers": {"text": ["Philip Segal", "Philip Segal", "Philip Segal"], "answer_start": [116, 116, 116]}, "prediction_text": "Philip Segal"}
{"answers": {"text": ["the Fox Network", "Fox Network", "the Fox Network"], "answer_start": [418, 422, 418]}, "prediction_text": "Fox"}
{"answers": {"text": ["9.1 million", "9.1 million", "9.1 million"], "answer_start": [570, 570, 570]}, "prediction_text": "9.1 million"}
{"answers": {"text": ["the United States", "United States", "the United States"], "answer_start": [610, 614, 610]}, "prediction_text": "United States"}
{"answers": {"text": ["Rose", "Rose", "Rose"], "answer_start": [46, 46, 46]}, "prediction_text": "Rose"}
{"answers": {"text": ["2005", "2005", "2005"], "answer_start": [75, 75, 75]}, "prediction_text": "2005"}
{"answers": {"text": ["2009", "2009", "2009"], "answer_start": [230, 230, 230]}, "prediction_text": "2009"}
{"answers": {"text": ["Chris Chibnall", "Chris Chibnall", "Chris Chibnall"], "answer_start": [481, 481, 481]}, "prediction_text": "Chris Chibnall"}
{"answers": {"text": ["Christmas Day specials", "Christmas Day specials", "Christmas Day specials"], "answer_start": [155, 155, 155]}, "prediction_text": "Christmas specials."}
{"answers": {"text": ["1963\u20131989", "1963\u20131989", "1963\u20131989"], "answer_start": [77, 77, 77]}, "prediction_text": "1963-1989"}
{"answers": {"text": ["The 2005 version", "The 2005 version of Doctor Who", "The 2005 version of Doctor Who"], "answer_start": [0, 0, 0]}, "prediction_text": "Mission Impossible"}
{"answers": {"text": ["1996", "1996", "1996"], "answer_start": [110, 110, 110]}, "prediction_text": "2005"}
{"answers": {"text": ["Battlestar Galactica and Bionic Woman", "Battlestar Galactica and Bionic Woman", "Battlestar Galactica and Bionic Woman"], "answer_start": [280, 280, 280]}, "prediction_text": "Battlestar Galactica, Bionic Woman."}
{"answers": {"text": ["Mission Impossible,", "Mission Impossible", "Mission Impossible"], "answer_start": [169, 169, 169]}, "prediction_text": "Mission Impossible, Battlestar Galactica, Bionic Woman."}
{"answers": {"text": ["30 November 1963", "30 November 1963", "30 November 1963"], "answer_start": [464, 464, 464]}, "prediction_text": "30 November 1963"}
{"answers": {"text": ["eighty seconds", "ten minutes", "ten minutes"], "answer_start": [236, 78, 78]}, "prediction_text": "Eighty seconds."}
{"answers": {"text": ["ten minutes", "eighty seconds", "ten minutes"], "answer_start": [78, 236, 78]}, "prediction_text": "John F. Kennedy"}
{"answers": {"text": ["the assassination of US President John F. Kennedy", "assassination of US President John F. Kennedy", "the assassination of US President John F. Kennedy"], "answer_start": [123, 127, 123]}, "prediction_text": "assassination of US President John F. Kennedy"}
{"answers": {"text": ["a series of power blackouts across the country", "power blackouts", "a series of power blackouts across the country"], "answer_start": [385, 397, 385]}, "prediction_text": "The assassination of US President John F. Kennedy."}
{"answers": {"text": ["Hiding behind (or 'watching from behind') the sofa", "Hiding behind (or 'watching from behind') the sofa", "\"Hiding behind (or 'watching from behind') the sofa\""], "answer_start": [12, 12, 11]}, "prediction_text": "\"Hiding behind (or 'watching from behind') the sofa\""}
{"answers": {"text": ["the Museum of the Moving Image", "Museum of the Moving Image", "the Museum of the Moving Image"], "answer_start": [375, 379, 375]}, "prediction_text": "Museum of the Moving Image"}
{"answers": {"text": ["Behind the Sofa", "Behind the Sofa", "Behind the Sofa"], "answer_start": [466, 466, 466]}, "prediction_text": "Behind the Sofa"}
{"answers": {"text": ["scariest TV show of all time", "scariest TV show of all time", "scariest TV show of all time"], "answer_start": [768, 768, 768]}, "prediction_text": "Scariest TV show of all time."}
{"answers": {"text": ["Digital Spy", "Digital Spy", "Digital Spy"], "answer_start": [733, 733, 733]}, "prediction_text": "Digital Spy"}
{"answers": {"text": ["Doctor Who", "Doctor Who", "Doctor Who"], "answer_start": [247, 247, 247]}, "prediction_text": "Doctor Who"}
{"answers": {"text": ["3%", "3%", "3%"], "answer_start": [368, 368, 368]}, "prediction_text": "3%"}
{"answers": {"text": ["Philip Howard", "Philip Howard", "Philip Howard"], "answer_start": [531, 531, 531]}, "prediction_text": "Philip Howard"}
{"answers": {"text": ["Monopoly", "Monopoly", "Monopoly"], "answer_start": [795, 795, 795]}, "prediction_text": "Monopoly"}
{"answers": {"text": ["The Times newspaper", "The Times", "The Times newspaper"], "answer_start": [499, 499, 499]}, "prediction_text": "The Times newspaper."}
{"answers": {"text": ["the TARDIS", "the TARDIS", "the TARDIS"], "answer_start": [13, 13, 13]}, "prediction_text": "The TARDIS."}
{"answers": {"text": ["blue police box", "a police box", "blue police box"], "answer_start": [310, 217, 310]}, "prediction_text": "The TARDIS looks like a police box."}
{"answers": {"text": ["time machine", "time machine", "time machine"], "answer_start": [235, 235, 235]}, "prediction_text": "Time machine."}
{"answers": {"text": ["the Metropolitan Police Authority", "Metropolitan Police Authority", "the Metropolitan Police Authority"], "answer_start": [387, 391, 387]}, "prediction_text": "Metropolitan Police Authority"}
{"answers": {"text": ["2002", "2002", "2002"], "answer_start": [472, 472, 472]}, "prediction_text": "2002"}
{"answers": {"text": ["26", "26", "26"], "answer_start": [30, 30, 30]}, "prediction_text": "26"}
{"answers": {"text": ["6 December 1989", "6 December 1989", "6 December 1989"], "answer_start": [81, 81, 81]}, "prediction_text": "6 December 1989"}
{"answers": {"text": ["12", "12", "12"], "answer_start": [327, 327, 327]}, "prediction_text": "12 episodes."}
{"answers": {"text": ["The Master", "The Master", "The Master"], "answer_start": [823, 823, 823]}, "prediction_text": "The Master"}
{"answers": {"text": ["Black Guardian Trilogy", "Black Guardian Trilogy", "Black Guardian Trilogy"], "answer_start": [952, 952, 952]}, "prediction_text": "Black Guardian Trilogy."}
{"answers": {"text": ["2005", "2005", "2005"], "answer_start": [34, 34, 34]}, "prediction_text": "2005"}
{"answers": {"text": ["60 minutes", "45-minute", "60 minutes"], "answer_start": [126, 90, 126]}, "prediction_text": "60 minutes."}
{"answers": {"text": ["Christmas Day", "Christmas Day", "Christmas Day"], "answer_start": [222, 222, 222]}, "prediction_text": "Christmas Day."}
{"answers": {"text": ["Journey's End", "Journey's End", "Journey's End"], "answer_start": [574, 574, 574]}, "prediction_text": "Journey's End"}
{"answers": {"text": ["2010", "2010", "2010"], "answer_start": [628, 628, 628]}, "prediction_text": "2010"}
{"answers": {"text": ["826", "826", "826"], "answer_start": [0, 0, 0]}, "prediction_text": "826"}
{"answers": {"text": ["25-minute", "25-minute", "25-minute"], "answer_start": [75, 75, 75]}, "prediction_text": "25-minute episodes."}
{"answers": {"text": ["eight", "eight", "eight"], "answer_start": [321, 321, 321]}, "prediction_text": "4"}
{"answers": {"text": ["72 minutes", "72 minutes", "72 minutes"], "answer_start": [384, 384, 384]}, "prediction_text": "60 minutes."}
{"answers": {"text": ["2009", "2009", "2009"], "answer_start": [977, 977, 977]}, "prediction_text": "2009"}
{"answers": {"text": ["William Hartnell and Patrick Troughton", "William Hartnell and Patrick Troughton", "William Hartnell and Patrick Troughton"], "answer_start": [340, 340, 340]}, "prediction_text": "William Hartnell and Patrick Troughton."}
{"answers": {"text": ["97", "79", "97"], "answer_start": [388, 539, 388]}, "prediction_text": "79"}
{"answers": {"text": ["3, 4, & 5", "seasons 3, 4, & 5", "3, 4, & 5"], "answer_start": [517, 509, 517]}, "prediction_text": "3, 4, & 5."}
{"answers": {"text": ["1978", "1978", "1978"], "answer_start": [645, 645, 645]}, "prediction_text": "1978"}
{"answers": {"text": ["Between about 1964 and 1973", "the first six years", "Between about 1964 and 1973"], "answer_start": [0, 423, 0]}, "prediction_text": "1964-1973"}
{"answers": {"text": ["bought prints for broadcast", "bought prints for broadcast", "bought prints for broadcast"], "answer_start": [85, 85, 85]}, "prediction_text": "Private individuals."}
{"answers": {"text": ["fans", "fans", "fans"], "answer_start": [226, 226, 226]}, "prediction_text": "Fans"}
{"answers": {"text": ["Mission to the Unknown", "Mission to the Unknown", "Mission to the Unknown"], "answer_start": [547, 547, 547]}, "prediction_text": "Mission to the Unknown"}
{"answers": {"text": ["8 mm cine film", "8 mm cine film and clips that were shown on other programmes", "8 mm cine film"], "answer_start": [316, 316, 316]}, "prediction_text": "8 mm cine film"}
{"answers": {"text": ["home viewers who made tape recordings of the show", "home viewers who made tape recordings of the show", "home viewers who made tape recordings"], "answer_start": [432, 432, 432]}, "prediction_text": "Home viewers."}
{"answers": {"text": ["the BBC", "BBC", "BBC"], "answer_start": [54, 58, 58]}, "prediction_text": "BBC"}
{"answers": {"text": ["Cosgrove Hall", "Cosgrove Hall", "Cosgrove Hall"], "answer_start": [163, 163, 163]}, "prediction_text": "Cosgrove Hall"}
{"answers": {"text": ["1968", "1968", "1968"], "answer_start": [238, 238, 238]}, "prediction_text": "1968"}
{"answers": {"text": ["Theta-Sigma", "Theta-Sigma", "Theta-Sigma"], "answer_start": [461, 461, 461]}, "prediction_text": "Theta-Sigma"}
{"answers": {"text": ["November 2006", "November 2006", "2006"], "answer_start": [367, 367, 376]}, "prediction_text": "November 2006"}
{"answers": {"text": ["regeneration", "regeneration", "renewal"], "answer_start": [36, 186, 341]}, "prediction_text": "Change of appearance."}
{"answers": {"text": ["the Doctor's third on-screen regeneration", "the Doctor's third on-screen regeneration", "the Doctor's third on-screen regeneration"], "answer_start": [237, 237, 237]}, "prediction_text": "The term \"regeneration\" was first used in the episode \"The Doctor's Third\"."}
{"answers": {"text": ["William Hartnell's poor health", "to permit the recasting of the main character", "William Hartnell's poor health"], "answer_start": [137, 49, 137]}, "prediction_text": "William Hartnell's poor health."}
{"answers": {"text": ["renewal", "renewal", "renewal"], "answer_start": [341, 341, 341]}, "prediction_text": "Doctor's third on-screen regeneration."}
{"answers": {"text": ["change of appearance", "change of appearance", "change of appearance"], "answer_start": [386, 386, 386]}, "prediction_text": "Second Doctor"}
{"answers": {"text": ["12", "12", "12"], "answer_start": [131, 131, 131]}, "prediction_text": "13 times."}
{"answers": {"text": ["13", "13", "13"], "answer_start": [156, 156, 156]}, "prediction_text": "13"}
{"answers": {"text": ["The Time of the Doctor", "The Time of the Doctor", "The Time of the Doctor"], "answer_start": [408, 408, 408]}, "prediction_text": "The Time of the Doctor."}
{"answers": {"text": ["The Deadly Assassin and Mawdryn Undead", "Deadly Assassin and Mawdryn Undead", "Deadly Assassin and Mawdryn Undead"], "answer_start": [12, 16, 16]}, "prediction_text": "The Deadly Assassin and Mawdryn Undead."}
{"answers": {"text": ["1996", "1996", "1996"], "answer_start": [59, 59, 59]}, "prediction_text": "1996"}
{"answers": {"text": ["John Hurt", "John Hurt", "John Hurt"], "answer_start": [138, 138, 138]}, "prediction_text": "John Hurt"}
{"answers": {"text": ["The Day of the Doctor", "The Day of the Doctor", "The Day of the Doctor"], "answer_start": [288, 288, 288]}, "prediction_text": "The Day of the Doctor"}
{"answers": {"text": ["Michael Jayston", "Michael Jayston", "Michael Jayston"], "answer_start": [659, 659, 659]}, "prediction_text": "Michael Jayston"}
{"answers": {"text": ["The Trial of a Time Lord", "The Trial of a Time Lord", "The Trial of a Time Lord"], "answer_start": [627, 627, 627]}, "prediction_text": "The Trial of a Time Lord"}
{"answers": {"text": ["McGann and Eccleston's Doctors", "McGann and Eccleston", "McGann and Eccleston's"], "answer_start": [447, 447, 447]}, "prediction_text": "McGann and Eccleston"}
{"answers": {"text": ["the War Doctor", "an unknown incarnation of himself", "the War Doctor"], "answer_start": [1800, 1692, 1800]}, "prediction_text": "The War Doctor."}
{"answers": {"text": ["The Three Doctors", "The Three Doctors", "The Three Doctors"], "answer_start": [117, 117, 117]}, "prediction_text": "The Three Doctors."}
{"answers": {"text": ["Peter Davison", "Peter Davison", "Peter Davison"], "answer_start": [531, 531, 531]}, "prediction_text": "Peter Davison."}
{"answers": {"text": ["The Space Museum", "The Space Museum", "The Space Museum"], "answer_start": [1058, 1058, 1058]}, "prediction_text": "The Space Museum."}
{"answers": {"text": ["The Day of the Doctor", "The Day of the Doctor", "The Day of the Doctor"], "answer_start": [685, 685, 685]}, "prediction_text": "The Day of the Doctor."}
{"answers": {"text": ["Peter Davison, Colin Baker and Sylvester McCoy", "Peter Davison, Colin Baker and Sylvester McCoy", "Peter Davison, Colin Baker and Sylvester McCoy"], "answer_start": [109, 109, 109]}, "prediction_text": "Peter Davison, Colin Baker, Sylvester McCoy."}
{"answers": {"text": ["Zagreus", "Zagreus", "Zagreus"], "answer_start": [287, 287, 287]}, "prediction_text": "Zagreus"}
{"answers": {"text": ["Peter Davison, Colin Baker, Sylvester McCoy and Paul McGann", "Peter Davison, Colin Baker, Sylvester McCoy and Paul McGann", "Peter Davison, Colin Baker, Sylvester McCoy and Paul McGann"], "answer_start": [544, 544, 544]}, "prediction_text": "Colin Baker, Sylvester McCoy, Paul McGann, Peter Davison."}
{"answers": {"text": ["Colin Baker and Sylvester McCoy", "Colin Baker and Sylvester McCoy", "Colin Baker and Sylvester McCoy"], "answer_start": [444, 444, 444]}, "prediction_text": "Colin Baker, Sylvester McCoy, Paul McGann."}
{"answers": {"text": ["2003", "2003", "2003"], "answer_start": [259, 259, 259]}, "prediction_text": "2003"}
{"answers": {"text": ["The Time of the Doctor", "The Time of the Doctor", "The Time of the Doctor"], "answer_start": [904, 904, 904]}, "prediction_text": "The Lodger (2013)"}
{"answers": {"text": ["The Brain of Morbius", "The Five Doctors", "The Brain of Morbius"], "answer_start": [128, 595, 128]}, "prediction_text": "The Brain of Morbius (1976)"}
{"answers": {"text": ["Mawdryn Undead", "Mawdryn Undead", "Mawdryn Undead"], "answer_start": [420, 420, 420]}, "prediction_text": "Mawdryn Undead (1983)"}
{"answers": {"text": ["The Lodger", "The Lodger", "The Time of the Doctor"], "answer_start": [870, 870, 904]}, "prediction_text": "The Lodger (2013)"}
{"answers": {"text": ["1983", "1983", "1983"], "answer_start": [563, 563, 563]}, "prediction_text": "1983"}
{"answers": {"text": ["An Unearthly Child", "An Unearthly Child", "An Unearthly Child"], "answer_start": [30, 30, 30]}, "prediction_text": "An Unearthly Child"}
{"answers": {"text": ["Susan Foreman", "Susan Foreman", "Susan Foreman"], "answer_start": [93, 93, 93]}, "prediction_text": "Susan Foreman"}
{"answers": {"text": ["2005", "2005"], "answer_start": [351, 351]}, "prediction_text": "2005"}
{"answers": {"text": ["destroyed", "destroyed", "destroyed"], "answer_start": [472, 472, 472]}, "prediction_text": "Destroyed."}
{"answers": {"text": ["Smith and Jones", "Smith and Jones", "Smith and Jones"], "answer_start": [686, 686, 686]}, "prediction_text": "\"Fear Her\" (2006)"}
{"answers": {"text": ["a human", "a human", "usually human, or humanoid aliens"], "answer_start": [33, 33, 1257]}, "prediction_text": "Human"}
{"answers": {"text": ["The Deadly Assassin", "The Deadly Assassin", "The Deadly Assassin"], "answer_start": [649, 649, 649]}, "prediction_text": "The Deadly Assassin."}
{"answers": {"text": ["his granddaughter Susan Foreman", "Susan Foreman", "his granddaughter"], "answer_start": [255, 273, 255]}, "prediction_text": "Susan Foreman"}
{"answers": {"text": ["teachers", "to remind the Doctor of his \"moral duty\"", "teachers"], "answer_start": [313, 163, 313]}, "prediction_text": "Teachers."}
{"answers": {"text": ["Romana", "Romana", "Sarah Jane Smith"], "answer_start": [722, 722, 770]}, "prediction_text": "Mary Tamm"}
{"answers": {"text": ["female", "female", "female"], "answer_start": [68, 68, 68]}, "prediction_text": "Female"}
{"answers": {"text": ["Mickey Smith (Noel Clarke) and Jack Harkness (John Barrowman)", "Mickey Smith (Noel Clarke) and Jack Harkness (John Barrowman)", "Mickey Smith (Noel Clarke) and Jack Harkness (John Barrowman)"], "answer_start": [444, 444, 444]}, "prediction_text": "Mickey Smith, Jack Harkness, and John Barrowman."}
{"answers": {"text": ["The Eleventh", "The Eleventh Doctor", "The Eleventh Doctor"], "answer_start": [548, 548, 548]}, "prediction_text": "Amy Pond"}
{"answers": {"text": ["Pearl Mackie as Bill", "Pearl Mackie", "Pearl Mackie as Bill"], "answer_start": [832, 832, 832]}, "prediction_text": "Pearl Mackie"}
{"answers": {"text": ["Catherine Tate", "Catherine Tate", "Catherine Tate"], "answer_start": [423, 423, 423]}, "prediction_text": "Catherine Tate"}
{"answers": {"text": ["Russell T Davies", "Russell T Davies", "Russell T Davies"], "answer_start": [49, 49, 49]}, "prediction_text": "Russell T. Davies"}
{"answers": {"text": ["series 1", "series 1", "series 1,"], "answer_start": [206, 206, 206]}, "prediction_text": "2005"}
{"answers": {"text": ["Cybermen", "Cybermen", "Cybermen"], "answer_start": [216, 216, 216]}, "prediction_text": "Daleks, Macra, Master, Sontarans, Davros, Time Lords, Silurians, Great Intelligence, Ice Warriors, Zygons."}
{"answers": {"text": ["3", "series 3", "3"], "answer_start": [273, 266, 273]}, "prediction_text": "Cybermen"}
{"answers": {"text": ["Zygons", "Zygons", "Zygons"], "answer_start": [550, 550, 550]}, "prediction_text": "Zygons"}
{"answers": {"text": ["The Dalek race", "The Dalek race", "The Dalek race"], "answer_start": [0, 0, 0]}, "prediction_text": "The Daleks."}
{"answers": {"text": ["Skaro", "Skaro", "Skaro"], "answer_start": [146, 146, 146]}, "prediction_text": "Skaro"}
{"answers": {"text": ["to \"exterminate\" all non-Dalek beings", "to \"exterminate\" all non-Dalek beings", "to \"exterminate\" all non-Dalek beings"], "answer_start": [674, 674, 674]}, "prediction_text": "Exterminate non-Dalek beings."}
{"answers": {"text": ["Davros", "Davros", "Davros"], "answer_start": [178, 178, 178]}, "prediction_text": "Davros"}
{"answers": {"text": ["their eyestalk", "their eyestalk", "their eyestalk"], "answer_start": [466, 466, 466]}, "prediction_text": "Eyestalk."}
{"answers": {"text": ["The Master", "The Master", "The Master"], "answer_start": [0, 0, 0]}, "prediction_text": "The Master"}
{"answers": {"text": ["Time Lord", "Time Lord", "Time Lord"], "answer_start": [49, 49, 49]}, "prediction_text": "Professor Moriarty"}
{"answers": {"text": ["Eric Roberts", "Eric Roberts", "Eric Roberts"], "answer_start": [690, 690, 690]}, "prediction_text": "Eric Roberts"}
{"answers": {"text": ["Professor Moriarty to the Doctor's Sherlock Holmes", "Professor Moriarty to the Doctor's Sherlock Holmes", "Professor Moriarty to the Doctor's Sherlock Holmes"], "answer_start": [107, 107, 107]}, "prediction_text": "Sherlock Holmes."}
{"answers": {"text": ["Roger Delgado", "Roger Delgado", "Roger Delgado"], "answer_start": [359, 359, 359]}, "prediction_text": "Roger Delgado"}
{"answers": {"text": ["Derek Jacobi", "Derek Jacobi", "John Simm"], "answer_start": [38, 38, 169]}, "prediction_text": "John Simm"}
{"answers": {"text": ["Utopia", "Utopia", "Utopia"], "answer_start": [113, 113, 113]}, "prediction_text": "\"Utopia\""}
{"answers": {"text": ["2014", "Dark Water", "2014"], "answer_start": [264, 278, 264]}, "prediction_text": "2007"}
{"answers": {"text": ["Missy", "Missy", "Missy"], "answer_start": [393, 393, 393]}, "prediction_text": "Missy"}
{"answers": {"text": ["Michelle Gomez", "Michelle Gomez", "Michelle Gomez"], "answer_start": [489, 489, 489]}, "prediction_text": "Michelle Gomez"}
{"answers": {"text": ["Ron Grainer", "Ron Grainer", "Ron Grainer"], "answer_start": [35, 35, 35]}, "prediction_text": "Ron Grainer"}
{"answers": {"text": ["the BBC Radiophonic Workshop", "BBC Radiophonic Workshop", "the BBC Radiophonic Workshop"], "answer_start": [83, 87, 83]}, "prediction_text": "BBC Radiophonic Workshop"}
{"answers": {"text": ["musique concr\u00e8te", "musique concr\u00e8te techniques", "musique concr\u00e8te techniques"], "answer_start": [184, 184, 184]}, "prediction_text": "Musique concr\u00e8te techniques."}
{"answers": {"text": ["17", "season 17", "17"], "answer_start": [415, 408, 415]}, "prediction_text": "1979-80"}
{"answers": {"text": ["Did I write that?", "Did I write that?", "Did I write that?"], "answer_start": [1054, 1054, 1054]}, "prediction_text": "Ron Grainer"}
{"answers": {"text": ["Peter Howell", "Peter Howell", "Peter Howell"], "answer_start": [40, 40, 40]}, "prediction_text": "Peter Howell"}
{"answers": {"text": ["Dominic Glynn", "Dominic Glynn", "Dominic Glynn's"], "answer_start": [105, 105, 105]}, "prediction_text": "John Debney"}
{"answers": {"text": ["Seventh", "the Seventh Doctor", "the Seventh"], "answer_start": [258, 254, 254]}, "prediction_text": "The Seventh Doctor"}
{"answers": {"text": ["Murray Gold", "Murray Gold", "Murray Gold"], "answer_start": [502, 502, 502]}, "prediction_text": "John Debney"}
{"answers": {"text": ["The Christmas Invasion", "The Christmas Invasion", "The Christmas Invasion"], "answer_start": [647, 647, 647]}, "prediction_text": "Doctor Who"}
{"answers": {"text": ["Voyage of the Damned", "Voyage of the Damned", "Voyage of the Damned"], "answer_start": [107, 107, 107]}, "prediction_text": "Voyage of the Damned"}
{"answers": {"text": ["Classic FM's Hall of Fame", "Classic FM", "Classic FM's Hall of Fame"], "answer_start": [360, 360, 360]}, "prediction_text": "Classic FM"}
{"answers": {"text": ["2010", "the 2010 series", "the 2010 series"], "answer_start": [164, 160, 160]}, "prediction_text": "The 2010 series."}
{"answers": {"text": ["228", "228", "228"], "answer_start": [339, 339, 339]}, "prediction_text": "228."}
{"answers": {"text": ["Gold", "Gold", "Gold"], "answer_start": [444, 444, 46]}, "prediction_text": "Gold"}
{"answers": {"text": ["Jon Pertwee", "Jon Pertwee", "Jon Pertwee"], "answer_start": [108, 108, 108]}, "prediction_text": "Jon Pertwee"}
{"answers": {"text": ["Mankind", "Mankind", "Mankind"], "answer_start": [350, 350, 350]}, "prediction_text": "The Justified Ancients of Mu Mu (The KLF)"}
{"answers": {"text": ["number 24", "24", "number 24"], "answer_start": [373, 380, 373]}, "prediction_text": "No. 24."}
{"answers": {"text": ["Doctorin' the Tardis", "Doctorin' the Tardis", "Doctorin' the Tardis"], "answer_start": [496, 496, 496]}, "prediction_text": "\"Doctorin' the Tardis\""}
{"answers": {"text": ["Doctorin' the Tardis", "Doctorin' the Tardis", "Doctorin' the Tardis"], "answer_start": [496, 496, 496]}, "prediction_text": "Who Is the Doctor."}
{"answers": {"text": ["Dudley Simpson", "Dudley Simpson", "Dudley Simpson"], "answer_start": [68, 68, 68]}, "prediction_text": "Dudley Simpson"}
{"answers": {"text": ["Planet of Giants", "Planet of Giants", "Planet of Giants"], "answer_start": [292, 292, 292]}, "prediction_text": "Planet of Giants (1964)"}
{"answers": {"text": ["the 1960s and 1970s", "the 1960s and 1970s", "1960s and 1970s"], "answer_start": [369, 369, 373]}, "prediction_text": "1960s and 1970s"}
{"answers": {"text": ["The Horns of Nimon", "The Horns of Nimon", "The Horns of Nimon"], "answer_start": [470, 470, 470]}, "prediction_text": "The Horns of Nimon (1979)"}
{"answers": {"text": ["The Talons of Weng-Chiang", "The Talons of Weng-Chiang", "The Talons of Weng-Chiang"], "answer_start": [532, 532, 532]}, "prediction_text": "The Talons of Weng-Chiang."}
{"answers": {"text": ["the BBC National Orchestra of Wales", "BBC National Orchestra of Wales", "the BBC National Orchestra of Wales"], "answer_start": [127, 131, 127]}, "prediction_text": "Ben Foster"}
{"answers": {"text": ["the BBC National Orchestra of Wales", "BBC National Orchestra of Wales", "the BBC National Orchestra of Wales"], "answer_start": [127, 131, 127]}, "prediction_text": "David Tennant"}
{"answers": {"text": ["27 July 2008", "27 July 2008", "27 July 2008"], "answer_start": [691, 691, 691]}, "prediction_text": "27 July 2008"}
{"answers": {"text": ["Music of the Spheres", "Music of the Spheres", "Music of the Spheres"], "answer_start": [1174, 1174, 1174]}, "prediction_text": "Music of the Spheres"}
{"answers": {"text": ["Murray Gold and Ben Foster", "Murray Gold and Ben Foster", "Murray Gold and Ben Foster"], "answer_start": [74, 74, 74]}, "prediction_text": "Murray Gold"}
{"answers": {"text": ["Six", "Six", "Six"], "answer_start": [0, 0, 0]}, "prediction_text": "Six."}
{"answers": {"text": ["the first two series", "the first two series", "the first two"], "answer_start": [86, 86, 86]}, "prediction_text": "The first series."}
{"answers": {"text": ["music from the 2008\u20132010 specials", "the 2008\u20132010 specials", "music from the 2008\u20132010 specials"], "answer_start": [277, 288, 277]}, "prediction_text": "The fourth soundtrack featured music from the 2008-2010 specials."}
{"answers": {"text": ["A Christmas Carol", "A Christmas Carol", "A Christmas Carol"], "answer_start": [490, 490, 490]}, "prediction_text": "A Christmas Carol"}
{"answers": {"text": ["8 November 2010", "8 November 2010", "8 November 2010"], "answer_start": [396, 396, 396]}, "prediction_text": "8 November 2010"}
{"answers": {"text": ["The original logo", "The original logo used for the First Doctor", "The original logo"], "answer_start": [0, 0, 0]}, "prediction_text": "The logo used for the 50th Anniversary special."}
{"answers": {"text": ["The logo for the Twelfth Doctor", "The logo for the Twelfth Doctor", "The logo for the Twelfth Doctor"], "answer_start": [933, 933, 933]}, "prediction_text": "The logo used for the Eleventh Doctor."}
{"answers": {"text": ["the logo used for the Third and Eighth Doctors", "the Third and Eighth Doctors", "the logo used for the Third and Eighth Doctors"], "answer_start": [1080, 1098, 1080]}, "prediction_text": "The current Doctor Who logo."}
{"answers": {"text": ["The logo from 1973\u201380", "The logo from 1973\u201380", "The logo from 1973\u201380"], "answer_start": [340, 340, 340]}, "prediction_text": "The logo used for the Third Doctor's final season."}
{"answers": {"text": ["the Eleventh Doctor", "the Eleventh Doctor", "the Eleventh"], "answer_start": [185, 185, 185]}, "prediction_text": "The Eleventh Doctor."}
{"answers": {"text": ["the assassination of John F. Kennedy", "the assassination of John F. Kennedy", "the assassination of John F. Kennedy"], "answer_start": [25, 25, 25]}, "prediction_text": "John F. Kennedy assassination."}
{"answers": {"text": ["on the BBC's mainstream BBC One channel", "BBC One", "BBC One"], "answer_start": [193, 217, 217]}, "prediction_text": "BBC One."}
{"answers": {"text": ["the late 1970s", "the late 1970s", "the late 1970s"], "answer_start": [695, 695, 695]}, "prediction_text": "1970s"}
{"answers": {"text": ["circa 1964\u20131965", "circa 1964\u20131965", "circa 1964\u20131965"], "answer_start": [517, 517, 517]}, "prediction_text": "1964-1965"}
{"answers": {"text": ["BBC Three", "BBC Three", "BBC Three"], "answer_start": [349, 349, 349]}, "prediction_text": "BBC Three"}
{"answers": {"text": ["During the ITV network strike of 1979", "During the ITV network strike of 1979", "1979"], "answer_start": [0, 0, 33]}, "prediction_text": "1979"}
{"answers": {"text": ["Its late 1980s performance of three to five million viewers", "performance of three to five million viewers was seen as poor", "performance"], "answer_start": [254, 269, 269]}, "prediction_text": "Coronation Street."}
{"answers": {"text": ["Coronation Street", "Coronation Street", "Coronation Street"], "answer_start": [540, 540, 540]}, "prediction_text": "Coronation Street"}
{"answers": {"text": ["the most popular show at the time", "the most popular show", "the most popular show at the time"], "answer_start": [559, 559, 559]}, "prediction_text": "3 to 5 million."}
{"answers": {"text": ["After the series' revival in 2005", "the series' revival in 2005", "2005"], "answer_start": [594, 600, 623]}, "prediction_text": "2005"}
{"answers": {"text": ["PBS", "PBS", "PBS"], "answer_start": [221, 221, 221]}, "prediction_text": "PBS"}
{"answers": {"text": ["New Zealand", "New Zealand", "TVNZ"], "answer_start": [374, 374, 366]}, "prediction_text": "New Zealand"}
{"answers": {"text": ["Edmonton, Canada", "Edmonton, Canada", "Edmonton, Canada"], "answer_start": [572, 572, 572]}, "prediction_text": "Edmonton, Canada"}
{"answers": {"text": ["15 days", "15", "15"], "answer_start": [590, 590, 590]}, "prediction_text": "15 days."}
{"answers": {"text": ["23 November", "23 November", "23 November"], "answer_start": [157, 157, 157]}, "prediction_text": "23 November 1983"}
{"answers": {"text": ["Australian Broadcasting Corporation (ABC)", "Australian Broadcasting Corporation", "the Australian Broadcasting Corporation"], "answer_start": [111, 111, 107]}, "prediction_text": "ABC"}
{"answers": {"text": ["partial funding", "partial funding", "partial funding"], "answer_start": [541, 541, 541]}, "prediction_text": "Funding."}
{"answers": {"text": ["SyFy", "SyFy", "SyFy"], "answer_start": [745, 745, 745]}, "prediction_text": "BBC UKTV"}
{"answers": {"text": ["weekly screenings of all available classic episodes", "screenings of all available classic episodes", "repeated episodes"], "answer_start": [234, 241, 198]}, "prediction_text": "The Five Doctors."}
{"answers": {"text": ["ABC1", "Australian Broadcasting Corporation", "the Australian Broadcasting Corporation"], "answer_start": [491, 111, 107]}, "prediction_text": "ABC"}
{"answers": {"text": ["1976", "1976", "1976"], "answer_start": [32, 32, 32]}, "prediction_text": "1976"}
{"answers": {"text": ["The Three Doctors", "The Three Doctors", "The Three Doctors"], "answer_start": [52, 52, 52]}, "prediction_text": "The Three Doctors"}
{"answers": {"text": ["Space", "Space", "Space"], "answer_start": [662, 662, 662]}, "prediction_text": "YTV"}
{"answers": {"text": ["The Talons of Weng-Chiang", "The Talons of Weng-Chiang", "The Talons of Weng-Chiang"], "answer_start": [412, 412, 412]}, "prediction_text": "The Talons of Weng-Chiang"}
{"answers": {"text": ["Judith Merril", "Judith Merril", "Judith Merril"], "answer_start": [215, 215, 215]}, "prediction_text": "Judith Merril"}
{"answers": {"text": ["Christopher Eccleston", "Christopher Eccleston", "Christopher Eccleston"], "answer_start": [28, 28, 28]}, "prediction_text": "Christopher Eccleston"}
{"answers": {"text": ["excerpts from the Doctor Who Confidential documentary", "excerpts from the Doctor Who Confidential", "excerpts from the Doctor Who Confidential documentary"], "answer_start": [166, 166, 166]}, "prediction_text": "excerpts from the Doctor Who Confidential documentary."}
{"answers": {"text": ["The Christmas Invasion", "The Christmas Invasion", "The Christmas Invasion"], "answer_start": [280, 280, 280]}, "prediction_text": "The Christmas Invasion"}
{"answers": {"text": ["9 October 2006", "9 October 2006", "9 October 2006"], "answer_start": [408, 408, 408]}, "prediction_text": "9 October 2006"}
{"answers": {"text": ["Thanksgiving", "Thanksgiving", "Thanksgiving"], "answer_start": [520, 520, 520]}, "prediction_text": "Thanksgiving"}
{"answers": {"text": ["the United Kingdom, Australia, Canada and the United States", "United Kingdom, Australia, Canada and the United States", "United Kingdom, Australia, Canada and the United States"], "answer_start": [76, 80, 80]}, "prediction_text": "United Kingdom, Australia, Canada, and the United States."}
{"answers": {"text": ["Eight original series serials", "Eight original series serials", "Eight original series serials"], "answer_start": [341, 341, 341]}, "prediction_text": "Doctor Who (The Infinite Quest)"}
{"answers": {"text": ["The Infinite Quest", "The Infinite Quest", "The Infinite Quest"], "answer_start": [495, 495, 495]}, "prediction_text": "The Infinite Quest."}
{"answers": {"text": ["Spearhead from Space", "Spearhead from Space", "Spearhead from Space"], "answer_start": [622, 622, 622]}, "prediction_text": "Spearhead from Space"}
{"answers": {"text": ["from 2009 onwards", "series from 2009 onwards", "from 2009 onwards"], "answer_start": [552, 545, 552]}, "prediction_text": "1970, 2005, 2009."}
{"answers": {"text": ["Trevor Martin", "Trevor Martin", "Trevor Martin"], "answer_start": [69, 69, 69]}, "prediction_text": "Trevor Martin"}
{"answers": {"text": ["Doctor Who \u2013 The Ultimate Adventure", "Doctor Who \u2013 The Ultimate Adventure", "Doctor Who \u2013 The Ultimate Adventure"], "answer_start": [280, 280, 280]}, "prediction_text": "Doctor Who \u2013 The Ultimate Adventure"}
{"answers": {"text": ["The Curse of the Daleks", "The Curse of the Daleks", "The Curse of the Daleks"], "answer_start": [554, 554, 554]}, "prediction_text": "The Curse of the Daleks."}
{"answers": {"text": ["Doctor Who and the Daleks in the Seven Keys to Doomsday", "Doctor Who and the Daleks in the Seven Keys to Doomsday", "Doctor Who and the Daleks in the Seven Keys to Doomsday"], "answer_start": [102, 102, 102]}, "prediction_text": "Doctor Who \u2013 The Ultimate Adventure"}
{"answers": {"text": ["David Banks", "David Banks", "David Banks"], "answer_start": [362, 362, 362]}, "prediction_text": "David Banks"}
{"answers": {"text": ["Torchwood", "Torchwood", "Torchwood"], "answer_start": [143, 143, 143]}, "prediction_text": "Torchwood (an anagram of \"Doctor Who\")"}
{"answers": {"text": ["22 October 2006", "22 October 2006", "22 October 2006"], "answer_start": [290, 290, 290]}, "prediction_text": "2006"}
{"answers": {"text": ["2008", "2008", "2008"], "answer_start": [724, 724, 724]}, "prediction_text": "2008"}
{"answers": {"text": ["Children of Earth", "Children of Earth", "Children of Earth"], "answer_start": [937, 937, 937]}, "prediction_text": "Children of Earth"}
{"answers": {"text": ["Torchwood: Miracle Day", "Torchwood: Miracle Day", "Torchwood: Miracle Day"], "answer_start": [1005, 1005, 1005]}, "prediction_text": "Miracle Day"}
{"answers": {"text": ["Elisabeth Sladen", "Elisabeth Sladen", "Elisabeth Sladen"], "answer_start": [36, 36, 36]}, "prediction_text": "Elisabeth Sladen"}
{"answers": {"text": ["24 September 2007", "24 September 2007", "24 September 2007"], "answer_start": [210, 210, 210]}, "prediction_text": "2007"}
{"answers": {"text": ["2009", "2009", "2009"], "answer_start": [357, 357, 357]}, "prediction_text": "2010"}
{"answers": {"text": ["2010", "2010", "2010"], "answer_start": [454, 454, 454]}, "prediction_text": "2010"}
{"answers": {"text": ["due to the death of Elisabeth Sladen", "the death of Elisabeth Sladen", "the death of Elisabeth Sladen"], "answer_start": [688, 695, 695]}, "prediction_text": "Death of Elisabeth Sladen."}
{"answers": {"text": ["Dimensions in Time", "Dimensions in Time", "Dimensions in Time"], "answer_start": [79, 79, 79]}, "prediction_text": "Dimensions in Time"}
{"answers": {"text": ["Children in Need", "Children in Need", "Children in Need"], "answer_start": [115, 115, 115]}, "prediction_text": "Children in Need"}
{"answers": {"text": ["EastEnders", "EastEnders", "EastEnders"], "answer_start": [279, 279, 279]}, "prediction_text": "EastEnders"}
{"answers": {"text": ["glasses with one darkened lens", "glasses with one darkened lens", "one darkened lens"], "answer_start": [524, 524, 537]}, "prediction_text": "Glasses with one darkened lens."}
{"answers": {"text": ["the Pulfrich effect", "Pulfrich effect", "Pulfrich effect"], "answer_start": [494, 498, 498]}, "prediction_text": "Pulfrich effect"}
{"answers": {"text": ["Doctor Who and the Curse of Fatal Death", "Curse of Fatal Death,", "Doctor Who and the Curse of Fatal Death"], "answer_start": [26, 45, 26]}, "prediction_text": "Doctor Who and the Curse of Fatal Death."}
{"answers": {"text": ["four", "four", "four"], "answer_start": [187, 187, 187]}, "prediction_text": "4"}
{"answers": {"text": ["Rowan Atkinson", "Rowan Atkinson", "Richard E. Grant"], "answer_start": [430, 430, 629]}, "prediction_text": "Jim Broadbent"}
{"answers": {"text": ["Joanna Lumley", "Joanna Lumley", "Joanna Lumley"], "answer_start": [677, 677, 677]}, "prediction_text": "Joanna Lumley"}
{"answers": {"text": ["head writer and executive producer", "Steven Moffat", "head writer and executive producer"], "answer_start": [745, 718, 745]}, "prediction_text": "Steven Moffat"}
{"answers": {"text": ["The Neutral Zone", "The Neutral Zone", "The Neutral Zone"], "answer_start": [140, 140, 140]}, "prediction_text": "The Neutral Zone."}
{"answers": {"text": ["\"Blue Harvest\" and \"420\"", "\"Blue Harvest\" and \"420\"", "\"Blue Harvest\" and \"420\""], "answer_start": [828, 828, 828]}, "prediction_text": "\"Blue Harvest\" and \"420\"."}
{"answers": {"text": ["Queer as Folk", "Queer as Folk", "Queer as Folk"], "answer_start": [197, 197, 197]}, "prediction_text": "Queer as Folk (created by later Doctor Who executive producer Russell T. Davies)"}
{"answers": {"text": ["Oliver", "Oliver", "Oliver"], "answer_start": [470, 470, 470]}, "prediction_text": "Oliver"}
{"answers": {"text": ["Brisingr and High Wizardry,", "Brisingr and High Wizardry", "Brisingr and High Wizardry"], "answer_start": [681, 681, 681]}, "prediction_text": "Brisingr, High Wizardry, EastEnders, Robot Chicken, Family Guy, RuneScape."}
{"answers": {"text": ["The Chase", "The Chase", "The Chase"], "answer_start": [120, 120, 120]}, "prediction_text": "The Chase"}
{"answers": {"text": ["21-minute", "21-minute", "21-minute"], "answer_start": [52, 52, 52]}, "prediction_text": "21 minutes."}
{"answers": {"text": ["Doctor Who and the Pescatons", "Doctor Who and the Pescatons", "Doctor Who and the Pescatons"], "answer_start": [228, 228, 228]}, "prediction_text": "The Chase"}
{"answers": {"text": ["1981", "1966", "1981"], "answer_start": [398, 142, 398]}, "prediction_text": "1988"}
{"answers": {"text": ["Slipback", "Doctor Who and the Pescatons", "Slipback"], "answer_start": [453, 228, 453]}, "prediction_text": "Slipback"}
{"answers": {"text": ["the Fifth, Sixth and Seventh Doctors", "Fifth, Sixth and Seventh Doctors", "the Fifth, Sixth and Seventh Doctors"], "answer_start": [132, 136, 132]}, "prediction_text": "Fifth, Sixth, Seventh, Eighth."}
{"answers": {"text": ["Destiny of the Doctor", "The 2013 series", "Destiny of the Doctor"], "answer_start": [473, 457, 473]}, "prediction_text": "Destiny of the Doctor"}
{"answers": {"text": ["Big Finish Productions", "Big Finish Productions", "Big Finish Productions"], "answer_start": [12, 12, 12]}, "prediction_text": "Big Finish Productions"}
{"answers": {"text": ["1999", "1999", "1999"], "answer_start": [6, 6, 6]}, "prediction_text": "1999"}
{"answers": {"text": ["2012", "2012", "2012"], "answer_start": [288, 288, 288]}, "prediction_text": "2012"}
{"answers": {"text": ["1991", "1991", "1991"], "answer_start": [197, 197, 197]}, "prediction_text": "1965"}
{"answers": {"text": ["the mid-sixties", "the mid-sixties", "1965"], "answer_start": [42, 42, 91]}, "prediction_text": "1965"}
{"answers": {"text": ["since 1979", "1979", "since 1979"], "answer_start": [618, 624, 618]}, "prediction_text": "1979"}
{"answers": {"text": ["Panini", "Panini", "Panini"], "answer_start": [651, 651, 651]}, "prediction_text": "Panini"}
{"answers": {"text": ["BBC Books", "BBC Books", "BBC Books"], "answer_start": [401, 401, 401]}, "prediction_text": "BBC Books"}
{"answers": {"text": ["the early 1960s", "the early 1960s", "the early 1960s"], "answer_start": [68, 68, 68]}, "prediction_text": "1960s"}
{"answers": {"text": ["BBC Television", "BBC Television", "BBC Television"], "answer_start": [50, 50, 50]}, "prediction_text": "BBC Television"}
{"answers": {"text": ["producers of the show", "producers of the show", "producers of the show"], "answer_start": [522, 522, 522]}, "prediction_text": "BBC"}
{"answers": {"text": ["the BBC", "the BBC", "the BBC"], "answer_start": [450, 450, 450]}, "prediction_text": "BBC"}
{"answers": {"text": ["2006", "2006", "2006"], "answer_start": [96, 96, 96]}, "prediction_text": "2006"}
{"answers": {"text": ["2005\u20132010", "2005\u20132010", "2005\u20132010"], "answer_start": [178, 178, 178]}, "prediction_text": "2005-2010"}
{"answers": {"text": ["2011", "2011", "2011"], "answer_start": [288, 288, 288]}, "prediction_text": "2011"}
{"answers": {"text": ["Michelle Gomez", "Michelle Gomez", "Michelle Gomez"], "answer_start": [402, 402, 402]}, "prediction_text": "Michelle Gomez"}
{"answers": {"text": ["Best Supporting Actress", "Best Supporting Actress", "Best Supporting Actress"], "answer_start": [497, 497, 497]}, "prediction_text": "BAFTA Television Award"}
{"answers": {"text": ["Guinness World Records", "Guinness World Records", "Guinness World Records"], "answer_start": [202, 202, 202]}, "prediction_text": "Guinness World Records"}
{"answers": {"text": ["Doctor Who", "Doctor Who", "Doctor Who"], "answer_start": [37, 37, 37]}, "prediction_text": "50th anniversary special."}
{"answers": {"text": ["electronic", "electronic music", "electronic"], "answer_start": [647, 647, 647]}, "prediction_text": "Electronic music."}
{"answers": {"text": ["2013", "2013", "2013"], "answer_start": [3, 3, 3]}, "prediction_text": "2013"}
{"answers": {"text": ["50th anniversary special", "50th anniversary special", "50th anniversary special"], "answer_start": [490, 490, 490]}, "prediction_text": "50th anniversary special."}
{"answers": {"text": ["Season 11", "1975", "11"], "answer_start": [9, 3, 16]}, "prediction_text": "Season 11."}
{"answers": {"text": ["Doctor Who", "Doctor Who", "Doctor Who"], "answer_start": [269, 269, 269]}, "prediction_text": "Doctor Who"}
{"answers": {"text": ["third", "third", "third"], "answer_start": [447, 447, 447]}, "prediction_text": "Third."}
{"answers": {"text": ["SFX magazine", "SFX magazine", "SFX magazine"], "answer_start": [659, 659, 659]}, "prediction_text": "SFX Magazine"}
{"answers": {"text": ["eight", "eight", "eight"], "answer_start": [858, 858, 858]}, "prediction_text": "Third."}
{"answers": {"text": ["Best Drama Series", "Best Drama Series", "Best Drama Series"], "answer_start": [146, 146, 146]}, "prediction_text": "BAFTA TV Awards"}
{"answers": {"text": ["five", "25", "five", "five"], "answer_start": [114, 329, 114, 114]}, "prediction_text": "Five."}
{"answers": {"text": ["25", "25", "25", "25"], "answer_start": [329, 329, 329, 329]}, "prediction_text": "Five."}
{"answers": {"text": ["2009", "2009", "2009", "2009"], "answer_start": [552, 552, 552, 552]}, "prediction_text": "2009"}
{"answers": {"text": ["a Mind Award at the 2010 Mind Mental Health Media Awards", "Mind Award", "Mind Award", "Mind Award"], "answer_start": [721, 723, 723, 723]}, "prediction_text": "BAFTA TV Awards"}
{"answers": {"text": ["six", "six", "six"], "answer_start": [139, 139, 139]}, "prediction_text": "Six times."}
{"answers": {"text": ["over 200", "over 200", "over 200"], "answer_start": [596, 596, 596]}, "prediction_text": "200"}
{"answers": {"text": ["over a hundred", "over a hundred", "over a hundred"], "answer_start": [624, 624, 624]}, "prediction_text": "200"}
{"answers": {"text": ["Matt Smith", "Matt Smith", "Matt Smith"], "answer_start": [452, 452, 452]}, "prediction_text": "Matt Smith"}
{"answers": {"text": ["The Waters of Mars", "The Waters of Mars", "The Waters of Mars"], "answer_start": [331, 331, 331]}, "prediction_text": "The Pandorica Opens"}
{"answers": {"text": ["Spike Milligan", "Spike Milligan", "Spike Milligan"], "answer_start": [83, 83, 83]}, "prediction_text": "Spike Milligan"}
{"answers": {"text": ["Jon Culshaw", "Jon Culshaw", "Jon Culshaw"], "answer_start": [191, 191, 191]}, "prediction_text": "Jon Culshaw"}
{"answers": {"text": ["a soap sponge", "a soap sponge", "soap sponge"], "answer_start": [153, 153, 155]}, "prediction_text": "soap sponge"}
{"answers": {"text": ["Doctor Who fandom", "Doctor Who", "Doctor Who"], "answer_start": [277, 277, 277]}, "prediction_text": "Jon Culshaw"}
{"answers": {"text": ["BBC Dead Ringers", "Dead Ringers", "Dead Ringers"], "answer_start": [252, 256, 256]}, "prediction_text": "The Chaser's War on Everything."}
{"answers": {"text": ["a private research university", "private research", "private research university", "private research university"], "answer_start": [60, 62, 62, 62]}, "prediction_text": "Private research university"}
{"answers": {"text": ["1890", "1890", "1890", "1890"], "answer_start": [133, 133, 133, 133]}, "prediction_text": "1890"}
{"answers": {"text": ["seven", "seven", "seven", "seven"], "answer_start": [272, 272, 272, 272]}, "prediction_text": "7"}
{"answers": {"text": ["four", "four", "four", "four"], "answer_start": [235, 235, 235, 235]}, "prediction_text": "Four."}
{"answers": {"text": ["5,000", "around 15,000", "15,000", "15,000"], "answer_start": [728, 762, 769, 769]}, "prediction_text": "5,000"}
{"answers": {"text": ["various academic disciplines", "various academic disciplines", "the world's first man-made, self-sustaining nuclear reaction", "the world's first man-made, self-sustaining nuclear reaction"], "answer_start": [78, 78, 404, 404]}, "prediction_text": "academic disciplines"}
{"answers": {"text": ["Chicago's physics department", "Chicago's physics department", "Chicago's physics department", "Chicago's physics department"], "answer_start": [360, 360, 360, 360]}, "prediction_text": "Chicago's physics department."}
{"answers": {"text": ["beneath the university's Stagg Field", "Stagg Field", "beneath the university's Stagg Field", "beneath the university's Stagg Field"], "answer_start": [465, 490, 465, 465]}, "prediction_text": "Stagg Field"}
{"answers": {"text": ["University of Chicago Press", "University of Chicago Press", "University of Chicago Press", "University of Chicago Press"], "answer_start": [741, 741, 741, 741]}, "prediction_text": "University of Chicago Press"}
{"answers": {"text": ["2020", "2020", "2020", "2020"], "answer_start": [858, 858, 858, 858]}, "prediction_text": "2020"}
{"answers": {"text": ["the American Baptist Education Society", "American Baptist Education Society", "American Baptist Education Society", "American Baptist Education Society"], "answer_start": [11, 15, 15, 15]}, "prediction_text": "American Baptist Education Society"}
{"answers": {"text": ["John D. Rockefeller", "John D. Rockefeller", "John D. Rockefeller", "John D. Rockefeller"], "answer_start": [113, 113, 113, 113]}, "prediction_text": "John D. Rockefeller"}
{"answers": {"text": ["William Rainey Harper", "William Rainey Harper", "William Rainey Harper", "William Rainey Harper"], "answer_start": [186, 186, 186, 186]}, "prediction_text": "William Rainey Harper"}
{"answers": {"text": ["1891", "1891", "1891", "1891"], "answer_start": [251, 251, 251, 251]}, "prediction_text": "1891"}
{"answers": {"text": ["1892", "1892", "1892", "1892"], "answer_start": [292, 292, 292, 292]}, "prediction_text": "1892"}
{"answers": {"text": ["Marshall Field", "Marshall Field", "Marshall Field", "Marshall Field"], "answer_start": [235, 235, 235, 235]}, "prediction_text": "John D. Rockefeller"}
{"answers": {"text": ["Silas B. Cobb", "Silas B. Cobb", "Silas B. Cobb", "Silas B. Cobb"], "answer_start": [497, 497, 497, 497]}, "prediction_text": "Silas B. Cobb"}
{"answers": {"text": ["Cobb Lecture Hall", "Cobb Lecture Hall", "Cobb Lecture Hall", "Cobb Lecture Hall"], "answer_start": [566, 566, 566, 566]}, "prediction_text": "Cobb Lecture Hall"}
{"answers": {"text": ["$100,000", "$100,000", "$100,000", "$100,000"], "answer_start": [624, 624, 624, 624]}, "prediction_text": "$100,000"}
{"answers": {"text": ["Charles L. Hutchinson", "Charles L. Hutchinson", "Charles L. Hutchinson", "Charles L. Hutchinson"], "answer_start": [679, 679, 679, 679]}, "prediction_text": "Silas B. Cobb"}
{"answers": {"text": ["several regional colleges and universities", "Des Moines College, Kalamazoo College, Butler University, and Stetson University", "Des Moines College, Kalamazoo College, Butler University, and Stetson University", "Des Moines College, Kalamazoo College, Butler University, and Stetson University"], "answer_start": [149, 193, 193, 193]}, "prediction_text": "Shimer College."}
{"answers": {"text": ["1896", "1896", "1896", "1896"], "answer_start": [278, 278, 278, 278]}, "prediction_text": "1896"}
{"answers": {"text": ["made a grade of A for all four years", "made a grade of A for all four years, and on any other graduate who took twelve weeks additional study at the University of Chicago", "made a grade of A for all four years", "made a grade of A for all four years"], "answer_start": [796, 796, 796, 796]}, "prediction_text": "make a grade of A for all four years."}
{"answers": {"text": ["passed", "Several University of Chicago professors", "Several University of Chicago professors"], "answer_start": [1751, 1541, 1541]}, "prediction_text": "University of Chicago professors."}
{"answers": {"text": ["1910", "1910", "Several University of Chicago professors", "1910"], "answer_start": [1774, 1774, 1541, 1774]}, "prediction_text": "1910"}
{"answers": {"text": ["Robert Maynard Hutchins", "Robert Maynard Hutchins", "Robert Maynard Hutchins", "Robert Maynard Hutchins"], "answer_start": [43, 43, 43, 43]}, "prediction_text": "Robert Maynard Hutchins"}
{"answers": {"text": ["the Common Core", "Common Core", "Common Core", "Common Core"], "answer_start": [328, 332, 332, 332]}, "prediction_text": "Common Core"}
{"answers": {"text": ["to emphasize academics over athletics", "emphasize academics", "an attempt to emphasize academics over athletics", "to emphasize academics over athletics,"], "answer_start": [217, 220, 206, 217]}, "prediction_text": "To emphasize academics over athletics."}
{"answers": {"text": ["24-year tenure", "24-year", "24-year tenure.", "24-year"], "answer_start": [130, 130, 130, 130]}, "prediction_text": "24 years."}
{"answers": {"text": ["1929", "1929", "1929", "1929"], "answer_start": [3, 3, 3, 3]}, "prediction_text": "1929"}
{"answers": {"text": ["1950s", "early 1950s", "the early 1950s", "the early 1950s"], "answer_start": [13, 7, 3, 3]}, "prediction_text": "1950s"}
{"answers": {"text": ["a result of increasing crime and poverty", "increasing crime and poverty in the Hyde Park neighborhood", "increasing crime and poverty in the Hyde Park neighborhood", "increasing crime and poverty in the Hyde Park neighborhood"], "answer_start": [53, 65, 65, 65]}, "prediction_text": "Crime and poverty."}
{"answers": {"text": ["after their second year", "early 1950s", "after their second year", "after their second year"], "answer_start": [576, 7, 576, 576]}, "prediction_text": "After their second year."}
{"answers": {"text": ["Hyde Park", "Hyde Park", "Hyde Park", "Hyde Park"], "answer_start": [221, 101, 101, 101]}, "prediction_text": "Hyde Park"}
{"answers": {"text": ["allowed very young students to attend college", "allowed very young students to attend college", "allowed very young students to attend college", "allowed very young students to attend college"], "answer_start": [420, 420, 420, 420]}, "prediction_text": "Enabled students to attend college."}
{"answers": {"text": ["1962", "1962", "1962", "1962"], "answer_start": [86, 86, 86, 86]}, "prediction_text": "1962"}
{"answers": {"text": ["the university's off-campus rental policies.", "the university's off-campus rental policies", "the university's off-campus rental policies", "the university's off-campus rental policies"], "answer_start": [166, 166, 166, 166]}, "prediction_text": "The student decided to occupy the president's office in protest over the university's off-campus rental policies."}
{"answers": {"text": ["1967", "1967", "1967", "1967"], "answer_start": [262, 262, 262, 262]}, "prediction_text": "1967"}
{"answers": {"text": ["a two-page statement", "two", "two", "two"], "answer_start": [326, 328, 328, 328]}, "prediction_text": "2 pages."}
{"answers": {"text": ["social and political action", "social and political action", "social and political action", "social and political action"], "answer_start": [378, 378, 378, 378]}, "prediction_text": "The Kelven Report contained the following policy:"}
{"answers": {"text": ["mid-2000s", "mid-2000s", "mid-2000s", "the mid-2000s"], "answer_start": [9, 9, 9, 5]}, "prediction_text": "2008"}
{"answers": {"text": ["Milton Friedman Institute", "Milton Friedman Institute", "Milton Friedman Institute", "Milton Friedman Institute"], "answer_start": [161, 161, 161, 161]}, "prediction_text": "Milton Friedman Institute"}
{"answers": {"text": ["around $200 million", "$200 million", "$200 million", "$200 million"], "answer_start": [291, 298, 298, 298]}, "prediction_text": "$200 million"}
{"answers": {"text": ["the Chicago Theological Seminary", "Chicago Theological Seminary", "Chicago Theological Seminary", "Chicago Theological Seminary"], "answer_start": [339, 343, 343, 343]}, "prediction_text": "Chicago Theological Seminary, Booth School of Business, Jules and Gwen Knapp Center for Biomedical Discovery, and further additions to the medical campus of the University of Chicago Medical Center."}
{"answers": {"text": ["David G. Booth", "David G. Booth", "David G. Booth", "David G. Booth"], "answer_start": [404, 404, 404, 404]}, "prediction_text": "David G. Booth"}
{"answers": {"text": ["the Main Quadrangles", "Main Quadrangles", "the Main Quadrangles", "the Main Quadrangles"], "answer_start": [92, 96, 92, 92]}, "prediction_text": "Main Quadrangles"}
{"answers": {"text": ["six", "six quadrangles", "six", "six"], "answer_start": [273, 273, 273, 273]}, "prediction_text": "Six."}
{"answers": {"text": ["Cobb, Shepley, Rutan and Coolidge, Holabird & Roche,", "Henry Ives Cobb", "Cobb, Shepley, Rutan and Coolidge, Holabird & Roche, and other architectural firms", "Cobb, Shepley, Rutan and Coolidge, Holabird & Roche, and other architectural firms"], "answer_start": [408, 224, 408, 408]}, "prediction_text": "Henry Ives Cobb"}
{"answers": {"text": ["Oxford's Magdalen Tower", "Magdalen Tower", "Magdalen Tower", "Magdalen Tower"], "answer_start": [660, 669, 669, 669]}, "prediction_text": "Magdalen Tower"}
{"answers": {"text": ["Christ Church Hall", "Christ Church Hall", "Christ Church Hall", "Christ Church Hall"], "answer_start": [741, 741, 741, 741]}, "prediction_text": "Christ Church Hall"}
{"answers": {"text": ["the 1940s", "After the 1940s"], "answer_start": [6, 0]}, "prediction_text": "1940s"}
{"answers": {"text": ["Eero Saarinen", "Eero Saarinen", "Eero Saarinen"], "answer_start": [89, 89, 89]}, "prediction_text": "Eero Saarinen"}
{"answers": {"text": ["School of Social Service Administration", "School of Social Service Administration", "the university's School of Social Service Administration"], "answer_start": [401, 401, 384]}, "prediction_text": "School of Social Service Administration"}
{"answers": {"text": ["Harris School of Public Policy Studies", "Harris School of Public Policy Studies", "the Harris School of Public Policy Studies"], "answer_start": [489, 489, 485]}, "prediction_text": "School of Social Service Administration"}
{"answers": {"text": ["2003", "2003"], "answer_start": [813, 813]}, "prediction_text": "2003"}
{"answers": {"text": ["Singapore, London, and the downtown Streeterville neighborhood of Chicago", "Singapore, London, and the downtown Streeterville neighborhood of Chicago", "Singapore, London, and the downtown Streeterville neighborhood of Chicago"], "answer_start": [144, 144, 144]}, "prediction_text": "Singapore, London, and the downtown Streeterville neighborhood of Chicago."}
{"answers": {"text": ["Seine", "Seine", "the Seine"], "answer_start": [281, 281, 277]}, "prediction_text": "Seine"}
{"answers": {"text": ["2010", "2010", "2010"], "answer_start": [362, 362, 362]}, "prediction_text": "2014"}
{"answers": {"text": ["Renmin University", "Renmin University", "Renmin University"], "answer_start": [432, 432, 432]}, "prediction_text": "Renmin University"}
{"answers": {"text": ["2015", "2015", "2015"], "answer_start": [604, 604, 604]}, "prediction_text": "2015"}
{"answers": {"text": ["a board of trustees", "board of trustees", "The Board of Trustees"], "answer_start": [41, 43, 62]}, "prediction_text": "The Board of Trustees."}
{"answers": {"text": ["50", "50", "50"], "answer_start": [199, 199, 199]}, "prediction_text": "50"}
{"answers": {"text": ["fourteen", "fourteen", "fourteen"], "answer_start": [294, 294, 294]}, "prediction_text": "14"}
{"answers": {"text": ["Andrew Alper", "Andrew Alper", "Andrew Alper"], "answer_start": [616, 616, 616]}, "prediction_text": "Andrew Alper"}
{"answers": {"text": ["Robert Zimmer", "Daniel Diermeier", "Daniel Diermeier"], "answer_start": [669, 851, 851]}, "prediction_text": "Daniel Diermeier"}
{"answers": {"text": ["The Higher Learning Commission", "The Higher Learning Commission", "The Higher Learning Commission"], "answer_start": [499, 499, 499]}, "prediction_text": "The Higher Learning Commission"}
{"answers": {"text": ["four", "four", "four"], "answer_start": [73, 73, 73]}, "prediction_text": "four divisions of graduate research."}
{"answers": {"text": ["seven", "seven", "seven"], "answer_start": [113, 113, 113]}, "prediction_text": "7"}
{"answers": {"text": ["50", "50", "50"], "answer_start": [100, 100, 100]}, "prediction_text": "50"}
{"answers": {"text": ["28", "28", "28"], "answer_start": [123, 123, 123]}, "prediction_text": "28"}
{"answers": {"text": ["five", "five", "five"], "answer_start": [175, 175, 175]}, "prediction_text": "5"}
{"answers": {"text": ["the New Collegiate Division", "New Collegiate Division", "New Collegiate Division"], "answer_start": [470, 474, 474]}, "prediction_text": "New Collegiate Division"}
{"answers": {"text": ["the Common Core", "Common Core", "Common Core"], "answer_start": [123, 127, 127]}, "prediction_text": "Common Core"}
{"answers": {"text": ["17", "17", "17"], "answer_start": [198, 198, 198]}, "prediction_text": "17"}
{"answers": {"text": ["the most rigorous, intense", "the most rigorous, intense"], "answer_start": [756, 756]}, "prediction_text": "rigorous, intense."}
{"answers": {"text": ["Uni in the USA", "Uni in the USA", "Uni in the USA"], "answer_start": [565, 565, 565]}, "prediction_text": "Uni in the USA."}
{"answers": {"text": ["University of Chicago Laboratory Schools", "University of Chicago Laboratory Schools", "University of Chicago Laboratory Schools"], "answer_start": [138, 138, 138]}, "prediction_text": "Sonia Shankman Orthogenic School"}
{"answers": {"text": ["the Sonia Shankman Orthogenic School", "Sonia Shankman Orthogenic School", "the Sonia Shankman Orthogenic School"], "answer_start": [234, 238, 234]}, "prediction_text": "Sonia Shankman Orthogenic School"}
{"answers": {"text": ["four", "four", "four"], "answer_start": [359, 359, 359]}, "prediction_text": "Four."}
{"answers": {"text": ["four public charter schools", "four public charter schools", "public charter schools on the South Side of Chicago"], "answer_start": [359, 359, 364]}, "prediction_text": "The Urban Education Institute helps run the University of Chicago Laboratory Schools, the Sonia Shankman Orthogenic School, the Sonia Shankman Orthogenic School, and the four public charter schools."}
{"answers": {"text": ["the University of Chicago campus", "University of Chicago campus", "the University of Chicago campus"], "answer_start": [589, 593, 589]}, "prediction_text": "Hyde Park."}
{"answers": {"text": ["six", "six", "six"], "answer_start": [53, 53, 53]}, "prediction_text": "6"}
{"answers": {"text": ["9.8 million", "9.8 million", "9.8 million"], "answer_start": [91, 91, 91]}, "prediction_text": "9.8 million."}
{"answers": {"text": ["the Regenstein Library", "Regenstein Library", "the Regenstein Library"], "answer_start": [203, 207, 203]}, "prediction_text": "Regenstein Library"}
{"answers": {"text": ["2011", "2011", "2011"], "answer_start": [356, 356, 356]}, "prediction_text": "2011"}
{"answers": {"text": ["more than 1.3 million", "1.3 million", "more than 1.3 million"], "answer_start": [474, 484, 474]}, "prediction_text": "1.3 million."}
{"answers": {"text": ["12", "12", "12"], "answer_start": [24, 24, 24]}, "prediction_text": "12"}
{"answers": {"text": ["113", "113", "113"], "answer_start": [51, 51, 51]}, "prediction_text": "113"}
{"answers": {"text": ["the Oriental Institute", "Oriental Institute", "the Oriental Institute"], "answer_start": [99, 103, 99]}, "prediction_text": "Oriental Institute"}
{"answers": {"text": ["Fermilab", "Fermilab", "Fermilab"], "answer_start": [581, 581, 581]}, "prediction_text": "Fermilab"}
{"answers": {"text": ["Sunspot, New Mexico", "Sunspot, New Mexico", "Sunspot, New Mexico"], "answer_start": [683, 683, 683]}, "prediction_text": "Sunspot, New Mexico."}
{"answers": {"text": ["shaping ideas about the free market", "shaping ideas about the free market", "shaping ideas about the free market"], "answer_start": [159, 159, 159]}, "prediction_text": "Shaping ideas about the free market."}
{"answers": {"text": ["Chicago Pile-1", "Chicago Pile-1"], "answer_start": [537, 537]}, "prediction_text": "Manhattan Project"}
{"answers": {"text": ["Miller\u2013Urey experiment", "Miller\u2013Urey experiment", "Chicago Pile-1"], "answer_start": [872, 872, 537]}, "prediction_text": "Miller-Urey experiment"}
{"answers": {"text": ["1953", "1953", "1953"], "answer_start": [975, 975, 975]}, "prediction_text": "1953"}
{"answers": {"text": ["1933", "1933", "1933"], "answer_start": [509, 509, 509]}, "prediction_text": "1933"}
{"answers": {"text": ["2000", "2000", "2000"], "answer_start": [550, 550, 550]}, "prediction_text": "2000"}
{"answers": {"text": ["1996", "1996", "1996"], "answer_start": [803, 803, 803]}, "prediction_text": "2002"}
{"answers": {"text": ["2002", "2002", "2002"], "answer_start": [844, 844, 844]}, "prediction_text": "2002"}
{"answers": {"text": ["Several thousand", "Several thousand", "Several thousand"], "answer_start": [1047, 1047, 1047]}, "prediction_text": "Several thousand."}
{"answers": {"text": ["5,792", "5,792", "5,792"], "answer_start": [64, 64, 64]}, "prediction_text": "5,792"}
{"answers": {"text": ["3,468", "3,468", "3,468"], "answer_start": [95, 95, 95]}, "prediction_text": "3,468"}
{"answers": {"text": ["5,984", "5,984", "5,984"], "answer_start": [142, 142, 142]}, "prediction_text": "5,984"}
{"answers": {"text": ["15,244", "15,244", "15,244"], "answer_start": [190, 190, 190]}, "prediction_text": "5,792"}
{"answers": {"text": ["international students", "international students", "international students"], "answer_start": [243, 243, 243]}, "prediction_text": "International students."}
{"answers": {"text": ["the University Athletic Association", "University Athletic Association (UAA)", "University Athletic Association (UAA)"], "answer_start": [61, 65, 65]}, "prediction_text": "UAA"}
{"answers": {"text": ["NCAA's Division III", "NCAA's Division III"], "answer_start": [27, 27]}, "prediction_text": "NCAA Division III"}
{"answers": {"text": ["the Big Ten Conference", "Big Ten Conference", "the Big Ten Conference"], "answer_start": [144, 148, 144]}, "prediction_text": "Big Ten Conference"}
{"answers": {"text": ["Jay Berwanger", "Jay Berwanger", "Jay Berwanger"], "answer_start": [406, 406, 406]}, "prediction_text": "Jay Berwanger"}
{"answers": {"text": ["Robert Maynard Hutchins de-emphasized varsity athletics", "University President Robert Maynard Hutchins de-emphasized varsity athletics", "University President Robert Maynard Hutchins de-emphasized varsity athletics in 1939"], "answer_start": [564, 543, 543]}, "prediction_text": "De-emphasized varsity athletics."}
{"answers": {"text": ["over 400", "over 400", "over 400"], "answer_start": [42, 42, 42]}, "prediction_text": "400 clubs."}
{"answers": {"text": ["Recognized Student Organizations", "Recognized Student Organizations (RSOs)"], "answer_start": [84, 84]}, "prediction_text": "Recognized Student Organizations (RSOs)"}
{"answers": {"text": ["the University of Chicago College Bowl Team", "University of Chicago College Bowl Team", "University of Chicago College Bowl Team"], "answer_start": [270, 274, 274]}, "prediction_text": "University of Chicago College Bowl Team"}
{"answers": {"text": ["Doc Films", "Doc Films", "Doc Films"], "answer_start": [625, 625, 625]}, "prediction_text": "Doc Films"}
{"answers": {"text": ["Off-Off Campus", "Off-Off Campus", "Off-Off Campus"], "answer_start": [905, 905, 905]}, "prediction_text": "Off-Off Campus"}
{"answers": {"text": ["graduate and undergraduate students", "graduate and undergraduate students", "graduate and undergraduate students elected to represent members from their respective academic unit"], "answer_start": [256, 256, 256]}, "prediction_text": "Graduate and undergraduate students."}
{"answers": {"text": ["an Executive Committee", "Executive Committee", "an Executive Committee"], "answer_start": [371, 374, 371]}, "prediction_text": "President"}
{"answers": {"text": ["two", "two", "two"], "answer_start": [441, 441, 441]}, "prediction_text": "2"}
{"answers": {"text": ["greater than $2 million", "$2 million", "greater than $2 million"], "answer_start": [599, 612, 599]}, "prediction_text": "$2 million"}
{"answers": {"text": ["fifteen", "fifteen", "fifteen"], "answer_start": [10, 10, 10]}, "prediction_text": "Four."}
{"answers": {"text": ["seven", "seven", "seven"], "answer_start": [35, 35, 35]}, "prediction_text": "Seven."}
{"answers": {"text": ["Alpha Phi Omega", "Alpha Phi Omega", "Alpha Phi Omega"], "answer_start": [133, 133, 133]}, "prediction_text": "Alpha Phi Omega"}
{"answers": {"text": ["Alpha Phi Omega", "Four", "Four"], "answer_start": [133, 150, 150]}, "prediction_text": "Four."}
{"answers": {"text": ["ten", "ten", "ten"], "answer_start": [229, 229, 229]}, "prediction_text": "Four."}
{"answers": {"text": ["May", "May", "May"], "answer_start": [6, 6, 6]}, "prediction_text": "May"}
{"answers": {"text": ["1987", "1987", "1987"], "answer_start": [16, 16, 16]}, "prediction_text": "1987"}
{"answers": {"text": ["Festival of the Arts", "Festival of the Arts", "Festival of the Arts"], "answer_start": [206, 206, 206]}, "prediction_text": "Festival of the Arts"}
{"answers": {"text": ["Kuviasungnerk/Kangeiko", "Kuviasungnerk/Kangeiko", "Kuviasungnerk/Kangeiko"], "answer_start": [382, 382, 382]}, "prediction_text": "Kuviasungnerk/Kangeiko"}
{"answers": {"text": ["Summer Breeze", "Summer Breeze", "Summer Breeze"], "answer_start": [547, 547, 547]}, "prediction_text": "Kuviasungnerk/Kangeiko"}
{"answers": {"text": ["Satya Nadella", "Satya Nadella", "Satya Nadella"], "answer_start": [50, 50, 50]}, "prediction_text": "Satya Nadella"}
{"answers": {"text": ["Larry Ellison", "Larry Ellison", "Larry Ellison"], "answer_start": [129, 129, 129]}, "prediction_text": "Larry Ellison"}
{"answers": {"text": ["Larry Ellison", "Larry Ellison", "Larry Ellison"], "answer_start": [129, 129, 129]}, "prediction_text": "Larry Ellison"}
{"answers": {"text": ["Jon Corzine", "Jon Corzine", "Jon Corzine"], "answer_start": [217, 217, 217]}, "prediction_text": "Larry Ellison"}
{"answers": {"text": ["James O. McKinsey", "James O. McKinsey", "James O. McKinsey"], "answer_start": [312, 312, 312]}, "prediction_text": "James O. McKinsey"}
{"answers": {"text": ["Saul Alinsky", "Saul Alinsky", "Saul Alinsky"], "answer_start": [106, 106, 106]}, "prediction_text": "Saul Alinsky"}
{"answers": {"text": ["David Axelrod", "David Axelrod", "David Axelrod"], "answer_start": [195, 195, 195]}, "prediction_text": "David Axelrod"}
{"answers": {"text": ["Robert Bork", "Robert Bork", "Robert Bork"], "answer_start": [245, 245, 245]}, "prediction_text": "Robert Bork"}
{"answers": {"text": ["Masaaki Shirakawa", "Masaaki Shirakawa", "Masaaki Shirakawa"], "answer_start": [487, 487, 487]}, "prediction_text": "Masaaki Shirakawa"}
{"answers": {"text": ["Eliot Ness", "Eliot Ness", "Eliot Ness"], "answer_start": [307, 307, 307]}, "prediction_text": "Eliot Ness"}
{"answers": {"text": ["Allan Bloom", "Allan Bloom", "Allan Bloom"], "answer_start": [335, 335, 335]}, "prediction_text": "Allan Bloom, Studs Terkel, Susan Sontag, Kurt Vonnegut."}
{"answers": {"text": ["Kurt Vonnegut", "Kurt Vonnegut", "Kurt Vonnegut"], "answer_start": [598, 598, 598]}, "prediction_text": "Studs Terkel"}
{"answers": {"text": ["Lauren Oliver", "Lauren Oliver", "Lauren Oliver"], "answer_start": [69, 69, 69]}, "prediction_text": "Saul Bellow"}
{"answers": {"text": ["Studs Terkel", "Studs Terkel", "Studs Terkel"], "answer_start": [371, 371, 371]}, "prediction_text": "Studs Terkel, Allan Bloom, Susan Sontag, Kurt Vonnegut."}
{"answers": {"text": ["Philip Roth", "Philip Roth", "Philip Roth"], "answer_start": [116, 116, 116]}, "prediction_text": "Philip Roth"}
{"answers": {"text": ["Philip Glass", "Philip Glass", "Philip Glass"], "answer_start": [51, 51, 51]}, "prediction_text": "Philip Glass"}
{"answers": {"text": ["Alex Seropian", "Alex Seropian", "Alex Seropian"], "answer_start": [210, 210, 210]}, "prediction_text": "Alex Seropian"}
{"answers": {"text": ["Halo", "Halo", "Halo"], "answer_start": [187, 187, 187]}, "prediction_text": "Halo"}
{"answers": {"text": ["Ed Asner", "Ed Asner", "Ed Asner"], "answer_start": [257, 257, 257]}, "prediction_text": "Ed Asner"}
{"answers": {"text": ["Mike Nichols", "Mike Nichols", "Mike Nichols"], "answer_start": [417, 417, 417]}, "prediction_text": "Ed Asner"}
{"answers": {"text": ["Carl Sagan", "Carl Sagan", "Carl Sagan"], "answer_start": [39, 39, 39]}, "prediction_text": "Carl Sagan, Edwin Hubble, John M. Grunsfeld, James Watson, Luis Alvarez, Jeannette Piccard, David Suzuki, Ernest Everett Just, Lynn Margulis, John B. Goodenough, Paul Joseph Cohen, Clair Cameron Patterson, Stanton Friedman."}
{"answers": {"text": ["John M. Grunsfeld", "John M. Grunsfeld", "John M. Grunsfeld"], "answer_start": [187, 187, 187]}, "prediction_text": "John M. Grunsfeld"}
{"answers": {"text": ["David Suzuki,", "David Suzuki", "David Suzuki"], "answer_start": [358, 358, 358]}, "prediction_text": "David Suzuki"}
{"answers": {"text": ["John B. Goodenough", "John B. Goodenough", "John B. Goodenough"], "answer_start": [551, 551, 551]}, "prediction_text": "John B. Goodenough"}
{"answers": {"text": ["Clair Cameron Patterson", "Clair Cameron Patterson", "Clair Cameron Patterson"], "answer_start": [646, 646, 646]}, "prediction_text": "Clair Cameron Patterson"}
{"answers": {"text": ["Milton Friedman", "Milton Friedman", "Milton Friedman"], "answer_start": [72, 72, 72]}, "prediction_text": "Gary Becker"}
{"answers": {"text": ["George Stigler", "George Stigler", "George Stigler"], "answer_start": [207, 207, 207]}, "prediction_text": "Margaret Thatcher"}
{"answers": {"text": ["Paul Samuelson", "Paul Samuelson", "Paul Samuelson"], "answer_start": [475, 475, 475]}, "prediction_text": "Milton Friedman"}
{"answers": {"text": ["Eugene Fama", "Eugene Fama", "Eugene Fama"], "answer_start": [568, 568, 568]}, "prediction_text": "Herbert A. Simon"}
{"answers": {"text": ["David Graeber and Donald Johanson", "David Graeber", "David Graeber and Donald Johanson"], "answer_start": [47, 47, 47]}, "prediction_text": "David Graeber, Donald Johanson, John B. Watson, Harold Innis, Samuel Reshevsky, Samuel P. Huntington."}
{"answers": {"text": ["Samuel Reshevsky", "Samuel Reshevsky", "Samuel Reshevsky"], "answer_start": [373, 373, 373]}, "prediction_text": "Samuel Reshevsky"}
{"answers": {"text": ["Samuel P. Huntington", "Samuel P. Huntington", "Samuel P. Huntington"], "answer_start": [523, 523, 523]}, "prediction_text": "Samuel P. Huntington"}
{"answers": {"text": ["A. A. Michelson", "A. A. Michelson", "A. A. Michelson"], "answer_start": [71, 71, 71]}, "prediction_text": "A. Michelson"}
{"answers": {"text": ["Arthur H. Compton", "Robert A. Millikan", "Arthur H. Compton"], "answer_start": [170, 117, 170]}, "prediction_text": "Arthur H. Compton"}
{"answers": {"text": ["Enrico Fermi", "Enrico Fermi", "Enrico Fermi"], "answer_start": [230, 230, 230]}, "prediction_text": "Enrico Fermi"}
{"answers": {"text": ["Edward Teller", "Edward Teller", "Edward Teller"], "answer_start": [278, 278, 278]}, "prediction_text": "Edward Teller"}
{"answers": {"text": ["Maria Goeppert-Mayer", "Maria Goeppert-Mayer", "Maria Goeppert-Mayer"], "answer_start": [478, 478, 478]}, "prediction_text": "Maria Goeppert-Mayer"}
{"answers": {"text": ["James Henry Breasted", "James Henry Breasted", "James Henry Breasted"], "answer_start": [45, 45, 45]}, "prediction_text": "James Henry Breasted"}
{"answers": {"text": ["Alberto Calder\u00f3n", "Alberto Calder\u00f3n", "Alberto Calder\u00f3n"], "answer_start": [81, 81, 81]}, "prediction_text": "James Henry Breasted"}
{"answers": {"text": ["Ted Fujita", "Ted Fujita", "Ted Fujita"], "answer_start": [194, 194, 194]}, "prediction_text": "Ted Fujita"}
{"answers": {"text": ["Yuan T. Lee", "Yuan T. Lee", "Glenn T. Seaborg"], "answer_start": [294, 294, 215]}, "prediction_text": "Ted Fujita"}
{"answers": {"text": ["Charles Brenton Huggins and Janet Rowley", "Charles Brenton Huggins and Janet Rowley", "Charles Brenton Huggins and Janet Rowley"], "answer_start": [414, 414, 414]}, "prediction_text": "Charles Brenton Huggins, Janet Rowley, Gerard Kuiper, Edward Sapir, James O. McKinsey."}
{"answers": {"text": ["Raghuram Rajan", "Raghuram Rajan", "Raghuram Rajan"], "answer_start": [473, 473, 473]}, "prediction_text": "Raghuram Rajan"}
{"answers": {"text": ["Goldman Sachs", "Goldman Sachs", "Goldman Sachs"], "answer_start": [549, 549, 549]}, "prediction_text": "Goldman Sachs"}
{"answers": {"text": ["David Bevington", "David Bevington", "John Mearsheimer and Robert Pape"], "answer_start": [704, 704, 755]}, "prediction_text": "David Bevington"}
{"answers": {"text": ["John Mearsheimer and Robert Pape", "John Mearsheimer and Robert Pape"], "answer_start": [755, 755]}, "prediction_text": "John Mearsheimer and Robert Pape."}
{"answers": {"text": ["Neil Shubin and Paul Sereno", "Neil Shubin and Paul Sereno", "Neil Shubin and Paul Sereno"], "answer_start": [107, 107, 107]}, "prediction_text": "Neil Shubin, Paul Sereno, Jerry Coyne, Yoichiro Nambu, James Heckman, Lars Peter Hansen, Roger Myerson, James Cronin, Eugene Fama, James Heckman, James Cronin, James Heckman, James Heckman, James Heckman, James Heckman, James Heckman, James Heckman, James Heckman, James Heckman, James Heckman, James Heckman, James Heckman, James Heckman, James Heckman, James Heckman, James Heckman, James Heckman, James Heckman, James Heckman, James Heckman, James Heckman, James Heckman, James Heckman, James Heckman, James Heckman, James Heckman, James Heck"}
{"answers": {"text": ["Yu\u00e1n Ch\u00e1o", "Yu\u00e1n Ch\u00e1o", "\u5143\u671d"], "answer_start": [39, 39, 27]}, "prediction_text": "Great Yuan"}
{"answers": {"text": ["the Great Yuan", "the Great Yuan", "the Great Yuan"], "answer_start": [62, 62, 62]}, "prediction_text": "Great Yuan (Chinese: \u5927\u5143; pinyin: D\u00e0 Yu\u00e1n; Mongolian: Yehe Yuan Ulus[a])"}
{"answers": {"text": ["Kublai Khan", "Kublai Khan", "Kublai Khan"], "answer_start": [196, 196, 196]}, "prediction_text": "Kublai Khan"}
{"answers": {"text": ["Kublai Khan", "Kublai Khan", "Kublai Khan"], "answer_start": [196, 196, 196]}, "prediction_text": "Kublai Khan"}
{"answers": {"text": ["1271", "1271", "1271"], "answer_start": [351, 351, 351]}, "prediction_text": "1271"}
{"answers": {"text": ["Mongol Empire", "Mongol Empire", "Mongol Empire"], "answer_start": [55, 55, 55]}, "prediction_text": "Mongol Empire"}
{"answers": {"text": ["Song dynasty", "Mongol Empire", "the Song dynasty"], "answer_start": [291, 55, 287]}, "prediction_text": "Ming dynasty"}
{"answers": {"text": ["Ming dynasty", "Ming dynasty", "the Ming dynasty"], "answer_start": [322, 322, 318]}, "prediction_text": "Ming dynasty"}
{"answers": {"text": ["Genghis Khan", "Genghis Khan", "Genghis Khan"], "answer_start": [410, 410, 410]}, "prediction_text": "Genghis Khan"}
{"answers": {"text": ["1271", "1271", "1271"], "answer_start": [3, 3, 3]}, "prediction_text": "1271"}
{"answers": {"text": ["the Commentaries on the Classic of Changes (I Ching)", "the Commentaries on the Classic of Changes"], "answer_start": [228, 228]}, "prediction_text": "Commentaries on the Classic of Changes (I Ching) section."}
{"answers": {"text": ["Dai \u00d6n Ulus, also rendered as Ikh Yuan \u00dcls or Yekhe Yuan Ulus", "Dai \u00d6n Ulus", "Dai \u00d6n Ulus"], "answer_start": [351, 351, 351]}, "prediction_text": "Mongol dynasty"}
{"answers": {"text": ["Great Mongol State", "Great Mongol State", "Great Mongol State"], "answer_start": [514, 514, 514]}, "prediction_text": "Great Yuan Great Mongol State."}
{"answers": {"text": ["Great Khan", "Great Khan", "Great Khan"], "answer_start": [1012, 1012, 1012]}, "prediction_text": "Great Khan"}
{"answers": {"text": ["Mongol and Turkic tribes", "Mongol and Turkic tribes", "the Mongol and Turkic tribes"], "answer_start": [24, 24, 20]}, "prediction_text": "Mongols, Turkics."}
{"answers": {"text": ["1206", "1206", "1206"], "answer_start": [89, 89, 89]}, "prediction_text": "1251"}
{"answers": {"text": ["\u00d6gedei Khan", "\u00d6gedei Khan", "\u00d6gedei Khan"], "answer_start": [196, 196, 196]}, "prediction_text": "\u00d6gedei Khan"}
{"answers": {"text": ["1251", "1251", "1251"], "answer_start": [627, 627, 627]}, "prediction_text": "1251"}
{"answers": {"text": ["nephew", "nephew", "nephew"], "answer_start": [319, 319, 319]}, "prediction_text": "Uncle."}
{"answers": {"text": ["the Jin", "Jin", "the Jin"], "answer_start": [69, 73, 69]}, "prediction_text": "Shi Tianze"}
{"answers": {"text": ["Xiao Zhala", "Xiao Zhala", "Xiao Zhala"], "answer_start": [155, 155, 155]}, "prediction_text": "Xiao Zhala (\u856d\u672d\u524c)"}
{"answers": {"text": ["Shi Tianze, Liu Heima", "Shi Tianze, Liu Heima", "Shi Tianze, Liu Heima"], "answer_start": [103, 103, 103]}, "prediction_text": "Shi Tianze"}
{"answers": {"text": ["10,000", "10,000", "10,000"], "answer_start": [424, 424, 424]}, "prediction_text": "10,000 troops."}
{"answers": {"text": ["3", "three"], "answer_start": [377, 552]}, "prediction_text": "3"}
{"answers": {"text": ["Han Chinese", "Han Chinese", "Han Chinese"], "answer_start": [17, 17, 17]}, "prediction_text": "Han"}
{"answers": {"text": ["Jin dynasty", "Jin dynasty", "Jin"], "answer_start": [46, 46, 46]}, "prediction_text": "Jin dynasty"}
{"answers": {"text": ["between Han and Jurchen", "marriage between Han and Jurchen", "Han and Jurchen"], "answer_start": [80, 71, 88]}, "prediction_text": "Interethnic marriage between Han and Jurchen."}
{"answers": {"text": ["Shi Bingzhi", "Shi Bingzhi", "Shi Bingzhi"], "answer_start": [147, 147, 147]}, "prediction_text": "Shi Bingzhi"}
{"answers": {"text": ["Song dynasty", "Song dynasty", "the Song dynasty"], "answer_start": [1141, 1141, 1137]}, "prediction_text": "Song dynasty"}
{"answers": {"text": ["M\u00f6ngke Khan", "M\u00f6ngke Khan", "M\u00f6ngke Khan"], "answer_start": [0, 0, 0]}, "prediction_text": "M\u00f6ngke Khan"}
{"answers": {"text": ["southern China", "southern China", "in southern China"], "answer_start": [78, 78, 75]}, "prediction_text": "Southern China."}
{"answers": {"text": ["1259", "1259", "in 1259"], "answer_start": [226, 226, 223]}, "prediction_text": "1259"}
{"answers": {"text": ["Ariq B\u00f6ke", "Ariq B\u00f6ke", "Ariq B\u00f6ke"], "answer_start": [333, 333, 333]}, "prediction_text": "Ariq B\u00f6ke"}
{"answers": {"text": ["Zhongtong", "Zhongtong. Ariq B\u00f6ke", "Zhongtong"], "answer_start": [809, 809, 809]}, "prediction_text": "Zhongtong"}
{"answers": {"text": ["Ogedei", "Ogedei", "Ogedei"], "answer_start": [61, 61, 61]}, "prediction_text": "Kaidu's grandfather was Kublai Khan."}
{"answers": {"text": ["south", "the south", "the western frontier"], "answer_start": [235, 231, 128]}, "prediction_text": "The Song dynasty continued to cause problems for Kublai."}
{"answers": {"text": ["Wonjong", "prince Wonjong", "prince Wonjong"], "answer_start": [319, 312, 312]}, "prediction_text": "Wonjong"}
{"answers": {"text": ["northeast", "northeast", "the northeast"], "answer_start": [261, 261, 257]}, "prediction_text": "Northeast border."}
{"answers": {"text": ["1262", "1262", "1262"], "answer_start": [523, 523, 523]}, "prediction_text": "1262"}
{"answers": {"text": ["preserving Mongol interests in China and satisfying the demands of his Chinese subjects", "preserving Mongol interests in China and satisfying the demands of his Chinese subjects", "preserving Mongol interests in China and satisfying the demands of his Chinese subjects"], "answer_start": [56, 56, 56]}, "prediction_text": "preserving Mongol interests in China and satisfying the demands of his Chinese subjects."}
{"answers": {"text": ["local administrative structure of past Chinese dynasties", "local administrative structure", "local"], "answer_start": [390, 390, 390]}, "prediction_text": "Local administrative structure."}
{"answers": {"text": ["Han Chinese", "Han Chinese", "the Han Chinese"], "answer_start": [601, 601, 597]}, "prediction_text": "Han Chinese"}
{"answers": {"text": ["three, later four", "four", "three"], "answer_start": [565, 578, 565]}, "prediction_text": "Four."}
{"answers": {"text": ["salt and iron", "salt and iron", "salt and iron"], "answer_start": [325, 325, 325]}, "prediction_text": "Salt and iron."}
{"answers": {"text": ["Karakorum", "Karakorum", "Karakorum"], "answer_start": [51, 51, 51]}, "prediction_text": "Karakorum"}
{"answers": {"text": ["Khanbaliq", "Khanbaliq", "Khanbaliq"], "answer_start": [76, 76, 76]}, "prediction_text": "Khanbaliq"}
{"answers": {"text": ["1264", "1264", "1264"], "answer_start": [89, 89, 89]}, "prediction_text": "1264"}
{"answers": {"text": ["Zhongdu", "Zhongdu", "Zhongdu"], "answer_start": [151, 151, 151]}, "prediction_text": "Zhongdu"}
{"answers": {"text": ["Confucian propriety and ancestor veneration", "Confucian propriety and ancestor veneration", "Confucian propriety and ancestor veneration"], "answer_start": [889, 889, 889]}, "prediction_text": "Confucian propriety and ancestor veneration."}
{"answers": {"text": ["commercial, scientific, and cultural", "commercial, scientific, and cultural", "commercial, scientific, and cultural"], "answer_start": [21, 21, 21]}, "prediction_text": "Commercial, scientific, cultural growth."}
{"answers": {"text": ["Mongol peace", "Mongol peace", "Mongol peace"], "answer_start": [316, 316, 316]}, "prediction_text": "Peace."}
{"answers": {"text": ["southern China", "southern China", "southern China"], "answer_start": [456, 456, 456]}, "prediction_text": "Southern China."}
{"answers": {"text": ["Daidu in the north", "Daidu", "Daidu"], "answer_start": [474, 474, 474]}, "prediction_text": "Daidu in the north."}
{"answers": {"text": ["Marco Polo", "Marco Polo", "Marco Polo"], "answer_start": [615, 615, 615]}, "prediction_text": "Marco Polo"}
{"answers": {"text": ["the Song Emperor", "Song Emperor", "the Song Emperor to Quzhou"], "answer_start": [117, 121, 117]}, "prediction_text": "Song Emperor"}
{"answers": {"text": ["1115", "1115", "1115"], "answer_start": [186, 186, 186]}, "prediction_text": "1115-1234"}
{"answers": {"text": ["1234", "1234", "1234"], "answer_start": [191, 191, 191]}, "prediction_text": "1234."}
{"answers": {"text": ["Kong Duancao", "Kong Duancao", "Kong Duancao"], "answer_start": [243, 243, 243]}, "prediction_text": "Kong Duancao"}
{"answers": {"text": ["30,000", "30,000", "30,000"], "answer_start": [777, 777, 777]}, "prediction_text": "30,000"}
{"answers": {"text": ["northern China", "northern China", "northern China"], "answer_start": [38, 38, 38]}, "prediction_text": "Northern China."}
{"answers": {"text": ["between 1268 and 1273", "between 1268 and 1273", "between 1268 and 1273"], "answer_start": [246, 246, 246]}, "prediction_text": "1268-1273"}
{"answers": {"text": ["Yangzi River basin", "Yangzi River basin", "the Song dynasty"], "answer_start": [318, 318, 189]}, "prediction_text": "Yangzi River basin."}
{"answers": {"text": ["Hangzhou", "Hangzhou", "Hangzhou"], "answer_start": [445, 445, 445]}, "prediction_text": "Hangzhou"}
{"answers": {"text": ["drowned", "drowned", "drowned"], "answer_start": [675, 675, 675]}, "prediction_text": "Drowned."}
{"answers": {"text": ["after 1279", "after 1279", "after 1279"], "answer_start": [49, 49, 49]}, "prediction_text": "1279"}
{"answers": {"text": ["an inauspicious typhoon", "inauspicious typhoon", "inauspicious typhoon"], "answer_start": [343, 346, 346]}, "prediction_text": "Disease, an inhospitable climate, and a tropical terrain unsuitable for the mounted warfare of the Mongols."}
{"answers": {"text": ["Annam (Dai Viet)", "Annam", "Annam"], "answer_start": [641, 641, 641]}, "prediction_text": "Vietnam \u0110\u1ea1i Vi\u1ec7t."}
{"answers": {"text": ["Battle of B\u1ea1ch \u0110\u1eb1ng", "Battle of B\u1ea1ch \u0110\u1eb1ng", "the Battle of B\u1ea1ch \u0110\u1eb1ng"], "answer_start": [698, 698, 694]}, "prediction_text": "B\u1ea1ch \u0110\u1eb1ng (1288)"}
{"answers": {"text": ["1288", "1288", "1288"], "answer_start": [719, 719, 719]}, "prediction_text": "1288."}
{"answers": {"text": ["1253", "1253", "1253"], "answer_start": [34, 34, 34]}, "prediction_text": "1253"}
{"answers": {"text": ["his eldest son, Zhenjin", "Zhenjin", "Zhenjin"], "answer_start": [420, 436, 436]}, "prediction_text": "Zhenjin"}
{"answers": {"text": ["before Kublai in 1285", "1285", "1285"], "answer_start": [478, 495, 495]}, "prediction_text": "1285"}
{"answers": {"text": ["Emperor Chengzong", "Emperor Chengzong", "Emperor Chengzong"], "answer_start": [640, 640, 640]}, "prediction_text": "Emperor Chengzong"}
{"answers": {"text": ["1294 to 1307", "1294 to 1307", "from 1294 to 1307"], "answer_start": [664, 664, 659]}, "prediction_text": "1294-1307."}
{"answers": {"text": ["Buyantu Khan", "Buyantu Khan", "Buyantu Khan"], "answer_start": [25, 25, 25]}, "prediction_text": "Buyantu Khan"}
{"answers": {"text": ["actively support and adopt mainstream Chinese culture", "actively support and adopt mainstream Chinese culture", "adopt mainstream Chinese culture"], "answer_start": [111, 111, 132]}, "prediction_text": "He did not support mainstream Chinese culture."}
{"answers": {"text": ["Li Meng", "Li Meng", "Li Meng"], "answer_start": [256, 256, 256]}, "prediction_text": "Li Meng"}
{"answers": {"text": ["the Department of State Affairs", "Department of State Affairs", "the Department of State Affairs"], "answer_start": [338, 342, 338]}, "prediction_text": "Department of State Affairs (Chinese: \u5c1a\u66f8\u7701)"}
{"answers": {"text": ["1313", "1313", "1313"], "answer_start": [472, 472, 472]}, "prediction_text": "1313"}
{"answers": {"text": ["Gegeen Khan", "Gegeen Khan", "Gegeen Khan"], "answer_start": [8, 8, 8]}, "prediction_text": "Baiju"}
{"answers": {"text": ["1321 to 1323", "1321 to 1323", "from 1321 to 1323"], "answer_start": [85, 85, 80]}, "prediction_text": "1321"}
{"answers": {"text": ["Baiju", "Baiju", "Baiju"], "answer_start": [248, 248, 248]}, "prediction_text": "Baiju"}
{"answers": {"text": ["\"the comprehensive institutions of the Great Yuan\"", "the comprehensive institutions of the Great Yuan", "the comprehensive institutions of the Great Yuan"], "answer_start": [310, 311, 311]}, "prediction_text": "The Da Yuan Tong Zhi meant the Da Yuan Tong Zhi (Chinese: \u5927\u5143\u901a\u5236, \"the comprehensive institutions of the Great Yuan\") and the Da Yuan Tong Zhi (Chinese: \u5927\u5143\u901a\u5236, \"the comprehensive institutions of the Great Yuan\") were officially promulgated."}
{"answers": {"text": ["five", "five", "five"], "answer_start": [517, 517, 517]}, "prediction_text": "Five."}
{"answers": {"text": ["Shangdu", "Shangdu", "Shangdu"], "answer_start": [25, 25, 25]}, "prediction_text": "Shangdu"}
{"answers": {"text": ["the War of the Two Capitals", "War of the Two Capitals", "the War of the Two Capitals"], "answer_start": [490, 494, 490]}, "prediction_text": "War of the Two Capitals"}
{"answers": {"text": ["four days", "four days", "four days"], "answer_start": [712, 712, 712]}, "prediction_text": "Four days."}
{"answers": {"text": ["El Tem\u00fcr", "El Tem\u00fcr"], "answer_start": [795, 795]}, "prediction_text": "El Tem\u00fcr"}
{"answers": {"text": ["Tugh Tem\u00fcr", "Tugh Tem\u00fcr", "Tugh Tem\u00fcr"], "answer_start": [809, 809, 743]}, "prediction_text": "Ragibagh"}
{"answers": {"text": ["his cultural contribution", "his cultural contribution", "his cultural contribution"], "answer_start": [88, 88, 88]}, "prediction_text": "Cultural contribution."}
{"answers": {"text": ["Academy of the Pavilion of the Star of Literature", "Academy of the Pavilion of the Star of Literature", "Academy of the Pavilion of the Star of Literature"], "answer_start": [281, 281, 281]}, "prediction_text": "Academy of the Pavilion of the Star of Literature (Chinese: \u594e\u7ae0\u95a3\u5b78\u58eb\u9662)"}
{"answers": {"text": ["spring of 1329", "1329", "the spring of 1329"], "answer_start": [375, 385, 371]}, "prediction_text": "1329"}
{"answers": {"text": ["Jingshi Dadian", "Jingshi Dadian", "Jingshi Dadian"], "answer_start": [704, 704, 704]}, "prediction_text": "Jingshi Dadian (Chinese: \u7d93\u4e16\u5927\u5178)"}
{"answers": {"text": ["supported Zhu Xi's Neo-Confucianism and also devoted himself in Buddhism", "Buddhism", "Neo-Confucianism and also devoted himself in Buddhism"], "answer_start": [747, 811, 766]}, "prediction_text": "Confucianism, Buddhism."}
{"answers": {"text": ["1332", "1332", "1332"], "answer_start": [33, 33, 33]}, "prediction_text": "1332"}
{"answers": {"text": ["Emperor Ningzong", "Emperor Ningzong", "Emperor Ningzong"], "answer_start": [74, 74, 74]}, "prediction_text": "Emperor Huizong"}
{"answers": {"text": ["13", "13", "13"], "answer_start": [111, 111, 111]}, "prediction_text": "13 years old."}
{"answers": {"text": ["nine", "nine", "nine"], "answer_start": [171, 171, 171]}, "prediction_text": "Nine."}
{"answers": {"text": ["Liao, Jin, and Song", "Liao, Jin, and Song", "the Liao, Jin, and Song"], "answer_start": [872, 872, 868]}, "prediction_text": "Liao, Jin, and Song dynasties."}
{"answers": {"text": ["struggle, famine, and bitterness", "struggle, famine, and bitterness", "struggle, famine, and bitterness"], "answer_start": [51, 51, 51]}, "prediction_text": "Struggle, famine, and bitterness."}
{"answers": {"text": ["Mongols beyond the Middle Kingdom saw them as too Chinese", "saw them as too Chinese"], "answer_start": [202, 236]}, "prediction_text": "The Yuan emperors lost control of the rest of the Mongol empire."}
{"answers": {"text": ["both the army and the populace", "army and the populace", "the army and the populace"], "answer_start": [455, 464, 460]}, "prediction_text": "The army and the populace."}
{"answers": {"text": ["Outlaws ravaged the country", "Outlaws", "Outlaws"], "answer_start": [532, 532, 532]}, "prediction_text": "Outlaws."}
{"answers": {"text": ["administration", "administration", "administration"], "answer_start": [414, 414, 414]}, "prediction_text": "Administration."}
{"answers": {"text": ["From the late 1340s onwards", "1340s onwards", "the late 1340s"], "answer_start": [0, 14, 5]}, "prediction_text": "1351"}
{"answers": {"text": ["the Red Turban Rebellion", "Red Turban Rebellion", "Red Turban Rebellion"], "answer_start": [237, 241, 241]}, "prediction_text": "Red Turban Rebellion"}
{"answers": {"text": ["fear of betrayal", "fear of betrayal", "fear of betrayal"], "answer_start": [420, 420, 420]}, "prediction_text": "For fear of betrayal."}
{"answers": {"text": ["the Red Turban rebels", "Red Turban rebels", "the Red Turban rebels"], "answer_start": [357, 361, 357]}, "prediction_text": "Toghtogha."}
{"answers": {"text": ["1368\u20131644", "1368\u20131644", "1368\u20131644"], "answer_start": [849, 849, 849]}, "prediction_text": "1368-1644"}
{"answers": {"text": ["The political unity of China and much of central Asia", "political unity of China and much of central Asia", "political unity"], "answer_start": [186, 190, 190]}, "prediction_text": "The political unity of China and much of central Asia."}
{"answers": {"text": ["The Mongols' extensive West Asian and European contacts", "The Mongols' extensive West Asian and European contacts"], "answer_start": [278, 278]}, "prediction_text": "The political unity of China and much of central Asia."}
{"answers": {"text": ["the Ilkhanate", "Ilkhanate"], "answer_start": [627, 631]}, "prediction_text": "Ilkhanate"}
{"answers": {"text": ["carrots, turnips, new varieties of lemons, eggplants, and melons, high-quality granulated sugar, and cotton", "carrots, turnips, new varieties of lemons, eggplants, and melons, high-quality granulated sugar, and cotton", "carrots, turnips, new varieties of lemons, eggplants, and melons, high-quality granulated sugar, and cotton"], "answer_start": [966, 966, 966]}, "prediction_text": "Carrots, turnips, new varieties of lemons, eggplants, melons, high-quality granulated sugar, cotton."}
{"answers": {"text": ["Western", "Western", "Western"], "answer_start": [0, 0, 0]}, "prediction_text": "Western musical instruments."}
{"answers": {"text": ["Nestorianism and Roman Catholicism", "Nestorianism and Roman Catholicism", "Nestorianism and Roman Catholicism"], "answer_start": [217, 217, 217]}, "prediction_text": "Taoism, Buddhism, Confucianism."}
{"answers": {"text": ["Taoism", "Taoism", "Taoism"], "answer_start": [349, 349, 349]}, "prediction_text": "Taoism"}
{"answers": {"text": ["Confucian", "Confucian governmental practices and examinations", "Confucian"], "answer_start": [432, 432, 432]}, "prediction_text": "Confucian governmental practices and examinations."}
{"answers": {"text": ["travel literature, cartography, geography, and scientific education", "travel literature, cartography, geography, and scientific education", "literature, cartography, geography, and scientific education"], "answer_start": [715, 715, 722]}, "prediction_text": "Travel literature, cartography, geography, and scientific education."}
{"answers": {"text": ["Marco Polo", "Marco Polo", "Marco Polo"], "answer_start": [135, 135, 135]}, "prediction_text": "Marco Polo"}
{"answers": {"text": ["Cambaluc", "Cambaluc", "Cambaluc"], "answer_start": [177, 177, 177]}, "prediction_text": "Cambaluc"}
{"answers": {"text": ["Travels of Marco Polo", "Travels of Marco Polo", "the Travels of Marco Polo"], "answer_start": [350, 350, 346]}, "prediction_text": "Il milione"}
{"answers": {"text": ["Il milione", "Il milione", "Il milione"], "answer_start": [297, 297, 297]}, "prediction_text": "Il milione"}
{"answers": {"text": ["through contact with Persian traders", "through contact with Persian traders", "through contact with Persian traders"], "answer_start": [753, 753, 753]}, "prediction_text": "Through contact with Persian traders."}
{"answers": {"text": ["Guo Shoujing", "Guo Shoujing", "Guo Shoujing"], "answer_start": [111, 111, 111]}, "prediction_text": "Guo Shoujing"}
{"answers": {"text": ["26 seconds off the modern Gregorian calendar", "365.2425 days of the year", "365.2425 days of the year"], "answer_start": [290, 248, 248]}, "prediction_text": "26 seconds off the modern Gregorian calendar."}
{"answers": {"text": ["granaries were ordered built throughout the empire", "granaries were ordered built throughout the empire", "granaries were ordered built"], "answer_start": [448, 448, 448]}, "prediction_text": "ordered granaries throughout the empire."}
{"answers": {"text": ["Beijing", "Beijing", "Beijing"], "answer_start": [644, 644, 644]}, "prediction_text": "Beijing"}
{"answers": {"text": ["sorghum", "sorghum", "sorghum"], "answer_start": [1071, 1071, 1071]}, "prediction_text": "Sorghum."}
{"answers": {"text": ["non-native Chinese people", "non-native Chinese people", "non-native Chinese"], "answer_start": [41, 41, 41]}, "prediction_text": "Non-native Chinese people."}
{"answers": {"text": ["the Eternal Heaven", "Eternal Heaven", "the Eternal Heaven"], "answer_start": [230, 234, 230]}, "prediction_text": "Eternal Heaven."}
{"answers": {"text": ["Song", "Song dynasty", "the Song dynasty"], "answer_start": [641, 641, 637]}, "prediction_text": "Song dynasty"}
{"answers": {"text": ["Ming", "Ming dynasty", "the Ming dynasty"], "answer_start": [662, 662, 658]}, "prediction_text": "Ming dynasty"}
{"answers": {"text": ["a period of foreign domination", "continuation of the Mongol Empire", "a period of foreign domination"], "answer_start": [1239, 159, 1239]}, "prediction_text": "Foreign domination."}
{"answers": {"text": ["Han Chinese, Khitans, Jurchens, Mongols, and Tibetan Buddhists", "Han Chinese, Khitans, Jurchens, Mongols, and Tibetan Buddhists.", "the Han Chinese, Khitans, Jurchens, Mongols, and Tibetan Buddhists"], "answer_start": [113, 113, 109]}, "prediction_text": "Han Chinese, Khitans, Jurchens, Mongols, Tibetan Buddhists."}
{"answers": {"text": ["Tang, Song, as well as Khitan Liao and Jurchen Jin dynasties", "native Chinese dynasties", "Tang, Song, as well as Khitan Liao and Jurchen Jin dynasties"], "answer_start": [478, 292, 478]}, "prediction_text": "Khitans, Jurchens, Mongols, Tibetan Buddhists."}
{"answers": {"text": ["Liu Bingzhong and Yao Shu", "Liu Bingzhong and Yao Shu", "Liu Bingzhong and Yao Shu"], "answer_start": [565, 565, 565]}, "prediction_text": "Liu Bingzhong and Yao Shu."}
{"answers": {"text": ["tripartite", "tripartite division", "tripartite"], "answer_start": [787, 787, 787]}, "prediction_text": "Three-part division of power."}
{"answers": {"text": ["civil, military, and censorial offices", "civil, military, and censorial", "civil, military, and censorial offices"], "answer_start": [826, 826, 826]}, "prediction_text": "Civil, military, and censorial."}
{"answers": {"text": ["the Privy Council", "Privy Council", "the Privy Council"], "answer_start": [785, 789, 785]}, "prediction_text": "Privy Council"}
{"answers": {"text": ["since the Sui and Tang dynasties", "Sui and Tang dynasties", "since the Sui and Tang dynasties"], "answer_start": [110, 120, 110]}, "prediction_text": "Sui and Tang dynasties."}
{"answers": {"text": ["Mongols and Semuren", "Mongols and Semuren", "Mongols and Semuren"], "answer_start": [451, 451, 451]}, "prediction_text": "Semuren"}
{"answers": {"text": ["the Ministry of War", "Ministry of War", "the Ministry of War"], "answer_start": [669, 673, 669]}, "prediction_text": "Privy Council"}
{"answers": {"text": ["1269", "1269", "1269"], "answer_start": [23, 23, 23]}, "prediction_text": "1269"}
{"answers": {"text": ["Mongolian, Tibetan, and Chinese", "Mongolian, Tibetan, and Chinese", "Mongolian, Tibetan, and Chinese"], "answer_start": [81, 81, 81]}, "prediction_text": "Mongolian, Tibetan, and Chinese."}
{"answers": {"text": ["could not master written Chinese, but they could generally converse well", "could not master written Chinese, but they could generally converse well", "well"], "answer_start": [202, 202, 270]}, "prediction_text": "Well."}
{"answers": {"text": ["Tugh Temur", "Tugh Temur", "Tugh Temur"], "answer_start": [456, 456, 456]}, "prediction_text": "Tugh Temur"}
{"answers": {"text": ["Emperor Wenzong", "Emperor Wenzong", "Wenzong"], "answer_start": [723, 723, 731]}, "prediction_text": "Tugh Temur"}
{"answers": {"text": ["1290", "1290", "1290"], "answer_start": [283, 283, 283]}, "prediction_text": "1290"}
{"answers": {"text": ["1291", "1291", "1291"], "answer_start": [430, 430, 430]}, "prediction_text": "1291"}
{"answers": {"text": ["income from the harvests of their Chinese tenants", "harvests of their Chinese tenants", "harvests of their Chinese tenants"], "answer_start": [114, 130, 130]}, "prediction_text": "From the harvests of their Chinese tenants."}
{"answers": {"text": ["painting, mathematics, calligraphy, poetry, and theater", "painting, mathematics, calligraphy, poetry, and theater", "painting, mathematics, calligraphy, poetry, and theater"], "answer_start": [151, 151, 151]}, "prediction_text": "Chinese painting, calligraphy, poetry, theater."}
{"answers": {"text": ["painting, poetry, and calligraphy", "painting, poetry, and calligraphy", "painting, poetry, and calligraphy"], "answer_start": [294, 294, 294]}, "prediction_text": "painting, poetry, calligraphy, theater."}
{"answers": {"text": ["Song", "Song", "the Song dynasty"], "answer_start": [658, 658, 654]}, "prediction_text": "Song dynasty"}
{"answers": {"text": ["the qu", "the qu", "qu"], "answer_start": [934, 934, 938]}, "prediction_text": "Qu."}
{"answers": {"text": ["zaju", "zaju", "zaju"], "answer_start": [1281, 1281, 1281]}, "prediction_text": "Sanqu."}
{"answers": {"text": ["western", "Yuan dynasty", "western"], "answer_start": [221, 243, 221]}, "prediction_text": "Western khanates."}
{"answers": {"text": ["Buddhism, especially the Tibetan variants", "Buddhism", "Buddhism"], "answer_start": [345, 345, 345]}, "prediction_text": "Buddhism"}
{"answers": {"text": ["Tibetan Buddhism", "Tibetan Buddhism", "Tibetan Buddhism"], "answer_start": [401, 401, 401]}, "prediction_text": "Buddhism"}
{"answers": {"text": ["Bureau of Buddhist and Tibetan Affairs", "Bureau of Buddhist and Tibetan Affairs", "the Bureau of Buddhist and Tibetan Affairs"], "answer_start": [526, 526, 522]}, "prediction_text": "Bureau of Buddhist and Tibetan Affairs (Xuanzheng Yuan)"}
{"answers": {"text": ["Sakya", "Sakya sect", "Sakya sect"], "answer_start": [710, 710, 710]}, "prediction_text": "Sakya."}
{"answers": {"text": ["1249", "1249", "1249"], "answer_start": [110, 110, 110]}, "prediction_text": "1249"}
{"answers": {"text": ["1314", "1314", "1314"], "answer_start": [115, 115, 115]}, "prediction_text": "1314"}
{"answers": {"text": ["matrices", "matrices", "matrices"], "answer_start": [240, 240, 240]}, "prediction_text": "Matrices."}
{"answers": {"text": ["polynomial algebra", "polynomial algebra", "polynomial algebra"], "answer_start": [12, 12, 12]}, "prediction_text": "Polynomial algebra"}
{"answers": {"text": ["1303", "1303", "1303"], "answer_start": [440, 440, 440]}, "prediction_text": "1303"}
{"answers": {"text": ["applied mathematics to the construction of calendars", "applied mathematics", "applied mathematics"], "answer_start": [13, 13, 13]}, "prediction_text": "He applied mathematics to the construction of calendars."}
{"answers": {"text": ["a cubic interpolation formula", "cubic interpolation formula", "derived a cubic interpolation formula"], "answer_start": [162, 164, 154]}, "prediction_text": "Shen Kuo"}
{"answers": {"text": ["Shoushi Li", "\u6388\u6642\u66a6", "\u6388\u6642\u66a6"], "answer_start": [245, 257, 257]}, "prediction_text": "Shoushi Li (\u6388\u6642\u66a6)"}
{"answers": {"text": ["Calendar for Fixing the Seasons", "Calendar for Fixing the Seasons", "Calendar for Fixing the Seasons"], "answer_start": [265, 265, 265]}, "prediction_text": "Shoushi Li (\u6388\u6642\u66a6)"}
{"answers": {"text": ["1281", "1281", "1281"], "answer_start": [318, 318, 318]}, "prediction_text": "1281"}
{"answers": {"text": ["non-Mongol physicians", "non-Mongol physicians", "non-Mongol physicians"], "answer_start": [87, 87, 87]}, "prediction_text": "Non-Mongol physicians."}
{"answers": {"text": ["herbal remedies", "herbal", "herbal remedies"], "answer_start": [212, 212, 212]}, "prediction_text": "Herbal remedies."}
{"answers": {"text": ["spiritual cures", "spiritual", "spiritual"], "answer_start": [262, 262, 262]}, "prediction_text": "Herbal remedies."}
{"answers": {"text": ["Imperial Academy of Medicine", "Imperial Academy of Medicine", "Imperial Academy of Medicine"], "answer_start": [421, 421, 421]}, "prediction_text": "Imperial Academy of Medicine"}
{"answers": {"text": ["it ensured a high income and medical ethics were compatible with Confucian virtues", "it ensured a high income and medical ethics were compatible with Confucian virtues", "it ensured a high income and medical ethics were compatible with Confucian virtues"], "answer_start": [580, 580, 580]}, "prediction_text": "High income, medical ethics."}
{"answers": {"text": ["four", "Four", "Four"], "answer_start": [117, 47, 47]}, "prediction_text": "Four."}
{"answers": {"text": ["inherited from the Jin dynasty", "inherited from the Jin dynasty", "inherited from the Jin dynasty"], "answer_start": [81, 81, 81]}, "prediction_text": "The Yuan came to have the 4 schools of medicine through the following:"}
{"answers": {"text": ["Chinese physicians were brought along military campaigns by the Mongols", "to other parts of the empire", "Under the Mongols"], "answer_start": [331, 301, 242]}, "prediction_text": "Under the Mongols."}
{"answers": {"text": ["acupuncture, moxibustion, pulse diagnosis, and various herbal drugs and elixirs", "acupuncture, moxibustion, pulse diagnosis, and various herbal drugs and elixirs", "acupuncture, moxibustion, pulse diagnosis, and various herbal drugs"], "answer_start": [473, 473, 473]}, "prediction_text": "Acupuncture, moxibustion, pulse diagnosis, herbal drugs, and elixirs."}
{"answers": {"text": ["1347", "1347", "1347"], "answer_start": [711, 711, 711]}, "prediction_text": "1347"}
{"answers": {"text": ["Muslim medicine", "Muslim medicine", "Muslim medicine"], "answer_start": [136, 136, 136]}, "prediction_text": "Muslim medicine."}
{"answers": {"text": ["Jesus the Interpreter", "Jesus the Interpreter", "Jesus the Interpreter"], "answer_start": [177, 177, 177]}, "prediction_text": "Jesus the Interpreter"}
{"answers": {"text": ["1263", "1263", "1263"], "answer_start": [241, 241, 241]}, "prediction_text": "1263"}
{"answers": {"text": ["its humoral system", "its humoral system", "humoral system"], "answer_start": [451, 451, 455]}, "prediction_text": "Humoral system."}
{"answers": {"text": ["yin-yang and wuxing", "yin-yang and wuxing", "yin-yang and wuxing philosophy"], "answer_start": [487, 487, 487]}, "prediction_text": "Yin-yang, wuxing, and traditional Chinese medicine."}
{"answers": {"text": ["through Kingdom of Qocho and Tibetan intermediaries", "through Kingdom of Qocho and Tibetan intermediaries", "through Kingdom of Qocho and Tibetan intermediaries"], "answer_start": [116, 116, 116]}, "prediction_text": "Through Kingdom of Qocho and Tibetan intermediaries."}
{"answers": {"text": ["Wang Zhen", "Wang Zhen", "Wang Zhen"], "answer_start": [197, 197, 197]}, "prediction_text": "Wang Zhen"}
{"answers": {"text": ["in the 12th century", "12th century", "the 12th century"], "answer_start": [284, 291, 287]}, "prediction_text": "12th century."}
{"answers": {"text": ["T\u00f6regene Khatun", "T\u00f6regene Khatun", "T\u00f6regene Khatun"], "answer_start": [462, 462, 462]}, "prediction_text": "T\u00f6regene Khatun"}
{"answers": {"text": ["1273", "1273", "In 1273"], "answer_start": [557, 557, 554]}, "prediction_text": "1273"}
{"answers": {"text": ["chao", "chao", "the chao"], "answer_start": [68, 68, 64]}, "prediction_text": "Chao"}
{"answers": {"text": ["bark of mulberry trees", "bark of mulberry trees", "bark of mulberry trees"], "answer_start": [127, 127, 127]}, "prediction_text": "Mulberry trees."}
{"answers": {"text": ["1275", "1275", "1275"], "answer_start": [242, 242, 242]}, "prediction_text": "1275"}
{"answers": {"text": ["woodblocks", "woodblocks", "woodblocks"], "answer_start": [176, 176, 176]}, "prediction_text": "Woodblocks"}
{"answers": {"text": ["1294", "1294", "1294"], "answer_start": [545, 545, 545]}, "prediction_text": "1294"}
{"answers": {"text": ["patrimonial feudalism", "patrimonial feudalism", "patrimonial feudalism"], "answer_start": [111, 111, 111]}, "prediction_text": "Patrimonial feudalism."}
{"answers": {"text": ["traditional Chinese autocratic-bureaucratic system", "autocratic-bureaucratic", "autocratic-bureaucratic system"], "answer_start": [141, 161, 161]}, "prediction_text": "Traditional Chinese autocratic-bureaucratic system."}
{"answers": {"text": ["allied groups from Central Asia and the western end of the empire", "various allied groups from Central Asia and the western end of the empire", "various allied groups"], "answer_start": [474, 466, 466]}, "prediction_text": "Central Asia and western end of the empire."}
{"answers": {"text": ["colonial", "somewhat strong \"colonial\" coloration", "colonial"], "answer_start": [662, 645, 662]}, "prediction_text": "The unequal treatment of Chinese versus Mongols in the Yuan made the dynasty seem like a \"colonial\" coloration."}
{"answers": {"text": ["Ilkhanate", "Ilkhanate", "reaching the highest-post in the government"], "answer_start": [1130, 1130, 1028]}, "prediction_text": "North China and Southern China."}
{"answers": {"text": ["Central Asian Muslims", "Central Asian Muslims", "Central Asian Muslims"], "answer_start": [38, 38, 38]}, "prediction_text": "Han Chinese and Khitans."}
{"answers": {"text": ["Han Chinese and Khitans", "Han Chinese and Khitans", "Han Chinese and Khitans"], "answer_start": [119, 119, 119]}, "prediction_text": "Han Chinese and Khitans."}
{"answers": {"text": ["Besh Baliq, Almaliq, and Samarqand", "Besh Baliq, Almaliq, and Samarqand", "Bukhara"], "answer_start": [361, 361, 211]}, "prediction_text": "Besh Baliq, Almaliq, and Samarqand."}
{"answers": {"text": ["artisans and farmers", "artisans and farmers", "artisans and farmers"], "answer_start": [432, 432, 432]}, "prediction_text": "Artisans and farmers."}
{"answers": {"text": ["a Qara-Khitay (Khitan", "Ahai", "Ahai"], "answer_start": [1136, 1226, 1226]}, "prediction_text": "Qi Kongzhi (Ch'i Kung-chih)"}
{"answers": {"text": ["restricting Halal slaughter and other Islamic practices like circumcision", "circumcision", "restricting Halal slaughter and other Islamic practices like circumcision"], "answer_start": [116, 177, 116]}, "prediction_text": "Halal slaughter, circumcision, kosher butchering for Jews."}
{"answers": {"text": ["Kosher butchering", "Kosher butchering"], "answer_start": [202, 202]}, "prediction_text": "Circumcision."}
{"answers": {"text": ["Zhu Yuanzhang", "Zhu Yuanzhang", "thanks"], "answer_start": [426, 426, 624]}, "prediction_text": "Zhu Yuanzhang"}
{"answers": {"text": ["thanks", "thanks", "thanks"], "answer_start": [624, 624, 624]}, "prediction_text": "Thanks."}
{"answers": {"text": ["Muslims in the semu class", "Muslims in the semu class", "Muslims in the semu class"], "answer_start": [919, 919, 919]}, "prediction_text": "Hui Muslims Feng Sheng."}
{"answers": {"text": ["Frederick W. Mote", "Frederick W. Mote", "Frederick W. Mote"], "answer_start": [14, 14, 14]}, "prediction_text": "Frederick W. Mote"}
{"answers": {"text": ["degrees of privilege", "degrees of privilege", "degrees of privilege"], "answer_start": [256, 256, 256]}, "prediction_text": "Degrees of privilege."}
{"answers": {"text": ["rich and well socially standing", "rich and well", "rich and well socially standing"], "answer_start": [433, 433, 433]}, "prediction_text": "Rich and well socially standing."}
{"answers": {"text": ["lived in poverty and were ill treated", "poverty", "poverty and were ill treated"], "answer_start": [552, 561, 561]}, "prediction_text": "Poor and ill treated."}
{"answers": {"text": ["Northern", "Northern Chinese", "Northern"], "answer_start": [316, 316, 316]}, "prediction_text": "Southern China"}
{"answers": {"text": ["Southern", "Southern Chinese", "southern"], "answer_start": [356, 356, 399]}, "prediction_text": "Southern China"}
{"answers": {"text": ["southern China withstood and fought to the last", "because southern China withstood and fought to the last before caving in", "withstood and fought to the last"], "answer_start": [399, 391, 414]}, "prediction_text": "Southern China withstood and fought to the last before caving in."}
{"answers": {"text": ["The earlier they surrendered to the Mongols, the higher they were placed", "The earlier they surrendered to the Mongols", "they surrendered"], "answer_start": [187, 187, 199]}, "prediction_text": "Higher."}
{"answers": {"text": ["private southern Chinese manufacturers and merchants", "private southern Chinese manufacturers and merchants", "southern Chinese manufacturers and merchants"], "answer_start": [534, 534, 542]}, "prediction_text": "Southern Chinese."}
{"answers": {"text": ["Uighurs", "Uighurs of the Kingdom of Qocho"], "answer_start": [28, 28]}, "prediction_text": "The Uighurs."}
{"answers": {"text": ["the Karluk Kara-Khanid ruler", "Karluk Kara-Khanid", "the Karluk Kara-Khanid ruler"], "answer_start": [237, 241, 237]}, "prediction_text": "Karluk Kara-Khanid ruler."}
{"answers": {"text": ["the Korean King", "Korean King", "the Korean King"], "answer_start": [302, 306, 302]}, "prediction_text": "Korean King"}
{"answers": {"text": ["the Uighurs surrendered peacefully without violently resisting", "Uighurs surrendered peacefully without violently resisting", "surrendered peacefully without violently resisting"], "answer_start": [480, 484, 492]}, "prediction_text": "Ranked higher."}
{"answers": {"text": ["The Central Region", "Central Region", "Central"], "answer_start": [0, 4, 4]}, "prediction_text": "Central Region"}
{"answers": {"text": ["the Central Secretariat", "Central Secretariat", "the Central Secretariat"], "answer_start": [259, 263, 259]}, "prediction_text": "Zhongshu Sheng"}
{"answers": {"text": ["Khanbaliq", "Khanbaliq", "Khanbaliq"], "answer_start": [306, 306, 306]}, "prediction_text": "Khanbaliq (modern Beijing)"}
{"answers": {"text": ["Beijing", "Beijing", "Beijing"], "answer_start": [324, 324, 324]}, "prediction_text": "Beijing"}
{"answers": {"text": ["Zhongshu Sheng", "Zhongshu Sheng", "Zhongshu Sheng"], "answer_start": [287, 287, 287]}, "prediction_text": "Zhongshu Sheng"}
{"answers": {"text": ["in Africa", "Africa"], "answer_start": [92, 95]}, "prediction_text": "Equator."}
{"answers": {"text": ["East African Community", "East African Community", "East African Community"], "answer_start": [131, 131, 131]}, "prediction_text": "East African Community (EAC)"}
{"answers": {"text": ["Nairobi", "Nairobi", "Nairobi"], "answer_start": [193, 193, 193]}, "prediction_text": "Nairobi"}
{"answers": {"text": ["Tanzania", "Tanzania", "Tanzania"], "answer_start": [464, 464, 464]}, "prediction_text": "Uganda"}
{"answers": {"text": ["45 million people", "45 million people", "approximately 45 million"], "answer_start": [671, 671, 657]}, "prediction_text": "45 million"}
{"answers": {"text": ["a warm and humid tropical climate on its Indian Ocean coastline", "warm and humid tropical climate", "tropical"], "answer_start": [10, 12, 27]}, "prediction_text": "Tropical climate"}
{"answers": {"text": ["The climate is cooler", "cooler", "cooler"], "answer_start": [75, 90, 90]}, "prediction_text": "cooler"}
{"answers": {"text": ["Mount Kenya", "Mount Kenya,", "Mount Kenya"], "answer_start": [183, 183, 183]}, "prediction_text": "Mount Kenya"}
{"answers": {"text": ["Somalia and Ethiopia", "Somalia and Ethiopia", "Somalia and Ethiopia"], "answer_start": [536, 536, 536]}, "prediction_text": "Somalia and Ethiopia."}
{"answers": {"text": ["its safaris, diverse climate and geography, and expansive wildlife reserves and national parks", "safaris, diverse climate and geography", "safaris, diverse climate and geography, and expansive wildlife reserves"], "answer_start": [634, 638, 638]}, "prediction_text": "Safaris, diverse climate, geography, wildlife reserves, national parks, world heritage sites."}
{"answers": {"text": ["Lower Paleolithic period", "since the Lower Paleolithic period", "since the Lower Paleolithic period"], "answer_start": [97, 87, 87]}, "prediction_text": "Since the Lower Paleolithic period."}
{"answers": {"text": ["By the first millennium AD", "By the first millennium AD", "first millennium AD"], "answer_start": [123, 123, 130]}, "prediction_text": "The Bantu expansion reached the area from West-Central Africa in the first millennium AD."}
{"answers": {"text": ["Bantu and Nilotic", "Bantu and Nilotic", "Bantu and Nilotic"], "answer_start": [424, 424, 424]}, "prediction_text": "The Bantu expansion reached the area from West-Central Africa."}
{"answers": {"text": ["19th century", "19th century", "19th century"], "answer_start": [642, 642, 642]}, "prediction_text": "19th century."}
{"answers": {"text": ["December 1963", "December 1963", "December 1963"], "answer_start": [809, 809, 809]}, "prediction_text": "December 1963"}
{"answers": {"text": ["Mount Kenya", "Mount Kenya", "Mount Kenya"], "answer_start": [37, 37, 37]}, "prediction_text": "Mount Kenya"}
{"answers": {"text": ["Kirinyaga, Kirenyaa and Kiinyaa", "Kikuyu, Embu and Kamba", "Kikuyu, Embu and Kamba"], "answer_start": [148, 119, 119]}, "prediction_text": "Kirinyaga, Kirenyaa, Kiinyaa."}
{"answers": {"text": ["God's resting place", "God's resting place", "God's resting place"], "answer_start": [192, 192, 192]}, "prediction_text": "God's resting place."}
{"answers": {"text": ["both Kenia and Kegnia", "Kenia and Kegnia", "Kenia and Kegnia"], "answer_start": [34, 39, 39]}, "prediction_text": "Kenia"}
{"answers": {"text": ["a very precise notation of a correct African pronunciation", "precise", "precise"], "answer_start": [155, 162, 162]}, "prediction_text": "Kenia and Kegnia believed that the pronunciation /\u02c8k\u025bnj\u0259/ was a corruption of the Kamba version."}
{"answers": {"text": ["Joseph Thompsons", "Joseph Thompsons", "Joseph Thompsons"], "answer_start": [245, 245, 245]}, "prediction_text": "Joseph Thompsons"}
{"answers": {"text": ["1862", "1862", "1862"], "answer_start": [334, 334, 334]}, "prediction_text": "1862"}
{"answers": {"text": ["The \"Big Five\"", "Big Five", "Big Five"], "answer_start": [0, 5, 5]}, "prediction_text": "Lion, leopard, buffalo, rhinoceros, and elephant."}
{"answers": {"text": ["lion, leopard, buffalo, rhinoceros, and elephant", "lion, leopard, buffalo, rhinoceros, and elephant", "lion, leopard, buffalo, rhinoceros, and elephant"], "answer_start": [51, 51, 51]}, "prediction_text": "Lion, leopard, buffalo, rhinoceros, and elephant."}
{"answers": {"text": ["Masai Mara", "national parks", "Masai Mara"], "answer_start": [134, 247, 134]}, "prediction_text": "Kenya and the Masai Mara."}
{"answers": {"text": ["between June and September", "June and September", "between June and September"], "answer_start": [331, 339, 331]}, "prediction_text": "June to September."}
{"answers": {"text": ["2,900 kilometres (1,802 mi)", "2,900 kilometres", "2,900 kilometres"], "answer_start": [478, 478, 478]}, "prediction_text": "2,900 km (1,802 mi)"}
{"answers": {"text": ["more than 20 million years ago", "20 million years ago", "20 million years ago"], "answer_start": [61, 71, 71]}, "prediction_text": "20 million years ago."}
{"answers": {"text": ["in the Pleistocene epoch", "Pleistocene epoch", "Pleistocene epoch"], "answer_start": [328, 335, 335]}, "prediction_text": "1.8 to 350,000 years ago."}
{"answers": {"text": ["Richard Leakey", "Kamoya Kimeu", "Kamoya Kimeu"], "answer_start": [418, 445, 445]}, "prediction_text": "Richard Leakey"}
{"answers": {"text": [".6-million-year-old", "1.6-million-year-old", "1.6-million-year-old"], "answer_start": [489, 488, 488]}, "prediction_text": "1.6 million years old."}
{"answers": {"text": ["Mary Leakey and Louis Leakey", "Mary Leakey and Louis Leakey", "Mary Leakey and Louis Leakey"], "answer_start": [611, 611, 611]}, "prediction_text": "Mary Leakey and Louis Leakey."}
{"answers": {"text": ["The Swahili", "Swahili", "Swahili"], "answer_start": [0, 4, 4]}, "prediction_text": "Swahili"}
{"answers": {"text": ["Mombasa", "Mombasa", "Mombasa"], "answer_start": [18, 18, 18]}, "prediction_text": "Mombasa"}
{"answers": {"text": ["Duarte Barbosa", "Duarte Barbosa", "Duarte Barbosa"], "answer_start": [213, 213, 213]}, "prediction_text": "Duarte Barbosa"}
{"answers": {"text": ["the Kenyan Coast", "Kenyan Coast", "Kenyan Coast"], "answer_start": [26, 30, 30]}, "prediction_text": "The Kenyan Coast."}
{"answers": {"text": ["City of Malindi", "City of Malindi", "City of Malindi"], "answer_start": [143, 143, 143]}, "prediction_text": "City of Malindi"}
{"answers": {"text": ["14th century", "14th century", "14th century"], "answer_start": [218, 218, 218]}, "prediction_text": "14th century."}
{"answers": {"text": ["August 1914", "August 1914", "August 1914"], "answer_start": [34, 34, 34]}, "prediction_text": "August 1914"}
{"answers": {"text": ["governors of British East Africa (as the Protectorate was generally known) and German East Africa", "British East Africa (as the Protectorate was generally known) and German East Africa", "British East Africa (as the Protectorate was generally known) and German East Africa"], "answer_start": [51, 64, 64]}, "prediction_text": "British East Africa and German East Africa."}
{"answers": {"text": ["Lt Col Paul von Lettow-Vorbeck", "Lt Col Paul von Lettow-Vorbeck", "Paul von Lettow-Vorbeck"], "answer_start": [232, 232, 239]}, "prediction_text": "Lt Col Paul von Lettow-Vorbeck"}
{"answers": {"text": ["effective guerrilla warfare campaign, living off the land, capturing British supplies, and remaining undefeated", "guerrilla warfare campaign", "guerrilla warfare"], "answer_start": [426, 436, 436]}, "prediction_text": "Living off the land."}
{"answers": {"text": ["Northern Rhodesia", "Northern Rhodesia", "Northern Rhodesia"], "answer_start": [568, 568, 568]}, "prediction_text": "Northern Rhodesia (today Zambia)"}
{"answers": {"text": ["The central highlands", "central highlands", "central highlands"], "answer_start": [0, 4, 4]}, "prediction_text": "The central highlands."}
{"answers": {"text": ["as itinerant farmers", "itinerant farmers", "itinerant farmers"], "answer_start": [148, 151, 151]}, "prediction_text": "As itinerant farmers."}
{"answers": {"text": ["banned the growing of coffee, introduced a hut tax, and the landless were granted less and less land in exchange for their labour", "banned the growing of coffee, introduced a hut tax, and the landless were granted less and less land", "banned the growing of coffee, introduced a hut tax, and the landless were granted less and less land"], "answer_start": [211, 211, 211]}, "prediction_text": "banned coffee, introduced hut tax, and landless were granted less and less land in exchange for their labour."}
{"answers": {"text": ["80,000", "80,000", "80,000"], "answer_start": [452, 452, 452]}, "prediction_text": "80,000"}
{"answers": {"text": ["15 January 1954", "15 January 1954", "15 January 1954"], "answer_start": [52, 52, 52]}, "prediction_text": "15 January 1954"}
{"answers": {"text": ["the subsequent interrogation led to a better understanding of the Mau Mau command structure", "better understanding of the Mau Mau command structure", "a better understanding of the Mau Mau command structure"], "answer_start": [72, 110, 108]}, "prediction_text": "To better understand the Mau Mau command structure."}
{"answers": {"text": ["24 April 1954", "24 April 1954", "24 April 1954"], "answer_start": [191, 191, 191]}, "prediction_text": "24 April 1954"}
{"answers": {"text": ["4,686 Mau Mau", "4,686", "4,686"], "answer_start": [652, 652, 652]}, "prediction_text": "4,686"}
{"answers": {"text": ["the Swynnerton Plan, which was used to both reward loyalists and punish Mau Mau.", "Swynnerton Plan", "Swynnerton Plan"], "answer_start": [967, 971, 971]}, "prediction_text": "Swynnerton Plan"}
{"answers": {"text": ["1957", "1957", "1957"], "answer_start": [87, 87, 87]}, "prediction_text": "1957"}
{"answers": {"text": ["Kenya African National Union (KANU) of Jomo Kenyatta", "Kenya African National Union", "Kenya African National Union"], "answer_start": [171, 171, 171]}, "prediction_text": "Jomo Kenyatta"}
{"answers": {"text": ["12 December 1963", "12 December 1963", "8 October 1963"], "answer_start": [323, 323, 483]}, "prediction_text": "1963"}
{"answers": {"text": ["1963", "Kenya Independence Act", "1963"], "answer_start": [810, 787, 810]}, "prediction_text": "12 December 1964."}
{"answers": {"text": ["Republic of Kenya", "Republic of Kenya", "Republic of Kenya"], "answer_start": [923, 923, 923]}, "prediction_text": "Republic of Kenya"}
{"answers": {"text": ["where voters were supposed to line up behind their favoured candidates instead of a secret ballot", "voters were supposed to line up behind their favoured candidates", "voters were supposed to line up behind their favoured candidates"], "answer_start": [75, 81, 81]}, "prediction_text": "Queuing system."}
{"answers": {"text": ["agitation for constitutional reform", "widespread agitation for constitutional reform", "widespread agitation for constitutional reform"], "answer_start": [257, 246, 246]}, "prediction_text": "Democratic, multiparty elections."}
{"answers": {"text": ["Daniel arap Moi", "Daniel arap Moi", "Daniel arap Moi"], "answer_start": [470, 470, 470]}, "prediction_text": "Daniel arap Moi"}
{"answers": {"text": ["a presidential representative democratic republic", "presidential representative democratic republic", "presidential representative democratic republic"], "answer_start": [9, 11, 11]}, "prediction_text": "Presidential representative democratic republic."}
{"answers": {"text": ["the head of state and head of government", "head of state and head of government", "head of state and head of government"], "answer_start": [82, 86, 86]}, "prediction_text": "Head of state and head of government."}
{"answers": {"text": ["exercised by the government", "government", "government."], "answer_start": [172, 189, 189]}, "prediction_text": "The government."}
{"answers": {"text": ["both the government and the National Assembly and the Senate", "government and the National Assembly and the Senate", "both the government and the National Assembly and the Senate"], "answer_start": [232, 241, 232]}, "prediction_text": "The National Assembly and the Senate."}
{"answers": {"text": ["The Judiciary", "Judiciary", "Judiciary"], "answer_start": [294, 298, 298]}, "prediction_text": "Judiciary"}
{"answers": {"text": ["low", "low", "low"], "answer_start": [12, 12, 12]}, "prediction_text": "139th out of 176 total countries."}
{"answers": {"text": ["gauge the prevalence of public sector corruption in various countries", "public sector corruption", "public sector corruption"], "answer_start": [110, 134, 134]}, "prediction_text": "Public sector corruption."}
{"answers": {"text": ["139th out of 176 total countries", "139th", "139th"], "answer_start": [208, 208, 208]}, "prediction_text": "139th out of 176 total countries."}
{"answers": {"text": ["the establishment of a new and independent Ethics and Anti-Corruption Commission", "Ethics and Anti-Corruption Commission", "establishment of a new and independent Ethics and Anti-Corruption Commission"], "answer_start": [413, 456, 417]}, "prediction_text": "Establish EACC."}
{"answers": {"text": ["Party of National Unity", "Party of National Unity", "Party of National Unity"], "answer_start": [58, 58, 58]}, "prediction_text": "Party of National Unity (PNU)"}
{"answers": {"text": ["the Orange Democratic Movement (ODM)", "Kalonzo Musyoka", "Orange Democratic Movement"], "answer_start": [137, 444, 141]}, "prediction_text": "Orange Democratic Movement (ODM)"}
{"answers": {"text": ["Kibaki closed the gap and then overtook his opponent by a substantial margin after votes from his stronghold arrived later", "Kibaki closed the gap and then overtook his opponent", "votes from his stronghold arrived later"], "answer_start": [757, 757, 840]}, "prediction_text": "Odinga declaring himself the \"people's president\" and calling for a recount."}
{"answers": {"text": ["Odinga", "Odinga", "Odinga"], "answer_start": [957, 957, 957]}, "prediction_text": "Raila Odinga"}
{"answers": {"text": ["programmes to avoid similar disasters in the future", "programmes", "programmes"], "answer_start": [81, 81, 81]}, "prediction_text": "programmes"}
{"answers": {"text": ["Truth, Justice and Reconciliation Commission", "Truth, Justice and Reconciliation Commission", "Truth, Justice and Reconciliation Commission"], "answer_start": [304, 304, 304]}, "prediction_text": "Truth, Justice and Reconciliation Commission"}
{"answers": {"text": ["Evangelical Lutheran Church", "Evangelical Lutheran Church", "Evangelical Lutheran Church"], "answer_start": [384, 384, 384]}, "prediction_text": "Evangelical Lutheran Church in Kenya"}
{"answers": {"text": ["Kenya National Dialogue and Reconciliation process", "Kenya National Dialogue and Reconciliation", "Kenya National Dialogue and Reconciliation process"], "answer_start": [452, 452, 452]}, "prediction_text": "Community dialogues."}
{"answers": {"text": ["28 February 2008", "28 February 2008", "28 February 2008"], "answer_start": [3, 3, 3]}, "prediction_text": "28 February 2008"}
{"answers": {"text": ["Prime Minister", "second Prime Minister", "Prime Minister"], "answer_start": [146, 139, 146]}, "prediction_text": "Prime Minister"}
{"answers": {"text": ["both PNU and ODM camps", "PNU and ODM camps", "PNU and ODM camps"], "answer_start": [229, 234, 234]}, "prediction_text": "PNU and ODM camps."}
{"answers": {"text": ["depending on each party's strength in Parliament", "each party's strength in Parliament", "each party's strength in Parliament"], "answer_start": [252, 265, 265]}, "prediction_text": "The agreement stipulated that the cabinet would include a vice-president and two deputy Prime Ministers."}
{"answers": {"text": ["until the end of the current Parliament or if either of the parties withdraws from the deal before then", "end of the current Parliament", "until the end of the current Parliament or if either of the parties withdraws from the deal before then"], "answer_start": [476, 486, 476]}, "prediction_text": "Until the end of the current Parliament."}
{"answers": {"text": ["PM will have power and authority to co-ordinate and supervise the functions of the Government", "co-ordinate and supervise the functions of the Government", "co-ordinate and supervise the functions of the Government"], "answer_start": [22, 58, 58]}, "prediction_text": "The power to co-ordinate and supervise the functions of the Government."}
{"answers": {"text": ["Annan and his UN-backed panel and African Union chairman Jakaya Kikwete", "Annan and his UN-backed panel and African Union chairman Jakaya Kikwete", "Jakaya Kikwete"], "answer_start": [258, 258, 315]}, "prediction_text": "Annan"}
{"answers": {"text": ["the steps of Nairobi's Harambee House", "Nairobi's Harambee House", "steps of Nairobi's Harambee House"], "answer_start": [430, 443, 434]}, "prediction_text": "Nairobi's Harambee House."}
{"answers": {"text": ["29 February 2008", "29 February 2008", "29 February 2008"], "answer_start": [472, 472, 472]}, "prediction_text": "29 February 2008"}
{"answers": {"text": ["the two political parties would share power equally", "two political parties would share power equally", "two political parties would share power equally"], "answer_start": [872, 876, 876]}, "prediction_text": "To salvage a country usually seen as one of the most stable and prosperous in Africa."}
{"answers": {"text": ["eliminate the position of Prime Minister and simultaneously reduce the powers of the President", "eliminate the position of Prime Minister", "eliminate the position of Prime Minister"], "answer_start": [50, 50, 50]}, "prediction_text": "Elimination of Prime Minister and reduction of powers of the President."}
{"answers": {"text": ["August 2010", "4 August 2010", "4 August 2010"], "answer_start": [210, 208, 208]}, "prediction_text": "4 August 2010"}
{"answers": {"text": ["delegates more power to local governments and gives Kenyans a bill of rights", "delegates more power to local governments and gives Kenyans a bill of rights", "delegates more power to local governments and gives Kenyans a bill of rights"], "answer_start": [314, 314, 314]}, "prediction_text": "Powers of the President."}
{"answers": {"text": ["27 August 2010", "27 August 2010", "27 August 2010"], "answer_start": [414, 414, 414]}, "prediction_text": "27 August 2010"}
{"answers": {"text": ["the Second Republic", "Second Republic", "Second Republic"], "answer_start": [650, 654, 654]}, "prediction_text": "Second Republic"}
{"answers": {"text": ["December 2014", "December 2014", "December 2014"], "answer_start": [3, 3, 3]}, "prediction_text": "December 2014"}
{"answers": {"text": ["to guard against armed groups", "guard against armed groups", "guard against armed groups"], "answer_start": [134, 137, 137]}, "prediction_text": "To guard against armed groups."}
{"answers": {"text": ["Opposition politicians, human rights groups, and nine Western countries", "Opposition politicians, human rights groups, and nine Western countries", "Opposition politicians, human rights groups, and nine Western countries"], "answer_start": [165, 165, 165]}, "prediction_text": "Opposition politicians, human rights groups, and nine Western countries."}
{"answers": {"text": ["it infringed on democratic freedoms", "infringed on democratic freedoms", "arguing that it infringed on democratic freedoms"], "answer_start": [280, 283, 267]}, "prediction_text": "Nine Western countries."}
{"answers": {"text": ["of the United States, Britain, Germany and France", "governments", "governments of the United States, Britain, Germany and France"], "answer_start": [333, 321, 321]}, "prediction_text": "Governments of the United States, Britain, Germany, France."}
{"answers": {"text": ["h International Criminal Court trial dates in 2013 for both President Kenyatta and Deputy President William Ruto", "International Criminal Court trial dates", "International Criminal Court trial dates"], "answer_start": [3, 5, 5]}, "prediction_text": "Kenyatta elected."}
{"answers": {"text": ["US President Barack Obama", "US President Barack Obama", "US President Barack Obama"], "answer_start": [156, 156, 156]}, "prediction_text": "Barack Obama"}
{"answers": {"text": ["China", "China", "China"], "answer_start": [285, 285, 285]}, "prediction_text": "China"}
{"answers": {"text": ["In July 2015", "July 2015", "July 2015"], "answer_start": [411, 414, 414]}, "prediction_text": "July 2015"}
{"answers": {"text": ["in peacekeeping missions around the world", "peacekeeping", "peacekeeping"], "answer_start": [40, 43, 43]}, "prediction_text": "Peacekeeping missions."}
{"answers": {"text": ["violence that subsequently engulfed the country", "violence", "violence"], "answer_start": [160, 160, 160]}, "prediction_text": "Violence."}
{"answers": {"text": ["human rights violations", "human rights violations", "human rights violations"], "answer_start": [383, 383, 383]}, "prediction_text": "Human rights violations."}
{"answers": {"text": ["Kenya\u2019s armed forces", "Kenya\u2019s armed forces", "armed"], "answer_start": [0, 0, 8]}, "prediction_text": "Kenya\u2019s armed forces."}
{"answers": {"text": ["Because the operations of the armed forces have been traditionally cloaked by the ubiquitous blanket of \u201cstate security\u201d", "state security", "state security"], "answer_start": [117, 222, 222]}, "prediction_text": "Less in public view."}
{"answers": {"text": ["credible claims of corruption were made with regard to recruitment  and procurement of Armoured Personnel Carriers", "recruitment  and procurement of Armoured Personnel Carriers", "recruitment  and procurement of Armoured Personnel Carriers"], "answer_start": [435, 490, 490]}, "prediction_text": "recruitment and procurement of Armoured Personnel Carriers."}
{"answers": {"text": [", the wisdom and prudence of certain decisions of procurement", "wisdom and prudence of certain decisions", "wisdom and prudence of certain decisions of procurement"], "answer_start": [558, 564, 564]}, "prediction_text": "The wisdom and prudence of certain decisions of procurement."}
{"answers": {"text": ["0.519, ranked 145 out of 186 in the world", "0.519", "0.519"], "answer_start": [162, 162, 162]}, "prediction_text": "0.519"}
{"answers": {"text": ["Kenya", "Kenya", "Kenya"], "answer_start": [9, 9, 9]}, "prediction_text": "Kenya"}
{"answers": {"text": ["less than $1.25 a day", "less than $1.25", "less than $1.25"], "answer_start": [243, 243, 243]}, "prediction_text": "$1.25 a day."}
{"answers": {"text": ["a frontier market or occasionally an emerging market", "frontier", "frontier"], "answer_start": [482, 484, 484]}, "prediction_text": "Frontier market or emerging market."}
{"answers": {"text": ["rapid expansion in telecommunication and financial activity", "rapid expansion in telecommunication and financial activity", "rapid expansion in telecommunication and financial activity"], "answer_start": [105, 105, 105]}, "prediction_text": "Rapid expansion in telecommunication and financial activity."}
{"answers": {"text": ["food security", "food security", "food security"], "answer_start": [408, 408, 408]}, "prediction_text": "Food security."}
{"answers": {"text": ["Industry and manufacturing", "Industry and manufacturing", "Industry and manufacturing"], "answer_start": [537, 537, 537]}, "prediction_text": "Agriculture"}
{"answers": {"text": ["75% of the labour force", "75%", "75%"], "answer_start": [303, 303, 303]}, "prediction_text": "75%"}
{"answers": {"text": ["61%", "61%", "61%"], "answer_start": [43, 43, 43]}, "prediction_text": "61%"}
{"answers": {"text": ["tourism", "tourism", "tourism"], "answer_start": [71, 71, 71]}, "prediction_text": "Tourism"}
{"answers": {"text": ["steady growth", "growth", "growth"], "answer_start": [113, 120, 120]}, "prediction_text": "Growth"}
{"answers": {"text": ["the coastal beaches and the game reserves", "coastal beaches and the game reserves,", "coastal beaches and the game reserves"], "answer_start": [341, 345, 345]}, "prediction_text": "Coastal beaches and game reserves."}
{"answers": {"text": ["Germany and the United Kingdom", "Germany and the United Kingdom", "Germany and the United Kingdom"], "answer_start": [285, 285, 285]}, "prediction_text": "Germany and the United Kingdom."}
{"answers": {"text": ["24%", "24%", "24%"], "answer_start": [180, 180, 180]}, "prediction_text": "24%"}
{"answers": {"text": ["tea, horticultural produce, and coffee", "tea, horticultural produce, and coffee", "tea, horticultural produce, and coffee"], "answer_start": [292, 292, 292]}, "prediction_text": "Tea, horticultural produce, and coffee."}
{"answers": {"text": ["Agriculture", "Agriculture", "Agriculture"], "answer_start": [0, 0, 0]}, "prediction_text": "Agriculture"}
{"answers": {"text": ["weather-related fluctuations", "weather-related", "weather"], "answer_start": [513, 513, 513]}, "prediction_text": "Weather-related fluctuations."}
{"answers": {"text": ["International Crops Research Institute for the Semi-Arid Tropics (ICRISAT)", "ICRISAT", "International Crops Research Institute for the Semi-Arid Tropics"], "answer_start": [24, 90, 24]}, "prediction_text": "ICRISAT"}
{"answers": {"text": ["Pigeon peas are very drought resistant,", "drought resistant", "very drought resistant"], "answer_start": [215, 236, 231]}, "prediction_text": "To grow peas."}
{"answers": {"text": ["by stimulating the growth of local seed production and agro-dealer networks for distribution and marketing", "stimulating the growth of local seed production", "stimulating the growth of local seed production"], "answer_start": [384, 387, 387]}, "prediction_text": "Stimulated the growth of local seed production and agro-dealer networks for distribution and marketing."}
{"answers": {"text": [", helped to increase local producer prices by 20\u201325%", "increase local producer prices by 20\u201325%", "increase"], "answer_start": [550, 562, 562]}, "prediction_text": "20-25%"}
{"answers": {"text": ["enabling some farmers to buy assets", "enabling some farmers to buy assets", "enabling some farmers to buy assets"], "answer_start": [674, 674, 674]}, "prediction_text": "Local producer prices."}
{"answers": {"text": ["the fertile highlands", "fertile highlands", "highlands"], "answer_start": [60, 64, 72]}, "prediction_text": "Africa"}
{"answers": {"text": ["Tea, coffee, sisal, pyrethrum, corn, and wheat", "ea, coffee, sisal, pyrethrum, corn, and wheat", "Tea, coffee, sisal, pyrethrum, corn, and wheat"], "answer_start": [0, 1, 0]}, "prediction_text": "Tea, coffee, sisal, pyrethrum, corn, and wheat."}
{"answers": {"text": ["the semi-arid savanna to the north and east", "savanna to the north and east", "semi-arid savanna to the north and east"], "answer_start": [179, 193, 183]}, "prediction_text": "The livestock of the country normally dominate in the fertile highlands."}
{"answers": {"text": ["53% of the population", "53%", "53%"], "answer_start": [497, 497, 497]}, "prediction_text": "53%"}
{"answers": {"text": ["Kenyans for Kenya", "Kenyans for Kenya", "Kenyans for Kenya"], "answer_start": [955, 955, 955]}, "prediction_text": "Kenya initiative for Kenya."}
{"answers": {"text": ["Kenya", "Kenya", "Kenya"], "answer_start": [9, 9, 9]}, "prediction_text": "Kenya"}
{"answers": {"text": ["14%", "14%", "14%"], "answer_start": [131, 131, 131]}, "prediction_text": "14%"}
{"answers": {"text": ["Nairobi, Mombasa and Kisumu", "Nairobi, Mombasa and Kisumu", ", Nairobi, Mombasa and Kisumu"], "answer_start": [221, 221, 219]}, "prediction_text": "Nairobi, Mombasa, and Kisumu."}
{"answers": {"text": ["small-scale manufacturing of household goods, motor-vehicle parts, and farm implements", "small-scale manufacturing", "small-scale manufacturing"], "answer_start": [702, 702, 702]}, "prediction_text": "Small-scale manufacturing of household goods, motor-vehicle parts, and farm implements."}
{"answers": {"text": ["Kenya's inclusion among the beneficiaries of the US Government's African Growth and Opportunity Act (AGOA)", "AGOA", "African Growth and Opportunity Act"], "answer_start": [0, 101, 65]}, "prediction_text": "AGOA"}
{"answers": {"text": ["2000", "2000", "2000"], "answer_start": [185, 185, 185]}, "prediction_text": "2000"}
{"answers": {"text": ["hydroelectric stations at dams", "hydroelectric stations at dams", "dams"], "answer_start": [59, 59, 85]}, "prediction_text": "hydroelectric stations along the upper Tana River."}
{"answers": {"text": ["Tana River, as well as the Turkwel Gorge Dam", "upper Tana River, as well as the Turkwel Gorge", "along the upper Tana River"], "answer_start": [106, 100, 90]}, "prediction_text": "Upper Tana River."}
{"answers": {"text": ["1997", "1997", "1997"], "answer_start": [473, 473, 473]}, "prediction_text": "1997"}
{"answers": {"text": ["in Turkana", "Turkana", "Turkana"], "answer_start": [33, 36, 36]}, "prediction_text": "Turkana"}
{"answers": {"text": ["around 10 billion barrels.", "10 billion", "10 billion"], "answer_start": [142, 149, 149]}, "prediction_text": "10 billion barrels."}
{"answers": {"text": ["Exploration", "Exploration", "Exploration"], "answer_start": [169, 169, 169]}, "prediction_text": "Exploration."}
{"answers": {"text": ["r 20% to 25%", "20% to 25%", "20% to 25%"], "answer_start": [478, 480, 480]}, "prediction_text": "20% to 25%"}
{"answers": {"text": ["$474 million", "$474 million", "$474 million"], "answer_start": [201, 201, 201]}, "prediction_text": "$474 million"}
{"answers": {"text": ["Kenya's largest source of foreign direct investment", "largest source of foreign direct investment"], "answer_start": [228, 236]}, "prediction_text": "$474 million."}
{"answers": {"text": ["support from China for a planned $2.5 billion railway from the southern Kenyan port of Mombasa to neighboring Uganda", "support from China for a planned $2.5 billion railway", "support from China for a planned $2.5 billion railway"], "answer_start": [422, 422, 422]}, "prediction_text": "Support for a $2.5 billion railway project."}
{"answers": {"text": ["Base Titanium, a subsidiary of Base resources of Australia", "Base Titanium", "Base Titanium"], "answer_start": [662, 662, 662]}, "prediction_text": "Base Titanium"}
{"answers": {"text": ["environmental and social problems", "environmental and social problems", "environmental and social problems"], "answer_start": [973, 973, 973]}, "prediction_text": "Environmental and social problems."}
{"answers": {"text": ["Vision 2030", "Vision 2030", "Vision 2030"], "answer_start": [40, 40, 40]}, "prediction_text": "Vision 2030"}
{"answers": {"text": ["an economic development programme it hopes will put the country in the same league as the Asian Economic Tigers by the year 2030", "economic development programme", "an economic development programme"], "answer_start": [53, 56, 53]}, "prediction_text": "Economic development programme."}
{"answers": {"text": ["National Climate Change Action Plan", "2013", "National Climate Change Action Plan"], "answer_start": [206, 186, 206]}, "prediction_text": "National Climate Change Action Plan"}
{"answers": {"text": ["having acknowledged that omitting climate as a key development issue in Vision 2030 was an oversight", "oversight", "oversight"], "answer_start": [243, 334, 334]}, "prediction_text": "To ensure climate change is treated as an economy-wide issue."}
{"answers": {"text": ["climate will be a central issue in the renewed Medium Term Plan that will be launched in the coming months", "Medium Term Plan", "Medium Term Plan"], "answer_start": [669, 716, 716]}, "prediction_text": "Climate change."}
{"answers": {"text": ["in agriculture", "agriculture", "agriculture"], "answer_start": [66, 69, 69]}, "prediction_text": "Agriculture."}
{"answers": {"text": ["up to 30%", "30%", "up to 30%"], "answer_start": [113, 119, 113]}, "prediction_text": "30%"}
{"answers": {"text": ["9\u201318.", "9\u201318", "9\u201318"], "answer_start": [264, 264, 264]}, "prediction_text": "9-18."}
{"answers": {"text": ["poverty, the lack of access to education and weak government institutions", "poverty, the lack of access to education and weak government institutions", "poverty, the lack of access to education and weak government institutions"], "answer_start": [394, 394, 394]}, "prediction_text": "poverty, lack of access to education, weak government institutions."}
{"answers": {"text": ["Kenya's various ethnic groups typically speak their mother tongues within their own communities", "mother tongues", "mother tongues"], "answer_start": [0, 52, 52]}, "prediction_text": "English"}
{"answers": {"text": ["English and Swahili", "English and Swahili", "English and Swahili,"], "answer_start": [125, 125, 125]}, "prediction_text": "English and Swahili."}
{"answers": {"text": ["in commerce, schooling and government", "commerce, schooling and government.", "commerce, schooling and government"], "answer_start": [252, 255, 255]}, "prediction_text": "Commerce, schooling, government."}
{"answers": {"text": ["in the country", "country", "in the country"], "answer_start": [441, 448, 441]}, "prediction_text": "The country."}
{"answers": {"text": ["Christian", "Christian", "Christian"], "answer_start": [33, 33, 33]}, "prediction_text": "Christian"}
{"answers": {"text": ["Protestant", "Protestant", "Protestant"], "answer_start": [85, 85, 85]}, "prediction_text": "Christian"}
{"answers": {"text": ["3 million followers", "3 million", "3 million"], "answer_start": [186, 186, 186]}, "prediction_text": "3 million."}
{"answers": {"text": ["Nairobi", "Nairobi", "Nairobi"], "answer_start": [630, 630, 630]}, "prediction_text": "Nairobi"}
{"answers": {"text": ["2.4%", "2.4%", "2.4%"], "answer_start": [103, 103, 103]}, "prediction_text": "2.4%"}
{"answers": {"text": ["Sixty percent", "Sixty percent", "Sixty percent"], "answer_start": [109, 109, 109]}, "prediction_text": "50%"}
{"answers": {"text": ["mostly Christian", "mostly Christian", "Christian"], "answer_start": [378, 378, 385]}, "prediction_text": "Christian"}
{"answers": {"text": ["around 300,000", "300,000", "300,000"], "answer_start": [587, 594, 594]}, "prediction_text": "300,000"}
{"answers": {"text": ["Nurses", "Nurses", "Nurses"], "answer_start": [0, 0, 0]}, "prediction_text": "Clinical officers, medical officers, medical practitioners."}
{"answers": {"text": ["clinical officers, medical officers and medical practitioners", "clinical officers, medical officers and medical practitioners", "clinical officers, medical officers and medical practitioners"], "answer_start": [167, 167, 167]}, "prediction_text": "Clinical officers, medical officers, and medical practitioners."}
{"answers": {"text": ["65,000", "65,000", "65,000"], "answer_start": [303, 303, 303]}, "prediction_text": "65,000"}
{"answers": {"text": ["7,000 doctors", "7,000", "7,000"], "answer_start": [382, 382, 382]}, "prediction_text": "7,000"}
{"answers": {"text": ["Diseases of poverty", "Diseases of poverty", "Diseases of poverty"], "answer_start": [0, 0, 0]}, "prediction_text": "Diseases of poverty."}
{"answers": {"text": ["Half", "Half", "Half"], "answer_start": [102, 102, 102]}, "prediction_text": "Half."}
{"answers": {"text": ["diseases like malaria, HIV/AIDS, pneumonia, diarrhoea and malnutrition", "Preventable diseases like malaria, HIV/AIDS, pneumonia, diarrhoea and malnutrition", "Preventable diseases"], "answer_start": [160, 148, 148]}, "prediction_text": "Malnutrition, weak policies, corruption, inadequate health workers, weak management, and poor leadership in the public health sector."}
{"answers": {"text": ["weak policies, corruption, inadequate health workers, weak management and poor leadership in the public health sector", "weak policies, corruption, inadequate health workers, weak management and poor leadership", "weak policies, corruption, inadequate health workers, weak management and poor leadership"], "answer_start": [312, 312, 312]}, "prediction_text": "Weak policies, corruption, inadequate health workers, weak management, and poor leadership in the public health sector."}
{"answers": {"text": ["15 million", "15 million", "15 million"], "answer_start": [731, 731, 731]}, "prediction_text": "15 million."}
{"answers": {"text": ["British colonists.", "British colonists", "British colonists"], "answer_start": [52, 52, 52]}, "prediction_text": "British colonists"}
{"answers": {"text": ["12 December 1963", "12 December 1963", "12 December 1963"], "answer_start": [101, 101, 101]}, "prediction_text": "12 December 1963"}
{"answers": {"text": ["Ominde Commission", "Ominde Commission", "Ominde Commission"], "answer_start": [142, 142, 142]}, "prediction_text": "Ominde Commission"}
{"answers": {"text": ["focused on identity and unity, which were critical issues at the time", "focused on identity and unity", "focused on identity and unity"], "answer_start": [252, 252, 252]}, "prediction_text": "Introduced changes to the subject content of history and geography."}
{"answers": {"text": ["the 7\u20134\u20132\u20133 system was adopted", "7\u20134\u20132\u20133 system", "7\u20134\u20132\u20133"], "answer_start": [442, 446, 446]}, "prediction_text": "7-4-2-3"}
{"answers": {"text": ["look at both the possibilities of setting up a second university in Kenya as well as the reforming of the entire education system", "look at both the possibilities", "look at both the possibilities of setting up a second university in Kenya as well as the reforming of the entire education system"], "answer_start": [85, 85, 85]}, "prediction_text": "To look at both the possibilities of setting up a second university in Kenya and reforming the entire education system."}
{"answers": {"text": ["8\u20134\u20134 system", "8\u20134\u20134 system", "an 8\u20134\u20134 system"], "answer_start": [283, 283, 280]}, "prediction_text": "Reformed."}
{"answers": {"text": ["8\u20134\u20134 system", "7\u20134\u20132\u20133 system", "7\u20134\u20132\u20133"], "answer_start": [467, 251, 494]}, "prediction_text": "8\u20134\u20134 system"}
{"answers": {"text": ["1992", "1992", "1992"], "answer_start": [672, 672, 672]}, "prediction_text": "1992"}
{"answers": {"text": ["January 1985", "January 1985", "January 1985"], "answer_start": [41, 41, 41]}, "prediction_text": "January 1985"}
{"answers": {"text": ["vocational subjects", "vocational subjects", "vocational subjects"], "answer_start": [79, 79, 79]}, "prediction_text": "vocational subjects"}
{"answers": {"text": ["the new structure would enable school drop-outs at all levels either to be self-employed or to secure employment in the informal sector", "enable school drop-outs at all levels either to be self-employed or to secure employment", "enable school drop-outs at all levels either to be self-employed or to secure employment"], "answer_start": [122, 146, 146]}, "prediction_text": "To enable school drop-outs at all levels."}
{"answers": {"text": ["January 2003", "January 2003", "January 2003"], "answer_start": [262, 262, 262]}, "prediction_text": "2003"}
{"answers": {"text": ["increased by about 70%.", "70%", "70%"], "answer_start": [392, 411, 411]}, "prediction_text": "70%"}
{"answers": {"text": ["age six years", "six", "six"], "answer_start": [33, 37, 37]}, "prediction_text": "Six years."}
{"answers": {"text": ["eight years in primary school and four years in high school or secondary school.", "eight years in primary school and four years in high school", "eight years in primary school and four years in high school"], "answer_start": [77, 77, 77]}, "prediction_text": "Primary school: free, secondary school: free, high school: free, vocational youth/village polytechnic: free, technical college: free, university: free."}
{"answers": {"text": ["join a vocational youth/village polytechnic or make their own arrangements for an apprenticeship program", "join a vocational youth/village polytechnic", "four years in high school"], "answer_start": [236, 236, 111]}, "prediction_text": "They join a vocational youth/village polytechnic or make their own arrangements for an apprenticeship program."}
{"answers": {"text": ["join a polytechnic or other technical college and study for three years or proceed directly to the university and study for four years", "join a polytechnic or other technical college", "join a vocational youth/village polytechnic"], "answer_start": [492, 492, 236]}, "prediction_text": "Join a polytechnic or other technical college."}
{"answers": {"text": ["85%", "85%", "85%"], "answer_start": [39, 39, 39]}, "prediction_text": "85%"}
{"answers": {"text": ["age three to five", "three to five", "age three to five"], "answer_start": [107, 111, 107]}, "prediction_text": "3-5 years."}
{"answers": {"text": ["a key requirement for admission to Standard One (First Grade)", "admission to Standard One", "admission to Standard One"], "answer_start": [182, 204, 204]}, "prediction_text": "Admission to Standard One (First Grade)"}
{"answers": {"text": ["those who proceed to secondary school or vocational training", "those who proceed to secondary school or vocational training", "those who proceed to secondary school or vocational training"], "answer_start": [357, 357, 357]}, "prediction_text": "The KCPE determines the following:"}
{"answers": {"text": ["the Kenya Certificate of Secondary Education", "Kenya Certificate of Secondary Education", "determines those proceeding to the universities"], "answer_start": [634, 638, 693]}, "prediction_text": "Kenya Certificate of Secondary Education (KCSE)"}
{"answers": {"text": ["the Kenya National Library Service", "Kenya National Library Service", "Kenya National Library Service"], "answer_start": [99, 103, 103]}, "prediction_text": "KNLS"}
{"answers": {"text": ["establish, equip, manage and maintain national and public libraries in the country", "establish, equip, manage and maintain national and public libraries", "establish, equip, manage and maintain national and public libraries"], "answer_start": [171, 171, 171]}, "prediction_text": "Establishing, equipping, managing and maintaining national and public libraries."}
{"answers": {"text": ["a peoples university", "peoples university", "peoples university"], "answer_start": [565, 567, 567]}, "prediction_text": "People of all walks of life."}
{"answers": {"text": ["it is open to all irrespective of age, literacy level and has materials relevant to people of all walks of life", "open to all irrespective of age, literacy level", "open to all irrespective of age, literacy level"], "answer_start": [592, 598, 598]}, "prediction_text": "Open to all irrespective of age, literacy level, and materials relevant to people of all walks of life."}
{"answers": {"text": ["cricket, rallying, football, rugby union and boxing", "cricket, rallying, football, rugby union and boxing", "cricket, rallying, football, rugby union and boxing"], "answer_start": [46, 46, 46]}, "prediction_text": "Cricket, rallying, football, rugby union, boxing."}
{"answers": {"text": ["its dominance in middle-distance and long-distance athletics", "middle-distance and long-distance athletics", "dominance in middle-distance and long-distance athletics"], "answer_start": [132, 149, 136]}, "prediction_text": "Middle-distance and long-distance athletics."}
{"answers": {"text": ["Kenyan athletes (particularly Kalenjin)", "Kenyan athletes", "Kenya"], "answer_start": [380, 380, 0]}, "prediction_text": "Kalenjin"}
{"answers": {"text": ["Morocco and Ethiopia", "Morocco and Ethiopia", "Morocco and Ethiopia"], "answer_start": [498, 498, 498]}, "prediction_text": "Morocco and Ethiopia."}
{"answers": {"text": ["six gold", "several", "six"], "answer_start": [54, 10, 54]}, "prediction_text": "Six."}
{"answers": {"text": ["Africa's most successful nation in the 2008 Olympics", "Africa's most successful nation", "Africa's most successful nation"], "answer_start": [103, 103, 103]}, "prediction_text": "Six medals."}
{"answers": {"text": ["IAAF Golden League jackpot", "IAAF Golden League jackpot", "IAAF Golden League jackpot"], "answer_start": [268, 268, 268]}, "prediction_text": "800m gold medalist."}
{"answers": {"text": ["the defection of a number of Kenyan athletes to represent other countries", "defection of a number of Kenyan athletes", "a number of Kenyan athletes to represent other countries"], "answer_start": [641, 645, 658]}, "prediction_text": "Defection of Kenyan athletes to represent other countries."}
{"answers": {"text": ["economic or financial factors", "economic or financial factors", "economic or financial factors"], "answer_start": [953, 953, 953]}, "prediction_text": "Economic or financial factors."}
{"answers": {"text": ["women's volleyball within Africa", "volleyball", "volleyball"], "answer_start": [35, 43, 43]}, "prediction_text": "Volleyball"}
{"answers": {"text": ["Cricket", "Cricket", "Cricket"], "answer_start": [293, 293, 293]}, "prediction_text": "Cricket"}
{"answers": {"text": ["2003", "2003", "2003"], "answer_start": [485, 485, 485]}, "prediction_text": "2003"}
{"answers": {"text": ["Rakep Patel", "Rakep Patel", "Rakep Patel"], "answer_start": [635, 635, 635]}, "prediction_text": "Rakep Patel"}
{"answers": {"text": ["March 2007", "March 2007", "March 2007"], "answer_start": [1296, 1296, 1296]}, "prediction_text": "March 2007"}
{"answers": {"text": ["the world famous Safari Rally", "Safari Rally", "world famous Safari Rally"], "answer_start": [46, 63, 50]}, "prediction_text": "Safari Rally"}
{"answers": {"text": ["one of the toughest rallies in the world", "toughest rallies in the world", "toughest rallies in the world."], "answer_start": [102, 113, 113]}, "prediction_text": "World Rally Championship"}
{"answers": {"text": ["Bj\u00f6rn Waldeg\u00e5rd, Hannu Mikkola, Tommi M\u00e4kinen, Shekhar Mehta, Carlos Sainz and Colin McRae", "Bj\u00f6rn Waldeg\u00e5rd, Hannu Mikkola, Tommi M\u00e4kinen, Shekhar Mehta, Carlos Sainz and Colin McRae", "Bj\u00f6rn Waldeg\u00e5rd, Hannu Mikkola, Tommi M\u00e4kinen, Shekhar Mehta, Carlos Sainz and Colin McRae"], "answer_start": [369, 369, 369]}, "prediction_text": "Bj\u00f6rn Waldeg\u00e5rd, Hannu Mikkola, Tommi M\u00e4kinen, Shekhar Mehta, Carlos Sainz, Colin McRae."}
{"answers": {"text": ["three meals in a day", "three", "three"], "answer_start": [23, 23, 23]}, "prediction_text": "3 meals."}
{"answers": {"text": ["10 o'clock tea (chai ya saa nne) and 4 pm tea", "10 o'clock", "10 o'clock tea (chai ya saa nne) and 4 pm"], "answer_start": [234, 234, 234]}, "prediction_text": "10 o'clock."}
{"answers": {"text": ["tea or porridge with bread, chapati, mahamri, boiled sweet potatoes or yams", "porridge with bread, chapati, mahamri, boiled sweet potatoes or yams", "porridge with bread, chapati, mahamri, boiled sweet potatoes or yams"], "answer_start": [321, 328, 328]}, "prediction_text": "Bread, chapati, mahamri, boiled sweet potatoes, yams."}
{"answers": {"text": ["Ugali with vegetables, sour milk, meat, fish or any other stew", "Ugali with vegetables, sour milk, meat, fish or any other stew", "Ugali with vegetables, sour milk, meat, fish or any other stew"], "answer_start": [398, 398, 398]}, "prediction_text": "Ugali."}
{"answers": {"text": ["the United Nations", "the United Nations", "the United Nations"], "answer_start": [114, 114, 114]}, "prediction_text": "United Nations Organization (UN)"}
{"answers": {"text": ["the World Meteorological Organization (WMO) and the United Nations Environment Programme (UNEP)", "World Meteorological Organization (WMO) and the United Nations Environment Programme (UNEP),", "the World Meteorological Organization (WMO) and the United Nations Environment Programme (UNEP)"], "answer_start": [249, 253, 249]}, "prediction_text": "WMO and UNEP."}
{"answers": {"text": ["greenhouse gas concentrations in the atmosphere", "greenhouse gas concentrations in the atmosphere", "stabilize greenhouse gas concentrations in the atmosphere"], "answer_start": [716, 716, 706]}, "prediction_text": "greenhouse gas concentrations in the atmosphere."}
{"answers": {"text": ["United Nations Framework Convention on Climate Change", "United Nations Framework Convention on Climate Change (UNFCCC)", "the United Nations Framework Convention on Climate Change (UNFCCC),"], "answer_start": [540, 540, 536]}, "prediction_text": "UNFCCC"}
{"answers": {"text": ["Resolution 43/53", "Resolution 43/53", "Resolution 43/53"], "answer_start": [412, 412, 412]}, "prediction_text": "Resolution 43/53."}
{"answers": {"text": ["Hoesung Lee", "Hoesung Lee", "Hoesung Lee"], "answer_start": [17, 17, 17]}, "prediction_text": "Hoesung Lee"}
{"answers": {"text": ["Korean", "onomist", "Korean"], "answer_start": [0, 9, 0]}, "prediction_text": "Korean"}
{"answers": {"text": ["Ismail El Gizouli", "Ismail El Gizouli", "Ismail El Gizouli"], "answer_start": [181, 181, 181]}, "prediction_text": "Hoesung Lee"}
{"answers": {"text": ["Bert Bolin", "Bert Bolin", "Bert Bolin"], "answer_start": [391, 391, 391]}, "prediction_text": "Hoesung Lee"}
{"answers": {"text": ["February 2015", "February 2015", "February 2015"], "answer_start": [281, 281, 281]}, "prediction_text": "February 2015"}
{"answers": {"text": ["representatives appointed by governments and organizations", "representatives appointed by governments and organizations", "representatives appointed by governments and organizations"], "answer_start": [30, 30, 30]}, "prediction_text": "Government officials and climate change experts."}
{"answers": {"text": ["350", "350", "350"], "answer_start": [494, 494, 494]}, "prediction_text": "350"}
{"answers": {"text": ["government officials and climate change experts", "government officials and climate change experts", "government officials and climate change experts"], "answer_start": [498, 498, 498]}, "prediction_text": "Government officials."}
{"answers": {"text": ["about seven-eighths", "seven-eighths", "about seven-eighths"], "answer_start": [692, 698, 692]}, "prediction_text": "Seven-eighths."}
{"answers": {"text": ["1989", "1989", "1989"], "answer_start": [70, 70, 70]}, "prediction_text": "1989"}
{"answers": {"text": ["the United Nations Environment Programme (UNEP) and the World Meteorological Organization (WMO)", "United Nations Environment Programme (UNEP) and the World Meteorological Organization (WMO)", "the United Nations Environment Programme (UNEP) and the World Meteorological Organization (WMO),"], "answer_start": [78, 82, 78]}, "prediction_text": "United Nations Environment Programme (UNEP)"}
{"answers": {"text": ["United Nations Environment Programme", "UNEP", "UNEP"], "answer_start": [82, 260, 260]}, "prediction_text": "WMO"}
{"answers": {"text": ["the Financial Regulations and Rules of the WMO", "Financial Regulations and Rules of the WMO", "Financial Regulations and Rules of the WMO"], "answer_start": [586, 590, 590]}, "prediction_text": "Financial Regulations and Rules of the WMO."}
{"answers": {"text": ["World Meteorological Organization", "WMO", "the WMO"], "answer_start": [134, 249, 245]}, "prediction_text": "WMO"}
{"answers": {"text": ["does not carry out research nor does it monitor climate related data", "research", "carry out research nor does it monitor climate related data"], "answer_start": [9, 28, 18]}, "prediction_text": "Research."}
{"answers": {"text": ["available information about climate change based on published sources", "published sources", "peer-reviewed sources"], "answer_start": [119, 171, 252]}, "prediction_text": "Published sources."}
{"answers": {"text": ["non-peer-reviewed sources", "non-peer-reviewed sources", "non-peer-reviewed sources"], "answer_start": [296, 296, 296]}, "prediction_text": "Model results, reports from government agencies, and non-governmental organizations."}
{"answers": {"text": ["model results, reports from government agencies and non-governmental organizations, and industry journals", "model results", "model results, reports from government agencies and non-governmental organizations, and industry journals"], "answer_start": [439, 439, 439]}, "prediction_text": "Model results, reports from government agencies, and industry journals."}
{"answers": {"text": ["two", "two", "two"], "answer_start": [119, 119, 119]}, "prediction_text": "2"}
{"answers": {"text": ["ten to fifteen", "ten to fifteen", "ten to fifteen"], "answer_start": [152, 152, 152]}, "prediction_text": "2"}
{"answers": {"text": ["a somewhat larger number", "a somewhat larger number of \"contributing authors\"", "a somewhat larger number"], "answer_start": [187, 187, 187]}, "prediction_text": "10-15"}
{"answers": {"text": ["The coordinating lead authors", "coordinating lead authors", "coordinating lead authors"], "answer_start": [239, 243, 243]}, "prediction_text": "The coordinating lead authors."}
{"answers": {"text": ["the Working Group chairs", "Working Group chairs", "Working Group chairs"], "answer_start": [420, 424, 424]}, "prediction_text": "Working Group chairs"}
{"answers": {"text": ["substantially increasing the atmospheric concentrations", "substantially increasing the atmospheric concentrations of the greenhouse gases", "substantially increasing the atmospheric concentrations"], "answer_start": [139, 139, 139]}, "prediction_text": "Increasing atmospheric concentrations of greenhouse gases."}
{"answers": {"text": ["additional warming of the Earth's surface", "warming of the Earth's surface", "additional warming of the Earth's surface"], "answer_start": [247, 258, 247]}, "prediction_text": "additional warming of the Earth's surface."}
{"answers": {"text": ["over half", "over half", "over half"], "answer_start": [355, 355, 355]}, "prediction_text": "Half."}
{"answers": {"text": ["\"business as usual\" (BAU)", "enhanced greenhouse effect", "\"business as usual\" (BAU)"], "answer_start": [423, 369, 423]}, "prediction_text": "BAU (business as usual)"}
{"answers": {"text": ["increased by 0.3 to 0.6 \u00b0C", "0.3 to 0.6 \u00b0C", "0.3 to 0.6 \u00b0C"], "answer_start": [607, 620, 620]}, "prediction_text": "0.3 to 0.6 \u00b0C."}
{"answers": {"text": ["2001", "2001", "2001"], "answer_start": [3, 3, 3]}, "prediction_text": "2001"}
{"answers": {"text": ["16 national science academies", "16", "16"], "answer_start": [9, 9, 9]}, "prediction_text": "16"}
{"answers": {"text": ["Science", "Science", "Science"], "answer_start": [787, 787, 787]}, "prediction_text": "Science"}
{"answers": {"text": ["at least 90%", "at least 90%", "at least 90% certain"], "answer_start": [849, 849, 849]}, "prediction_text": "90% certain."}
{"answers": {"text": ["between 1.4 and 5.8 \u00b0C above 1990 levels", "1.4 and 5.8 \u00b0C", "between 1.4 and 5.8 \u00b0C"], "answer_start": [976, 984, 976]}, "prediction_text": "1.4 to 5.8 \u00b0C."}
{"answers": {"text": ["Richard Lindzen", "Richard Lindzen", "Richard Lindzen"], "answer_start": [12, 12, 12]}, "prediction_text": "Richard Lindzen"}
{"answers": {"text": ["does not faithfully summarize the full WGI report", "understates the uncertainty associated with climate models", "does not faithfully summarize the full WGI report"], "answer_start": [157, 249, 157]}, "prediction_text": "The SPM understates the uncertainty associated with climate models."}
{"answers": {"text": ["John Houghton", "John Houghton", "John Houghton,"], "answer_start": [309, 309, 309]}, "prediction_text": "John Houghton"}
{"answers": {"text": ["a co-chair of TAR WGI", "co-chair of TAR WGI", "co-chair of TAR WGI"], "answer_start": [332, 334, 334]}, "prediction_text": "Co-chair of TAR WGI."}
{"answers": {"text": ["scientific evidence", "must be supported by scientific evidence", "must be supported by scientific evidence"], "answer_start": [559, 538, 538]}, "prediction_text": "Scientific evidence."}
{"answers": {"text": ["the same procedures as for IPCC Assessment Reports", "follows the same procedures as for IPCC Assessment Reports", "the same procedures as for IPCC Assessment Reports"], "answer_start": [176, 168, 176]}, "prediction_text": "The IPCC prepares Special Reports on specific topics."}
{"answers": {"text": ["2011", "2011", "2011"], "answer_start": [240, 240, 240]}, "prediction_text": "2011"}
{"answers": {"text": ["2011", "2011", "2011"], "answer_start": [240, 240, 240]}, "prediction_text": "2011"}
{"answers": {"text": ["requested by governments", "requested by governments", "requested by governments."], "answer_start": [514, 514, 514]}, "prediction_text": "Governments."}
{"answers": {"text": ["the Data Distribution Centre and the National Greenhouse Gas Inventories Programme", "Data Distribution Centre and the National Greenhouse Gas Inventories Programme", "Data Distribution Centre and the National Greenhouse Gas Inventories Programme"], "answer_start": [329, 333, 333]}, "prediction_text": "Data Distribution Centre, National Greenhouse Gas Inventories Programme."}
{"answers": {"text": ["default emission factors", "default emission factors", "default emission factors"], "answer_start": [465, 465, 465]}, "prediction_text": "Default emission factors."}
{"answers": {"text": ["fuel consumption, industrial production and so on", "Greenhouse Gas Inventories", "fuel consumption, industrial production and so on"], "answer_start": [567, 375, 567]}, "prediction_text": "Fuel consumption, industrial production, and so on."}
{"answers": {"text": ["WMO Executive Council and UNEP Governing Council", "WMO Executive Council and UNEP Governing Council", "WMO Executive Council and UNEP Governing Council"], "answer_start": [81, 81, 81]}, "prediction_text": "WMO Executive Council, UNEP Governing Council."}
{"answers": {"text": ["the date", "the date", "the date"], "answer_start": [109, 109, 109]}, "prediction_text": "The date of 2035."}
{"answers": {"text": ["\"the poor application of well-established IPCC procedures in this instance\"", "the poor application of well-established IPCC procedures", "poor application of well-established IPCC procedures"], "answer_start": [229, 230, 234]}, "prediction_text": "The IPCC apologized for \"the poor application of well-established IPCC procedures in this instance\"."}
{"answers": {"text": ["the WWF report", "the IPCC from the WWF report", "the WWF report"], "answer_start": [366, 352, 366]}, "prediction_text": "WWF report"}
{"answers": {"text": ["\"Variations of Snow and Ice in the past and at present on a Global and Regional Scale\"", "the IPCC from the WWF report", "the WWF report"], "answer_start": [433, 352, 366]}, "prediction_text": "WWF report"}
{"answers": {"text": ["IPCC chairman", "chairman", "chairman"], "answer_start": [7, 12, 12]}, "prediction_text": "Former IPCC chairman."}
{"answers": {"text": ["making it seem like climate change is more serious by overstating the impact", "the direction of making it seem like climate change is more serious", "making it seem like climate change is more serious by overstating the impact"], "answer_start": [102, 85, 102]}, "prediction_text": "The direction of making it seem like climate change is more serious."}
{"answers": {"text": ["co-chair of the IPCC working group II", "co-chair of the IPCC working group II", "co-chair of the IPCC working group II"], "answer_start": [319, 319, 319]}, "prediction_text": "Climate expert."}
{"answers": {"text": ["Himalayan glaciers", "Himalayan glaciers"], "answer_start": [417, 417]}, "prediction_text": "Himalayan glaciers"}
{"answers": {"text": ["\"generally unfounded and also marginal to the assessment\"", "generally unfounded and also marginal to the assessment", "generally unfounded and also marginal to the assessment"], "answer_start": [545, 546, 546]}, "prediction_text": "Marginal to the assessment."}
{"answers": {"text": ["1999", "1999", "1999"], "answer_start": [142, 142, 142]}, "prediction_text": "1999"}
{"answers": {"text": ["Michael E. Mann, Raymond S. Bradley and Malcolm K. Hughes", "Michael E. Mann, Raymond S. Bradley and Malcolm K. Hughes", "Michael E. Mann, Raymond S. Bradley and Malcolm K. Hughes"], "answer_start": [156, 156, 156]}, "prediction_text": "Michael E. Mann"}
{"answers": {"text": ["the \"hockey stick graph\"", "hockey stick graph", "hockey stick graph"], "answer_start": [253, 258, 258]}, "prediction_text": "Hockey stick."}
{"answers": {"text": ["Jones et al. 1998, Pollack, Huang & Shen 1998, Crowley & Lowery 2000 and Briffa 2000", "Jones et al. and Briffa reconstructions", "temperatures increased on the basis of documentary evidence of Medieval vineyards in England"], "answer_start": [1049, 1173, 755]}, "prediction_text": "Jones et al. 1998, Pollack, Huang & Shen 1998, Crowley & Lowery 2000, Briffa 2000."}
{"answers": {"text": ["between 1000 and 1900", "1000 and 1900", "1000 and 1900"], "answer_start": [130, 138, 138]}, "prediction_text": "1000-1900."}
{"answers": {"text": ["Fred Singer", "Fred Singer", "Fred Singer"], "answer_start": [281, 281, 281]}, "prediction_text": "Fred Singer"}
{"answers": {"text": ["Capitol Hill, Washington, D.C.", "Capitol Hill, Washington, D.C.", "Capitol Hill, Washington, D.C."], "answer_start": [358, 358, 358]}, "prediction_text": "Capitol Hill."}
{"answers": {"text": ["18 July 2000", "May 2000", "18 July 2000"], "answer_start": [560, 272, 560]}, "prediction_text": "18 July 2000."}
{"answers": {"text": ["United States Senate Committee on Commerce, Science and Transportation", "Committee on Commerce, Science and Transportation", "United States Senate Committee on Commerce, Science and Transportation"], "answer_start": [478, 499, 478]}, "prediction_text": "Commerce, Science and Transportation."}
{"answers": {"text": ["Rep. Joe Barton", "Rep. Joe Barton", "Rep. Joe Barton"], "answer_start": [17, 17, 17]}, "prediction_text": "Joe Barton"}
{"answers": {"text": ["Ed Whitfield", "Ed Whitfield", "Ed Whitfield"], "answer_start": [114, 114, 114]}, "prediction_text": "Ed Whitfield"}
{"answers": {"text": ["23 June 2005", "23 June 2005", "23 June 2005"], "answer_start": [3, 3, 3]}, "prediction_text": "23 June 2005"}
{"answers": {"text": ["Sherwood Boehlert", "Sherwood Boehlert", "Sherwood Boehlert"], "answer_start": [330, 330, 330]}, "prediction_text": "Sherwood Boehlert"}
{"answers": {"text": ["Sherwood Boehlert", "Sherwood Boehlert", "Sherwood Boehlert, chairman of the House Science Committee"], "answer_start": [330, 330, 330]}, "prediction_text": "Sherwood Boehlert"}
{"answers": {"text": ["2001", "2001", "2001"], "answer_start": [166, 166, 166]}, "prediction_text": "2001"}
{"answers": {"text": ["2007", "2007", "2007"], "answer_start": [53, 53, 53]}, "prediction_text": "2007"}
{"answers": {"text": ["Ten", "14", "Ten"], "answer_start": [463, 476, 463]}, "prediction_text": "14"}
{"answers": {"text": ["divergence", "divergence problem", "divergence"], "answer_start": [740, 740, 740]}, "prediction_text": "Divergence problem."}
{"answers": {"text": ["14", "Ten", "14"], "answer_start": [476, 463, 476]}, "prediction_text": "14"}
{"answers": {"text": ["1 February 2007", "1 February 2007", "1 February 2007"], "answer_start": [3, 3, 3]}, "prediction_text": "2007"}
{"answers": {"text": ["temperatures and sea levels have been rising at or above the maximum rates", "temperatures and sea levels have been rising at or above the maximum rates proposed", "temperatures and sea levels have been rising at or above the maximum rates proposed"], "answer_start": [120, 120, 120]}, "prediction_text": "Temperature rise."}
{"answers": {"text": ["actual temperature rise was near the top end of the range given", "temperature rise was near the top end of the range given", "near the top end of the range given by IPCC's 2001 projection"], "answer_start": [369, 376, 397]}, "prediction_text": "Top end of the range."}
{"answers": {"text": ["actual sea level rise was above the top of the range", "actual sea level rise was above the top of the range", "the actual sea level rise was above the top of the range"], "answer_start": [468, 468, 464]}, "prediction_text": "Over the six years studied, the actual sea level rise was above the top end of the range given by IPCC's 2001 projection."}
{"answers": {"text": ["projected rises in sea levels", "rises in sea levels", "projected rises in sea levels"], "answer_start": [176, 186, 176]}, "prediction_text": "Sea levels."}
{"answers": {"text": ["9\u201388 cm", "0.5\u20131.4 m", "9\u201388 cm"], "answer_start": [478, 398, 478]}, "prediction_text": "0.5-1.4 m."}
{"answers": {"text": ["50\u2013140 cm", "9\u201388 cm", "0.5\u20131.4 m [50\u2013140 cm]"], "answer_start": [409, 478, 398]}, "prediction_text": "0.5-1.4 m."}
{"answers": {"text": ["2001", "2001", "2001"], "answer_start": [563, 563, 563]}, "prediction_text": "2001"}
{"answers": {"text": ["coordinating lead author of the Fifth Assessment Report", "participant in the IPCC and coordinating lead author of the Fifth Assessment Report", "coordinating lead author of the Fifth Assessment Report"], "answer_start": [61, 33, 61]}, "prediction_text": "Coordinating lead author."}
{"answers": {"text": ["Science Magazine", "Science Magazine", "Science Magazine's"], "answer_start": [129, 129, 129]}, "prediction_text": "Michael Oppenheimer"}
{"answers": {"text": ["concurring, smaller assessments of special problems", "smaller assessments of special problems instead of the large scale approach", "concurring, smaller assessments of special problems"], "answer_start": [239, 251, 239]}, "prediction_text": "Smaller assessments of special problems."}
{"answers": {"text": ["the Montreal Protocol", "Montreal Protocol", "global regulation based on the Montreal Protocol"], "answer_start": [238, 242, 211]}, "prediction_text": "Montreal Protocol"}
{"answers": {"text": ["Climate Change", "Climate Change", "Climate Change"], "answer_start": [292, 292, 292]}, "prediction_text": "Global warming."}
{"answers": {"text": ["states and governments", "states and governments", "states and governments"], "answer_start": [488, 488, 488]}, "prediction_text": "Governments."}
{"answers": {"text": ["Sheldon Ungar", "Sheldon Ungar", "Sheldon Ungar"], "answer_start": [13, 13, 13]}, "prediction_text": "Sheldon Ungar"}
{"answers": {"text": ["varying regional cost-benefit analysis and burden-sharing conflicts with regard to the distribution of emission reductions", "varying regional cost-benefit analysis and burden-sharing conflicts with regard to the distribution of emission reductions"], "answer_start": [619, 619]}, "prediction_text": "regional burden sharing conflicts."}
{"answers": {"text": ["regional burden sharing conflicts", "regional burden sharing conflicts", "varying regional cost-benefit analysis and burden-sharing conflicts with regard to the distribution of emission reductions"], "answer_start": [513, 513, 619]}, "prediction_text": "regional burden sharing conflicts."}
{"answers": {"text": ["the UK government", "the UK government", "the UK government"], "answer_start": [946, 946, 946]}, "prediction_text": "UK government"}
{"answers": {"text": ["other scientific bodies", "scientific papers and independently documented results", "scientific papers and independently documented results from other scientific bodies"], "answer_start": [140, 80, 80]}, "prediction_text": "Scientific papers."}
{"answers": {"text": ["significant new evidence or events that change our understanding of climate science", "significant new evidence or events that change our understanding", "significant new evidence or events that change our understanding of climate science"], "answer_start": [311, 311, 311]}, "prediction_text": "Significant new evidence or events."}
{"answers": {"text": ["IPCC", "IPCC", "the IPCC"], "answer_start": [439, 439, 6]}, "prediction_text": "IPCC"}
{"answers": {"text": ["five", "five", "five"], "answer_start": [97, 97, 97]}, "prediction_text": "Five."}
{"answers": {"text": ["the journal Nature", "the journal Nature", "the journal Nature"], "answer_start": [179, 179, 179]}, "prediction_text": "Nature"}
{"answers": {"text": ["turning the whole climate science assessment process into a moderated \"living\" Wikipedia-IPCC", "turning the whole climate science assessment process into a moderated \"living\" Wikipedia-IPCC", "turning the whole climate science assessment process into a moderated \"living\" Wikipedia-IPCC"], "answer_start": [407, 407, 407]}, "prediction_text": "Tightening selection of lead authors and contributors."}
{"answers": {"text": ["remove government oversight from its processes", "remove government oversight", "remove government oversight from its processes"], "answer_start": [577, 577, 577]}, "prediction_text": "Turned the whole climate science assessment process into a moderated \"living\" Wikipedia-IPCC."}
{"answers": {"text": ["to conduct photosynthesis", "conduct photosynthesis", "conduct photosynthesis"], "answer_start": [27, 30, 30]}, "prediction_text": "Photosynthesis."}
{"answers": {"text": ["energy", "energy", "energy"], "answer_start": [170, 170, 112]}, "prediction_text": "Energy."}
{"answers": {"text": ["energy", "energy", "energy"], "answer_start": [170, 170, 112]}, "prediction_text": "Oxygen."}
{"answers": {"text": ["the Calvin cycle", "Calvin cycle", "Calvin cycle"], "answer_start": [342, 346, 346]}, "prediction_text": "Calvin cycle"}
{"answers": {"text": ["1", "1", "1"], "answer_start": [554, 554, 554]}, "prediction_text": "1"}
{"answers": {"text": ["pinch in two", "pinch in two", "pinch in two"], "answer_start": [105, 105, 105]}, "prediction_text": "Chloroplasts reproduce by pinch."}
{"answers": {"text": ["environmental factors like light color and intensity", "environmental factors", "environmental factors like light color and intensity"], "answer_start": [173, 173, 173]}, "prediction_text": "Environmental factors."}
{"answers": {"text": ["contain their own DNA", "contain their own DNA", "contain their own DNA,"], "answer_start": [260, 260, 260]}, "prediction_text": "They contain their own DNA."}
{"answers": {"text": ["a photosynthetic cyanobacterium that was engulfed by an early eukaryotic cell", "a photosynthetic cyanobacterium", "photosynthetic cyanobacterium that was engulfed by an early eukaryotic cell"], "answer_start": [336, 336, 338]}, "prediction_text": "Cyanobacterium"}
{"answers": {"text": ["must be inherited by each daughter cell during cell division", "inherited", "inherited by each daughter cell during cell division"], "answer_start": [465, 473, 473]}, "prediction_text": "From an early eukaryotic cell."}
{"answers": {"text": ["plants and algae", "plants and algae", "plants and algae."], "answer_start": [590, 590, 590]}, "prediction_text": "Plants and algae."}
{"answers": {"text": ["Russian", "Russian", "Russian"], "answer_start": [418, 418, 418]}, "prediction_text": "Russian"}
{"answers": {"text": ["biologist", "biologist", "biologist"], "answer_start": [426, 426, 426]}, "prediction_text": "Russian biologist"}
{"answers": {"text": ["1905", "1905", "1905"], "answer_start": [464, 464, 464]}, "prediction_text": "1905"}
{"answers": {"text": ["Andreas Schimper", "Andreas Schimper", "Andreas Schimper"], "answer_start": [475, 475, 475]}, "prediction_text": "Konstantin Mereschkowski"}
{"answers": {"text": ["Cyanobacteria", "Cyanobacteria", "Cyanobacteria"], "answer_start": [0, 0, 0]}, "prediction_text": "Cyanobacteria"}
{"answers": {"text": ["prokaryotes", "bacteria", "prokaryotes"], "answer_start": [124, 166, 124]}, "prediction_text": "Prokaryotes"}
{"answers": {"text": ["they have two cell membranes", "two cell membranes", "they have two cell membranes."], "answer_start": [251, 261, 251]}, "prediction_text": "Thicker cell membranes."}
{"answers": {"text": ["peptidoglycan", "peptidoglycan", "peptidoglycan"], "answer_start": [310, 310, 310]}, "prediction_text": "Thicker."}
{"answers": {"text": ["blue-green algae", "blue-green algae", "sometimes called blue-green algae"], "answer_start": [86, 86, 69]}, "prediction_text": "Cyanobacteria are sometimes wrongly described as blue-green algae."}
{"answers": {"text": ["eukaryotic", "an early eukaryotic cell", "eukaryotic"], "answer_start": [84, 75, 84]}, "prediction_text": "Early eukaryotic cell."}
{"answers": {"text": ["around a billion years ago", "a billion years ago", "around a billion years ago"], "answer_start": [10, 17, 10]}, "prediction_text": "A billion years ago."}
{"answers": {"text": ["two innermost lipid-bilayer membranes", "phagocytic vacuole", "lipid-bilayer membranes"], "answer_start": [214, 170, 228]}, "prediction_text": "Lipid-bilayer membranes."}
{"answers": {"text": ["phagosomal", "phagosomal", "phagosomal"], "answer_start": [398, 398, 398]}, "prediction_text": "Phagosomal membrane."}
{"answers": {"text": ["many of its genes were lost or transferred to the nucleus of the host", "many of its genes were lost or transferred", "many of its genes were lost or transferred to the nucleus of the host"], "answer_start": [640, 640, 640]}, "prediction_text": "The cyanobacterium was assimilated."}
{"answers": {"text": ["almost the same thing as chloroplast", "chloroplast", "chloroplast"], "answer_start": [147, 172, 172]}, "prediction_text": "Primary plastids."}
{"answers": {"text": ["three", "three", "three"], "answer_start": [228, 228, 228]}, "prediction_text": "3"}
{"answers": {"text": ["red algal chloroplast", "red algal chloroplast lineage", "red algal chloroplast lineage"], "answer_start": [311, 311, 311]}, "prediction_text": "Red algae."}
{"answers": {"text": ["green chloroplast", "green chloroplast lineage", "green chloroplast lineage"], "answer_start": [369, 369, 369]}, "prediction_text": "Green chloroplast lineage."}
{"answers": {"text": ["the green chloroplast lineage", "the green chloroplast lineage", "green chloroplast lineage"], "answer_start": [432, 432, 436]}, "prediction_text": "Green chloroplast lineage"}
{"answers": {"text": ["glaucophyte", "glaucophyte", "glaucophyte chloroplast group"], "answer_start": [23, 23, 110]}, "prediction_text": "Glaucophyte."}
{"answers": {"text": ["alga", "alga", "glaucophyte"], "answer_start": [4, 4, 23]}, "prediction_text": "Glaucophyte"}
{"answers": {"text": ["glaucophyte chloroplasts", "glaucophyte chloroplasts", "glaucophyte chloroplasts"], "answer_start": [439, 439, 439]}, "prediction_text": "Glaucophyte chloroplasts."}
{"answers": {"text": ["a carboxysome", "carboxysome", "carboxysome"], "answer_start": [580, 582, 582]}, "prediction_text": "A carboxysome."}
{"answers": {"text": ["icosahedral", "icosahedral", "icosahedral"], "answer_start": [599, 599, 599]}, "prediction_text": "icosahedral"}
{"answers": {"text": ["chlorophyll a and phycobilins", "phycobilin", "phycobilin"], "answer_start": [229, 67, 67]}, "prediction_text": "Phycobilins, chlorophyll a, phycobilins, pyrenoids, and red phycoerytherin."}
{"answers": {"text": ["phycobilisomes", "phycobilisomes", "phycobilisomes"], "answer_start": [102, 102, 102]}, "prediction_text": "Phycobilisomes."}
{"answers": {"text": ["the phycobilin phycoerytherin", "phycobilin phycoerytherin", "phycobilin phycoerytherin"], "answer_start": [288, 292, 292]}, "prediction_text": "Phycobilins."}
{"answers": {"text": ["catch more sunlight in deep water", "catch more sunlight in deep water", "catch more sunlight in deep water"], "answer_start": [585, 585, 585]}, "prediction_text": "Red color."}
{"answers": {"text": ["a form of starch", "a form of starch", "a form of starch"], "answer_start": [770, 770, 770]}, "prediction_text": "Starch granules."}
{"answers": {"text": ["phycobilisomes", "phycobilisomes", "phycobilisomes"], "answer_start": [281, 281, 281]}, "prediction_text": "Phycobilisomes."}
{"answers": {"text": ["accessory pigments that override the chlorophylls' green colors", "accessory pigments", "accessory pigments that override the chlorophylls' green colors"], "answer_start": [449, 449, 449]}, "prediction_text": "accessory pigments."}
{"answers": {"text": ["the peptidoglycan wall", "the peptidoglycan wall", "peptidoglycan wall between their double membrane"], "answer_start": [553, 553, 557]}, "prediction_text": "phycobilisomes"}
{"answers": {"text": ["chloroplast division", "chloroplast division", "chloroplast division"], "answer_start": [785, 785, 785]}, "prediction_text": "chloroplast division."}
{"answers": {"text": ["chlorophyll b", "chlorophyll b", "chlorophyll b"], "answer_start": [309, 309, 309]}, "prediction_text": "Chlorophyll b."}
{"answers": {"text": ["double", "double membrane", "double membrane"], "answer_start": [34, 34, 34]}, "prediction_text": "Double membrane."}
{"answers": {"text": ["additional membranes outside of the original two", "additional membranes", "have additional membranes outside of the original two"], "answer_start": [114, 114, 109]}, "prediction_text": "Additional membranes outside of the original two."}
{"answers": {"text": ["a nonphotosynthetic eukaryote engulfed a chloroplast-containing alga but failed to digest it", "a nonphotosynthetic eukaryote engulfed a chloroplast-containing alga but failed to digest it", "nonphotosynthetic eukaryote engulfed a chloroplast-containing alga but failed to digest it"], "answer_start": [219, 219, 221]}, "prediction_text": "Nonphotosynthetic eukaryote."}
{"answers": {"text": ["sometimes the eaten alga's cell membrane, and the phagosomal vacuole from the host's cell membrane", "sometimes the eaten alga's cell membrane", "the two cyanobacterial membranes, sometimes the eaten alga's cell membrane, and the phagosomal vacuole from the host's cell membrane"], "answer_start": [568, 568, 534]}, "prediction_text": "Outside of the original two."}
{"answers": {"text": ["its chloroplast, and sometimes its cell membrane and nucleus", "its chloroplast", "chloroplast"], "answer_start": [421, 421, 425]}, "prediction_text": "The chloroplast."}
{"answers": {"text": ["chloroplasts derived from a green alga", "green alga", "green alga"], "answer_start": [70, 98, 98]}, "prediction_text": "Euglenophyte chloroplasts."}
{"answers": {"text": ["common flagellated", "common flagellated", "common flagellated"], "answer_start": [29, 29, 29]}, "prediction_text": "Euglenophytes are a group of common flagellated protists."}
{"answers": {"text": ["stacked in groups of three", "groups of three", "stacked in groups of three"], "answer_start": [368, 379, 368]}, "prediction_text": "Stacked in groups of three."}
{"answers": {"text": ["Starch", "Starch", "Starch"], "answer_start": [396, 396, 396]}, "prediction_text": "Starch."}
{"answers": {"text": ["the membrane of the primary endosymbiont", "the primary endosymbiont", "primary endosymbiont"], "answer_start": [176, 192, 196]}, "prediction_text": "The primary endosymbiont."}
{"answers": {"text": ["cryptomonads", "cryptomonads", "cryptomonads"], "answer_start": [17, 17, 17]}, "prediction_text": "Cryptomonads"}
{"answers": {"text": ["red-algal derived chloroplast", "red-algal", "red-algal derived"], "answer_start": [66, 66, 66]}, "prediction_text": "Chlorarachniophytes."}
{"answers": {"text": ["nucleomorph", "nucleomorph", "nucleomorph"], "answer_start": [132, 132, 132]}, "prediction_text": "Outermost."}
{"answers": {"text": ["in granules found in the periplastid space", "granules", "in granules found in the periplastid space"], "answer_start": [376, 379, 376]}, "prediction_text": "Granules."}
{"answers": {"text": ["stacks of two", "stacks of two", "in stacks of two"], "answer_start": [580, 580, 577]}, "prediction_text": "Stacks of two."}
{"answers": {"text": ["helicosproidia", "helicosproidia", "helicosproidia"], "answer_start": [61, 61, 61]}, "prediction_text": "Helicosproidia"}
{"answers": {"text": ["chromalveolates", "chromalveolates", "chromalveolates"], "answer_start": [35, 35, 35]}, "prediction_text": "Chromalveolates"}
{"answers": {"text": ["the malaria parasite", "apicomplexans", "malaria parasite"], "answer_start": [324, 290, 328]}, "prediction_text": "Malaria parasite."}
{"answers": {"text": ["a vestigial red algal derived chloroplast", "a vestigial red algal derived chloroplast", "red algal derived chloroplast"], "answer_start": [370, 370, 382]}, "prediction_text": "Stored energy in amylopectin starch granules."}
{"answers": {"text": ["in amylopectin starch granules that are located in their cytoplasm", "amylopectin starch granules", "amylopectin starch granules"], "answer_start": [589, 592, 592]}, "prediction_text": "Amylopectin starch granules."}
{"answers": {"text": ["fatty acids, isopentenyl pyrophosphate, iron-sulfur clusters", "fatty acids, isopentenyl pyrophosphate, iron-sulfur clusters", "fatty acids, isopentenyl pyrophosphate, iron-sulfur clusters"], "answer_start": [516, 516, 516]}, "prediction_text": "Isopentenyl pyrophosphate."}
{"answers": {"text": ["apicomplexan-related diseases", "apicomplexan-related diseases", "apicomplexan-related"], "answer_start": [683, 683, 683]}, "prediction_text": "Apicomplexan-related diseases."}
{"answers": {"text": ["isopentenyl pyrophosphate synthesis", "isopentenyl pyrophosphate synthesis", "isopentenyl pyrophosphate synthesis"], "answer_start": [756, 756, 756]}, "prediction_text": "Isopentenyl pyrophosphate synthesis."}
{"answers": {"text": ["photosynthetic pigments or true thylakoids", "photosynthetic function", "all photosynthetic function"], "answer_start": [66, 26, 22]}, "prediction_text": "photosynthetic pigments and true thylakoids."}
{"answers": {"text": ["four", "four", "four"], "answer_start": [130, 130, 130]}, "prediction_text": "Four."}
{"answers": {"text": ["Peridinin", "peridinin", "peridinin"], "answer_start": [191, 113, 113]}, "prediction_text": "Chlorophyll a and chlorophyll c2."}
{"answers": {"text": ["peridinin-type chloroplast", "chloroplasts", "chloroplasts"], "answer_start": [45, 132, 132]}, "prediction_text": "Chloroplasts."}
{"answers": {"text": ["triplet-stacked", "triplet-stacked", "triplet-stacked"], "answer_start": [495, 495, 495]}, "prediction_text": "Triplet-stacked."}
{"answers": {"text": ["the red algal endosymbiont's original cell membrane", "the red algal endosymbiont's original cell membrane", "red algal endosymbiont's original cell membrane"], "answer_start": [338, 338, 342]}, "prediction_text": "The original cell membrane."}
{"answers": {"text": ["fucoxanthin dinophyte", "fucoxanthin dinophyte", "fucoxanthin dinophyte"], "answer_start": [4, 4, 4]}, "prediction_text": "Karlodinium lineage."}
{"answers": {"text": ["fucoxanthin dinophyte", "fucoxanthin dinophyte", "fucoxanthin dinophyte"], "answer_start": [4, 4, 4]}, "prediction_text": "Karlodinium"}
{"answers": {"text": ["four", "four", "four"], "answer_start": [310, 310, 310]}, "prediction_text": "4 membranes."}
{"answers": {"text": ["a six membraned chloroplast", "a six membraned chloroplast", "six membraned chloroplast"], "answer_start": [377, 377, 379]}, "prediction_text": "Six membraned chloroplast."}
{"answers": {"text": ["a cryptophyte", "cryptophyte", "cryptophyte"], "answer_start": [84, 86, 86]}, "prediction_text": "Cryptophyte."}
{"answers": {"text": ["its nucleomorph and outermost two membranes", "its nucleomorph and outermost two membranes", "nucleomorph and outermost two membranes"], "answer_start": [235, 235, 239]}, "prediction_text": "Nucleomorph and outermost two membranes."}
{"answers": {"text": ["a phycobilin-containing chloroplast", "phycobilin-containing chloroplast", "phycobilin-containing"], "answer_start": [37, 39, 39]}, "prediction_text": "Cryptophyte"}
{"answers": {"text": ["a two-membraned chloroplast", "a two-membraned chloroplast", "a two-membraned chloroplast"], "answer_start": [293, 293, 293]}, "prediction_text": "Two-membraned chloroplast."}
{"answers": {"text": ["heterokontophyte", "heterokontophyte", "heterokontophyte"], "answer_start": [68, 68, 68]}, "prediction_text": "Heterokontophyte"}
{"answers": {"text": ["a diatom (heterokontophyte) derived chloroplast", "diatom (heterokontophyte) derived", "diatom (heterokontophyte) derived"], "answer_start": [58, 60, 60]}, "prediction_text": "Diatom endosymbiont."}
{"answers": {"text": ["up to five", "up to five", "five"], "answer_start": [141, 141, 147]}, "prediction_text": "Five."}
{"answers": {"text": ["the entire diatom endosymbiont as the chloroplast", "the entire diatom endosymbiont", "diatom endosymbiont"], "answer_start": [195, 195, 206]}, "prediction_text": "The diatom endosymbiont."}
{"answers": {"text": ["granules in the dinophyte host's cytoplasm", "in the dinophyte host", "granules in the dinophyte host's cytoplasm"], "answer_start": [662, 671, 662]}, "prediction_text": "Granules in the dinophyte host's cytoplasm."}
{"answers": {"text": ["the dinophyte nucleus", "the dinophyte nucleus", "dinophyte nucleus"], "answer_start": [417, 417, 421]}, "prediction_text": "Dinophyte nucleus."}
{"answers": {"text": ["Lepidodinium", "Lepidodinium", "Lepidodinium"], "answer_start": [198, 198, 198]}, "prediction_text": "Lepidodinium."}
{"answers": {"text": ["their original peridinin chloroplast", "nucleomorph", "their original peridinin chloroplast"], "answer_start": [69, 354, 69]}, "prediction_text": "peridinin chloroplast."}
{"answers": {"text": ["a green algal derived chloroplast", "prasinophyte", "green algal derived chloroplast (more specifically, a prasinophyte)"], "answer_start": [127, 183, 129]}, "prediction_text": "A green algal derived chloroplast."}
{"answers": {"text": ["a green algal derived chloroplast", "a green algal derived chloroplast", "green algal derived chloroplast"], "answer_start": [127, 127, 129]}, "prediction_text": "Lepidodinium viride"}
{"answers": {"text": ["first set of endosymbiotic events", "first set of endosymbiotic events", "first set of endosymbiotic events"], "answer_start": [44, 44, 44]}, "prediction_text": "Endosymbiotic events."}
{"answers": {"text": ["acquired a photosynthetic cyanobacterial endosymbiont more recently", "acquired a photosynthetic cyanobacterial endosymbiont more recently", "acquired a photosynthetic cyanobacterial endosymbiont more recently"], "answer_start": [125, 125, 125]}, "prediction_text": "The photosynthetic cyanobacterial endosymbiont."}
{"answers": {"text": ["about a million", "about a million", "about a million"], "answer_start": [659, 659, 659]}, "prediction_text": "850 base pairs."}
{"answers": {"text": ["around 850", "850", "850"], "answer_start": [703, 710, 710]}, "prediction_text": "850"}
{"answers": {"text": ["three million", "150,000", "three million"], "answer_start": [755, 840, 755]}, "prediction_text": "3 million."}
{"answers": {"text": ["ctDNA, or cpDNA", "ctDNA", "ctDNA, or cpDNA"], "answer_start": [54, 54, 54]}, "prediction_text": "ctDNA"}
{"answers": {"text": ["the plastome", "cpDNA", "plastome"], "answer_start": [91, 64, 95]}, "prediction_text": "Plastome"}
{"answers": {"text": ["1962", "1962", "1962"], "answer_start": [139, 139, 139]}, "prediction_text": "1986"}
{"answers": {"text": ["1986", "1986", "1986"], "answer_start": [168, 168, 168]}, "prediction_text": "1986"}
{"answers": {"text": ["two Japanese research teams", "two Japanese research teams", "two Japanese research teams"], "answer_start": [178, 178, 178]}, "prediction_text": "Japanese research teams."}
{"answers": {"text": ["The inverted repeat regions", "inverted repeat regions", "inverted repeat regions"], "answer_start": [0, 4, 4]}, "prediction_text": "Inverted repeats."}
{"answers": {"text": ["direct repeats", "direct repeats", "direct repeats"], "answer_start": [367, 367, 367]}, "prediction_text": "Direct repeats."}
{"answers": {"text": ["stabilize the rest of the chloroplast genome", "stabilize the rest of the chloroplast", "help stabilize the rest of the chloroplast genome"], "answer_start": [430, 430, 425]}, "prediction_text": "Stabilize the rest of the chloroplast genome."}
{"answers": {"text": ["electron microscopy", "via electron microscopy", "electron microscopy"], "answer_start": [197, 193, 197]}, "prediction_text": "Electron microscopy."}
{"answers": {"text": ["two", "two", "two"], "answer_start": [96, 96, 96]}, "prediction_text": "2"}
{"answers": {"text": ["a theta intermediary form", "theta intermediary form", "a theta intermediary form"], "answer_start": [423, 425, 423]}, "prediction_text": "theta intermediary form"}
{"answers": {"text": ["a Cairns replication intermediate", "double displacement loop", "double displacement loop"], "answer_start": [464, 332, 332]}, "prediction_text": "Double displacement loop (D-loop)"}
{"answers": {"text": ["with a rolling circle mechanism", "a rolling circle mechanism", "with a rolling circle mechanism"], "answer_start": [525, 530, 525]}, "prediction_text": "Rolling circle mechanism."}
{"answers": {"text": ["A \u2192 G deamination", "A \u2192 G deamination gradients", "A \u2192 G deamination"], "answer_start": [28, 28, 28]}, "prediction_text": "A \u2192 G deamination gradients."}
{"answers": {"text": ["when it is single stranded", "when it is single stranded", "replication forks form"], "answer_start": [103, 103, 136]}, "prediction_text": "Single stranded."}
{"answers": {"text": ["linear", "linear", "linear"], "answer_start": [622, 622, 622]}, "prediction_text": "Linear and replicates through homologous recombination."}
{"answers": {"text": ["homologous recombination", "homologous recombination", "through homologous recombination"], "answer_start": [652, 652, 644]}, "prediction_text": "Through homologous recombination."}
{"answers": {"text": ["in branched, linear, or other complex structures", "branched, linear, or other complex structures", "in branched, linear, or other complex structures"], "answer_start": [793, 796, 793]}, "prediction_text": "In branched, linear, or other complex structures."}
{"answers": {"text": ["bacteriophage T4", "bacteriophage T4.", "bacteriophage T4"], "answer_start": [162, 162, 162]}, "prediction_text": "T4 bacteriophage."}
{"answers": {"text": ["linear", "linear", "linear"], "answer_start": [226, 226, 226]}, "prediction_text": "Linear."}
{"answers": {"text": ["circular", "circular", "circular"], "answer_start": [1063, 987, 1063]}, "prediction_text": "Linear."}
{"answers": {"text": ["via a D loop mechanism", "via a D loop mechanism", "via a D loop mechanism"], "answer_start": [1099, 1099, 1099]}, "prediction_text": "via a D loop mechanism."}
{"answers": {"text": ["Endosymbiotic gene transfer", "Endosymbiotic gene transfer", "Endosymbiotic gene transfer"], "answer_start": [0, 0, 0]}, "prediction_text": "Genes."}
{"answers": {"text": ["the lost chloroplast's existence", "for the lost chloroplast's existence", "the lost chloroplast's existence"], "answer_start": [228, 224, 228]}, "prediction_text": "The presence of genes in the nucleus."}
{"answers": {"text": ["a red algal derived chloroplast", "red algal", "red algal derived"], "answer_start": [319, 321, 321]}, "prediction_text": "Red algae."}
{"answers": {"text": ["green algal derived chloroplast", "green algal", "green algal derived"], "answer_start": [512, 373, 512]}, "prediction_text": "Red algae."}
{"answers": {"text": ["nonfunctional pseudogenes", "nonfunctional", "most became nonfunctional pseudogenes"], "answer_start": [326, 326, 314]}, "prediction_text": "Nonfunctional pseudogenes."}
{"answers": {"text": ["around half", "half", "half"], "answer_start": [11, 18, 18]}, "prediction_text": "Half."}
{"answers": {"text": ["participating in cell division, protein routing, and even disease resistance", "cell division, protein routing, and even disease resistance", "cell division, protein routing, and even disease resistance"], "answer_start": [168, 185, 185]}, "prediction_text": "participating in cell division, protein routing, and disease resistance."}
{"answers": {"text": ["the cell membrane", "the cell membrane", "the cell membrane"], "answer_start": [748, 748, 748]}, "prediction_text": "The outermost membrane."}
{"answers": {"text": ["a ribosome", "a ribosome", "ribosome"], "answer_start": [50, 50, 52]}, "prediction_text": "Ribosome"}
{"answers": {"text": ["in the cytosol", "in the cytosol", "cytosol"], "answer_start": [61, 61, 68]}, "prediction_text": "Ribosome in the cytosol."}
{"answers": {"text": ["helps many proteins bind the polypeptide", "helps many proteins bind the polypeptide", "helps many proteins bind the polypeptide"], "answer_start": [236, 236, 236]}, "prediction_text": "Helps proteins bind the polypeptide."}
{"answers": {"text": ["keeping it from folding prematurely", "keeping it from folding prematurely", "prevents chloroplast proteins from assuming their active form and carrying out their chloroplast functions in the wrong place"], "answer_start": [278, 278, 344]}, "prediction_text": "Keeps the polypeptide from folding prematurely."}
{"answers": {"text": ["lens-shaped", "lens-shaped", "lens-shaped"], "answer_start": [43, 43, 43]}, "prediction_text": "lens-shaped"}
{"answers": {"text": ["5\u20138 \u03bcm in diameter", "5\u20138 \u03bcm", "5\u20138 \u03bcm in diameter"], "answer_start": [56, 56, 56]}, "prediction_text": "5-8 \u03bcm."}
{"answers": {"text": ["1\u20133 \u03bcm", "1\u20133 \u03bcm", "1\u20133 \u03bcm"], "answer_start": [79, 79, 79]}, "prediction_text": "1-3 \u03bcm."}
{"answers": {"text": ["a net", "a net", "net"], "answer_start": [222, 222, 224]}, "prediction_text": "Net shape."}
{"answers": {"text": ["a cup", "a cup", "cup"], "answer_start": [248, 248, 250]}, "prediction_text": "Cup."}
{"answers": {"text": ["a double membrane", "surrounded by a double membrane", "surrounded by a double membrane"], "answer_start": [131, 117, 117]}, "prediction_text": "Double membranes."}
{"answers": {"text": ["the product of the host's cell membrane infolding to form a vesicle to surround the ancestral cyanobacterium", "is the product of the host's cell membrane", "is the product of the host's cell membrane infolding to form a vesicle to surround the ancestral cyanobacterium"], "answer_start": [309, 306, 306]}, "prediction_text": "The misconception that the outer chloroplast membrane is the product of the host's cell membrane infolding to form a vesicle to surround the ancestral cyanobacterium."}
{"answers": {"text": ["homologous", "homologous", "homologous"], "answer_start": [467, 467, 467]}, "prediction_text": "Homologous."}
{"answers": {"text": ["the mitochondrial double membrane", "the mitochondrial double membrane", "mitochondrial double membrane"], "answer_start": [58, 58, 62]}, "prediction_text": "Mitochondrial double membrane"}
{"answers": {"text": ["run proton pumps and carry out oxidative phosphorylation", "to run proton pumps", "run proton pumps and carry out oxidative phosphorylation across to generate ATP energy"], "answer_start": [167, 164, 167]}, "prediction_text": "Synthesizes some materials."}
{"answers": {"text": ["generate ATP energy", "generate ATP energy", "generate ATP energy"], "answer_start": [234, 234, 234]}, "prediction_text": "Generate ATP energy."}
{"answers": {"text": ["the internal thylakoid system", "the internal thylakoid system", "internal thylakoid system"], "answer_start": [325, 325, 329]}, "prediction_text": "Thylakoid system."}
{"answers": {"text": ["the inner chloroplast membrane", "the inner chloroplast membrane", "inner chloroplast membrane"], "answer_start": [547, 547, 551]}, "prediction_text": "Internal thylakoid system."}
{"answers": {"text": ["Stromules", "Stromules", "Stromules"], "answer_start": [118, 118, 118]}, "prediction_text": "Stromules."}
{"answers": {"text": ["stroma-containing tubule", "stroma-containing tubule", "stroma-containing tubule"], "answer_start": [92, 92, 92]}, "prediction_text": "Chloroplast membranes."}
{"answers": {"text": ["to increase the chloroplast's surface area for cross-membrane transport", "to increase the chloroplast's surface area", "to increase the chloroplast's surface area for cross-membrane transport"], "answer_start": [286, 286, 286]}, "prediction_text": "To increase the chloroplast's surface area for cross-membrane transport."}
{"answers": {"text": ["1962", "1962", "1962"], "answer_start": [468, 468, 468]}, "prediction_text": "1962"}
{"answers": {"text": ["in the chloroplasts of C4 plants", "in the chloroplasts of C4 plants,", "chloroplasts of C4 plants"], "answer_start": [101, 101, 108]}, "prediction_text": "Chloroplasts."}
{"answers": {"text": ["in some C3 angiosperms, and even some gymnosperms", "C3 angiosperms", "C3 angiosperms, and even some gymnosperms"], "answer_start": [165, 173, 173]}, "prediction_text": "C3 angiosperms."}
{"answers": {"text": ["The chloroplast peripheral reticulum", "The chloroplast peripheral reticulum"], "answer_start": [216, 216]}, "prediction_text": "Chloroplast peripheral reticulum."}
{"answers": {"text": ["increase the chloroplast's surface area for cross-membrane transport", "to increase the chloroplast's surface area", "increase the chloroplast's surface area for cross-membrane transport between its stroma and the cell cytoplasm"], "answer_start": [449, 446, 449]}, "prediction_text": "Increase the chloroplast's surface area for cross-membrane transport."}
{"answers": {"text": ["the thylakoids and intermembrane space", "thylakoids and intermembrane space", "thylakoids and intermembrane space"], "answer_start": [656, 660, 660]}, "prediction_text": "Thylakoids and intermembrane space."}
{"answers": {"text": ["synthesize a small fraction of their proteins", "to synthesize a small fraction of their proteins", "synthesize a small fraction of their proteins"], "answer_start": [57, 54, 57]}, "prediction_text": "Synthesis of proteins."}
{"answers": {"text": ["17 nm", "around 17 nm", "17 nm"], "answer_start": [189, 182, 189]}, "prediction_text": "Two-thirds."}
{"answers": {"text": ["25 nm", "25 nm", "25 nm"], "answer_start": [198, 198, 198]}, "prediction_text": "25 nm."}
{"answers": {"text": ["motifs for shine-dalgarno sequence recognition", "motifs for shine-dalgarno sequence recognition", "motifs for shine-dalgarno sequence recognition"], "answer_start": [538, 538, 538]}, "prediction_text": "motifs for shine-dalgarno sequence recognition."}
{"answers": {"text": ["is considered essential for translation initiation in most chloroplasts and prokaryotes", "translation initiation", "essential for translation initiation in most chloroplasts and prokaryotes"], "answer_start": [592, 620, 606]}, "prediction_text": "Chloroplast translation."}
{"answers": {"text": ["plastoglobulus, sometimes spelled plastoglobule(s)", "plastoglobulus", "plastoglobulus"], "answer_start": [24, 24, 24]}, "prediction_text": "Plastoglobulus"}
{"answers": {"text": ["spherical bubbles", "spherical", "spherical"], "answer_start": [81, 81, 81]}, "prediction_text": "Spherical bubbles of lipids and proteins."}
{"answers": {"text": ["lipids and proteins", "lipids and proteins", "lipids and proteins"], "answer_start": [102, 102, 102]}, "prediction_text": "Lipids and proteins."}
{"answers": {"text": ["45\u201360 nanometers across", "45\u201360 nanometers across", "45\u201360 nanometers across"], "answer_start": [128, 128, 128]}, "prediction_text": "45-60 nanometers."}
{"answers": {"text": ["a lipid monolayer", "lipid monolayer", "a lipid monolayer"], "answer_start": [176, 178, 176]}, "prediction_text": "Lipid monolayer."}
{"answers": {"text": ["either to a thylakoid or to another plastoglobulus attached to a thylakoid", "a thylakoid", "thylakoid or to another plastoglobulus attached to a thylakoid"], "answer_start": [124, 134, 136]}, "prediction_text": "Thylakoid"}
{"answers": {"text": ["the thylakoid network", "the thylakoid network", "thylakoid network"], "answer_start": [275, 275, 279]}, "prediction_text": "Thylakoid network."}
{"answers": {"text": ["singularly, attached directly to their parent thylakoid", "singularly", "singularly, attached directly to their parent thylakoid"], "answer_start": [369, 369, 369]}, "prediction_text": "Singularly."}
{"answers": {"text": ["In old or stressed chloroplasts", "In old or stressed chloroplasts", "In old or stressed chloroplasts"], "answer_start": [426, 426, 426]}, "prediction_text": "When they are anchored to a thylakoid."}
{"answers": {"text": ["The chloroplasts of some hornworts and algae", "some hornworts and algae", "chloroplasts of some hornworts and algae"], "answer_start": [0, 20, 4]}, "prediction_text": "Hornworts and algae."}
{"answers": {"text": ["roughly spherical", "roughly spherical", "spherical"], "answer_start": [133, 133, 141]}, "prediction_text": "Spherical"}
{"answers": {"text": ["highly refractive", "highly refractive", "roughly spherical and highly refractive"], "answer_start": [155, 155, 133]}, "prediction_text": "Pyrenoids."}
{"answers": {"text": ["starch", "starch", "starch"], "answer_start": [200, 200, 200]}, "prediction_text": "Starch."}
{"answers": {"text": ["divide to form new pyrenoids, or be produced \"de novo\"", "divide", "divide to form new pyrenoids, or be produced \"de novo\""], "answer_start": [568, 568, 568]}, "prediction_text": "Produce new pyrenoids."}
{"answers": {"text": ["the helical thylakoid model", "the helical thylakoid model", "helical thylakoid"], "answer_start": [3, 3, 7]}, "prediction_text": "Helical thylakoid model"}
{"answers": {"text": ["flattened circular", "flattened circular", "flattened circular"], "answer_start": [60, 60, 60]}, "prediction_text": "Pancakes"}
{"answers": {"text": ["anywhere from two to a hundred", "two to a hundred", "two to a hundred"], "answer_start": [145, 159, 159]}, "prediction_text": "2 to a hundred."}
{"answers": {"text": ["10\u201320", "10\u201320", "10\u201320"], "answer_start": [206, 206, 206]}, "prediction_text": "2 to a hundred."}
{"answers": {"text": ["helicoid stromal thylakoids", "helicoid stromal thylakoids", "helicoid stromal thylakoids"], "answer_start": [269, 269, 269]}, "prediction_text": "Helicoid stromal thylakoids."}
{"answers": {"text": ["light energy", "light energy", "light energy"], "answer_start": [236, 236, 236]}, "prediction_text": "Light energy."}
{"answers": {"text": ["light energy", "light energy", "light energy"], "answer_start": [236, 236, 236]}, "prediction_text": "Light energy."}
{"answers": {"text": ["energize electrons", "to energize electrons", "energize electrons"], "answer_start": [263, 260, 263]}, "prediction_text": "Energize electrons."}
{"answers": {"text": ["pump hydrogen ions into the thylakoid space", "pump hydrogen ions", "pump hydrogen ions into the thylakoid space"], "answer_start": [350, 350, 350]}, "prediction_text": "Hydrogen ions."}
{"answers": {"text": ["a dam turbine", "a dam turbine", "a dam turbine"], "answer_start": [646, 646, 646]}, "prediction_text": "Photosystem II"}
{"answers": {"text": ["two", "two", "two"], "answer_start": [10, 10, 10]}, "prediction_text": "2"}
{"answers": {"text": ["are arranged in grana", "arranged in grana", "arranged in grana"], "answer_start": [59, 63, 63]}, "prediction_text": "The flat tops and bottoms of granal thylakoids contain only the relatively flat photosystem II protein complex."}
{"answers": {"text": ["are in contact with the stroma", "in contact with the stroma", "helicoid sheets"], "answer_start": [112, 116, 257]}, "prediction_text": "Helicoid sheets."}
{"answers": {"text": ["pancake-shaped circular disks", "pancake", "pancake-shaped"], "answer_start": [166, 166, 166]}, "prediction_text": "Pancake-shaped."}
{"answers": {"text": ["about 300\u2013600 nanometers in diameter", "300\u2013600 nanometers", "300\u2013600 nanometers in diameter"], "answer_start": [196, 202, 202]}, "prediction_text": "300-600 nanometers."}
{"answers": {"text": ["about thirty", "about thirty", "thirty"], "answer_start": [134, 134, 140]}, "prediction_text": "Thirty."}
{"answers": {"text": ["help transfer and dissipate excess energy", "transfer and dissipate excess energy", "help transfer and dissipate excess energy"], "answer_start": [180, 185, 180]}, "prediction_text": "Transfer and dissipate excess energy."}
{"answers": {"text": ["their bright colors sometimes override the chlorophyll green", "override the chlorophyll green", "override the chlorophyll green"], "answer_start": [227, 257, 257]}, "prediction_text": "The leaves change color in the fall because of the excess energy that is dissipated by the carotenoids."}
{"answers": {"text": ["a bright red-orange carotenoid", "a bright red-orange carotenoid", "bright red-orange carotenoid found in nearly all chloroplasts"], "answer_start": [375, 375, 377]}, "prediction_text": "Bright red-orange carotenoid."}
{"answers": {"text": ["orange-red zeaxanthin", "zeaxanthin", "orange-red zeaxanthin"], "answer_start": [489, 500, 489]}, "prediction_text": "Zeaxanthin"}
{"answers": {"text": ["e a third group of pigments found in cyanobacteria", "a third group of pigments", "a third group of pigments found in cyanobacteria, and glaucophyte, red algal, and cryptophyte chloroplasts"], "answer_start": [14, 16, 16]}, "prediction_text": "Pigments."}
{"answers": {"text": ["red", "red", "red"], "answer_start": [227, 217, 217]}, "prediction_text": "Red"}
{"answers": {"text": ["red algae", "red algae", "algae"], "answer_start": [217, 217, 221]}, "prediction_text": "Phycobilins."}
{"answers": {"text": ["relatively large protein complexes", "large protein complexes", "large protein complexes"], "answer_start": [264, 275, 275]}, "prediction_text": "Phycobilisomes."}
{"answers": {"text": ["about 40 nanometers across", "about 40 nanometers across", "40 nanometers across"], "answer_start": [299, 299, 305]}, "prediction_text": "40 nanometers."}
{"answers": {"text": ["an enzyme called rubisco", "an enzyme called rubisco", "enzyme called rubisco"], "answer_start": [94, 94, 97]}, "prediction_text": "Rubisco"}
{"answers": {"text": ["it has trouble distinguishing between carbon dioxide and oxygen", "trouble distinguishing between carbon dioxide and oxygen", "has trouble distinguishing between carbon dioxide and oxygen"], "answer_start": [142, 149, 145]}, "prediction_text": "It has trouble distinguishing between carbon dioxide and oxygen."}
{"answers": {"text": ["at high oxygen concentrations, rubisco starts accidentally adding oxygen to sugar precursors", "at high oxygen concentrations, rubisco starts accidentally adding oxygen to sugar precursors", "at high oxygen concentrations, rubisco starts accidentally adding oxygen to sugar precursors"], "answer_start": [210, 210, 210]}, "prediction_text": "Wasted energy."}
{"answers": {"text": ["the Calvin cycle", "Calvin cycle", "Calvin cycle"], "answer_start": [539, 543, 543]}, "prediction_text": "Calvin cycle"}
{"answers": {"text": ["ATP energy", "ATP energy", "ATP energy"], "answer_start": [331, 331, 331]}, "prediction_text": "ATP energy."}
{"answers": {"text": ["light reactions", "each stage of photosynthesis", "light reactions"], "answer_start": [179, 92, 179]}, "prediction_text": "Light reactions."}
{"answers": {"text": ["rubisco", "rubisco", "rubisco"], "answer_start": [209, 209, 209]}, "prediction_text": "Rubisco."}
{"answers": {"text": ["normal grana and thylakoids", "normal grana and thylakoids", "grana and thylakoids"], "answer_start": [227, 227, 234]}, "prediction_text": "Rubisco."}
{"answers": {"text": ["a four-carbon compound", "a four-carbon compound", "a four-carbon compound"], "answer_start": [331, 331, 331]}, "prediction_text": "Four-carbon compound."}
{"answers": {"text": ["to carry out the Calvin cycle and make sugar", "the Calvin cycle", "cyclic electron flow"], "answer_start": [1138, 1151, 860]}, "prediction_text": "Light reactions."}
{"answers": {"text": ["All green parts", "green parts", "All green parts"], "answer_start": [61, 65, 61]}, "prediction_text": "Green parts."}
{"answers": {"text": ["the chlorophyll in them", "chlorophyll", "chlorophyll"], "answer_start": [149, 153, 153]}, "prediction_text": "Chloroplasts."}
{"answers": {"text": ["parenchyma cells", "photosynthetic", "parenchyma cells"], "answer_start": [285, 191, 285]}, "prediction_text": "Chlorenchyma cells."}
{"answers": {"text": ["collenchyma tissue", "in collenchyma tissue", "collenchyma tissue"], "answer_start": [344, 341, 344]}, "prediction_text": "Collenchyma tissue."}
{"answers": {"text": ["A plant cell which contains chloroplasts", "cell which contains chloroplasts", "plant cell which contains chloroplasts"], "answer_start": [364, 372, 366]}, "prediction_text": "A chlorenchyma cell."}
{"answers": {"text": ["in the stems", "stems", "stems"], "answer_start": [53, 60, 60]}, "prediction_text": "Stems."}
{"answers": {"text": ["concentrated in the leaves", "the leaves", "leaves"], "answer_start": [107, 123, 127]}, "prediction_text": "Leaves."}
{"answers": {"text": ["8\u201315 per cell", "around 8\u201315", "8\u201315"], "answer_start": [438, 431, 438]}, "prediction_text": "8-15 per cell."}
{"answers": {"text": ["half a million", "half a million", "half a million"], "answer_start": [184, 184, 184]}, "prediction_text": "Half a million."}
{"answers": {"text": ["the mesophyll layers", "mesophyll", "mesophyll"], "answer_start": [261, 265, 265]}, "prediction_text": "Mesophyll layers, guard cells."}
{"answers": {"text": ["low-light conditions", "low-light conditions", "In low-light conditions"], "answer_start": [101, 101, 98]}, "prediction_text": "Under intense light."}
{"answers": {"text": ["Under intense light", "intense light", "Under intense light"], "answer_start": [200, 206, 200]}, "prediction_text": "Under intense light."}
{"answers": {"text": ["photooxidative damage", "photooxidative damage", "photooxidative damage"], "answer_start": [410, 410, 410]}, "prediction_text": "photooxidative damage."}
{"answers": {"text": ["to distribute chloroplasts so that they can take shelter behind each other or spread out", "they can take shelter behind each other", "so that they can take shelter behind each other or spread out"], "answer_start": [446, 481, 473]}, "prediction_text": "To take shelter behind each other or spread out."}
{"answers": {"text": ["Mitochondria", "Mitochondria", "Mitochondria"], "answer_start": [760, 760, 760]}, "prediction_text": "Mitochondria."}
{"answers": {"text": ["two", "two", "two"], "answer_start": [12, 12, 12]}, "prediction_text": "2"}
{"answers": {"text": ["infected cells seal themselves off and undergo programmed cell death", "programmed cell death", "infected cells seal themselves off and undergo programmed cell death"], "answer_start": [76, 123, 76]}, "prediction_text": "Seal off."}
{"answers": {"text": ["infected cells release signals warning the rest of the plant of a pathogen's presence", "cells release signals", "infected cells release signals warning the rest of the plant of a pathogen's presence"], "answer_start": [186, 195, 186]}, "prediction_text": "Defense-molecule production."}
{"answers": {"text": ["by purposely damaging their photosynthetic system", "purposely damaging their photosynthetic system", "purposely damaging their photosynthetic system"], "answer_start": [311, 314, 314]}, "prediction_text": "By damaging their photosynthetic system."}
{"answers": {"text": ["reactive oxygen species", "reactive oxygen species", "reactive oxygen species"], "answer_start": [372, 372, 372]}, "prediction_text": "reactive oxygen species"}
{"answers": {"text": ["salicylic acid, jasmonic acid, nitric oxide and reactive oxygen species", "salicylic acid, jasmonic acid, nitric oxide and reactive oxygen species", "reactive oxygen species"], "answer_start": [156, 156, 204]}, "prediction_text": "reactive oxygen species"}
{"answers": {"text": ["After detecting stress in a cell", "After detecting stress in a cell", "After detecting stress in a cell"], "answer_start": [44, 44, 44]}, "prediction_text": "After detecting stress in a cell."}
{"answers": {"text": ["pass on their signal to an unknown second messenger molecule", "pass on their signal", "pass on their signal to an unknown second messenger molecule"], "answer_start": [391, 391, 391]}, "prediction_text": "They pass on their signal to an unknown second messenger molecule."}
{"answers": {"text": ["signals from the chloroplast that regulate gene expression in the nucleus", "signals from the chloroplast that regulate gene expression", "signals from the chloroplast that regulate gene expression in the nucleus"], "answer_start": [503, 503, 503]}, "prediction_text": "Gene expression."}
{"answers": {"text": ["photosynthesis", "photosynthesis", "its role in photosynthesis"], "answer_start": [60, 60, 48]}, "prediction_text": "Photosynthesis."}
{"answers": {"text": ["photosynthesis", "photosynthesis", "photosynthesis"], "answer_start": [60, 60, 60]}, "prediction_text": "Photosynthesis."}
{"answers": {"text": ["food in the form of sugars", "light", "food in the form of sugars"], "answer_start": [164, 97, 164]}, "prediction_text": "Sugar molecules."}
{"answers": {"text": ["Water (H2O) and carbon dioxide (CO2)", "Water (H2O) and carbon dioxide (CO2)", "Water (H2O) and carbon dioxide (CO2)"], "answer_start": [192, 192, 192]}, "prediction_text": "Water (H2O) and carbon dioxide (CO2)"}
{"answers": {"text": ["sugar and oxygen (O2)", "sugar and oxygen (O2)", "sugar and oxygen (O2)"], "answer_start": [261, 261, 261]}, "prediction_text": "Oxygen, sugar, and carbon dioxide."}
{"answers": {"text": ["generate ATP energy", "use the potential energy stored in an H+", "use the potential energy stored in an H+, or hydrogen ion gradient to generate ATP energy"], "answer_start": [102, 32, 32]}, "prediction_text": "Use the potential energy stored in an H+, or hydrogen ion gradient to generate ATP energy."}
{"answers": {"text": ["into the thylakoid space", "thylakoid space", "into the thylakoid space"], "answer_start": [347, 356, 347]}, "prediction_text": "Thylakoid space."}
{"answers": {"text": ["up to a thousand times", "up to a thousand times as many", "(up to a thousand times as many) inside the thylakoid system than in the stroma"], "answer_start": [433, 433, 432]}, "prediction_text": "Thousands times."}
{"answers": {"text": ["phosphorylate adenosine diphosphate", "phosphorylate adenosine diphosphate", "energy from the flowing hydrogen ions"], "answer_start": [725, 725, 684]}, "prediction_text": "ATP."}
{"answers": {"text": ["adenosine triphosphate", "adenosine triphosphate", "adenosine triphosphate"], "answer_start": [766, 766, 766]}, "prediction_text": "ATP is the energy source for the dark reactions."}
{"answers": {"text": ["NADP+", "photosystem I", "NADP+"], "answer_start": [220, 76, 220]}, "prediction_text": "NADP+"}
{"answers": {"text": ["cyclic photophosphorylation", "cyclic photophosphorylation", "cyclic photophosphorylation"], "answer_start": [404, 404, 404]}, "prediction_text": "Cyclic photophosphorylation."}
{"answers": {"text": ["in C4 plants", "in C4 plants", "in C4 plants"], "answer_start": [506, 506, 506]}, "prediction_text": "C4 plants."}
{"answers": {"text": ["more ATP than NADPH", "more ATP than NADPH", "more ATP than NADPH"], "answer_start": [531, 531, 531]}, "prediction_text": "More ATP."}
{"answers": {"text": ["The Calvin cycle", "The Calvin cycle", "Calvin cycle"], "answer_start": [0, 0, 4]}, "prediction_text": "Calvin cycle"}
{"answers": {"text": ["unstable six-carbon molecules that immediately break down", "unstable six-carbon molecules", "unstable six-carbon molecules"], "answer_start": [134, 134, 134]}, "prediction_text": "Stable six-carbon molecules."}
{"answers": {"text": ["three-carbon molecules called 3-phosphoglyceric acid", "three-carbon molecules", "three-carbon molecules called 3-phosphoglyceric acid, or 3-PGA"], "answer_start": [197, 197, 197]}, "prediction_text": "3-PGA"}
{"answers": {"text": ["one out of every six", "out of every six", "one out of every six"], "answer_start": [479, 483, 479]}, "prediction_text": "One out of every six."}
{"answers": {"text": ["glucose monomers in the chloroplast can be linked together", "glucose monomers", "glucose monomers in the chloroplast"], "answer_start": [15, 15, 15]}, "prediction_text": "Glucose monomers."}
{"answers": {"text": ["Under conditions such as high atmospheric CO2 concentrations", "high atmospheric CO2 concentrations", "Under conditions such as high atmospheric CO2 concentrations,"], "answer_start": [157, 182, 157]}, "prediction_text": "Under conditions such as high atmospheric CO2 concentrations."}
{"answers": {"text": ["distorting the grana and thylakoids", "distorting the grana and thylakoids", "displace the thylakoids, but leave them intact."], "answer_start": [260, 260, 317]}, "prediction_text": "Starch grains displace the thylakoids."}
{"answers": {"text": ["Waterlogged roots", "Waterlogged", "Waterlogged roots"], "answer_start": [365, 365, 365]}, "prediction_text": "Waterlogged roots."}
{"answers": {"text": ["another photosynthesis-depressing factor", "another photosynthesis-depressing factor", "another photosynthesis-depressing factor"], "answer_start": [835, 835, 835]}, "prediction_text": "Photosynthesis-depressing factor."}
{"answers": {"text": ["add O2 instead of CO2 to RuBP", "add O2 instead of CO2", "add O2 instead of CO2 to RuBP"], "answer_start": [165, 165, 165]}, "prediction_text": "Add O2 instead of CO2 to RuBP."}
{"answers": {"text": ["when the oxygen concentration is too high", "when the oxygen concentration is too high", "when the oxygen concentration is too high"], "answer_start": [27, 27, 27]}, "prediction_text": "When the oxygen concentration is too high."}
{"answers": {"text": ["it consumes ATP and oxygen, releases CO2, and produces no sugar", "it consumes ATP and oxygen, releases CO2, and produces no sugar", "consumes ATP and oxygen, releases CO2, and produces no sugar"], "answer_start": [250, 250, 253]}, "prediction_text": "The efficiency of photosynthesis is reduced."}
{"answers": {"text": ["up to half the carbon fixed by the Calvin cycle", "up to half", "half"], "answer_start": [328, 328, 334]}, "prediction_text": "half the carbon fixed by the Calvin cycle."}
{"answers": {"text": ["they exhibit a distinct chloroplast dimorphism", "exhibit a distinct chloroplast dimorphism", "exhibit a distinct chloroplast dimorphism"], "answer_start": [761, 766, 766]}, "prediction_text": "Chloroplast dimorphism."}
{"answers": {"text": ["in their stroma", "stroma", "in their stroma"], "answer_start": [65, 74, 65]}, "prediction_text": "Cytosol and mitochondria."}
{"answers": {"text": ["cysteine and methionine", "cysteine and methionine", "cysteine and methionine"], "answer_start": [120, 120, 120]}, "prediction_text": "Cysteine and methionine."}
{"answers": {"text": ["it has trouble crossing membranes to get to where it is needed", "it has trouble crossing membranes", "has trouble crossing membranes to get to where it is needed"], "answer_start": [279, 279, 282]}, "prediction_text": "It is difficult to cross membranes."}
{"answers": {"text": ["whether the organelle carries out the last leg of the pathway or if it happens in the cytosol", "whether the organelle carries out the last leg of the pathway", "whether the organelle carries out the last leg of the pathway or if it happens in the cytosol"], "answer_start": [423, 423, 423]}, "prediction_text": "The organelle carries out the last leg of the pathway."}
{"answers": {"text": ["Chloroplasts", "Chloroplasts", "Chloroplasts"], "answer_start": [0, 0, 0]}, "prediction_text": "Chloroplasts"}
{"answers": {"text": ["undifferentiated proplastids found in the zygote, or fertilized egg", "undifferentiated proplastids", "undifferentiated proplastids found in the zygote"], "answer_start": [255, 255, 255]}, "prediction_text": "Proplastids."}
{"answers": {"text": ["in an adult plant's apical meristems", "zygote", "adult plant's apical meristems"], "answer_start": [355, 297, 361]}, "prediction_text": "Zygote, fertilized egg."}
{"answers": {"text": ["the formation of starch-storing amyloplasts", "amyloplasts", "starch-storing amyloplasts"], "answer_start": [478, 510, 495]}, "prediction_text": "Amyloplasts."}
{"answers": {"text": ["proplastids may develop into an etioplast stage before becoming chloroplasts", "proplastids may develop into an etioplast stage before becoming chloroplasts", "proplastids may develop into an etioplast stage before becoming chloroplasts"], "answer_start": [86, 86, 86]}, "prediction_text": "Proplastids develop into an etioplast."}
{"answers": {"text": ["a plastid that lacks chlorophyll", "plastid that lacks chlorophyll", "plastid that lacks chlorophyll"], "answer_start": [180, 182, 182]}, "prediction_text": "A plastid lacking chlorophyll."}
{"answers": {"text": ["invaginations that form a lattice of tubes in their stroma", "invaginations", "lattice of tubes in their stroma, called a prolamellar body"], "answer_start": [237, 237, 263]}, "prediction_text": "Inner membrane invaginations."}
{"answers": {"text": ["a yellow chlorophyll precursor", "yellow chlorophyll precursor", "yellow chlorophyll precursor"], "answer_start": [369, 371, 371]}, "prediction_text": "A yellow chlorophyll precursor stocked."}
{"answers": {"text": ["Gymnosperms", "Gymnosperms", "Gymnosperms"], "answer_start": [635, 635, 635]}, "prediction_text": "Gymnosperms."}
{"answers": {"text": ["chromoplasts", "chromoplasts", "chromoplasts"], "answer_start": [119, 119, 119]}, "prediction_text": "Chromoplasts."}
{"answers": {"text": ["pigment-filled plastids responsible for the bright colors seen in flowers and ripe fruit", "pigment-filled plastids", "pigment-filled plastids responsible for the bright colors seen in flowers and ripe fruit"], "answer_start": [143, 143, 143]}, "prediction_text": "Pigment-filled plastids."}
{"answers": {"text": ["chromoplasts", "chromoplasts", "chromoplasts"], "answer_start": [285, 285, 285]}, "prediction_text": "Chromoplasts."}
{"answers": {"text": ["chromoplasts", "chromoplasts", "chromoplasts"], "answer_start": [359, 359, 359]}, "prediction_text": "Chromoplasts."}
{"answers": {"text": ["chloroplasts and other plastids", "chloroplasts and other plastids can turn back into proplastids", "chloroplasts and other plastids"], "answer_start": [588, 588, 588]}, "prediction_text": "Chloroplasts and amyloplasts."}
{"answers": {"text": ["filaments", "filaments", "filaments"], "answer_start": [76, 76, 76]}, "prediction_text": "Filaments"}
{"answers": {"text": ["proteins", "proteins", "proteins"], "answer_start": [37, 37, 37]}, "prediction_text": "Filaments."}
{"answers": {"text": ["a structure called a Z-ring", "a Z-ring", "Z-ring"], "answer_start": [129, 148, 150]}, "prediction_text": "Z-ring."}
{"answers": {"text": ["within the chloroplast's stroma", "within the chloroplast's stroma", "within the chloroplast's stroma"], "answer_start": [157, 157, 157]}, "prediction_text": "Chloroplast's stroma."}
{"answers": {"text": ["The Min system", "The Min system", "Min system"], "answer_start": [190, 190, 194]}, "prediction_text": "Min system"}
{"answers": {"text": ["plastid-dividing rings", "plastid-dividing rings", "plastid-dividing rings"], "answer_start": [14, 14, 14]}, "prediction_text": "Plastid-dividing rings."}
{"answers": {"text": ["two", "two", "two"], "answer_start": [10, 10, 10]}, "prediction_text": "2"}
{"answers": {"text": ["about 5 nanometers across", "about 5 nanometers across", "5 nanometers across"], "answer_start": [288, 288, 294]}, "prediction_text": "5 nanometers."}
{"answers": {"text": ["6.4 nanometers", "6.4 nanometers", "6.4 nanometers"], "answer_start": [332, 332, 332]}, "prediction_text": "6.4 nanometers."}
{"answers": {"text": ["chloroplasts have a third plastid-dividing ring", "chloroplasts have a third plastid-dividing ring", "chloroplasts have a third plastid-dividing ring located in the chloroplast's intermembrane space"], "answer_start": [486, 486, 486]}, "prediction_text": "Third plastid-dividing ring located in the chloroplast's intermembrane space."}
{"answers": {"text": ["Light", "Light", "Light"], "answer_start": [0, 0, 0]}, "prediction_text": "Bright white light."}
{"answers": {"text": ["bright white light", "white light", "bright white light"], "answer_start": [233, 240, 233]}, "prediction_text": "Bright white light."}
{"answers": {"text": ["large dumbbell-shaped", "dumbbell-shaped", "dumbbell-shaped"], "answer_start": [348, 354, 354]}, "prediction_text": "Large dumbbell-shaped chloroplasts."}
{"answers": {"text": ["poor quality green light", "green light", "poor quality green light"], "answer_start": [148, 161, 148]}, "prediction_text": "Bright white light."}
{"answers": {"text": ["transgenes in these plastids cannot be disseminated by pollen", "chloroplasts are not inherited from the male parent", "transgenes in these plastids cannot be disseminated by pollen"], "answer_start": [176, 123, 176]}, "prediction_text": "Developers of genetically modified crops."}
{"answers": {"text": ["environmental risks", "environmental risks", "environmental risks"], "answer_start": [418, 418, 418]}, "prediction_text": "Environmental risks."}
{"answers": {"text": ["3 in 1,000,000", "3 in 1,000,000", "3 in 1,000,000"], "answer_start": [778, 778, 778]}, "prediction_text": "3 in 1,000,000."}
{"answers": {"text": ["transplastomic", "genetically modified crops", "genetically modified plants"], "answer_start": [753, 62, 325]}, "prediction_text": "Transplastomic plants."}
{"answers": {"text": ["itself", "itself", "itself", "itself", "itself"], "answer_start": [110, 110, 110, 110, 110]}, "prediction_text": "5"}
{"answers": {"text": ["composite number", "composite number", "composite number", "primes"], "answer_start": [189, 189, 189, 439]}, "prediction_text": "primes"}
{"answers": {"text": ["The fundamental theorem of arithmetic", "fundamental theorem of arithmetic", "arithmetic", "fundamental theorem of arithmetic", "fundamental theorem of arithmetic"], "answer_start": [369, 373, 396, 373, 373]}, "prediction_text": "The fundamental theorem of arithmetic."}
{"answers": {"text": ["a product of primes", "product of primes that is unique up to ordering", "primes", "primes", "primes that is unique up to ordering"], "answer_start": [511, 513, 524, 524, 524]}, "prediction_text": "primes"}
{"answers": {"text": ["because one can include arbitrarily many instances of 1 in any factorization", "one can include arbitrarily many instances of 1 in any factorization", "can include arbitrarily many instances of 1 in any factorization", "one can include arbitrarily many instances of 1 in any factorization", "because one can include arbitrarily many instances of 1 in any factorization"], "answer_start": [625, 633, 637, 633, 625]}, "prediction_text": "1. One can include arbitrarily many instances of 1 in any factorization."}
{"answers": {"text": ["primality", "primality", "primality", "primality", "primality"], "answer_start": [47, 47, 47, 47, 47]}, "prediction_text": "Primality"}
{"answers": {"text": ["trial division", "trial division", "trial division", "trial division", "trial division"], "answer_start": [142, 142, 142, 142, 142]}, "prediction_text": "Trial division."}
{"answers": {"text": ["the Miller\u2013Rabin primality test", "Miller\u2013Rabin primality test", "Miller\u2013Rabin primality test", "Miller\u2013Rabin primality test", "Miller\u2013Rabin primality test"], "answer_start": [359, 363, 363, 363, 363]}, "prediction_text": "Miller-Rabin test"}
{"answers": {"text": ["the AKS primality test", "AKS primality test", "AKS primality test", "AKS primality test", "AKS primality test"], "answer_start": [448, 452, 452, 452, 452]}, "prediction_text": "Miller-Rabin test"}
{"answers": {"text": ["22,338,618 decimal digits", "22,338,618", "22,338,618", "22,338,618", "22,338,618"], "answer_start": [724, 724, 724, 724, 724]}, "prediction_text": "22,338,618."}
{"answers": {"text": ["infinitely many", "infinitely", "infinitely many", "infinitely", "infinitely many"], "answer_start": [10, 10, 10, 10, 10]}, "prediction_text": "Infinitely."}
{"answers": {"text": ["Euclid", "Euclid", "Euclid", "Euclid", "Euclid"], "answer_start": [53, 53, 53, 53, 53]}, "prediction_text": "Euclid"}
{"answers": {"text": ["the statistical behaviour", "distribution", "statistical", "statistical", "statistical"], "answer_start": [214, 174, 218, 218, 218]}, "prediction_text": "Statistical behavior."}
{"answers": {"text": ["the prime number theorem", "prime number theorem", "prime number", "prime number theorem", "prime number theorem"], "answer_start": [319, 323, 323, 323, 323]}, "prediction_text": "Prime number theorem."}
{"answers": {"text": ["at the end of the 19th century", "end of the 19th century", "end of the 19th century", "end of the 19th century", "end of the 19th century"], "answer_start": [352, 359, 359, 359, 359]}, "prediction_text": "19th century"}
{"answers": {"text": ["Goldbach's conjecture", "Goldbach's conjecture", "Goldbach's conjecture", "Goldbach's conjecture", "Goldbach's conjecture"], "answer_start": [60, 60, 60, 60, 60]}, "prediction_text": "Goldbach's conjecture."}
{"answers": {"text": ["the twin prime conjecture", "twin prime conjecture", "twin prime conjecture", "twin prime conjecture", "twin prime conjecture"], "answer_start": [170, 174, 174, 174, 174]}, "prediction_text": "Twin prime conjecture"}
{"answers": {"text": ["algebraic aspects", "algebraic", "algebraic", "algebraic", "algebraic aspects"], "answer_start": [369, 369, 369, 369, 369]}, "prediction_text": "Algebraic property of numbers."}
{"answers": {"text": ["public-key cryptography", "public-key cryptography", "public-key cryptography", "cryptography", "public-key cryptography"], "answer_start": [470, 470, 470, 481, 470]}, "prediction_text": "Public-key cryptography."}
{"answers": {"text": ["prime ideals", "prime elements", "prime elements", "prime elements"], "answer_start": [726, 707, 707, 707]}, "prediction_text": "Prime elements and prime ideals."}
{"answers": {"text": ["2", "2", "2", "2", "2"], "answer_start": [120, 120, 120, 120, 120]}, "prediction_text": "5"}
{"answers": {"text": ["1, 2, and n", "1, 2, and n", "1, 2, and n", "1, 2, and n", "1, 2, and n"], "answer_start": [217, 217, 217, 217, 217]}, "prediction_text": "1, 2, and n."}
{"answers": {"text": ["odd prime", "odd prime", "odd prime", "odd prime", "odd prime"], "answer_start": [286, 286, 286, 286, 286]}, "prediction_text": "Odd prime."}
{"answers": {"text": ["9", "9", "9", "9", "9"], "answer_start": [443, 443, 443, 443, 443]}, "prediction_text": "9"}
{"answers": {"text": ["even numbers", "even", "even numbers", "even", "even"], "answer_start": [452, 452, 452, 452, 452]}, "prediction_text": "Integers."}
{"answers": {"text": ["1", "1", "1", "1", "1"], "answer_start": [40, 40, 40, 40, 40]}, "prediction_text": "1"}
{"answers": {"text": ["Christian Goldbach", "Christian Goldbach", "Christian Goldbach", "mathematicians", "mathematicians"], "answer_start": [220, 220, 220, 143, 143]}, "prediction_text": "Christian Goldbach"}
{"answers": {"text": ["Leonhard Euler", "Leonhard Euler", "Leonhard Euler", "Leonhard Euler", "Leonhard Euler"], "answer_start": [301, 301, 301, 301, 301]}, "prediction_text": "Leonhard Euler"}
{"answers": {"text": ["10,006,721", "primes up to 10,006,721", "10,006,721", "10,006,721", "10,006,721"], "answer_start": [481, 468, 481, 481, 481]}, "prediction_text": "10,006,721"}
{"answers": {"text": ["its own special category as a \"unit\"", "unit", "a \"unit\"", "unit", "a \"unit"], "answer_start": [742, 773, 770, 773, 770]}, "prediction_text": "Unit."}
{"answers": {"text": ["Euclid's fundamental theorem of arithmetic", "Euclid's fundamental theorem of arithmetic", "arithmetic", "Euclid's fundamental theorem of arithmetic", "Euclid's fundamental theorem of arithmetic"], "answer_start": [83, 83, 115, 83, 83]}, "prediction_text": "Euclid's fundamental theorem of arithmetic."}
{"answers": {"text": ["if 1 were considered a prime", "1 were considered a prime", "1 were considered a prime", "if 1 were considered a prime", "if 1 were considered a prime"], "answer_start": [485, 488, 488, 485, 485]}, "prediction_text": "1 was considered a prime."}
{"answers": {"text": ["Euler's totient function", "sum of divisors function", "sum of divisors function", "the sum of divisors function", "sum of divisors"], "answer_start": [834, 866, 866, 862, 866]}, "prediction_text": "Euler's totient function."}
{"answers": {"text": ["the sum of divisors function", "relationship of the number to its corresponding value of Euler's totient function", "relationship of the number to its corresponding value of Euler's totient function", "relationship of the number to its corresponding value of Euler's totient function", "the relationship of the number to its corresponding value of Euler's totient function"], "answer_start": [862, 777, 777, 777, 773]}, "prediction_text": "Euler's totient function."}
{"answers": {"text": ["only the single number 1", "1", "only the single number 1", "eliminate all multiples of 1", "eliminate all multiples of 1 (that is, all other numbers) and produce as output only the single number 1."], "answer_start": [659, 682, 659, 579, 579]}, "prediction_text": "1."}
{"answers": {"text": ["the Rhind papyrus", "Rhind", "Rhind", "Egyptian fraction", "Rhind papyrus"], "answer_start": [149, 153, 153, 117, 153]}, "prediction_text": "Rhind papyrus"}
{"answers": {"text": ["the Ancient Greeks", "Ancient Greeks", "Greeks", "Ancient Greeks", "Ancient Greeks"], "answer_start": [329, 333, 341, 333, 333]}, "prediction_text": "Ancient Greeks"}
{"answers": {"text": ["Euclid's Elements", "Euclid's Elements", "Euclid's Elements", "Euclid's Elements", "Euclid's Elements"], "answer_start": [349, 349, 349, 349, 349]}, "prediction_text": "Euclid's Elements."}
{"answers": {"text": ["Euclid", "Euclid", "Euclid", "Euclid", "Euclid"], "answer_start": [501, 501, 501, 501, 501]}, "prediction_text": "Euclid"}
{"answers": {"text": ["compute primes", "compute primes", "compute primes", "compute primes", "compute primes"], "answer_start": [654, 654, 654, 654, 654]}, "prediction_text": "Computes primes."}
{"answers": {"text": ["In 1640", "1640", "1640", "1640", "1640"], "answer_start": [90, 93, 93, 93, 93]}, "prediction_text": "1640"}
{"answers": {"text": ["Euler", "Euler", "Euler", "Euler", "Euler"], "answer_start": [191, 191, 191, 191, 191]}, "prediction_text": "Euler"}
{"answers": {"text": ["22n + 1", "22n + 1", "22n + 1", "22n + 1", "22n + 1"], "answer_start": [252, 252, 252, 252, 252]}, "prediction_text": "22n + 1"}
{"answers": {"text": ["2p \u2212 1", "2p \u2212 1, with p a prime", "2p \u2212 1", "2p \u2212 1", "2p \u2212 1"], "answer_start": [591, 591, 591, 591, 591]}, "prediction_text": "2p \u2212 1"}
{"answers": {"text": ["up to n = 4 (or 216 + 1)", "up to n = 4 (or 216 + 1)", "216 + 1", "n = 4", "n = 4"], "answer_start": [324, 324, 340, 330, 330]}, "prediction_text": "up to n = 4 (or 216 + 1)"}
{"answers": {"text": ["trial division", "trial division", "trial division", "trial division", "trial division"], "answer_start": [79, 79, 79, 79, 79]}, "prediction_text": "Trial division."}
{"answers": {"text": ["if a complete list of primes up to  is known", "a complete list of primes up to  is known", "complete list of primes up to  is known", "if a complete list of primes up to  is known", "if a complete list of primes up to  is known"], "answer_start": [591, 594, 596, 591, 591]}, "prediction_text": "Complete list of primes up to ."}
{"answers": {"text": ["greater than 1", "1", "1", "is greater than 1 and less than or equal to the square root of n", "1"], "answer_start": [157, 170, 170, 154, 170]}, "prediction_text": "1. The square root of n."}
{"answers": {"text": ["only three divisions", "only for those m that are prime", "three", "three", "three"], "answer_start": [752, 676, 757, 757, 757]}, "prediction_text": "3 divisions."}
{"answers": {"text": ["less than or equal to the square root of n", "the square root of n", "square root of n", "the square root of n.", "the square root of n."], "answer_start": [176, 198, 202, 198, 198]}, "prediction_text": "The integer m must be less than or equal to 1."}
{"answers": {"text": ["two main classes", "two", "two", "two", "two"], "answer_start": [65, 65, 65, 65, 65]}, "prediction_text": "2"}
{"answers": {"text": ["probabilistic (or \"Monte Carlo\")", "probabilistic (or \"Monte Carlo\")", "probabilistic", "probabilistic", "probabilistic"], "answer_start": [83, 83, 83, 83, 83]}, "prediction_text": "Monte Carlo algorithms."}
{"answers": {"text": ["deterministic", "deterministic algorithms", "deterministic", "deterministic algorithms", "deterministic algorithms"], "answer_start": [120, 120, 120, 120, 120]}, "prediction_text": "Monte Carlo algorithms."}
{"answers": {"text": ["deterministic", "deterministic algorithm", "deterministic", "deterministic", "deterministic"], "answer_start": [275, 275, 275, 275, 275]}, "prediction_text": "Monte Carlo algorithm."}
{"answers": {"text": ["1/(1-p)n", "1/(1-p)n", "1/(1-p)n", "1/(1-p)n", "1/(1-p)n"], "answer_start": [833, 833, 833, 833, 833]}, "prediction_text": "1/(1-p)n."}
{"answers": {"text": ["the Fermat primality test,", "Fermat primality test", "Fermat primality test", "Fermat primality test", "the Fermat primality test"], "answer_start": [57, 61, 61, 61, 57]}, "prediction_text": "Fermat primality test."}
{"answers": {"text": ["np\u2261n (mod p)", "np\u2261n (mod p) for any n if p is a prime number", "np\u2261n (mod p) for any n if p is a prime number", "np\u2261n (mod p) for any n if p is a prime number", "the fact (Fermat's little theorem) that np\u2261n (mod p) for any n if p is a prime number"], "answer_start": [140, 140, 140, 140, 100]}, "prediction_text": "Fermat's little theorem."}
{"answers": {"text": ["composite numbers (the Carmichael numbers)", "Carmichael", "Carmichael", "Carmichael numbers", "Carmichael numbers"], "answer_start": [355, 378, 378, 378, 378]}, "prediction_text": "Carmichael numbers."}
{"answers": {"text": ["Baillie-PSW", "Baillie-PSW", "Baillie-PSW", "Baillie-PSW", "Baillie-PSW,"], "answer_start": [739, 739, 739, 739, 739]}, "prediction_text": "Baillie-PSW"}
{"answers": {"text": ["Solovay-Strassen tests", "Miller-Rabin", "Miller-Rabin", "Miller-Rabin", "Miller-Rabin"], "answer_start": [770, 752, 752, 752, 752]}, "prediction_text": "Baillie-PSW"}
{"answers": {"text": ["2p + 1", "2p + 1 with p prime", "2p + 1 with p prime", "2p + 1", "2p + 1"], "answer_start": [189, 189, 189, 189, 189]}, "prediction_text": "2p + 1"}
{"answers": {"text": ["2p \u2212 1", "2p \u2212 1", "2p \u2212 1, where p is an arbitrary prime", "2p \u2212 1", "2p \u2212 1,"], "answer_start": [308, 308, 308, 308, 308]}, "prediction_text": "2p \u2212 1"}
{"answers": {"text": ["The Lucas\u2013Lehmer test", "Lucas\u2013Lehmer", "Lucas\u2013Lehmer", "Lucas\u2013Lehmer", "Lucas\u2013Lehmer test"], "answer_start": [347, 351, 351, 351, 351]}, "prediction_text": "Lucas-Lehmer test."}
{"answers": {"text": ["primorial primes", "Fermat", "Sophie Germain", "Sophie Germain", "Sophie Germain"], "answer_start": [211, 229, 147, 147, 147]}, "prediction_text": "Sophie Germain primes."}
{"answers": {"text": ["Fermat primes", "Mersenne", "primorial primes", "primorial primes", "primorial primes"], "answer_start": [229, 247, 211, 211, 211]}, "prediction_text": "Sophie Germain primes."}
{"answers": {"text": ["distributed computing", "distributed computing", "distributed", "distributed computing", "distributed computing"], "answer_start": [118, 118, 118, 118, 118]}, "prediction_text": "Distributed computing."}
{"answers": {"text": ["In 2009", "2009", "2009", "2009", "2009"], "answer_start": [141, 144, 144, 144, 144]}, "prediction_text": "2009"}
{"answers": {"text": ["US$100,000", "US$100,000", "US$100,000", "$100,000", "US$100,000"], "answer_start": [213, 213, 213, 215, 213]}, "prediction_text": "US$100,000"}
{"answers": {"text": ["The Electronic Frontier Foundation", "Electronic Frontier Foundation", "Electronic Frontier Foundation", ". The Electronic Frontier Foundation", "$150,000"], "answer_start": [293, 297, 297, 291, 340]}, "prediction_text": "Electronic Frontier Foundation"}
{"answers": {"text": ["[256kn + 1, 256k(n + 1) \u2212 1]", "[256kn + 1, 256k(n + 1) \u2212 1]", "[256kn + 1, 256k(n + 1) \u2212 1]", "[256kn + 1, 256k(n + 1) \u2212 1]", "[256kn + 1, 256k(n + 1) \u2212 1]."], "answer_start": [765, 765, 765, 765, 765]}, "prediction_text": "[256kn + 1, 256k(n + 1) \u2212 1]"}
{"answers": {"text": ["the floor function", "floor", "floor", "floor function", "floor function"], "answer_start": [53, 57, 57, 57, 57]}, "prediction_text": "floor function"}
{"answers": {"text": ["Chebyshev", "Chebyshev", "Chebyshev", "Chebyshev", "Chebyshev"], "answer_start": [212, 212, 212, 212, 212]}, "prediction_text": "Chebyshev"}
{"answers": {"text": ["any natural number n > 3", "n > 3", "n > 3", "> 3.", "n > 3"], "answer_start": [315, 334, 334, 336, 334]}, "prediction_text": "3."}
{"answers": {"text": ["n < p < 2n \u2212 2", "n < p < 2n \u2212 2", "A or \u03bc", "n < p < 2n \u2212 2", "n < p < 2n \u2212 2"], "answer_start": [295, 295, 360, 295, 295]}, "prediction_text": "p = 2n \u2212 2."}
{"answers": {"text": ["Wilson's theorem", "Wilson's", "Wilson's", "Wilson's theorem", "Wilson's theorem"], "answer_start": [459, 459, 459, 459, 459]}, "prediction_text": "Wilson's theorem."}
{"answers": {"text": ["their greatest common divisor is one", "greatest common divisor is one", "their greatest common divisor is one", "their greatest common divisor is one"], "answer_start": [69, 75, 69, 69]}, "prediction_text": "The condition that infinitely many primes can exist only if a and q are coprime is stated as follows:"}
{"answers": {"text": ["Dirichlet's theorem", "Dirichlet's", "Dirichlet's theorem", "Dirichlet's theorem"], "answer_start": [149, 149, 149, 149]}, "prediction_text": "Dirichlet's theorem."}
{"answers": {"text": ["1/6", "1/6", "1/6", "1/6"], "answer_start": [713, 713, 713, 713]}, "prediction_text": "1/6."}
{"answers": {"text": ["at most one prime number", "one", "one", "at most one"], "answer_start": [469, 477, 477, 469]}, "prediction_text": "1."}
{"answers": {"text": ["infinitely many prime numbers", "infinitely many", "infinite", "infinitely many"], "answer_start": [550, 550, 550, 550]}, "prediction_text": "1."}
{"answers": {"text": ["The zeta function", "zeta", "zeta function", "zeta function"], "answer_start": [0, 4, 4, 4]}, "prediction_text": "Zeta function"}
{"answers": {"text": ["a finite value", "finite", "finite", "finite"], "answer_start": [233, 235, 235, 235]}, "prediction_text": "Finite."}
{"answers": {"text": ["diverges", "diverges", "exceeds any given number"], "answer_start": [304, 304, 320]}, "prediction_text": "The harmonic series 1 + 1/2 + 1/3 + 1/4 +... diverges."}
{"answers": {"text": ["exceeds any given number", "exceeds any given number", "exceeds any given number", "exceeds any given number"], "answer_start": [320, 320, 320, 320]}, "prediction_text": "The harmonic series diverges."}
{"answers": {"text": ["identity", "algebraic", "modern algebraic number theory", "modern algebraic number theory"], "answer_start": [506, 465, 458, 458]}, "prediction_text": "Zeta function"}
{"answers": {"text": ["1859", "1859", "1859", "1859"], "answer_start": [45, 45, 45, 45]}, "prediction_text": "1859"}
{"answers": {"text": ["s = \u22122, \u22124, ...,", "\u22122, \u22124, ...,", "\u22122, \u22124", "s = \u22122, \u22124"], "answer_start": [74, 78, 78, 74]}, "prediction_text": "\u22122, \u22124,..., s = \u22122, \u22124,..., s = \u22122."}
{"answers": {"text": ["random noise", "random noise", "random noise", "random noise"], "answer_start": [402, 402, 402, 402]}, "prediction_text": "Random noise."}
{"answers": {"text": ["asymptotic distribution", "asymptotic", "asymptotic distribution", "asymptotic distribution"], "answer_start": [474, 474, 474, 474]}, "prediction_text": "The Riemann hypothesis proposes that primes should have no significant irregularities without good reason."}
{"answers": {"text": ["asymptotic distribution", "asymptotic", "asymptotic distribution", "asymptotic distribution"], "answer_start": [474, 474, 474, 474]}, "prediction_text": "Regularly distributed."}
{"answers": {"text": ["Goldbach's conjecture", "Goldbach's", "Goldbach's", "Goldbach's"], "answer_start": [278, 278, 278, 278]}, "prediction_text": "Goldbach's conjecture."}
{"answers": {"text": ["1912", "1912", "1912", "1912"], "answer_start": [238, 238, 238, 238]}, "prediction_text": "1912"}
{"answers": {"text": ["all numbers up to n = 2 \u00b7 1017", "n = 2 \u00b7 1017", "n = 2", "n = 2"], "answer_start": [462, 480, 480, 480]}, "prediction_text": "4"}
{"answers": {"text": ["Vinogradov's theorem", "Vinogradov's", "Vinogradov's theorem", "Vinogradov's theorem"], "answer_start": [552, 552, 552, 552]}, "prediction_text": "Chen's theorem."}
{"answers": {"text": ["Chen's theorem", "Chen's", "Chen's theorem", "Chen's theorem"], "answer_start": [661, 661, 661, 661]}, "prediction_text": "Chen's theorem."}
{"answers": {"text": ["twin prime conjecture", "twin prime conjecture", "twin prime conjecture", "Polignac's"], "answer_start": [173, 173, 173, 197]}, "prediction_text": "Polignac's conjecture."}
{"answers": {"text": ["pairs of primes with difference 2", "pairs of primes with difference 2", "pairs of primes with difference 2", "pairs of primes with difference 2"], "answer_start": [138, 138, 138, 138]}, "prediction_text": "Twin primes."}
{"answers": {"text": ["Polignac's conjecture", "Polignac's", "Polignac's conjecture", "Polignac's"], "answer_start": [197, 197, 197, 197]}, "prediction_text": "Polignac's conjecture."}
{"answers": {"text": ["n2 + 1", "n2 + 1", "n2 + 1.", "n2 + 1"], "answer_start": [439, 439, 439, 439]}, "prediction_text": "Twin primes."}
{"answers": {"text": ["Brocard's conjecture", "Brocard's", "Brocard's conjecture", "Brocard's"], "answer_start": [521, 521, 521, 521]}, "prediction_text": "Legendre's conjecture."}
{"answers": {"text": ["number theory", "number theory", "number theory", "number theory"], "answer_start": [17, 17, 17, 17]}, "prediction_text": "Number theory."}
{"answers": {"text": ["G. H. Hardy", "G. H. Hardy", "G. H. Hardy", "G. H. Hardy"], "answer_start": [360, 360, 360, 360]}, "prediction_text": "G. H. Hardy"}
{"answers": {"text": ["the 1970s", "1970s", "1970s", "1970s"], "answer_start": [488, 492, 492, 492]}, "prediction_text": "1970s"}
{"answers": {"text": ["hash tables", "hash tables", "hash tables and pseudorandom number generators", "hash tables and pseudorandom number generators"], "answer_start": [664, 664, 664, 664]}, "prediction_text": "Hash tables."}
{"answers": {"text": ["pseudorandom number generators", "pseudorandom", "pseudorandom", "pseudorandom"], "answer_start": [680, 680, 680, 680]}, "prediction_text": "Pseudorandom number generators."}
{"answers": {"text": ["a recurring decimal", "recurring", "recurring", "recurring", "recurring"], "answer_start": [215, 217, 217, 217, 217]}, "prediction_text": "recurring decimal"}
{"answers": {"text": ["p \u2212 1", "p \u2212 1", "p \u2212 1 or a divisor of p \u2212 1", "p \u2212 1 or a divisor of p \u2212 1", "p \u2212 1 or a divisor of p \u2212 1"], "answer_start": [252, 252, 252, 252, 252]}, "prediction_text": "p \u2212 1."}
{"answers": {"text": ["(p \u2212 1)! + 1", "(p \u2212 1)! + 1", "(p \u2212 1)! + 1", "(p \u2212 1)! + 1", "(p \u2212 1)! + 1"], "answer_start": [495, 495, 495, 495, 495]}, "prediction_text": "(n \u2212 1)!"}
{"answers": {"text": ["(n \u2212 1)!", "(n \u2212 1)!", "(n \u2212 1)!", "(n \u2212 1)!", "(n \u2212 1)!"], "answer_start": [582, 582, 582, 582, 582]}, "prediction_text": "(n \u2212 1)!"}
{"answers": {"text": ["p is not a prime factor of q", "p is not a prime factor of q", "p is not a prime factor of q", "p is not a prime factor of q.", "p is not a prime factor of q."], "answer_start": [383, 383, 383, 383, 383]}, "prediction_text": "p is not a prime factor of q."}
{"answers": {"text": ["RSA", "RSA", "RSA", "RSA"], "answer_start": [52, 52, 52, 52]}, "prediction_text": "RSA"}
{"answers": {"text": ["the Diffie\u2013Hellman key exchange", "Diffie\u2013Hellman", "Diffie\u2013Hellman key exchange", "Diffie\u2013Hellman key exchange"], "answer_start": [60, 64, 64, 64]}, "prediction_text": "Diffie\u2013Hellman key exchange."}
{"answers": {"text": ["512-bit", "512", "512", "512"], "answer_start": [140, 140, 140, 140]}, "prediction_text": "512 bits."}
{"answers": {"text": ["modular exponentiation", "modular", "modular", "modular"], "answer_start": [541, 541, 541, 541]}, "prediction_text": "Modular exponentiation."}
{"answers": {"text": ["1024-bit", "1024", "1024", "1024"], "answer_start": [187, 187, 187, 187]}, "prediction_text": "1024 bits."}
{"answers": {"text": ["cicadas", "cicadas", "cicadas", "cicadas"], "answer_start": [34, 34, 34, 34]}, "prediction_text": "Magicicadas"}
{"answers": {"text": ["as grubs underground", "underground", "underground", "underground"], "answer_start": [133, 142, 142, 142]}, "prediction_text": "Underground."}
{"answers": {"text": ["17 years", "17", "17", "17"], "answer_start": [222, 222, 222, 222]}, "prediction_text": "17 years."}
{"answers": {"text": ["make it very difficult for predators to evolve that could specialize as predators", "difficult for predators to evolve that could specialize as predators on Magicicadas", "the prime number intervals between emergences make it very difficult for predators to evolve that could specialize as predators on Magicicadas", "the prime number intervals between emergences make it very difficult for predators to evolve"], "answer_start": [398, 411, 352, 352]}, "prediction_text": "The logic behind the cicadas prime number evolutionary strategy is that prime numbers make it very difficult for predators to evolve that could specialize as predators on Magicicadas."}
{"answers": {"text": ["up to 2% higher", "2%", "2%", "2%"], "answer_start": [775, 781, 781, 781]}, "prediction_text": "2% higher."}
{"answers": {"text": ["indecomposability", "minimality", "minimality or indecomposability", "minimality or indecomposability"], "answer_start": [170, 156, 156, 156]}, "prediction_text": "Minimality or indecomposability."}
{"answers": {"text": ["the smallest subfield", "the smallest subfield", "Q or the finite field with p elements", "the smallest subfield"], "answer_start": [246, 246, 319, 246]}, "prediction_text": "Q"}
{"answers": {"text": ["as a connected sum of prime knots", "as a connected sum of prime knots", "as a connected sum of prime knots", "as a connected sum of prime knots"], "answer_start": [728, 728, 728, 728]}, "prediction_text": "The prime knot."}
{"answers": {"text": ["any object can be, essentially uniquely, decomposed into its prime components", "any object can be, essentially uniquely, decomposed into its prime components", "any object can be, essentially uniquely, decomposed into its prime components", "any object can be, essentially uniquely, decomposed into its prime components"], "answer_start": [459, 459, 459, 459]}, "prediction_text": "That any object can be, essentially uniquely, decomposed into its prime components."}
{"answers": {"text": ["it cannot be written as the knot sum of two nontrivial knots", "cannot be written as the knot sum of two nontrivial knots", "cannot be written as the knot sum of two nontrivial knots", "it cannot be written as the knot sum of two nontrivial knots"], "answer_start": [631, 634, 634, 631]}, "prediction_text": "Indecomposable."}
{"answers": {"text": ["commutative ring R", "commutative ring", "ring R", "commutative ring R"], "answer_start": [83, 83, 95, 83]}, "prediction_text": "Algebraic structure."}
{"answers": {"text": ["prime elements", "prime elements", "prime elements", "prime elements"], "answer_start": [186, 186, 186, 186]}, "prediction_text": "prime elements"}
{"answers": {"text": ["irreducible elements", "irreducible elements", "irreducible elements", "irreducible elements"], "answer_start": [205, 205, 205, 205]}, "prediction_text": "prime elements"}
{"answers": {"text": ["it is neither zero nor a unit", "neither zero nor a unit", "it is neither zero nor a unit", "it is neither zero nor a unit"], "answer_start": [272, 278, 272, 272]}, "prediction_text": "Given x and y in R such that p divides the product xy, then p divides x or y."}
{"answers": {"text": ["cannot be written as a product of two ring elements that are not units", "not a unit and cannot be written as a product of two ring elements that are not units.", "it is not a unit and cannot be written as a product of two ring elements that are not units", "it is not a unit and cannot be written as a product of two ring elements that are not units"], "answer_start": [518, 503, 497, 497]}, "prediction_text": "It cannot be written as a product of two ring elements that are not units."}
{"answers": {"text": ["The fundamental theorem of arithmetic", "theorem of arithmetic", "fundamental theorem of arithmetic", "The fundamental theorem of arithmetic"], "answer_start": [0, 16, 4, 0]}, "prediction_text": "The fundamental theorem of arithmetic."}
{"answers": {"text": ["the Gaussian integers Z[i]", "Gaussian integers", "Gaussian integers Z[i],", "Gaussian integers Z[i]"], "answer_start": [120, 124, 124, 124]}, "prediction_text": "Gaussian integers Z[i]"}
{"answers": {"text": ["a + bi", "a + bi", "a + bi", "a + bi"], "answer_start": [196, 196, 196, 196]}, "prediction_text": "Z[i]"}
{"answers": {"text": ["arbitrary integers", "arbitrary integers", "arbitrary integers", "arbitrary integers"], "answer_start": [254, 254, 254, 254]}, "prediction_text": "arbitrary integers."}
{"answers": {"text": ["4k + 3", "4k + 3", "Z"], "answer_start": [522, 522, 507]}, "prediction_text": "4k + 1"}
{"answers": {"text": ["In ring theory", "ring", "ring theory", "ring theory"], "answer_start": [0, 3, 3, 3]}, "prediction_text": "Ring theory."}
{"answers": {"text": ["Prime ideals", "Prime", "Prime ideals", "Prime ideals"], "answer_start": [79, 79, 79, 79]}, "prediction_text": "Ideals."}
{"answers": {"text": ["algebraic number theory", "algebraic", "algebraic", "algebraic"], "answer_start": [276, 276, 276, 276]}, "prediction_text": "Algebraic number theory."}
{"answers": {"text": ["The fundamental theorem of arithmetic", "theorem of arithmetic", "fundamental theorem of arithmetic", "The fundamental theorem of arithmetic"], "answer_start": [413, 429, 417, 413]}, "prediction_text": "The fundamental theorem of arithmetic."}
{"answers": {"text": ["a Noetherian commutative ring", "Noetherian", "Noetherian commutative ring", "Noetherian"], "answer_start": [525, 527, 527, 527]}, "prediction_text": "Noetherian commutative ring."}
{"answers": {"text": ["Prime ideals", "Prime ideals", "Prime ideals", "Prime ideals"], "answer_start": [0, 0, 0, 0]}, "prediction_text": "The points of algebro-geometric objects are the points of the spectrum of a ring."}
{"answers": {"text": ["ramification in geometry", "ramification", "ramification", "ramification in geometry"], "answer_start": [378, 378, 378, 378]}, "prediction_text": "ramification in geometry."}
{"answers": {"text": ["ring of integers of quadratic number fields", "integers of quadratic number fields", "integers of quadratic number fields", "the ring of integers of quadratic number fields"], "answer_start": [538, 546, 546, 534]}, "prediction_text": "Number theory"}
{"answers": {"text": ["the solvability of quadratic equations", "solvability of quadratic equations", "solvability of quadratic equations", "the solvability of quadratic equations"], "answer_start": [654, 658, 658, 654]}, "prediction_text": "To prove quadratic reciprocity."}
{"answers": {"text": ["norm gets smaller", "gets smaller", "gets smaller", "gets smaller"], "answer_start": [20, 25, 25, 25]}, "prediction_text": "The norm gets smaller."}
{"answers": {"text": ["completed (or local) fields", "completed (or local) fields", "the completed (or local) fields", "the completed (or local) fields"], "answer_start": [552, 552, 548, 548]}, "prediction_text": "Q and more general global fields."}
{"answers": {"text": ["the absolute value", "the absolute value", "absolute value", "the absolute value"], "answer_start": [223, 223, 227, 223]}, "prediction_text": "p-adic norm."}
{"answers": {"text": ["local-global principle", "local-global", "local-global principle", "local-global principle"], "answer_start": [586, 586, 586, 586]}, "prediction_text": "Ostrowski's theorem."}
{"answers": {"text": ["Olivier Messiaen", "Olivier Messiaen", "Olivier Messiaen", "Olivier Messiaen"], "answer_start": [76, 76, 76, 76]}, "prediction_text": "Olivier Messiaen"}
{"answers": {"text": ["La Nativit\u00e9 du Seigneur", "La Nativit\u00e9 du Seigneur", "La Nativit\u00e9 du Seigneur", "La Nativit\u00e9 du Seigneur"], "answer_start": [184, 184, 184, 184]}, "prediction_text": "La Nativit\u00e9 du Seigneur (1935)"}
{"answers": {"text": ["Quatre \u00e9tudes de rythme", "Quatre \u00e9tudes de rythme", "Quatre \u00e9tudes de rythme", "Quatre \u00e9tudes de rythme"], "answer_start": [219, 219, 219, 219]}, "prediction_text": "La Nativit\u00e9 du Seigneur (1935)"}
{"answers": {"text": ["the third \u00e9tude", "third", "third", "third"], "answer_start": [405, 409, 409, 409]}, "prediction_text": "\"Neumes rythmiques\""}
{"answers": {"text": ["the movements of nature", "the movements of nature, movements of free and unequal durations", "the movements of nature, movements of free and unequal durations", "the movements of nature, movements of free and unequal durations"], "answer_start": [504, 504, 504, 504]}, "prediction_text": "movements of nature."}
{"answers": {"text": ["Swiss canton", "s in the Swiss canton of Graub\u00fcnden in the southeastern Swiss Alps,", "Graub\u00fcnden", "Graub\u00fcnden"], "answer_start": [110, 101, 126, 126]}, "prediction_text": "Switzerland"}
{"answers": {"text": ["North Sea", "the North Sea in the Netherlands", "North Sea", "North Sea"], "answer_start": [344, 340, 344, 344]}, "prediction_text": "North Sea"}
{"answers": {"text": ["Cologne, Germany", "Cologne, Germany", "Cologne"], "answer_start": [413, 413, 413]}, "prediction_text": "Cologne, Germany"}
{"answers": {"text": ["Danube", "the Danube", "Danube", "Danube"], "answer_start": [551, 547, 551, 551]}, "prediction_text": "Danube"}
{"answers": {"text": ["1,230 km (760 mi)", "1,230 km (760 mi)", "1,230 km", "1,230 km"], "answer_start": [569, 569, 569, 569]}, "prediction_text": "1,230 km (760 mi)"}
{"answers": {"text": ["Europe", "Central and Western Europe", "Germany", "Germany"], "answer_start": [533, 513, 422, 422]}, "prediction_text": "Switzerland"}
{"answers": {"text": ["Netherlands", "Germany", "Netherlands", "Netherlands"], "answer_start": [361, 422, 361, 361]}, "prediction_text": "Netherlands"}
{"answers": {"text": ["1,230 km", "1,230 km (760 mi)", "1,230 km", "1,230 km"], "answer_start": [569, 569, 569, 569]}, "prediction_text": "1,230 km (760 mi)"}
{"answers": {"text": ["Gaulish name R\u0113nos", "R\u0113nos", "the Gaulish name R\u0113nos", "R\u0113nos", "R\u0113nos"], "answer_start": [88, 101, 84, 101, 101]}, "prediction_text": "Gaulish."}
{"answers": {"text": ["Rhin", "Rhin", "Rhin", "Rhin", "Rhin"], "answer_start": [290, 290, 290, 290, 290]}, "prediction_text": "Rin."}
{"answers": {"text": ["R\u012bnaz", "R\u012bnaz", "*R\u012bnaz", "R\u012bnaz", "R\u012bnaz"], "answer_start": [430, 430, 429, 430, 430]}, "prediction_text": "*R\u012bnaz."}
{"answers": {"text": ["1st century BC", "1st", "Roman-era", "1st century BC"], "answer_start": [150, 150, 129, 150]}, "prediction_text": "1st century BC"}
{"answers": {"text": ["Gaulish name R\u0113nos", "Rhenus", "the Gaulish name R\u0113nos", "1st century BC", "R\u0113nos"], "answer_start": [88, 197, 84, 150, 101]}, "prediction_text": "The Rhine comes from the Gaulish name R\u0113nos."}
{"answers": {"text": ["Rhin", "Rhin", "Rhin", "Rhin", "Rhin"], "answer_start": [290, 290, 290, 290, 290]}, "prediction_text": "Rhein"}
{"answers": {"text": ["Rijn", "Rijn", "Rijn", "Rijn", "Rijn"], "answer_start": [505, 505, 505, 505, 505]}, "prediction_text": "Rhein."}
{"answers": {"text": ["R\u012bnaz", "R\u012bnaz", "R\u012bnaz", "R\u012bnaz", "R\u012bnaz"], "answer_start": [430, 430, 430, 430, 430]}, "prediction_text": "*R\u012bnaz."}
{"answers": {"text": ["Rhijn", "Rhijn", "Rhijn", "Rhijn", "Rhijn"], "answer_start": [533, 533, 533, 533, 533]}, "prediction_text": "Rhijn"}
{"answers": {"text": ["Rhine-kilometers", "Rhine-kilometers", "\"Rhine-kilometers\"", "Rhine-kilometers", "Rhine-kilometers"], "answer_start": [55, 55, 54, 55, 55]}, "prediction_text": "Rhine-kilometers (Rheinkilometer)"}
{"answers": {"text": ["1939", "1939", "1939", "1939", "1939"], "answer_start": [113, 113, 113, 113, 113]}, "prediction_text": "1939"}
{"answers": {"text": ["Old Rhine Bridge at Constance", "Old Rhine Bridge at Constance", "the Old Rhine Bridge at Constance", "Old Rhine Bridge at Constance", "Old Rhine Bridge"], "answer_start": [138, 138, 134, 138, 138]}, "prediction_text": "Constance (0 km)"}
{"answers": {"text": ["Hoek van Holland", "Hoek van Holland", "(1036.20 km)", "Hoek van Holland", "Hoek van Holland"], "answer_start": [178, 178, 195, 178, 178]}, "prediction_text": "Hoek van Holland"}
{"answers": {"text": ["canalisation projects", "canalisation projects", "the river's natural course due to number of canalisation projects completed in the 19th and 20th century", "canalisation projects", "canalisation projects"], "answer_start": [302, 302, 258, 302, 302]}, "prediction_text": "Canalisation projects completed in the 19th and 20th century."}
{"answers": {"text": ["Rhine-kilometers\"", "Rhine-kilometers", "kilometres", "Rhine-kilometers", "Rhine-kilometers"], "answer_start": [55, 55, 525, 55, 55]}, "prediction_text": "Rhine-kilometers (Rheinkilometer)"}
{"answers": {"text": ["1939", "1939", "1939", "1939"], "answer_start": [113, 113, 113, 113]}, "prediction_text": "1939"}
{"answers": {"text": ["Old Rhine Bridge at Constance", "Old Rhine Bridge at Constance", "the Old Rhine Bridge at Constance", "Old Rhine Bridge at Constance", "Old Rhine Bridge"], "answer_start": [138, 138, 134, 138, 138]}, "prediction_text": "Constance (0 km)"}
{"answers": {"text": ["canalisation projects", "canalisation projects", "from the river's natural course due to number of canalisation projects completed in the 19th and 20th century", "canalisation projects", "canalisation projects"], "answer_start": [302, 302, 253, 302, 302]}, "prediction_text": "Canalisation projects completed in the 19th and 20th century."}
{"answers": {"text": ["Hoek van Holland", "Hoek van Holland", "Hoek van Holland", "Hoek van Holland", "Hoek van Holland"], "answer_start": [178, 178, 178, 178, 178]}, "prediction_text": "Hoek van Holland"}
{"answers": {"text": ["north", "north", "north", "north", "north"], "answer_start": [132, 132, 132, 132, 132]}, "prediction_text": "North."}
{"answers": {"text": ["86 km long,", "86 km", "86 km long", "86 km", "86 km"], "answer_start": [172, 172, 172, 172, 172]}, "prediction_text": "86 km."}
{"answers": {"text": ["Rhine Valley", "Rhine Valley", "Rhine Valley", "Rhine Valley", "Rhine Valley"], "answer_start": [289, 289, 289, 289, 289]}, "prediction_text": "Lake Walen"}
{"answers": {"text": ["Sargans", "Near Sargans", "Sargans", "Sargans", "Sargans"], "answer_start": [327, 322, 327, 327, 327]}, "prediction_text": "Sargans"}
{"answers": {"text": ["Austria", "Liechtenstein", "Austria to the East.", "Austria", "Austria"], "answer_start": [664, 640, 664, 664, 664]}, "prediction_text": "Liechtenstein"}
{"answers": {"text": ["Chur", "Chur", "Chur", "Chur"], "answer_start": [143, 143, 143, 143]}, "prediction_text": "Near Chur."}
{"answers": {"text": ["86 km", "86 km", "86 km long", "86 km", "86 km"], "answer_start": [172, 172, 172, 172, 172]}, "prediction_text": "86 km."}
{"answers": {"text": ["599 m", "599 m to 396 m", "599 m to 396 m", "599 m", "599 m"], "answer_start": [214, 214, 214, 214, 214]}, "prediction_text": "599 m"}
{"answers": {"text": ["Rhine Valley", "Rhine Valley", "Rhine Valley", "Rhine Valley", "Rhine Valley"], "answer_start": [289, 289, 289, 289, 289]}, "prediction_text": "Rhine Valley (German: Rheintal)"}
{"answers": {"text": ["Switzerland", "Switzerland", "Liechtenstein", "Switzerland", "Switzerland"], "answer_start": [612, 612, 640, 612, 612]}, "prediction_text": "Liechtenstein"}
{"answers": {"text": ["Lake Constance", "Lake Constance", "Lake Constance", "Lake Constance"], "answer_start": [28, 28, 28, 28]}, "prediction_text": "Lake Constance"}
{"answers": {"text": ["Alter Rhein", "Alter Rhein", "the Alter Rhein", "Alter Rhein"], "answer_start": [108, 108, 104, 108]}, "prediction_text": "Alter Rhein"}
{"answers": {"text": ["modern canalized section", "modern canalized", "modern canalized section", "canalized section"], "answer_start": [155, 155, 155, 162]}, "prediction_text": "modern canalized section"}
{"answers": {"text": ["Isel", "Isel", "\"Isel\"", "Isel"], "answer_start": [474, 474, 473, 474]}, "prediction_text": "Isel."}
{"answers": {"text": ["Donkey", "Donkey", "\"Donkey\")", "Donkey"], "answer_start": [531, 531, 530, 531]}, "prediction_text": "Donkey"}
{"answers": {"text": ["Lake Constance", "Lake Constance", "West by the Alter Rhein", "Lake Constance"], "answer_start": [28, 28, 96, 28]}, "prediction_text": "Lake Constance"}
{"answers": {"text": ["modern canalized section", "modern canalized", "modern canalized section", "canalized section"], "answer_start": [155, 155, 155, 162]}, "prediction_text": "Modern canalized section."}
{"answers": {"text": ["Alter Rhein", "Alter Rhein", "Alter Rhein", "Alter Rhein"], "answer_start": [108, 108, 108, 108]}, "prediction_text": "Alter Rhein"}
{"answers": {"text": ["small islands", "small islands by precipitating sediments", "small islands", "islands"], "answer_start": [372, 372, 372, 378]}, "prediction_text": "islands"}
{"answers": {"text": ["Isel", "Isel", "\"Isel\"", "Isel"], "answer_start": [474, 474, 473, 474]}, "prediction_text": "Gai\u00dfau, H\u00f6chst, Fu\u00dfach."}
{"answers": {"text": ["Diepoldsau", "Diepoldsau", "near Diepoldsau", "Diepoldsau"], "answer_start": [67, 67, 62, 67]}, "prediction_text": "Diepoldsau"}
{"answers": {"text": ["Fu\u00dfach", "Fu\u00dfach", "Fu\u00dfach", "Fu\u00dfach"], "answer_start": [99, 99, 99, 99]}, "prediction_text": "Fu\u00dfach"}
{"answers": {"text": ["strong sedimentation", "strong sedimentation", "strong sedimentation in the western Rhine Delta", "strong sedimentation"], "answer_start": [156, 156, 156, 156]}, "prediction_text": "Sedimentation."}
{"answers": {"text": ["parallel to the canalized Rhine", "the canalized Rhine", "parallel to the canalized Rhine into the lake.", "into the lake"], "answer_start": [266, 278, 266, 298]}, "prediction_text": "Into the lake."}
{"answers": {"text": ["silt", "silt up the lake", "the continuous input of sediment into the lake will silt up the lake", "silt up the lake"], "answer_start": [502, 502, 450, 502]}, "prediction_text": "Silt up the lake."}
{"answers": {"text": ["Fu\u00dfach", "Fu\u00dfach", "Fu\u00dfach", "Fu\u00dfach"], "answer_start": [99, 99, 99, 99]}, "prediction_text": "Fu\u00dfach"}
{"answers": {"text": ["constant flooding", "constant flooding and strong sedimentation", "to counteract the constant flooding and strong sedimentation in the western Rhine Delta", "to counteract the constant flooding and strong sedimentation"], "answer_start": [134, 134, 116, 116]}, "prediction_text": "To counteract the constant flooding and strong sedimentation in the western Rhine Delta."}
{"answers": {"text": ["Diepoldsau", "Diepoldsau", "near Diepoldsau", "Diepoldsau"], "answer_start": [67, 67, 62, 67]}, "prediction_text": "Diepoldsau"}
{"answers": {"text": ["Dornbirner Ach", "Dornbirner Ach", "The Dornbirner Ach", "Dornbirner Ach"], "answer_start": [209, 209, 205, 209]}, "prediction_text": "Dornbirner Ach"}
{"answers": {"text": ["continuous input of sediment", "continuous input of sediment", "It is expected that the continuous input of sediment into the lake will silt up the lake", "continuous input of sediment into the lake"], "answer_start": [454, 454, 430, 454]}, "prediction_text": "Sedimentation."}
{"answers": {"text": ["three", "three", "three bodies of water:", "three"], "answer_start": [27, 27, 27, 27]}, "prediction_text": "3"}
{"answers": {"text": ["lower lake", "lower lake", "\"lower lake\"", "lower lake"], "answer_start": [93, 93, 92, 93]}, "prediction_text": "Lower lake."}
{"answers": {"text": ["Lake Rhine", "Seerhein", "Untersee", "Seerhein"], "answer_start": [168, 157, 82, 157]}, "prediction_text": "Lake Rhine"}
{"answers": {"text": ["Swiss-Austrian border", "Swiss-Austrian", "Swiss-Austrian border"], "answer_start": [478, 478, 478]}, "prediction_text": "Swiss-Austrian border"}
{"answers": {"text": ["upper lake", "upper lake", "(\"upper lake\"", "upper lake"], "answer_start": [64, 64, 62, 64]}, "prediction_text": "Upper lake"}
{"answers": {"text": ["three", "three", "three bodies of water", "three"], "answer_start": [27, 27, 27, 27]}, "prediction_text": "3"}
{"answers": {"text": ["Austria", "Austria", "Austria", "Austria"], "answer_start": [231, 231, 231, 231]}, "prediction_text": "Austria"}
{"answers": {"text": ["Alps", "Alps", "the Alps", "Alps"], "answer_start": [248, 248, 244, 248]}, "prediction_text": "Alps"}
{"answers": {"text": ["47\u00b039\u2032N 9\u00b019\u2032E\ufeff / \ufeff47.650\u00b0N 9.317\u00b0E\ufeff / 47.650; 9.317.", "47\u00b039\u2032N 9\u00b019\u2032E\ufeff / \ufeff47.650\u00b0N 9.317\u00b0E\ufeff / 47.650; 9.317", "47\u00b039\u2032N 9\u00b019\u2032E\ufeff / \ufeff47.650\u00b0N 9.317\u00b0E\ufeff / 47.650; 9.317"], "answer_start": [532, 532, 532]}, "prediction_text": "Swiss-Austrian border."}
{"answers": {"text": ["Baden-W\u00fcrttemberg", "Baden-W\u00fcrttemberg", "Baden-W\u00fcrttemberg", "Baden-W\u00fcrttemberg"], "answer_start": [323, 323, 323, 323]}, "prediction_text": "Baden-W\u00fcrttemberg"}
{"answers": {"text": ["greater density of cold water", "greater density of cold water", "because of the greater density of cold water"], "answer_start": [290, 290, 275]}, "prediction_text": "greater density of cold water."}
{"answers": {"text": ["Lake \u00dcberlingen", "Lake \u00dcberlingen", "Lake \u00dcberlingen"], "answer_start": [564, 564, 564]}, "prediction_text": "Lake \u00dcberlingen"}
{"answers": {"text": ["Rheinbrech", "Upper Lake", "Rheinbrech"], "answer_start": [216, 177, 216]}, "prediction_text": "Upper Lake."}
{"answers": {"text": ["entire length", "the entire length of the lake", "entire length of the lake"], "answer_start": [769, 765, 769]}, "prediction_text": "A small fraction."}
{"answers": {"text": ["Lindau", "Lindau", "Lindau"], "answer_start": [417, 417, 417]}, "prediction_text": "Lindau"}
{"answers": {"text": ["Rheinbrech", "Upper Lake", "Rheinrinne"], "answer_start": [216, 177, 639]}, "prediction_text": "Upper Lake."}
{"answers": {"text": ["Lindau", "Lindau", "Mainau"], "answer_start": [417, 417, 552]}, "prediction_text": "Lindau"}
{"answers": {"text": ["Lake \u00dcberlingen", "Lake \u00dcberlingen", "Lake \u00dcberlingen"], "answer_start": [564, 564, 564]}, "prediction_text": "Lake \u00dcberlingen"}
{"answers": {"text": ["Rhine Gutter", "Rhine Gutter", "Rhine Gutter"], "answer_start": [652, 652, 652]}, "prediction_text": "Rhine Gutter"}
{"answers": {"text": ["water level", "water level", "water level"], "answer_start": [698, 698, 698]}, "prediction_text": "Water level."}
{"answers": {"text": ["westward", "westward", "westward"], "answer_start": [55, 55, 55]}, "prediction_text": "generally westward."}
{"answers": {"text": ["river Aare", "Aare", "river Aare"], "answer_start": [149, 155, 149]}, "prediction_text": "Aare"}
{"answers": {"text": ["1,000 m3/s (35,000 cu ft/s)", "1,000 m3/s"], "answer_start": [241, 241]}, "prediction_text": "1,000 m3/s (35,000 cu ft/s)"}
{"answers": {"text": ["Finsteraarhorn", "summit of Finsteraarhorn", "Finsteraarhorn, t"], "answer_start": [412, 402, 412]}, "prediction_text": "Finsteraarhorn"}
{"answers": {"text": ["Basel", "Basel", "Basel, leaving Switzerland"], "answer_start": [688, 688, 688]}, "prediction_text": "Basel"}
{"answers": {"text": ["westward", "westward", "westward"], "answer_start": [55, 55, 55]}, "prediction_text": "generally westward"}
{"answers": {"text": ["Aare", "Aare", "river Aare"], "answer_start": [155, 155, 149]}, "prediction_text": "Aare"}
{"answers": {"text": ["1,000 m3/s (35,000 cu ft/s),", "1,000 m3/s"], "answer_start": [241, 241]}, "prediction_text": "1,000 m3/s (35,000 cu ft/s)"}
{"answers": {"text": ["Finsteraarhorn", "summit of Finsteraarhorn", "Finsteraarhorn"], "answer_start": [412, 402, 412]}, "prediction_text": "Finsteraarhorn"}
{"answers": {"text": ["German", "German-Swiss border", "German"], "answer_start": [494, 494, 494]}, "prediction_text": "Germany"}
{"answers": {"text": ["Basel", "Basel", "Basel,"], "answer_start": [17, 17, 17]}, "prediction_text": "Basel"}
{"answers": {"text": ["Rhine knee", "Rhine knee", "Rhine knee"], "answer_start": [90, 90, 90]}, "prediction_text": "Rhine knee"}
{"answers": {"text": ["Central Bridge", "Central Bridge", "Central Bridge"], "answer_start": [233, 233, 233]}, "prediction_text": "Central Bridge"}
{"answers": {"text": ["300 km long", "300 km long", "300 km long"], "answer_start": [381, 381, 381]}, "prediction_text": "300 km."}
{"answers": {"text": ["40 km wide", "40 km wide", "40 km"], "answer_start": [403, 403, 403]}, "prediction_text": "40 km."}
{"answers": {"text": ["Basel", "Basel", "Basel"], "answer_start": [17, 17, 17]}, "prediction_text": "Basel"}
{"answers": {"text": ["Rhine knee", "Rhine knee", "Rhine knee"], "answer_start": [90, 90, 90]}, "prediction_text": "Rhine knee"}
{"answers": {"text": ["North", "North", "North"], "answer_start": [187, 187, 187]}, "prediction_text": "North"}
{"answers": {"text": ["High Rhine", "High Rhine", "High Rhine"], "answer_start": [203, 203, 203]}, "prediction_text": "High Rhine."}
{"answers": {"text": ["Central Bridge", "Central Bridge", "Central Bridge"], "answer_start": [233, 233, 233]}, "prediction_text": "Central Bridge"}
{"answers": {"text": ["19th Century", "19th", "19th"], "answer_start": [89, 89, 89]}, "prediction_text": "19th Century"}
{"answers": {"text": ["increased", "increased", "rate of flow was increased"], "answer_start": [124, 124, 107]}, "prediction_text": "Increased."}
{"answers": {"text": ["fell significantly", "fell significantly", "level fell significantly"], "answer_start": [161, 161, 155]}, "prediction_text": "The ground water level fell significantly."}
{"answers": {"text": ["Grand Canal d'Alsace", "Grand Canal d'Alsace", "Grand Canal d'Alsace"], "answer_start": [293, 293, 293]}, "prediction_text": "Grand Canal d'Alsace"}
{"answers": {"text": ["large compensation pools", "large compensation pools", "compensation pools"], "answer_start": [426, 426, 432]}, "prediction_text": "The Bassin de compensation de Plobsheim in Alsace."}
{"answers": {"text": ["Upper Rhine", "Upper Rhine", "Upper Rhine region"], "answer_start": [4, 4, 4]}, "prediction_text": "Upper Rhine"}
{"answers": {"text": ["19th Century", "19th", "19th Century"], "answer_start": [89, 89, 89]}, "prediction_text": "19th Century"}
{"answers": {"text": ["increased", "increased", "was increased"], "answer_start": [124, 124, 120]}, "prediction_text": "The rate of flow increased."}
{"answers": {"text": ["fell significantly", "fell significantly", "level fell"], "answer_start": [161, 161, 155]}, "prediction_text": "The ground water level fell significantly."}
{"answers": {"text": ["Grand Canal d'Alsace", "Grand Canal d'Alsace", "Grand Canal d'Alsace"], "answer_start": [293, 293, 293]}, "prediction_text": "Grand Canal d'Alsace"}
{"answers": {"text": ["Germany", "Germany", "Germany"], "answer_start": [34, 34, 34]}, "prediction_text": "Germany"}
{"answers": {"text": ["300 m3/s (11,000 cu ft/s)", "300 m3/s", "300 m3/s"], "answer_start": [221, 221, 221]}, "prediction_text": "11,000 cu ft/s"}
{"answers": {"text": ["Rhine", "Rhine", "The Rhine"], "answer_start": [4, 4, 0]}, "prediction_text": "Rhine"}
{"answers": {"text": ["Moselle", "Moselle", "the Moselle"], "answer_start": [449, 296, 156]}, "prediction_text": "Neckar"}
{"answers": {"text": ["400 m (1,300 ft).", "400 m", "400 m"], "answer_start": [587, 587, 587]}, "prediction_text": "400 m."}
{"answers": {"text": ["Germany", "Germany", "Germany"], "answer_start": [34, 34, 34]}, "prediction_text": "Germany"}
{"answers": {"text": ["Germany", "Germany", "Germany"], "answer_start": [34, 34, 34]}, "prediction_text": "France"}
{"answers": {"text": ["Moselle", "Neckar", "Neckar"], "answer_start": [160, 127, 127]}, "prediction_text": "Neckar"}
{"answers": {"text": ["France", "France", "France"], "answer_start": [261, 261, 261]}, "prediction_text": "Luxembourg"}
{"answers": {"text": ["2,290 m3/s (81,000 cu ft/s)", "2,290 m3/s", "2,290 m3/s"], "answer_start": [535, 535, 535]}, "prediction_text": "2,290 m3/s (81,000 cu ft/s)"}
{"answers": {"text": ["Middle Rhine", "Middle Rhine", "Middle Rhine"], "answer_start": [29, 29, 29]}, "prediction_text": "Middle Rhine"}
{"answers": {"text": ["Rhine Gorge", "Rhine Gorge", "Rhine Gorge"], "answer_start": [60, 60, 60]}, "prediction_text": "Rhine Gorge"}
{"answers": {"text": ["erosion", "erosion", "by erosion"], "answer_start": [106, 106, 103]}, "prediction_text": "Erosion."}
{"answers": {"text": ["the Romantic Rhine", "the Romantic Rhine", "the Romantic Rhine"], "answer_start": [425, 425, 425]}, "prediction_text": "Romantic Rhine"}
{"answers": {"text": ["Middle Rhine", "Middle Rhine", "Middle Rhine"], "answer_start": [29, 29, 29]}, "prediction_text": "Middle Rhine"}
{"answers": {"text": ["Rhine Gorge", "Rhine Gorge", "Rhine Gorge"], "answer_start": [60, 60, 60]}, "prediction_text": "Rhine Gorge"}
{"answers": {"text": ["castles", "castles and vineyards", "castles and vineyards"], "answer_start": [346, 346, 346]}, "prediction_text": "Castles and fortresses."}
{"answers": {"text": ["Romantic Rhine", "the Romantic Rhine", "the Romantic Rhine"], "answer_start": [429, 425, 425]}, "prediction_text": "The Romantic Rhine."}
{"answers": {"text": ["plants and factories", "industry", "industry"], "answer_start": [85, 23, 23]}, "prediction_text": "Heavy industry."}
{"answers": {"text": ["Duisburg", "Duisburg", "Duisburg"], "answer_start": [298, 298, 298]}, "prediction_text": "Rotterdam"}
{"answers": {"text": ["Ruhr", "Ruhr", "The Ruhr"], "answer_start": [432, 432, 428]}, "prediction_text": "Ruhr"}
{"answers": {"text": ["drinking water", "drinking water", "drinking water"], "answer_start": [728, 728, 728]}, "prediction_text": "Drinking water."}
{"answers": {"text": ["Switzerland", "Lower Rhine", "Lower Rhine"], "answer_start": [143, 172, 172]}, "prediction_text": "Switzerland"}
{"answers": {"text": ["pollution", "pollution", "water pollution"], "answer_start": [60, 60, 54]}, "prediction_text": "Stricter environmental controls, a transition from heavy industry to light industry, cleanup measures."}
{"answers": {"text": ["Lower Rhine", "Lower Rhine", "Lower Rhine"], "answer_start": [172, 172, 172]}, "prediction_text": "Lower Rhine"}
{"answers": {"text": ["Switzerland", "Duisburg", "Switzerland"], "answer_start": [143, 288, 143]}, "prediction_text": "Germany"}
{"answers": {"text": ["Duisburg", "Duisburg", "Duisburg"], "answer_start": [298, 298, 298]}, "prediction_text": "Rotterdam"}
{"answers": {"text": ["Ruhr", "Ruhr", "The Ruhr"], "answer_start": [432, 432, 428]}, "prediction_text": "Ruhr"}
{"answers": {"text": ["tourism", "tourism", "tourism"], "answer_start": [75, 75, 75]}, "prediction_text": "Tourism"}
{"answers": {"text": ["R\u00fcdesheim am Rhein", "R\u00fcdesheim am Rhein", "R\u00fcdesheim am Rhein"], "answer_start": [108, 108, 108]}, "prediction_text": "R\u00fcdesheim am Rhein"}
{"answers": {"text": ["Lorelei", "Lorelei", "Lorelei"], "answer_start": [245, 245, 245]}, "prediction_text": "Lorelei"}
{"answers": {"text": ["Middle Rhine Valley", "Middle Rhine Valley", "Middle Rhine Valley"], "answer_start": [442, 442, 442]}, "prediction_text": "The Middle Rhine Valley."}
{"answers": {"text": ["tourism", "tourism", "tourism"], "answer_start": [75, 75, 75]}, "prediction_text": "Tourism"}
{"answers": {"text": ["UNESCO World Heritage Site.", "UNESCO World Heritage Site", "World Heritage Site"], "answer_start": [154, 154, 161]}, "prediction_text": "UNESCO World Heritage Site"}
{"answers": {"text": ["R\u00fcdesheim am Rhein", "R\u00fcdesheim am Rhein", "R\u00fcdesheim am Rhein"], "answer_start": [108, 108, 108]}, "prediction_text": "R\u00fcdesheim am Rhein"}
{"answers": {"text": ["Lorelei", "Lorelei", "Lorelei"], "answer_start": [245, 245, 245]}, "prediction_text": "Lorelei"}
{"answers": {"text": ["Sankt Goarshausen", "Near Sankt Goarshausen", "Lorelei"], "answer_start": [187, 182, 245]}, "prediction_text": "Lorelei"}
{"answers": {"text": ["Duisburg", "Duisburg", "Duisburg"], "answer_start": [321, 321, 321]}, "prediction_text": "Duisburg"}
{"answers": {"text": ["Wesel-Datteln Canal", "Wesel-Datteln Canal", "Wesel-Datteln Canal"], "answer_start": [547, 547, 547]}, "prediction_text": "Wesel-Datteln Canal"}
{"answers": {"text": ["Lippe", "Lippe", "Lippe"], "answer_start": [595, 595, 595]}, "prediction_text": "The Lippe."}
{"answers": {"text": ["Emmerich Rhine Bridge", "Emmerich Rhine Bridge", "Emmerich Rhine Bridge,"], "answer_start": [634, 634, 634]}, "prediction_text": "Duisburg"}
{"answers": {"text": ["400 m", "400 m", "400 m"], "answer_start": [711, 711, 711]}, "prediction_text": "400 m."}
{"answers": {"text": ["Lower Rhine", "Lower", "Lower Rhine"], "answer_start": [4, 4, 4]}, "prediction_text": "Lower Rhine"}
{"answers": {"text": ["Rhine-Ruhr", "Rhine-Ruhr", "Rhine-Ruhr region"], "answer_start": [251, 251, 251]}, "prediction_text": "Rhine-Ruhr"}
{"answers": {"text": ["Duisport", "Duisport", "Duisburg"], "answer_start": [369, 369, 321]}, "prediction_text": "Duisport"}
{"answers": {"text": ["Emmerich Rhine Bridge", "Emmerich Rhine Bridge", "Emmerich Rhine Bridge"], "answer_start": [634, 634, 634]}, "prediction_text": "Emmerich Rhine Bridge"}
{"answers": {"text": ["400 m wide", "400 m", "400 m"], "answer_start": [711, 711, 711]}, "prediction_text": "400 m."}
{"answers": {"text": ["Meuse", "Meuse", "Meuse"], "answer_start": [287, 287, 287]}, "prediction_text": "De Biesbosch"}
{"answers": {"text": ["Rijn", "Rijn", "Rijn"], "answer_start": [69, 69, 69]}, "prediction_text": "Rijn"}
{"answers": {"text": ["Two thirds", "Two thirds", "Two thirds"], "answer_start": [123, 123, 123]}, "prediction_text": "Two thirds."}
{"answers": {"text": ["west", "west", "west"], "answer_start": [186, 186, 186]}, "prediction_text": "West."}
{"answers": {"text": ["Waal", "Waal", "through the Waal"], "answer_start": [204, 204, 192]}, "prediction_text": "Waal and Merwede."}
{"answers": {"text": ["Meuse", "Meuse", "Meuse"], "answer_start": [287, 287, 287]}, "prediction_text": "The Merwede and Nieuwe Merwede."}
{"answers": {"text": ["The Oude Maas", "Oude Maas", "Oude Maas"], "answer_start": [629, 633, 633]}, "prediction_text": "Oude Maas"}
{"answers": {"text": ["Pannerdens Kanaal", "Pannerdens Kanaal", "the Pannerdens Kanaal"], "answer_start": [47, 47, 43]}, "prediction_text": "IJssel."}
{"answers": {"text": ["Nederrijn", "Nederrijn", "Nederrijn"], "answer_start": [101, 101, 101]}, "prediction_text": "IJsselmeer"}
{"answers": {"text": ["Lek", "Lek", "the Lek"], "answer_start": [406, 406, 402]}, "prediction_text": "Lek"}
{"answers": {"text": ["Noord River", "Nieuwe Maas", "Noord River"], "answer_start": [448, 469, 448]}, "prediction_text": "The Lek joins the Nederrijn at Wijk bij Duurstede."}
{"answers": {"text": ["Pannerdens Kanaal", "Pannerdens Kanaal", "the Pannerdens Kanaal"], "answer_start": [47, 47, 43]}, "prediction_text": "IJssel."}
{"answers": {"text": ["Nederrijn", "Nederrijn", "Nederrijn"], "answer_start": [101, 101, 101]}, "prediction_text": "IJssel."}
{"answers": {"text": ["one ninth", "one ninth", "one ninth"], "answer_start": [138, 138, 138]}, "prediction_text": "One ninth."}
{"answers": {"text": ["Lek", "Lek", "the Lek"], "answer_start": [406, 406, 402]}, "prediction_text": "Lek"}
{"answers": {"text": ["Wijk bij Duurstede", "at Wijk bij Duurstede", "Wijk bij Duurstede"], "answer_start": [339, 336, 339]}, "prediction_text": "Wijk bij Duurstede"}
{"answers": {"text": ["Rijn", "Rijn", "Rijn"], "answer_start": [9, 9, 9]}, "prediction_text": "Rijn"}
{"answers": {"text": ["draining the surrounding land", "draining the surrounding land and polders", "for draining the surrounding land and polders"], "answer_start": [243, 243, 239]}, "prediction_text": "Drain the surrounding land and polders."}
{"answers": {"text": ["Kromme Rijn", "Kromme Rijn", "Kromme Rijn"], "answer_start": [355, 355, 355]}, "prediction_text": "Kromme Rijn"}
{"answers": {"text": ["Bent Rhine", "Bent Rhine", "Bent Rhine"], "answer_start": [369, 369, 369]}, "prediction_text": "Kromme Rijn"}
{"answers": {"text": ["Old Rhine", "Oude Rijn", "Oude Rijn"], "answer_start": [456, 444, 444]}, "prediction_text": "Old Rhine"}
{"answers": {"text": ["Rhine-Meuse", "Rhine-Meuse Delta", "Rhine-Meuse"], "answer_start": [4, 4, 4]}, "prediction_text": "Rhine-Meuse Delta"}
{"answers": {"text": ["Millingen aan de Rijn,", "near Millingen aan de Rijn", "near Millingen aan de Rijn"], "answer_start": [88, 83, 83]}, "prediction_text": "Millingen aan de Rijn."}
{"answers": {"text": ["Rhine Delta", "Rhine Delta", "Rhine Delta"], "answer_start": [264, 264, 264]}, "prediction_text": "Rhine-Meuse Delta"}
{"answers": {"text": ["Nederrijn at Angeren", "Nederrijn at Angeren", "Nederrijn at Angeren"], "answer_start": [173, 173, 173]}, "prediction_text": "Nederrijn"}
{"answers": {"text": ["three", "three", "three"], "answer_start": [276, 276, 276]}, "prediction_text": "3"}
{"answers": {"text": ["Waal", "Waal", "Waal"], "answer_start": [381, 381, 381]}, "prediction_text": "Waal"}
{"answers": {"text": ["Old Meuse", "Old Meuse", "Old Meuse"], "answer_start": [973, 973, 973]}, "prediction_text": "Old Meuse"}
{"answers": {"text": ["the Rip", "the Rip", "the Rip"], "answer_start": [540, 540, 540]}, "prediction_text": "The Rip."}
{"answers": {"text": ["St. Elizabeth's", "St. Elizabeth's", "St. Elizabeth's flood"], "answer_start": [11, 11, 11]}, "prediction_text": "St. Elizabeth's flood (1421)"}
{"answers": {"text": ["1421", "1421", "1421"], "answer_start": [34, 34, 34]}, "prediction_text": "1421"}
{"answers": {"text": ["Merwede-Oude Maas", "Merwede-Oude Maas", "North Sea"], "answer_start": [85, 85, 110]}, "prediction_text": "South of today's line Merwede-Oude Maas to the North Sea."}
{"answers": {"text": ["1421 to 1904", "1421 to 1904", "From 1421 to 1904"], "answer_start": [321, 321, 316]}, "prediction_text": "1421 to 1904."}
{"answers": {"text": ["archipelago-like estuary", "archipelago-like estuary", "archipelago-like estuary"], "answer_start": [134, 134, 134]}, "prediction_text": "Archipelago-like estuary."}
{"answers": {"text": ["drainage channels", "drainage channels", "drainage channels"], "answer_start": [226, 226, 226]}, "prediction_text": "Drainage channels."}
{"answers": {"text": ["construction of Delta Works", "construction of Delta Works", "construction of Delta Works"], "answer_start": [274, 274, 274]}, "prediction_text": "Construction of Delta Works."}
{"answers": {"text": ["dammed", "dammed", "dammed"], "answer_start": [200, 200, 200]}, "prediction_text": "Dammed"}
{"answers": {"text": ["20th Century", "20th Century", "second half of the 20th Century"], "answer_start": [346, 346, 327]}, "prediction_text": "Second half of the 20th Century."}
{"answers": {"text": ["tidal delta", "tidal", "tidal delta"], "answer_start": [27, 27, 27]}, "prediction_text": "Tidal delta"}
{"answers": {"text": ["tidal currents", "tidal currents", "tidal currents"], "answer_start": [104, 104, 104]}, "prediction_text": "tidal currents"}
{"answers": {"text": ["tear huge areas of land into the sea.", "tidal currents", "tear huge areas of land into the sea"], "answer_start": [204, 104, 204]}, "prediction_text": "Tidal currents."}
{"answers": {"text": ["Zaltbommel", "Zaltbommel", "Zaltbommel"], "answer_start": [516, 516, 516]}, "prediction_text": "Zaltbommel"}
{"answers": {"text": ["Tethys sea", "Tethys", "Tethys sea"], "answer_start": [301, 301, 301]}, "prediction_text": "Tethys Ocean"}
{"answers": {"text": ["Jurassic Period", "Jurassic Period", "Jurassic Period"], "answer_start": [338, 338, 338]}, "prediction_text": "Jurassic Period"}
{"answers": {"text": ["Mediterranean geography", "Mediterranean geography", "Mediterranean geography"], "answer_start": [697, 697, 697]}, "prediction_text": "Mediterranean geography"}
{"answers": {"text": ["Mesozoic Era", "Triassic Period", "Triassic Period"], "answer_start": [68, 45, 45]}, "prediction_text": "Mesozoic Era"}
{"answers": {"text": ["Iberia", "Iberia", "Iberia"], "answer_start": [722, 722, 722]}, "prediction_text": "Italy"}
{"answers": {"text": ["N\u2013S", "N\u2013S", "N\u2013S rift system"], "answer_start": [61, 61, 61]}, "prediction_text": "N\u2013S rift system."}
{"answers": {"text": ["Upper Rhine Graben", "Upper Rhine Graben", "Upper Rhine Graben"], "answer_start": [141, 141, 141]}, "prediction_text": "Upper Rhine Graben, Lower Rhine Embayment."}
{"answers": {"text": ["Miocene", "By the time of the Miocene", "time of the Miocene"], "answer_start": [306, 287, 294]}, "prediction_text": "Miocene"}
{"answers": {"text": ["Danube", "Danube", "Danube"], "answer_start": [538, 538, 538]}, "prediction_text": "Danube"}
{"answers": {"text": ["stream capture", "stream capture", "stream capture"], "answer_start": [8, 8, 8]}, "prediction_text": "Through stream capture."}
{"answers": {"text": ["Pliocene period", "Pliocene", "Pliocene period"], "answer_start": [75, 75, 75]}, "prediction_text": "Pliocene period."}
{"answers": {"text": ["Vosges Mountains", "Vosges Mountains", "Vosges Mountains,"], "answer_start": [135, 135, 135]}, "prediction_text": "The Rhine captured streams from the Rh\u00f4ne, the Meuse, the Main, and the Neckar."}
{"answers": {"text": ["Ice Ages", "geological period", "geological period of the Ice Ages"], "answer_start": [88, 63, 63]}, "prediction_text": "Ice Ages"}
{"answers": {"text": ["six", "six", "six"], "answer_start": [137, 137, 137]}, "prediction_text": "6"}
{"answers": {"text": ["120 m", "120 m", "120 m"], "answer_start": [198, 198, 198]}, "prediction_text": "120 m (390 ft)"}
{"answers": {"text": ["northwest", "northwest", "northwest"], "answer_start": [326, 326, 326]}, "prediction_text": "Northwest."}
{"answers": {"text": ["Brest", "Brest, France", "offshore of Brest,"], "answer_start": [746, 746, 734]}, "prediction_text": "Brest, France"}
{"answers": {"text": ["74,000 (BP", "~74,000 (BP = Before Present)", "~74,000 (BP = Before Present)"], "answer_start": [27, 26, 26]}, "prediction_text": "74,000 (BP = Before Present)"}
{"answers": {"text": ["11,600 BP", "(~11,600 BP", "end of the Pleistocene (~11,600 BP)"], "answer_start": [92, 90, 67]}, "prediction_text": "~11,600 BP."}
{"answers": {"text": ["west", "west", "west"], "answer_start": [346, 346, 346]}, "prediction_text": "West."}
{"answers": {"text": ["120 m", "120 m", "120 m"], "answer_start": [593, 593, 593]}, "prediction_text": "120 m (390 ft) lower."}
{"answers": {"text": ["English Channel", "English Channel", "English Channel"], "answer_start": [474, 474, 474]}, "prediction_text": "Sea level."}
{"answers": {"text": ["glacier", "glacier", "a glacier"], "answer_start": [126, 126, 124]}, "prediction_text": "Glacier"}
{"answers": {"text": ["tundra", "tundra", "A tundra"], "answer_start": [137, 137, 135]}, "prediction_text": "A tundra."}
{"answers": {"text": ["22,000\u201314,000 yr BP", "22,000\u201314,000 yr BP", "ca. 22,000\u201314,000 yr BP"], "answer_start": [295, 295, 291]}, "prediction_text": "22,000\u201314,000 yr BP."}
{"answers": {"text": ["ice-sheets", "ice-sheets", "ice-sheets"], "answer_start": [321, 321, 321]}, "prediction_text": "Ice sheets."}
{"answers": {"text": ["loess", "loess", "loess"], "answer_start": [436, 436, 436]}, "prediction_text": "Loess"}
{"answers": {"text": ["22,000 years ago", "22,000 years ago", "22,000 years ago"], "answer_start": [49, 49, 49]}, "prediction_text": "22,000 years ago."}
{"answers": {"text": ["thaw", "thaw and fall-winter snow covers", "thaw"], "answer_start": [127, 127, 127]}, "prediction_text": "Thaw and fall-winter snow covers."}
{"answers": {"text": ["Rhine", "Rhine", "the Rhine"], "answer_start": [218, 218, 214]}, "prediction_text": "Rhine and its downstream extension."}
{"answers": {"text": ["13,000 BP", "13,000 BP", "13,000 BP"], "answer_start": [323, 323, 323]}, "prediction_text": "13,000 BP."}
{"answers": {"text": ["9000 BP", "9000 BP", "9000 BP"], "answer_start": [337, 337, 337]}, "prediction_text": "9000 BP."}
{"answers": {"text": ["7500 yr ago", "7500 yr ago", "7500 yr ago"], "answer_start": [6, 6, 6]}, "prediction_text": "7500 yr ago."}
{"answers": {"text": ["Rates of sea-level rise", "sea-level rise had dropped", "Rates of sea-level rise had dropped"], "answer_start": [93, 102, 93]}, "prediction_text": "The coast line."}
{"answers": {"text": ["last 7000 years", "7000 years", "last 7000 years"], "answer_start": [263, 268, 263]}, "prediction_text": "7000 years."}
{"answers": {"text": ["tectonic subsidence", "tectonic subsidence", "ongoing tectonic subsidence"], "answer_start": [371, 371, 363]}, "prediction_text": "Tectonic subsidence."}
{"answers": {"text": ["1\u20133 cm (0.39\u20131.18 in) per century", "1\u20133 cm (0.39\u20131.18 in) per century", "1\u20133 cm (0.39\u20131.18 in) per century"], "answer_start": [444, 444, 444]}, "prediction_text": "1-3 cm per century."}
{"answers": {"text": ["11,700 years ago", "~11,700 years ago", "~11,700 years ago"], "answer_start": [31, 30, 30]}, "prediction_text": "~11,700 years ago"}
{"answers": {"text": ["8,000 years ago", "~8,000 years ago", "~8,000 years ago"], "answer_start": [260, 259, 259]}, "prediction_text": "~8,000 years ago."}
{"answers": {"text": ["Late-Glacial valley", "Late-Glacial valley", "Late-Glacial valley"], "answer_start": [73, 73, 73]}, "prediction_text": "Late-Glacial valley."}
{"answers": {"text": ["Netherlands", "Netherlands", "Netherlands"], "answer_start": [191, 191, 191]}, "prediction_text": "Netherlands"}
{"answers": {"text": ["3000 yr BP", "~3000 yr BP", "~3000 yr BP"], "answer_start": [7, 6, 6]}, "prediction_text": "~3000 yr BP (= years Before Present)"}
{"answers": {"text": ["increased flooding and sedimentation", "Bronze Age agriculture", "increased flooding and sedimentation"], "answer_start": [281, 121, 281]}, "prediction_text": "Flooding and sedimentation."}
{"answers": {"text": ["sediment load", "sediment load", "sediment load"], "answer_start": [189, 189, 189]}, "prediction_text": "Sediment load."}
{"answers": {"text": ["11\u201313th century", "11\u201313th century AD", "11\u201313th century AD"], "answer_start": [805, 805, 805]}, "prediction_text": "11-13th century AD."}
{"answers": {"text": ["80", "80", "80"], "answer_start": [552, 552, 552]}, "prediction_text": "80."}
{"answers": {"text": ["North Sea", "North Sea", "North Sea", "North Sea"], "answer_start": [65, 65, 65, 65]}, "prediction_text": "North Sea."}
{"answers": {"text": ["Meuse estuary", "Meuse estuary", "Meuse estuary", "Meuse estuary"], "answer_start": [95, 95, 95, 95]}, "prediction_text": "The Waal and the Nederrijn-Lek discharge through the former Meuse estuary, near Rotterdam."}
{"answers": {"text": ["IJsselmeer", "IJsselmeer", "IJsselmeer", "IJsselmeer"], "answer_start": [184, 184, 184, 184]}, "prediction_text": "North Sea."}
{"answers": {"text": ["freshwater lake", "freshwater lake", "freshwater lake", "freshwater lake"], "answer_start": [260, 260, 260, 260]}, "prediction_text": "Lake."}
{"answers": {"text": ["three", "three", "three", "three"], "answer_start": [321, 321, 321, 321]}, "prediction_text": "3"}
{"answers": {"text": ["1st century BC", "1st century BC", "1st century BC"], "answer_start": [83, 83, 83]}, "prediction_text": "1st century BC"}
{"answers": {"text": ["Germania", "Germania", "Germania"], "answer_start": [176, 176, 176]}, "prediction_text": "Germania"}
{"answers": {"text": ["6th century BC", "6th century BC", "6th century BC"], "answer_start": [269, 269, 269]}, "prediction_text": "6th century BC."}
{"answers": {"text": ["Maurus Servius Honoratus", "Maurus Servius Honoratus"], "answer_start": [584, 584]}, "prediction_text": "Herodotus"}
{"answers": {"text": ["AD 14", "AD 14", "AD 14"], "answer_start": [30, 30, 30]}, "prediction_text": "AD 14"}
{"answers": {"text": ["Danube", "upper Danube", "upper Danube"], "answer_start": [136, 130, 130]}, "prediction_text": "Upper Danube"}
{"answers": {"text": ["the empire fell", "until the empire fell", "until the empire fell"], "answer_start": [455, 449, 449]}, "prediction_text": "AD 70."}
{"answers": {"text": ["eastwards", "eastwards", "eastwards"], "answer_start": [867, 867, 867]}, "prediction_text": "Eastwards."}
{"answers": {"text": ["southern", "between the modern Baden and W\u00fcrttemberg"], "answer_start": [476, 664]}, "prediction_text": "Frankfurt."}
{"answers": {"text": ["eight", "eight", "eight"], "answer_start": [16, 16, 16]}, "prediction_text": "8."}
{"answers": {"text": ["army of Germania Inferior", "army of Germania Inferior", "army of Germania Inferior"], "answer_start": [254, 254, 254]}, "prediction_text": "Germania Inferior."}
{"answers": {"text": ["Ubiorum", "oppidum Ubiorum", "oppidum Ubiorum"], "answer_start": [383, 375, 375]}, "prediction_text": "Colonia Agrippina"}
{"answers": {"text": ["threat of war", "whether a state or threat of war existed", "whether a state or threat of war existed"], "answer_start": [152, 133, 133]}, "prediction_text": "Whether a state or threat of war existed."}
{"answers": {"text": ["town of the Ubii", "town of the Ubii", "town of the Ubii"], "answer_start": [393, 393, 393]}, "prediction_text": "Colonia Agrippina."}
{"answers": {"text": ["5th century", "Migration period", "5th century"], "answer_start": [66, 41, 66]}, "prediction_text": "5th century."}
{"answers": {"text": ["kingdoms", "kingdoms", "kingdoms"], "answer_start": [95, 95, 95]}, "prediction_text": "kingdoms of Francia on the Lower Rhine, Burgundy on the Upper Rhine, Alemannia on the High Rhine."}
{"answers": {"text": ["dragons rock", "dragons rock", "dragons rock"], "answer_start": [371, 371, 371]}, "prediction_text": "Drachenfels (Siebengebirge)"}
{"answers": {"text": ["Siegfried", "Siegfried", "Siegfried"], "answer_start": [307, 307, 307]}, "prediction_text": "Siegfried"}
{"answers": {"text": ["Hagen", "Hagen", "Hagen"], "answer_start": [540, 540, 540]}, "prediction_text": "Hagen"}
{"answers": {"text": ["6th century", "6th century"], "answer_start": [7, 7]}, "prediction_text": "6th century."}
{"answers": {"text": ["10th century", "10th century", "10th century"], "answer_start": [153, 153, 153]}, "prediction_text": "10th century."}
{"answers": {"text": ["Lower Lorraine", "Lower Lorraine", "Lower Lorraine"], "answer_start": [248, 248, 248]}, "prediction_text": "Lower Lorraine."}
{"answers": {"text": ["Archduke Sigismund", "Archduke Sigismund of Austria", "Archduke Sigismund of Austria"], "answer_start": [659, 659, 659]}, "prediction_text": "Archduke Sigismund of Austria"}
{"answers": {"text": ["1469", "1469", "1469"], "answer_start": [692, 692, 692]}, "prediction_text": "1469"}
{"answers": {"text": ["Peace of Westphalia", "Peace of Westphalia", "Peace of Westphalia"], "answer_start": [10, 10, 10]}, "prediction_text": "Peace of Westphalia"}
{"answers": {"text": ["Establishing \"natural borders\"", "Establishing \"natural borders\"", "Establishing \"natural borders\" on the Rhine"], "answer_start": [103, 103, 103]}, "prediction_text": "Establishing \"natural borders\" on the Rhine."}
{"answers": {"text": ["Napoleon", "Napoleon", "Napoleon"], "answer_start": [461, 461, 461]}, "prediction_text": "Napoleon Bonaparte"}
{"answers": {"text": ["1806", "1806", "1806"], "answer_start": [500, 500, 500]}, "prediction_text": "1806"}
{"answers": {"text": ["1840", "1840", "1840"], "answer_start": [647, 647, 647]}, "prediction_text": "1840"}
{"answers": {"text": ["end of World War I", "end of World War I", "end of World War I"], "answer_start": [7, 7, 7]}, "prediction_text": "1935"}
{"answers": {"text": ["1935", "1930", "1935"], "answer_start": [142, 434, 142]}, "prediction_text": "1935"}
{"answers": {"text": ["German army", "German army", "German army"], "answer_start": [206, 206, 206]}, "prediction_text": "German army."}
{"answers": {"text": ["Adolf Hitler's rise to power", "helping Adolf Hitler's rise to power", "Adolf Hitler's rise to power"], "answer_start": [370, 362, 370]}, "prediction_text": "Hitler's rise to power."}
{"answers": {"text": ["1936", "1936", "1936"], "answer_start": [477, 477, 477]}, "prediction_text": "1936"}
{"answers": {"text": ["Arnhem", "Arnhem", "Arnhem"], "answer_start": [165, 165, 165]}, "prediction_text": "Arnhem"}
{"answers": {"text": ["formidable natural obstacle", "formidable natural obstacle", "natural obstacle"], "answer_start": [66, 66, 77]}, "prediction_text": "Germany"}
{"answers": {"text": ["September 1944", "September 1944", "September 1944"], "answer_start": [321, 321, 321]}, "prediction_text": "September 1944"}
{"answers": {"text": ["Ludendorff Bridge", "Ludendorff Bridge", "Ludendorff Bridge"], "answer_start": [483, 483, 483]}, "prediction_text": "Ludendorff Bridge."}
{"answers": {"text": ["Seven Days to the River Rhine", "plan for an invasion of Western Europe", "plan for an invasion of Western Europe during the Cold War"], "answer_start": [731, 783, 783]}, "prediction_text": "Seven Days to the River Rhine."}
{"answers": {"text": ["1,230 kilometres (764 miles)", "1,230 kilometres", "1,230 kilometres"], "answer_start": [58, 58, 58]}, "prediction_text": "1,230 kilometers (764 miles)"}
{"answers": {"text": ["Knaurs Lexikon", "Knaurs Lexikon", "Knaurs Lexikon"], "answer_start": [120, 120, 120]}, "prediction_text": "Knaurs Lexikon"}
{"answers": {"text": ["typographical error", "typographical error", "a typographical error"], "answer_start": [199, 199, 197]}, "prediction_text": "The length of the Rhine was changed from 1,230 kilometers (764 miles) to 1,232 kilometers (766 miles) in 1932."}
{"answers": {"text": ["1,320 kilometres (820 miles)", "1,320 kilometres", "1,320 kilometres"], "answer_start": [156, 156, 156]}, "prediction_text": "length"}
{"answers": {"text": ["2010", "2010", "2010"], "answer_start": [422, 422, 422]}, "prediction_text": "1932"}
{"answers": {"text": ["Following a referendum in 1997", "1998", "1998"], "answer_start": [0, 143, 143]}, "prediction_text": "1998"}
{"answers": {"text": ["Scotland Act 1998", "Scotland Act 1998", "Scotland Act"], "answer_start": [130, 130, 130]}, "prediction_text": "Scotland Act 1998"}
{"answers": {"text": ["in which it can make laws", "the areas in which it can make laws", "powers that are \"reserved\" to the Parliament of the United Kingdom"], "answer_start": [278, 268, 331]}, "prediction_text": "The areas in which it can make laws."}
{"answers": {"text": ["Parliament of the United Kingdom", "Parliament of the United Kingdom", "The British Parliament"], "answer_start": [365, 365, 512]}, "prediction_text": "The Parliament of the United Kingdom."}
{"answers": {"text": ["Westminster", "Westminster", "Westminster"], "answer_start": [499, 499, 499]}, "prediction_text": "Westminster"}
{"answers": {"text": ["lack of a Parliament of Scotland", "the lack of a Parliament of Scotland", "the lack of a Parliament of Scotland"], "answer_start": [189, 185, 185]}, "prediction_text": "Lack of a Parliament of Scotland."}
{"answers": {"text": ["three hundred", "three hundred years", "three hundred years"], "answer_start": [13, 13, 13]}, "prediction_text": "Three hundred years."}
{"answers": {"text": ["First World War.", "First World War", "the outbreak of the First World War"], "answer_start": [390, 390, 370]}, "prediction_text": "The outbreak of the First World War."}
{"answers": {"text": ["the late 1960s", "late 1960s", "late 1960s"], "answer_start": [454, 458, 458]}, "prediction_text": "late 1960s"}
{"answers": {"text": ["directly elected Scottish Assembly", "directly elected Scottish Assembly", "a directly elected Scottish Assembly"], "answer_start": [923, 923, 921]}, "prediction_text": "A directly elected Scottish Assembly."}
{"answers": {"text": ["North", "North Sea", "the North Sea"], "answer_start": [46, 46, 42]}, "prediction_text": "North Sea"}
{"answers": {"text": ["\"It's Scotland's oil\"", "It's Scotland's oil", "\"It's Scotland's oil\""], "answer_start": [74, 75, 74]}, "prediction_text": "It's Scotland's oil campaign."}
{"answers": {"text": ["1974", "1974", "1974"], "answer_start": [449, 449, 449]}, "prediction_text": "1974"}
{"answers": {"text": ["not benefitting Scotland as much as they should", "not benefitting Scotland as much as they should", "the revenues from the oil were not benefitting Scotland as much as they should"], "answer_start": [269, 269, 238]}, "prediction_text": "The SNP opined that the revenues from the oil were not benefitting Scotland as much as they should."}
{"answers": {"text": ["1978", "1978", "1978"], "answer_start": [481, 481, 481]}, "prediction_text": "1978"}
{"answers": {"text": ["Edinburgh", "Edinburgh", "Edinburgh"], "answer_start": [81, 81, 81]}, "prediction_text": "Edinburgh"}
{"answers": {"text": ["majority", "40%", "at least 40%"], "answer_start": [109, 227, 218]}, "prediction_text": "40%"}
{"answers": {"text": ["51.6%", "51.6%", "51.6%"], "answer_start": [368, 368, 368]}, "prediction_text": "51.6%"}
{"answers": {"text": ["failed", "failed", "failed"], "answer_start": [338, 338, 338]}, "prediction_text": "Failed."}
{"answers": {"text": ["32.9%", "32.9%", "32.9%"], "answer_start": [517, 517, 517]}, "prediction_text": "32.9%"}
{"answers": {"text": ["a Scottish Parliament", "Scottish Parliament", "Scottish Parliament grew"], "answer_start": [43, 45, 45]}, "prediction_text": "Scottish Parliament"}
{"answers": {"text": ["the Conservative Party", "Conservative Party", "the Conservative Party"], "answer_start": [142, 146, 142]}, "prediction_text": "Conservative Party"}
{"answers": {"text": ["1989", "1989", "1989"], "answer_start": [364, 364, 364]}, "prediction_text": "1989"}
{"answers": {"text": ["blueprint", "blueprint", "the Convention"], "answer_start": [536, 536, 570]}, "prediction_text": "The blueprint for devolution."}
{"answers": {"text": ["Scottish Parliament Building", "Scottish Parliament Building", "Edinburgh"], "answer_start": [82, 82, 136]}, "prediction_text": "Edinburgh"}
{"answers": {"text": ["Enric Miralles", "Enric Miralles", "Enric Miralles"], "answer_start": [214, 214, 214]}, "prediction_text": "Enric Miralles"}
{"answers": {"text": ["Spanish", "Spanish", "Spanish"], "answer_start": [196, 196, 196]}, "prediction_text": "Spanish"}
{"answers": {"text": ["leaf-shaped", "leaf-shaped", "leaf-shaped buildings"], "answer_start": [389, 389, 389]}, "prediction_text": "Leaf-shaped buildings."}
{"answers": {"text": ["Queen Elizabeth II", "Queen Elizabeth II", "Queen Elizabeth II"], "answer_start": [743, 743, 743]}, "prediction_text": "Queen Elizabeth II"}
{"answers": {"text": ["meeting of the Church's General Assembly", "meeting of the Church's General Assembly", "the meeting of the Church's General Assembly"], "answer_start": [409, 409, 405]}, "prediction_text": "The Parliament's temporary home."}
{"answers": {"text": ["General Assembly Hall of the Church of Scotland", "General Assembly Hall of the Church of Scotland", "the General Assembly Hall of the Church of Scotland"], "answer_start": [105, 105, 101]}, "prediction_text": "Holyrood"}
{"answers": {"text": ["courtyard", "courtyard adjoining the Assembly Hall", "the courtyard"], "answer_start": [249, 249, 245]}, "prediction_text": "The courtyard adjoining the Assembly Hall."}
{"answers": {"text": ["University of Aberdeen", "University of Aberdeen", "the University of Aberdeen"], "answer_start": [588, 588, 584]}, "prediction_text": "University of Aberdeen"}
{"answers": {"text": ["former Strathclyde Regional Council debating chamber in Glasgow", "former Strathclyde Regional Council debating chamber", "the former Strathclyde Regional Council debating chamber"], "answer_start": [512, 512, 508]}, "prediction_text": "Glasgow"}
{"answers": {"text": ["City of Edinburgh Council", "City of Edinburgh Council", "the City of Edinburgh Council"], "answer_start": [93, 93, 89]}, "prediction_text": "City of Edinburgh Council"}
{"answers": {"text": ["Lothian Regional Council", "Lothian Regional Council", "Lothian Regional Council on George IV Bridge"], "answer_start": [158, 158, 158]}, "prediction_text": "Lothian Regional Council"}
{"answers": {"text": ["demolished", "demolished", "demolished"], "answer_start": [292, 292, 292]}, "prediction_text": "Demolished."}
{"answers": {"text": ["Parliament Square, High Street and George IV Bridge in Edinburgh", "Parliament Square, High Street and George IV Bridge", "Parliament Square"], "answer_start": [350, 350, 350]}, "prediction_text": "Parliament Square, High Street, and George IV Bridge."}
{"answers": {"text": ["main", "main", "the main hall"], "answer_start": [558, 558, 554]}, "prediction_text": "Main hall"}
{"answers": {"text": ["one MSP", "MSP", "MSP"], "answer_start": [114, 118, 118]}, "prediction_text": "Tricia Marwick"}
{"answers": {"text": ["Tricia Marwick", "Tricia Marwick", "Tricia Marwick"], "answer_start": [194, 194, 194]}, "prediction_text": "Tricia Marwick"}
{"answers": {"text": ["secret", "secret", "a secret ballot"], "answer_start": [338, 338, 336]}, "prediction_text": "Secret ballot."}
{"answers": {"text": ["129", "129", "129"], "answer_start": [359, 359, 359]}, "prediction_text": "129"}
{"answers": {"text": ["A vote clerk", "vote clerk", "vote clerk"], "answer_start": [873, 875, 875]}, "prediction_text": "The parliamentary clerks."}
{"answers": {"text": ["Presiding Officer", "Presiding Officer", "the Presiding Officer"], "answer_start": [62, 62, 58]}, "prediction_text": "The Presiding Officer."}
{"answers": {"text": ["the Parliamentary Bureau", "Presiding Officer", "the Parliamentary Bureau"], "answer_start": [226, 345, 226]}, "prediction_text": "The Presiding Officer."}
{"answers": {"text": ["five", "five or more", "five or more seats"], "answer_start": [509, 509, 509]}, "prediction_text": "Five seats."}
{"answers": {"text": ["The Presiding Officer", "Presiding Officer", "The Presiding Officer"], "answer_start": [778, 782, 778]}, "prediction_text": "The Presiding Officer."}
{"answers": {"text": ["hemicycle", "hemicycle", "a hemicycle"], "answer_start": [74, 74, 72]}, "prediction_text": "Hemicycle"}
{"answers": {"text": ["encourage consensus amongst elected members", "encourage consensus amongst elected members", "reflects the desire to encourage consensus amongst elected members"], "answer_start": [114, 114, 91]}, "prediction_text": "To encourage consensus amongst elected members."}
{"answers": {"text": ["131", "131", "131"], "answer_start": [169, 169, 217]}, "prediction_text": "131 seats."}
{"answers": {"text": ["2", "2", "2"], "answer_start": [282, 282, 282]}, "prediction_text": "2"}
{"answers": {"text": ["vote", "vote", "vote"], "answer_start": [604, 604, 604]}, "prediction_text": "Vote."}
{"answers": {"text": ["Scottish rivers", "Scottish rivers", "Scottish rivers"], "answer_start": [127, 127, 127]}, "prediction_text": "Scottish rivers."}
{"answers": {"text": ["silver", "silver", "silver and inlaid with gold"], "answer_start": [87, 87, 87]}, "prediction_text": "Silver"}
{"answers": {"text": ["the Queen", "the Queen", "the Queen"], "answer_start": [533, 533, 533]}, "prediction_text": "Queen Elizabeth II"}
{"answers": {"text": ["Wisdom, Compassion, Justice and Integrity", "Wisdom, Compassion, Justice and Integrity", "Wisdom, Compassion, Justice and Integrity"], "answer_start": [173, 173, 173]}, "prediction_text": "Wisdom, Compassion, Justice, and Integrity."}
{"answers": {"text": ["a glass case suspended from the lid", "glass case", "a glass case"], "answer_start": [608, 610, 608]}, "prediction_text": "Glass case."}
{"answers": {"text": ["April", "April", "April"], "answer_start": [161, 161, 161]}, "prediction_text": "April"}
{"answers": {"text": ["debating chamber", "debating chamber", "Wednesday afternoons"], "answer_start": [204, 204, 243]}, "prediction_text": "Wednesday afternoons from 2 pm to 6 pm."}
{"answers": {"text": ["the public", "the public.", "public"], "answer_start": [372, 372, 376]}, "prediction_text": "The public."}
{"answers": {"text": ["free", "free", "free"], "answer_start": [393, 393, 393]}, "prediction_text": "Free."}
{"answers": {"text": ["the Official Report", "the Official Report", "the Official Report"], "answer_start": [649, 649, 649]}, "prediction_text": "Official Report"}
{"answers": {"text": ["Wednesdays", "Wednesdays", "Wednesdays"], "answer_start": [30, 30, 30]}, "prediction_text": "Wednesdays"}
{"answers": {"text": ["up to four minutes", "up to four minutes", "up to four minutes"], "answer_start": [114, 114, 114]}, "prediction_text": "Four minutes."}
{"answers": {"text": ["Presiding Officer", "Presiding Officer", "the Presiding Officer"], "answer_start": [506, 619, 502]}, "prediction_text": "The Presiding Officer."}
{"answers": {"text": ["religious beliefs", "religious beliefs", "the balance of religious beliefs"], "answer_start": [383, 383, 368]}, "prediction_text": "Religious beliefs."}
{"answers": {"text": ["nominate speakers", "nominate speakers", "nominate speakers"], "answer_start": [640, 640, 640]}, "prediction_text": "Nominate speakers."}
{"answers": {"text": ["The Presiding Officer", "Presiding Officer", "The Presiding Officer"], "answer_start": [0, 4, 0]}, "prediction_text": "The Presiding Officer."}
{"answers": {"text": ["amount of time for which they are allowed to speak", "amount of time for which they are allowed to speak", "the amount of time for which they are allowed to speak"], "answer_start": [98, 98, 94]}, "prediction_text": "The amount of time for which members are allowed to speak."}
{"answers": {"text": ["different viewpoints", "different viewpoints and political parties", "a balance between different viewpoints and political parties"], "answer_start": [217, 217, 199]}, "prediction_text": "A balance between different viewpoints and political parties."}
{"answers": {"text": ["ministers or party leaders", "ministers or party leaders", "ministers or party leaders"], "answer_start": [304, 304, 304]}, "prediction_text": "Ministers or party leaders."}
{"answers": {"text": ["Gaelic", "Gaelic", "Scots, Gaelic, or any other language with the agreement of the Presiding Officer"], "answer_start": [954, 954, 819]}, "prediction_text": "Gaelic"}
{"answers": {"text": ["5 pm", "5 pm", "5 pm"], "answer_start": [30, 30, 30]}, "prediction_text": "5 pm."}
{"answers": {"text": ["\"Decision Time\"", "Decision Time", "\"Decision Time\""], "answer_start": [118, 119, 118]}, "prediction_text": "Decision Time"}
{"answers": {"text": ["vote", "vote", "vote"], "answer_start": [292, 292, 292]}, "prediction_text": "Vote."}
{"answers": {"text": ["electronic consoles on their desks", "electronic consoles", "electronic consoles on their desks"], "answer_start": [649, 649, 649]}, "prediction_text": "By reading out the name of the motion or amendment, the Presiding Officer, and asking \"Are we all agreed?\", to which the chamber first votes orally."}
{"answers": {"text": ["seconds", "seconds", "seconds"], "answer_start": [870, 870, 870]}, "prediction_text": "seconds"}
{"answers": {"text": ["votes", "outcome of most votes", "outcome"], "answer_start": [20, 4, 4]}, "prediction_text": "The outcome of most votes."}
{"answers": {"text": ["political parties", "political parties", "political parties"], "answer_start": [60, 60, 60]}, "prediction_text": "Parties."}
{"answers": {"text": ["whips", "whips", "whips"], "answer_start": [159, 159, 159]}, "prediction_text": "MSPs"}
{"answers": {"text": ["moral", "moral", "moral issues"], "answer_start": [865, 865, 865]}, "prediction_text": "Moral issues."}
{"answers": {"text": ["deselected as official party candidates during future elections", "deselected as official party candidates", "deselected as official party candidates during future elections"], "answer_start": [401, 401, 401]}, "prediction_text": "The member is deselected as an official party candidate."}
{"answers": {"text": ["Immediately after Decision Time", "Immediately after Decision Time", "Immediately after Decision Time"], "answer_start": [0, 0, 0]}, "prediction_text": "Immediately after Decision Time."}
{"answers": {"text": ["not a Scottish minister", "may be of interest to a particular area such as a member's own constituency", "issues which may be of interest to a particular area such as a member's own constituency"], "answer_start": [155, 213, 200]}, "prediction_text": "To allow for the wind up of the debate."}
{"answers": {"text": ["45 minutes", "45 minutes", "45 minutes"], "answer_start": [76, 76, 76]}, "prediction_text": "45 minutes."}
{"answers": {"text": ["other members", "other members", "other members"], "answer_start": [426, 426, 426]}, "prediction_text": "Other members."}
{"answers": {"text": ["winds up", "winds up", "\"winds up\" the debate"], "answer_start": [548, 548, 547]}, "prediction_text": "Speaks after all other participants."}
{"answers": {"text": ["committee", "committee", "in committee"], "answer_start": [55, 55, 52]}, "prediction_text": "Committee meetings."}
{"answers": {"text": ["stronger", "stronger", "stronger in the Scottish Parliament than in other parliamentary systems"], "answer_start": [92, 92, 92]}, "prediction_text": "Stronger role of backbenchers."}
{"answers": {"text": ["no revising chamber", "no revising chamber", "take evidence from witnesses, conduct inquiries and scrutinise legislation"], "answer_start": [313, 313, 400]}, "prediction_text": "No revising chamber."}
{"answers": {"text": ["principal role", "principal role", "principal role"], "answer_start": [338, 338, 338]}, "prediction_text": "Evidence from witnesses."}
{"answers": {"text": ["other locations throughout Scotland", "other locations throughout Scotland", "other locations throughout Scotland"], "answer_start": [605, 605, 605]}, "prediction_text": "Other locations throughout Scotland."}
{"answers": {"text": ["a small number of MSPs", "a small number of MSPs", "a small number of MSPs"], "answer_start": [20, 20, 20]}, "prediction_text": "MSPs"}
{"answers": {"text": ["balance of parties", "balance of parties across Parliament", "the balance of parties across Parliament"], "answer_start": [75, 75, 71]}, "prediction_text": "The membership of the committees reflects the balance of parties across Parliament."}
{"answers": {"text": ["functions", "their functions", "their functions"], "answer_start": [155, 149, 149]}, "prediction_text": "functions"}
{"answers": {"text": ["Mandatory", "Mandatory", "Mandatory Committees"], "answer_start": [192, 192, 192]}, "prediction_text": "Mandatory Committees"}
{"answers": {"text": ["fourth", "fourth", "the fourth Session"], "answer_start": [379, 379, 375]}, "prediction_text": "Fourth Session."}
{"answers": {"text": ["beginning of each parliamentary session", "beginning of each parliamentary session", "at the beginning of each parliamentary session"], "answer_start": [42, 42, 35]}, "prediction_text": "At the beginning of each parliamentary session."}
{"answers": {"text": ["one", "one (or more", "one (or more) of the departments (or ministries) of the Scottish Government"], "answer_start": [215, 215, 215]}, "prediction_text": "1"}
{"answers": {"text": ["current Subject Committees", "Subject Committees", "Subject Committees"], "answer_start": [296, 304, 304]}, "prediction_text": "Department of Economy, Energy and Tourism."}
{"answers": {"text": ["Session", "Session", "Session"], "answer_start": [337, 337, 337]}, "prediction_text": "Economy, Energy, Tourism, Education, Culture, Health, Sport, Justice, Local Government, Regeneration, Rural Affairs, Climate Change, Environment, Welfare Reform, Infrastructure, Capital Investment."}
{"answers": {"text": ["type of committee", "committee", "committee"], "answer_start": [10, 18, 18]}, "prediction_text": "Private Bill Committees"}
{"answers": {"text": ["large-scale development projects", "large-scale development projects", "large-scale development projects"], "answer_start": [248, 248, 248]}, "prediction_text": "Infrastructure projects."}
{"answers": {"text": ["Scottish Government.", "Committees", "Private Bill Committees"], "answer_start": [194, 368, 355]}, "prediction_text": "Private Bill Committees."}
{"answers": {"text": ["Private Bill", "Private Bill", "Private Bill Committees"], "answer_start": [355, 355, 355]}, "prediction_text": "Private Bill Committee"}
{"answers": {"text": ["Scotland Act 1998", "The Scotland Act 1998", "The Scotland Act 1998"], "answer_start": [4, 0, 0]}, "prediction_text": "Scotland Act 1998"}
{"answers": {"text": ["Queen Elizabeth II", "Queen Elizabeth II", "Queen Elizabeth II"], "answer_start": [106, 106, 106]}, "prediction_text": "Queen Elizabeth II"}
{"answers": {"text": ["devolved competencies", "devolved competencies", "the devolved competencies"], "answer_start": [279, 279, 275]}, "prediction_text": "Devolved competencies."}
{"answers": {"text": ["Parliament of the United Kingdom at Westminster", "Parliament of the United Kingdom at Westminster", "the Parliament of the United Kingdom at Westminster"], "answer_start": [353, 353, 349]}, "prediction_text": "Westminster"}
{"answers": {"text": ["Scottish Parliament", "Parliament", "the Parliament"], "answer_start": [901, 1016, 1012]}, "prediction_text": "The Scottish Parliament."}
{"answers": {"text": ["Schedule 5", "Schedule 5", "Schedule 5"], "answer_start": [82, 82, 82]}, "prediction_text": "Schedule 5."}
{"answers": {"text": ["Scottish Parliament", "Scottish Parliament", "the Scottish Parliament"], "answer_start": [215, 215, 211]}, "prediction_text": "Scottish Parliament"}
{"answers": {"text": ["automatically devolved", "not specifically reserved", "All matters that are not specifically reserved are automatically devolved to the Scottish Parliament"], "answer_start": [185, 155, 134]}, "prediction_text": "They are not explicitly stated in Schedule 5 to the Scotland Act."}
{"answers": {"text": ["up to 3 pence in the pound", "up to 3 pence in the pound", "up to 3 pence in the pound"], "answer_start": [619, 619, 619]}, "prediction_text": "3 pence in the pound."}
{"answers": {"text": ["2012 Act", "2012 Act", "The 2012 Act"], "answer_start": [651, 651, 647]}, "prediction_text": "2012 Act"}
{"answers": {"text": ["Reserved", "Reserved", "Reserved matters"], "answer_start": [0, 0, 0]}, "prediction_text": "Reserved matters."}
{"answers": {"text": ["Scottish Parliament", "The Scottish Parliament", "The Scottish Parliament"], "answer_start": [106, 102, 102]}, "prediction_text": "Ministerial functions."}
{"answers": {"text": ["Westminster", "Westminster", "Westminster"], "answer_start": [205, 205, 205]}, "prediction_text": "Westminster"}
{"answers": {"text": ["UK Government ministers", "UK Government ministers", "Westminster"], "answer_start": [267, 267, 205]}, "prediction_text": "UK Government ministers."}
{"answers": {"text": ["Bills", "Bills", "Bills"], "answer_start": [0, 0, 0]}, "prediction_text": "Bills."}
{"answers": {"text": ["the Scottish Government", "Scottish Government", "the Scottish Government"], "answer_start": [59, 63, 59]}, "prediction_text": "Scottish Government"}
{"answers": {"text": ["a private member", "a private member", "private member"], "answer_start": [294, 294, 296]}, "prediction_text": "Private member."}
{"answers": {"text": ["an outside proposer", "an outside proposer", "an outside proposer"], "answer_start": [364, 364, 364]}, "prediction_text": "Outside proposer."}
{"answers": {"text": ["in a number of stages", "a number of stages", "in a number of stages"], "answer_start": [500, 503, 500]}, "prediction_text": "Through stages."}
{"answers": {"text": ["introductory", "introductory", "introductory stage of the bill"], "answer_start": [25, 25, 25]}, "prediction_text": "The first stage of a bill."}
{"answers": {"text": ["accompanying documents", "accompanying documents", "accompanying documents \u2013 Explanatory Notes"], "answer_start": [167, 167, 167]}, "prediction_text": "Explanatory Notes, Policy Memorandum, Financial Memorandum."}
{"answers": {"text": ["whether the bill is within the legislative competence of the Parliament", "whether the bill is within the legislative competence of the Parliament", "whether the bill is within the legislative competence of the Parliament"], "answer_start": [458, 458, 458]}, "prediction_text": "Whether the bill is within the legislative competence of the Parliament."}
{"answers": {"text": ["in the relevant committee or committees", "committee", "the whole Parliament"], "answer_start": [571, 587, 636]}, "prediction_text": "Committee or committees."}
{"answers": {"text": ["Stage 2", "Stage 2", "Stage 2"], "answer_start": [829, 829, 829]}, "prediction_text": "Stage 2."}
{"answers": {"text": ["Stage 3", "Stage 3", "Stage 3"], "answer_start": [0, 0, 0]}, "prediction_text": "Stage 3."}
{"answers": {"text": ["two", "two", "two"], "answer_start": [116, 116, 116]}, "prediction_text": "2"}
{"answers": {"text": ["final", "final", "final vote"], "answer_start": [194, 194, 194]}, "prediction_text": "Decision Time"}
{"answers": {"text": ["wrecking", "wrecking", "\"wrecking amendments\""], "answer_start": [248, 248, 247]}, "prediction_text": "Wrecking amendments."}
{"answers": {"text": ["Decision Time", "Decision Time", "After a general debate on the final form of the bill"], "answer_start": [491, 491, 410]}, "prediction_text": "Decision Time."}
{"answers": {"text": ["the Monarch", "Monarch", "the Monarch"], "answer_start": [82, 86, 82]}, "prediction_text": "The Monarch."}
{"answers": {"text": ["royal assent", "royal assent", "royal assent"], "answer_start": [98, 98, 98]}, "prediction_text": "Royal assent."}
{"answers": {"text": ["a 4-week period", "a 4-week period", "4-week period"], "answer_start": [191, 191, 193]}, "prediction_text": "4 weeks."}
{"answers": {"text": ["Supreme Court of the United Kingdom", "Supreme Court of the United Kingdom", "the Supreme Court"], "answer_start": [320, 320, 316]}, "prediction_text": "The Law Officers of the Scottish Government or UK Government."}
{"answers": {"text": ["[Date]", "[Date]", "\"The Bill for this Act of the Scottish Parliament was passed by the Parliament on [Date] and received royal assent on [Date]\"."], "answer_start": [633, 633, 551]}, "prediction_text": "The template for bills passed by the Scottish Parliament includes:"}
{"answers": {"text": ["hold the majority of seats", "hold the majority of seats", "The party, or parties, that hold the majority of seats in the Parliament"], "answer_start": [28, 28, 0]}, "prediction_text": "Scottish Government"}
{"answers": {"text": ["Any member", "Any member", "Any member"], "answer_start": [288, 288, 288]}, "prediction_text": "The leader of the largest party."}
{"answers": {"text": ["First Minister", "First Minister", "a First Minister"], "answer_start": [173, 173, 171]}, "prediction_text": "The leader of the largest party."}
{"answers": {"text": ["elected MSPs", "the elected MSPs", "amongst the elected MSPs"], "answer_start": [898, 894, 886]}, "prediction_text": "MSPs"}
{"answers": {"text": ["the Sovereign", "the Sovereign", "the Sovereign"], "answer_start": [1151, 1151, 1151]}, "prediction_text": "Sovereign"}
{"answers": {"text": ["Thursday", "Thursday", "Thursday"], "answer_start": [106, 106, 106]}, "prediction_text": "Thursday."}
{"answers": {"text": ["May", "May", "May"], "answer_start": [118, 971, 118]}, "prediction_text": "May"}
{"answers": {"text": ["the Monarch", "Monarch", "the Monarch"], "answer_start": [237, 241, 237]}, "prediction_text": "Monarch"}
{"answers": {"text": ["supplant it.", "supplant it", "reverts to the first Thursday in May, a multiple of four years after 1999"], "answer_start": [893, 893, 938]}, "prediction_text": "Reverts to the first Thursday in May."}
{"answers": {"text": ["28", "28", "28 days"], "answer_start": [499, 499, 499]}, "prediction_text": "28 days."}
{"answers": {"text": ["Several procedures", "Several procedures", "Several procedures"], "answer_start": [0, 0, 0]}, "prediction_text": "Procedures."}
{"answers": {"text": ["MSPs", "leaders of the opposition parties and other MSPs", "MSPs"], "answer_start": [173, 437, 173]}, "prediction_text": "MSPs"}
{"answers": {"text": ["legislative programme for the forthcoming year", "a statement", "a statement to the chamber setting out the Government's legislative programme for the forthcoming year"], "answer_start": [345, 289, 289]}, "prediction_text": "A statement."}
{"answers": {"text": ["issues related to the substance of the statement", "issues", "issues related to the substance of the statement"], "answer_start": [517, 517, 517]}, "prediction_text": "The leaders of the opposition parties and other MSPs question the First Minister about the substance of the statement."}
{"answers": {"text": ["Parliamentary time", "Parliamentary time", "Parliamentary time"], "answer_start": [0, 0, 0]}, "prediction_text": "Parliamentary time."}
{"answers": {"text": ["Thursday", "Thursday", "Thursday"], "answer_start": [126, 126, 126]}, "prediction_text": "Thursday."}
{"answers": {"text": ["any member of the Scottish Government", "any member of the Scottish Government", "ministers in departments that are selected for questioning that sitting day"], "answer_start": [204, 204, 342]}, "prediction_text": "Members of the Scottish Government."}
{"answers": {"text": ["issues under their jurisdiction", "issues under their jurisdiction", "the First Minister"], "answer_start": [668, 668, 637]}, "prediction_text": "The First Minister's Question Time."}
{"answers": {"text": ["four", "four", "four"], "answer_start": [938, 938, 938]}, "prediction_text": "4 General questions."}
{"answers": {"text": ["73", "73", "73"], "answer_start": [17, 17, 17]}, "prediction_text": "73"}
{"answers": {"text": ["2005", "2005", "2005"], "answer_start": [371, 371, 371]}, "prediction_text": "2005"}
{"answers": {"text": ["one", "one", "one"], "answer_start": [132, 132, 132]}, "prediction_text": "73"}
{"answers": {"text": ["dispersed population and distance", "dispersed population and distance", "their dispersed population and distance from the Scottish Parliament in Edinburgh"], "answer_start": [1004, 1004, 998]}, "prediction_text": "Dispersed population."}
{"answers": {"text": ["55,000", "55,000", "55,000"], "answer_start": [571, 571, 571]}, "prediction_text": "55,000"}
{"answers": {"text": ["proportionally to the number of votes received", "proportionally to the number of votes received", "proportionally to the number of votes received in the second vote of the ballot using the d'Hondt method"], "answer_start": [69, 69, 69]}, "prediction_text": "proportionally to the number of votes received in the second vote of the ballot."}
{"answers": {"text": ["the d'Hondt method", "d'Hondt", "the d'Hondt method"], "answer_start": [155, 159, 155]}, "prediction_text": "d'Hondt method"}
{"answers": {"text": ["quotient", "quotient", "quotient"], "answer_start": [421, 421, 421]}, "prediction_text": "quotient"}
{"answers": {"text": ["constituency seats", "constituency", "second"], "answer_start": [478, 478, 515]}, "prediction_text": "constituency seats"}
{"answers": {"text": ["iteratively", "iteratively", "iteratively"], "answer_start": [545, 545, 545]}, "prediction_text": "iteratively."}
{"answers": {"text": ["a number of qualifications", "a number of qualifications", "qualifications"], "answer_start": [28, 28, 83]}, "prediction_text": "Qualifications."}
{"answers": {"text": ["1981", "1981", "1981"], "answer_start": [199, 199, 199]}, "prediction_text": "1981"}
{"answers": {"text": ["over the age of 18", "over the age of 18", "18"], "answer_start": [235, 235, 251]}, "prediction_text": "18"}
{"answers": {"text": ["police and the armed forces", "police and the armed forces", "the police and the armed forces"], "answer_start": [483, 483, 479]}, "prediction_text": "Police and armed forces."}
{"answers": {"text": ["Mental Health (Care and Treatment) (Scotland) Act 2003", "Mental Health (Care and Treatment) (Scotland) Act 2003", "Mental Health (Care and Treatment) (Scotland) Act 2003"], "answer_start": [781, 781, 781]}, "prediction_text": "Mental Health (Care and Treatment) (Scotland) Act 2003."}
{"answers": {"text": ["a party has commanded a parliamentary majority", "a parliamentary majority", "a party has commanded a parliamentary majority"], "answer_start": [109, 131, 109]}, "prediction_text": "A majority SNP government."}
{"answers": {"text": ["Labour", "Labour", "Labour"], "answer_start": [184, 184, 184]}, "prediction_text": "Labour"}
{"answers": {"text": ["151 votes", "151 votes", "151 votes"], "answer_start": [309, 309, 309]}, "prediction_text": "151 votes."}
{"answers": {"text": ["eight", "eight", "eight"], "answer_start": [343, 343, 343]}, "prediction_text": "One."}
{"answers": {"text": ["Scottish independence", "Scottish independence", "Scottish independence"], "answer_start": [535, 535, 535]}, "prediction_text": "Scottish independence."}
{"answers": {"text": ["the Conservatives", "Conservatives", "Conservatives"], "answer_start": [4, 8, 8]}, "prediction_text": "David McLetchie"}
{"answers": {"text": ["Edinburgh Pentlands", "Edinburgh Pentlands", "former party leader"], "answer_start": [63, 63, 96]}, "prediction_text": "Edinburgh Pentlands"}
{"answers": {"text": ["five seats", "five seats", "five seats"], "answer_start": [241, 241, 241]}, "prediction_text": "Five seats."}
{"answers": {"text": ["Annabel Goldie", "Annabel Goldie", "Annabel Goldie"], "answer_start": [265, 265, 265]}, "prediction_text": "Annabel Goldie"}
{"answers": {"text": ["Cameron", "Cameron", "Cameron"], "answer_start": [399, 399, 399]}, "prediction_text": "Cameron"}
{"answers": {"text": ["able to vote on domestic legislation that applies only to England, Wales and Northern Ireland", "able to vote on domestic legislation that applies only to England, Wales and Northern Ireland", "procedural consequence"], "answer_start": [133, 133, 2]}, "prediction_text": "The consequence of establishing the Scottish Parliament applies to Scottish MPs sitting in the UK House of Commons."}
{"answers": {"text": ["domestic legislation of the Scottish Parliament", "domestic legislation of the Scottish Parliament.", "domestic legislation of the Scottish Parliament"], "answer_start": [322, 322, 322]}, "prediction_text": "Domestic legislation of the Scottish Parliament."}
{"answers": {"text": ["West Lothian question", "West Lothian question", "the West Lothian question"], "answer_start": [403, 403, 399]}, "prediction_text": "West Lothian question"}
{"answers": {"text": ["the Conservative", "Conservative", "Conservative"], "answer_start": [461, 465, 465]}, "prediction_text": "Conservative"}
{"answers": {"text": ["England", "England", "England"], "answer_start": [650, 650, 650]}, "prediction_text": "England"}
{"answers": {"text": ["Islamism", "Islamism", "Islamism"], "answer_start": [0, 0, 0]}, "prediction_text": "Islamism"}
{"answers": {"text": ["all spheres of life.", "all spheres of life", "all spheres"], "answer_start": [211, 211, 211]}, "prediction_text": "Moral conservatism, literalism, and the attempt \"to implement Islamic values in all spheres of life.\""}
{"answers": {"text": ["reordering", "reordering of government and society in accordance with the Shari'a", "reordering"], "answer_start": [253, 253, 253]}, "prediction_text": "Islamization of society and government."}
{"answers": {"text": ["poles", "two poles", "poles"], "answer_start": [403, 399, 403]}, "prediction_text": "State power and Islamization."}
{"answers": {"text": ["revolution or invasion", "revolution or invasion", "revolution"], "answer_start": [493, 493, 493]}, "prediction_text": "Revolution or invasion."}
{"answers": {"text": ["democratic", "democratic process", "democratic"], "answer_start": [64, 64, 64]}, "prediction_text": "democratic process"}
{"answers": {"text": ["Palestine", "Palestine", "Palestine"], "answer_start": [361, 361, 361]}, "prediction_text": "Palestine"}
{"answers": {"text": ["abolish the state of Israel", "abolish the state of Israel", "abolish the state of Israel"], "answer_start": [456, 456, 456]}, "prediction_text": "To abolish the state of Israel."}
{"answers": {"text": ["democracy", "democracy", "democracy"], "answer_start": [610, 610, 610]}, "prediction_text": "Democracy."}
{"answers": {"text": ["religious", "religious", "religious"], "answer_start": [778, 778, 778]}, "prediction_text": "Religious basis."}
{"answers": {"text": ["major division", "major division", "division"], "answer_start": [8, 8, 14]}, "prediction_text": "The Muslim Brotherhood."}
{"answers": {"text": ["Sunni pan-Islamism", "Sunni pan-Islamism", "Sunni pan-Islamism"], "answer_start": [299, 299, 299]}, "prediction_text": "Islamism."}
{"answers": {"text": ["sharia rather than the building of Islamic institutions,", "sharia", "sharia"], "answer_start": [527, 527, 527]}, "prediction_text": "Islamization of pan-Arabism."}
{"answers": {"text": ["democracy", "democracy", "democracy"], "answer_start": [706, 706, 706]}, "prediction_text": "Democracy."}
{"answers": {"text": ["to maintain their legitimacy", "to maintain their legitimacy", "to maintain their legitimacy"], "answer_start": [904, 904, 904]}, "prediction_text": "To maintain their legitimacy."}
{"answers": {"text": ["political", "political", "political"], "answer_start": [65, 65, 65]}, "prediction_text": "Political role."}
{"answers": {"text": ["Islam", "Islam", "Islam"], "answer_start": [157, 157, 157]}, "prediction_text": "Islam."}
{"answers": {"text": ["its supporters", "Scholars and observers", "Islamism"], "answer_start": [107, 238, 0]}, "prediction_text": "Fred Halliday, John Esposito, Muslim intellectuals."}
{"answers": {"text": ["illiberal Islamic regimes", "illiberal Islamic regimes", "illiberal Islamic regimes"], "answer_start": [517, 517, 517]}, "prediction_text": "Illiberal Islamic regimes."}
{"answers": {"text": ["religion from politics", "religion from politics", "religion from politics"], "answer_start": [607, 607, 607]}, "prediction_text": "Religion from politics."}
{"answers": {"text": ["Muslims", "Muslims"], "answer_start": [201, 201]}, "prediction_text": "Islamists"}
{"answers": {"text": ["Americans", "Americans", "Americans"], "answer_start": [393, 393, 393]}, "prediction_text": "Americans."}
{"answers": {"text": ["a historical fluke", "political Islam", "historical fluke"], "answer_start": [470, 358, 472]}, "prediction_text": "secular Arab nationalism"}
{"answers": {"text": ["between 1945 and 1970", "between 1945 and 1970", "between 1945 and 1970"], "answer_start": [555, 555, 555]}, "prediction_text": "1945-1970"}
{"answers": {"text": ["non-political Islam", "quietist/non-political Islam", "quietist/non-political Islam"], "answer_start": [598, 589, 589]}, "prediction_text": "Political Islam."}
{"answers": {"text": ["dangerous enemies", "dangerous enemies", "dangerous enemies"], "answer_start": [170, 170, 170]}, "prediction_text": "dangerous leftist/communist/nationalist insurgents/opposition"}
{"answers": {"text": ["During the 1970s", "the 1970s", "1970s"], "answer_start": [0, 7, 11]}, "prediction_text": "During the 1970s and sometimes later."}
{"answers": {"text": ["considerable impact", "experience, ideology, and weapons", "experience, ideology, and weapons"], "answer_start": [626, 582, 582]}, "prediction_text": "Experience, ideology, weapons."}
{"answers": {"text": ["the mujahideen Muslim Afghanistan", "mujahideen", "mujahideen Muslim Afghanistan"], "answer_start": [448, 452, 452]}, "prediction_text": "mujahideen Muslim Afghanistan enemies of the Soviet Union."}
{"answers": {"text": ["leftist/communist/nationalist insurgents/opposition", "leftist/communist/nationalist insurgents/opposition", "communist"], "answer_start": [306, 306, 314]}, "prediction_text": "Leftist/communist/nationalist insurgents/opposition."}
{"answers": {"text": ["considerable impact", "experience, ideology, and weapons", "experience, ideology, and weapons"], "answer_start": [626, 582, 582]}, "prediction_text": "Experience, ideology, weapons."}
{"answers": {"text": ["Anwar Sadat", "Anwar Sadat", "Anwar Sadat"], "answer_start": [19, 19, 19]}, "prediction_text": "Anwar Sadat"}
{"answers": {"text": ["peace", "peace"], "answer_start": [191, 191]}, "prediction_text": "Peace."}
{"answers": {"text": ["political support", "making peace with Israel", "political support"], "answer_start": [289, 184, 289]}, "prediction_text": "Political support in his struggle against leftists."}
{"answers": {"text": ["1975", "1975", "1975"], "answer_start": [563, 563, 563]}, "prediction_text": "1975"}
{"answers": {"text": ["assassinated", "Islamists came to completely dominate university student unions", "assassinated"], "answer_start": [664, 583, 664]}, "prediction_text": "Released."}
{"answers": {"text": ["conservative", "strict, conservative", "strict, conservative"], "answer_start": [69, 61, 61]}, "prediction_text": "Wahhabism"}
{"answers": {"text": ["hate", "hate them for their religion", "hate them for their religion"], "answer_start": [225, 225, 225]}, "prediction_text": "Hate them for their religion."}
{"answers": {"text": ["wars", "horrible wars", "all the horrible wars"], "answer_start": [329, 320, 312]}, "prediction_text": "Wars of the 20th century."}
{"answers": {"text": ["infidels", "infidels", "infidels"], "answer_start": [401, 401, 401]}, "prediction_text": "Non-Wahhabi Muslims."}
{"answers": {"text": ["Saudi", "the Saudi-interpretation", "Saudi"], "answer_start": [611, 607, 611]}, "prediction_text": "Saudi-interpretation of Islam."}
{"answers": {"text": ["Islamist", "Islamist", "Islamist"], "answer_start": [0, 0, 0]}, "prediction_text": "Islamist movements"}
{"answers": {"text": ["incompetent, inefficient, or neglectful", "incompetent, inefficient, or neglectful governments", "incompetent, inefficient, or neglectful"], "answer_start": [421, 421, 421]}, "prediction_text": "incompetent, inefficient, neglectful governments"}
{"answers": {"text": ["housing", "shelters, educational assistance, free or low cost medical clinics, housing assistance", "shelters, educational assistance, free or low cost medical clinics, housing assistance"], "answer_start": [149, 81, 81]}, "prediction_text": "Shelters"}
{"answers": {"text": ["rhetoric", "rhetoric", "rhetoric"], "answer_start": [522, 522, 522]}, "prediction_text": "Rhetoric"}
{"answers": {"text": ["avoid prohibitively costly dowry demands", "to avoid prohibitively costly dowry demands", "avoid prohibitively costly dowry demands"], "answer_start": [279, 276, 279]}, "prediction_text": "To avoid prohibitively costly dowry demands."}
{"answers": {"text": ["law and philosophy", "law and philosophy", "law and philosophy"], "answer_start": [15, 15, 15]}, "prediction_text": "Law and philosophy."}
{"answers": {"text": ["the All India Muslim League", "All India Muslim League", "All India Muslim League"], "answer_start": [104, 108, 108]}, "prediction_text": "All India Muslim League (AIML)"}
{"answers": {"text": ["the mainstream Indian nationalist and secularist Indian National Congress", "mainstream Indian nationalist and secularist Indian National Congress", "mainstream Indian nationalist and secularist Indian National Congress"], "answer_start": [466, 470, 470]}, "prediction_text": "Indian nationalist and secularist Indian National Congress."}
{"answers": {"text": ["1908", "1908", "1908"], "answer_start": [159, 159, 159]}, "prediction_text": "1908"}
{"answers": {"text": ["The Reconstruction of Religious Thought in Islam", "The Reconstruction of Religious Thought in Islam", "The Reconstruction of Religious Thought in Islam"], "answer_start": [639, 639, 639]}, "prediction_text": "The Reconstruction of Religious Thought in Islam."}
{"answers": {"text": ["secularism and secular nationalism", "secularism and secular nationalism", "secularism"], "answer_start": [42, 42, 42]}, "prediction_text": "secularism and secular nationalism."}
{"answers": {"text": ["crowd out", "crowd out Muslim heritage", "crowd out"], "answer_start": [188, 188, 188]}, "prediction_text": "crowd out Muslim heritage, culture, and political influence."}
{"answers": {"text": ["nationalist differences", "nationalist differences", "nationalist differences"], "answer_start": [406, 406, 406]}, "prediction_text": "nationalist differences."}
{"answers": {"text": ["1930", "1930", "1930"], "answer_start": [496, 496, 496]}, "prediction_text": "1930"}
{"answers": {"text": ["Pakistan movement", "the Pakistan movement", "Pakistan movement"], "answer_start": [754, 750, 754]}, "prediction_text": "Pakistan movement."}
{"answers": {"text": ["Sayyid Abul Ala Maududi", "Sayyid Abul Ala Maududi", "Sayyid Abul Ala Maududi"], "answer_start": [0, 0, 0]}, "prediction_text": "Sayyid Abul Ala Maududi"}
{"answers": {"text": ["journalism", "journalism", "journalism"], "answer_start": [207, 207, 207]}, "prediction_text": "Journalism"}
{"answers": {"text": ["1941", "1941", "1941"], "answer_start": [350, 350, 350]}, "prediction_text": "1941"}
{"answers": {"text": ["through his writing", "writing", "writing"], "answer_start": [429, 441, 441]}, "prediction_text": "Through his writing."}
{"answers": {"text": ["in a modern context", "a modern context", "modern context"], "answer_start": [566, 569, 571]}, "prediction_text": "Modern context."}
{"answers": {"text": ["Sayyid Abul Ala Maududi", "Sayyid Abul Ala Maududi", "Sayyid Abul Ala Maududi"], "answer_start": [0, 0, 0]}, "prediction_text": "Sayyid Abul Ala Maududi"}
{"answers": {"text": ["journalism", "journalism", "journalism"], "answer_start": [207, 207, 207]}, "prediction_text": "Journalism"}
{"answers": {"text": ["through his writing", "writing", "writing"], "answer_start": [429, 441, 441]}, "prediction_text": "Through his writing."}
{"answers": {"text": ["a modern context", "a modern context", "modern context"], "answer_start": [569, 569, 571]}, "prediction_text": "Modern context."}
{"answers": {"text": ["Sharia", "Sharia", "Sharia"], "answer_start": [71, 71, 71]}, "prediction_text": "Sharia."}
{"answers": {"text": ["an Islamic state", "an Islamic state", "an Islamic state"], "answer_start": [119, 119, 119]}, "prediction_text": "an Islamic state."}
{"answers": {"text": ["unity of God", "unity of God", "unity of God"], "answer_start": [214, 214, 214]}, "prediction_text": "Unity of God."}
{"answers": {"text": ["gradual", "Islamic revolution", "gradual"], "answer_start": [423, 305, 423]}, "prediction_text": "gradual changing the hearts and minds of individuals from the top of society downward through an educational process or da'wah."}
{"answers": {"text": ["an educational process", "an educational process or da'wah", "educational process or da'wah"], "answer_start": [517, 517, 520]}, "prediction_text": "Da'wah."}
{"answers": {"text": ["1928", "1928", "1928"], "answer_start": [104, 104, 104]}, "prediction_text": "1928"}
{"answers": {"text": ["Ismailiyah, Egypt", "Ismailiyah, Egypt", "Egypt"], "answer_start": [83, 83, 95]}, "prediction_text": "Ismailiyah, Egypt"}
{"answers": {"text": ["Hassan al Banna", "Hassan al Banna", "Hassan al Banna"], "answer_start": [112, 112, 112]}, "prediction_text": "Hassan al Banna"}
{"answers": {"text": ["the Qur'an", "the Qur'an", "Qur'an"], "answer_start": [252, 252, 256]}, "prediction_text": "The motto of the Muslim Brotherhood specifies that the Qur'an is their constitution."}
{"answers": {"text": ["imperialist", "imperialist influence", "imperialist"], "answer_start": [572, 572, 572]}, "prediction_text": "Imperialist influence."}
{"answers": {"text": ["violence", "violence", "violence"], "answer_start": [79, 79, 79]}, "prediction_text": "Engaged in violence against the government."}
{"answers": {"text": ["1949", "1949", "1949"], "answer_start": [157, 157, 157]}, "prediction_text": "1949"}
{"answers": {"text": ["Egypt's premier Mahmud Fami Naqrashi", "Mahmud Fami Naqrashi", "Mahmud Fami Naqrashi"], "answer_start": [202, 218, 218]}, "prediction_text": "Mahmud Fami Naqrashi"}
{"answers": {"text": ["1948", "1948", "1948"], "answer_start": [357, 357, 357]}, "prediction_text": "1948"}
{"answers": {"text": ["Gamal Abdul Nasser", "Gamal Abdul Nasser", "Gamal Abdul Nasser"], "answer_start": [435, 435, 435]}, "prediction_text": "Gamal Abdul Nasser"}
{"answers": {"text": ["one of the most influential movements", "one of the most influential movements", "one of the most influential"], "answer_start": [56, 56, 56]}, "prediction_text": "Semi-legal"}
{"answers": {"text": ["75% of the total seats", "75%", "75%"], "answer_start": [488, 488, 488]}, "prediction_text": "75%"}
{"answers": {"text": ["\"semi-legal\"", "semi-legal", "semi-legal"], "answer_start": [183, 184, 184]}, "prediction_text": "Semi-legal"}
{"answers": {"text": ["field candidates", "field candidates", "field candidates"], "answer_start": [247, 247, 247]}, "prediction_text": "Field candidates."}
{"answers": {"text": ["Mohamed Morsi", "Mohamed Morsi", "Mohamed Morsi"], "answer_start": [512, 512, 512]}, "prediction_text": "Mohamed Morsi"}
{"answers": {"text": ["quick and decisive", "quick and decisive defeat", "quick and decisive"], "answer_start": [4, 4, 4]}, "prediction_text": "The defeat of the Arab troops at the hand of the Israeli troops during the Six-Day War was a pivotal event in the Arab Muslim world."}
{"answers": {"text": ["a pivotal event", "a pivotal event in the Arab Muslim world", "pivotal event"], "answer_start": [102, 102, 104]}, "prediction_text": "A pivotal event."}
{"answers": {"text": ["economic", "economic stagnation", "economic"], "answer_start": [166, 166, 166]}, "prediction_text": "Economic stagnation."}
{"answers": {"text": ["A steep and steady decline", "A steep and steady decline", "steep and steady decline"], "answer_start": [279, 279, 281]}, "prediction_text": "Decline."}
{"answers": {"text": ["anti-democratic Islamist movements", "anti-democratic Islamist movements inspired by Maududi and Sayyid Qutb", "anti-democratic Islamist movements"], "answer_start": [482, 482, 482]}, "prediction_text": "Maududi and Sayyid Qutb."}
{"answers": {"text": ["ideological", "ideological", "ideological"], "answer_start": [101, 101, 101]}, "prediction_text": "Ideological father"}
{"answers": {"text": ["Ali Shariati", "Ali Shariati", "Ali Shariati"], "answer_start": [13, 13, 13]}, "prediction_text": "Ali Shariati"}
{"answers": {"text": ["somewhere between", "between", "somewhere between"], "answer_start": [195, 205, 195]}, "prediction_text": "Between Sunni Islamic thinkers."}
{"answers": {"text": ["the Prophet Mohammad", "Prophet Mohammad and his successors", "Prophet Mohammad"], "answer_start": [309, 313, 313]}, "prediction_text": "Mohammad Iqbal"}
{"answers": {"text": ["conspiracy", "Westernizing Muslims", "conspiracy"], "answer_start": [594, 434, 594]}, "prediction_text": "Western governments."}
{"answers": {"text": ["Islamic", "The Islamic Republic", "Islamic"], "answer_start": [4, 0, 4]}, "prediction_text": "Islamic Republic"}
{"answers": {"text": ["Shia terrorist", "Shia terrorist groups", "Shia terrorist"], "answer_start": [142, 142, 142]}, "prediction_text": "Shia terrorist groups."}
{"answers": {"text": ["economic", "economic", "economic"], "answer_start": [82, 82, 82]}, "prediction_text": "Economic sanctions."}
{"answers": {"text": ["During the 2006 Israel-Lebanon conflict", "the 2006 Israel-Lebanon conflict", "2006"], "answer_start": [290, 297, 301]}, "prediction_text": "2006"}
{"answers": {"text": ["President Mahmoud Ahmadinejad", "President Mahmoud Ahmadinejad", "President Mahmoud Ahmadinejad"], "answer_start": [489, 489, 489]}, "prediction_text": "Mahmoud Ahmadinejad"}
{"answers": {"text": ["the Soviet Union", "the Soviet Union", "Soviet Union"], "answer_start": [9, 9, 13]}, "prediction_text": "Soviet Union"}
{"answers": {"text": ["an Islamic rebellion", "an Islamic rebellion against an allied Marxist regime", "Islamic rebellion"], "answer_start": [90, 90, 93]}, "prediction_text": "Islamic rebellion."}
{"answers": {"text": ["send aid and sometimes to go themselves to fight for their faith", "send aid and sometimes to go themselves to fight for their faith", "send aid and sometimes to go themselves to fight for their faith"], "answer_start": [326, 326, 326]}, "prediction_text": "Send aid."}
{"answers": {"text": ["marginal", "marginal", "marginal"], "answer_start": [530, 530, 530]}, "prediction_text": "Marginal."}
{"answers": {"text": ["16,000 to 35,000", "16,000 to 35,000", "16,000 to 35,000"], "answer_start": [553, 553, 553]}, "prediction_text": "16,000 to 35,000."}
{"answers": {"text": ["worked to radicalize the Islamist movement", "radicalize the Islamist movement", "radicalize the Islamist movement"], "answer_start": [39, 49, 49]}, "prediction_text": "Radicalized the Islamist movement."}
{"answers": {"text": ["Saddam Hussein", "Saddam Hussein's", "Saddam Hussein's"], "answer_start": [222, 222, 222]}, "prediction_text": "Saddam Hussein"}
{"answers": {"text": ["Islamist", "Islamist", "Islamist"], "answer_start": [337, 337, 337]}, "prediction_text": "Islamist groups."}
{"answers": {"text": ["Saudi", "Saudi", "Saudi"], "answer_start": [529, 529, 529]}, "prediction_text": "Saudi Arabia"}
{"answers": {"text": ["the west", "the west", "the west"], "answer_start": [601, 601, 601]}, "prediction_text": "The west."}
{"answers": {"text": ["conservative Muslims", "Muslims", "conservative Muslims"], "answer_start": [29, 42, 29]}, "prediction_text": "Conservative Muslims."}
{"answers": {"text": ["domestic Islamists", "domestic Islamists", "domestic Islamists"], "answer_start": [350, 350, 350]}, "prediction_text": "domestic Islamists"}
{"answers": {"text": ["in the kingdom", "in the kingdom", "the kingdom"], "answer_start": [152, 152, 155]}, "prediction_text": "Saudi Arabia"}
{"answers": {"text": ["Algeria", "Algeria", "Algeria"], "answer_start": [739, 739, 739]}, "prediction_text": "Algeria"}
{"answers": {"text": ["Osama bin Laden", "Osama bin Laden", "Osama bin Laden"], "answer_start": [751, 751, 751]}, "prediction_text": "Osama bin Laden"}
{"answers": {"text": ["Qutb's", "Qutb", "Qutb's"], "answer_start": [6, 6, 6]}, "prediction_text": "Qutb"}
{"answers": {"text": ["1966", "1966", "1966"], "answer_start": [97, 97, 97]}, "prediction_text": "1966"}
{"answers": {"text": ["the Brotherhood", "the Brotherhood", "Brotherhood"], "answer_start": [121, 121, 125]}, "prediction_text": "Brotherhood"}
{"answers": {"text": ["Fringe or splinter", "Fringe or splinter movements", "Fringe"], "answer_start": [235, 235, 235]}, "prediction_text": "Fringe or splinter movements."}
{"answers": {"text": ["By the 1970s", "the 1970s", "1970s"], "answer_start": [452, 455, 459]}, "prediction_text": "1970s"}
{"answers": {"text": ["Egyptian Islamic Jihad organization", "the Egyptian Islamic Jihad organization", "Egyptian Islamic Jihad"], "answer_start": [68, 64, 68]}, "prediction_text": "Egyptian Islamic Jihad organization."}
{"answers": {"text": ["1981", "1981", "1981"], "answer_start": [156, 156, 156]}, "prediction_text": "1981"}
{"answers": {"text": ["apostate", "\"apostate\" leaders of Muslim states,", "apostate"], "answer_start": [273, 272, 273]}, "prediction_text": "Anwar Sadat, Gamal Abdel Nasser, and Muhammad Abd al-Salaam Farag."}
{"answers": {"text": ["promoted Western/foreign ideas and practices into Islamic societies", "held secular leanings or who had introduced or promoted Western/foreign ideas and practices into Islamic societies", "secular leanings"], "answer_start": [368, 321, 326]}, "prediction_text": "secular leanings or Western/foreign ideas and practices."}
{"answers": {"text": ["Muhammad Abd al-Salaam Farag", "Muhammad Abd al-Salaam Farag", "Muhammad Abd al-Salaam Farag"], "answer_start": [486, 486, 486]}, "prediction_text": "Muhammad Abd al-Salaam Farag"}
{"answers": {"text": ["violence", "violence", "violence"], "answer_start": [46, 46, 46]}, "prediction_text": "Violence."}
{"answers": {"text": ["al-Gama'a al-Islamiyya", "al-Gama'a al-Islamiyya", "Islamic Group"], "answer_start": [95, 95, 119]}, "prediction_text": "Jamaa Islamiya (al-Gama'a al-Islamiyya)"}
{"answers": {"text": ["in 2003", "2003", "2003"], "answer_start": [571, 574, 574]}, "prediction_text": "2003"}
{"answers": {"text": ["unsuccessful", "unsuccessful", "unsuccessful"], "answer_start": [466, 466, 466]}, "prediction_text": "Failed."}
{"answers": {"text": ["political figures", "political figures", "political figures"], "answer_start": [782, 782, 782]}, "prediction_text": "Political figure."}
{"answers": {"text": ["quiescent", "quiescent", "quiescent"], "answer_start": [108, 108, 108]}, "prediction_text": "Quiescent"}
{"answers": {"text": ["HAMAS", "HAMAS", "HAMAS"], "answer_start": [459, 459, 459]}, "prediction_text": "HAMAS"}
{"answers": {"text": ["destruction of Israel", "the destruction of Israel", "destruction of Israel"], "answer_start": [627, 623, 627]}, "prediction_text": "Jihad against Israel."}
{"answers": {"text": ["alcohol", "alcohol", "alcohol"], "answer_start": [1003, 1003, 1003]}, "prediction_text": "Alcohol"}
{"answers": {"text": ["Palestine", "Palestine", "Palestine"], "answer_start": [694, 694, 694]}, "prediction_text": "Palestine"}
{"answers": {"text": ["Hamas", "Hamas", "Hamas"], "answer_start": [0, 304, 0]}, "prediction_text": "Hamas"}
{"answers": {"text": ["542", "542", "542"], "answer_start": [83, 83, 83]}, "prediction_text": "542"}
{"answers": {"text": ["majority of the seats,", "the majority of the seats", "majority of the seats"], "answer_start": [239, 235, 239]}, "prediction_text": "Majority of seats."}
{"answers": {"text": ["2007", "2007", "2007"], "answer_start": [269, 269, 269]}, "prediction_text": "2007"}
{"answers": {"text": ["driving Israel out of the Gaza Strip", "driving Israel out of the Gaza Strip", "driving Israel out of the Gaza Strip"], "answer_start": [342, 342, 342]}, "prediction_text": "Driving Israel out of the Gaza Strip."}
{"answers": {"text": ["Islamist", "Islamist", "Islamist"], "answer_start": [29, 29, 29]}, "prediction_text": "Islamist regime"}
{"answers": {"text": ["Hassan al-Turabi", "Hassan al-Turabi", "Hassan al-Turabi"], "answer_start": [69, 69, 69]}, "prediction_text": "Hassan al-Turabi"}
{"answers": {"text": ["National Islamic Front", "National Islamic Front", "National Islamic Front"], "answer_start": [91, 91, 91]}, "prediction_text": "National Islamic Front"}
{"answers": {"text": ["money from foreign Islamist banking systems", "with money from foreign Islamist banking systems", "money from foreign Islamist banking systems"], "answer_start": [273, 268, 273]}, "prediction_text": "Money from foreign Islamist banking systems."}
{"answers": {"text": ["university and military academy", "the university and military academy", "university and military academy"], "answer_start": [461, 457, 461]}, "prediction_text": "University and military academy."}
{"answers": {"text": ["1985", "1985", "1985"], "answer_start": [35, 35, 35]}, "prediction_text": "1985"}
{"answers": {"text": ["with the help of the military", "with the help of the military", "military"], "answer_start": [160, 160, 181]}, "prediction_text": "Turabi."}
{"answers": {"text": ["sharia law", "sharia law", "sharia law"], "answer_start": [338, 338, 338]}, "prediction_text": "Sharia law."}
{"answers": {"text": ["Osama bin Laden", "Osama bin Laden", "Osama bin Laden"], "answer_start": [509, 509, 509]}, "prediction_text": "Osama bin Laden"}
{"answers": {"text": ["American attack on Iraq", "the American attack on Iraq", "American attack on Iraq"], "answer_start": [598, 594, 598]}, "prediction_text": "Osama bin Laden."}
{"answers": {"text": ["staying home", "staying home to alleviate the high rate of unemployment among young Algerian men", "staying home"], "answer_start": [681, 681, 681]}, "prediction_text": "Stay home."}
{"answers": {"text": ["1989", "1989", "1989"], "answer_start": [232, 232, 232]}, "prediction_text": "1989"}
{"answers": {"text": ["Algeria", "Algeria", "Afghanistan"], "answer_start": [182, 182, 61]}, "prediction_text": "Algeria"}
{"answers": {"text": ["Front Islamique de Salut", "Front Islamique de Salut", "Front Islamique de Salut"], "answer_start": [124, 124, 124]}, "prediction_text": "Front Islamique de Salut (Islamic Salvation Front)"}
{"answers": {"text": ["a military coup d'\u00e9tat", "a military coup d'\u00e9tat", "a military coup d'\u00e9tat"], "answer_start": [892, 892, 892]}, "prediction_text": "Military coup d'\u00e9tat."}
{"answers": {"text": ["justice and prosperity", "justice and prosperity", "justice and prosperity"], "answer_start": [95, 95, 95]}, "prediction_text": "Justice and prosperity."}
{"answers": {"text": ["vicious and destructive", "vicious and destructive", "civil"], "answer_start": [128, 128, 152]}, "prediction_text": "Civil war"}
{"answers": {"text": ["1992", "1992", "1992"], "answer_start": [262, 262, 262]}, "prediction_text": "1992"}
{"answers": {"text": ["one of the poorest countries on earth", "one of the poorest countries on earth", "one of the poorest countries on earth"], "answer_start": [220, 220, 220]}, "prediction_text": "poverty"}
{"answers": {"text": ["80%", "roughly 80%", "roughly 80%"], "answer_start": [587, 579, 579]}, "prediction_text": "80%"}
{"answers": {"text": ["The Taliban", "The Taliban", "Taliban"], "answer_start": [0, 0, 4]}, "prediction_text": "Deobandi movement"}
{"answers": {"text": ["Pakistan", "Pakistan", "Pakistan"], "answer_start": [188, 188, 188]}, "prediction_text": "Pakistan"}
{"answers": {"text": ["neofundamentalist", "Islamic fundamentalist or neofundamentalist", "neofundamentalist"], "answer_start": [335, 309, 335]}, "prediction_text": "Islamic fundamentalist or neofundamentalist."}
{"answers": {"text": ["Sharia", "an idealized and systematized version of conservative tribal village customs", "Sharia"], "answer_start": [476, 379, 476]}, "prediction_text": "Sharia."}
{"answers": {"text": ["Osama bin Laden", "Osama bin Laden", "Osama bin Laden"], "answer_start": [615, 615, 615]}, "prediction_text": "Osama bin Laden"}
{"answers": {"text": ["July 1977", "July 1977", "1977"], "answer_start": [3, 3, 8]}, "prediction_text": "July 1977"}
{"answers": {"text": ["alcohol and nightclubs", "alcohol and nightclubs", "alcohol and nightclubs"], "answer_start": [186, 186, 186]}, "prediction_text": "Alcohol and nightclubs."}
{"answers": {"text": ["Islamism", "Islamism", "Islamism"], "answer_start": [429, 429, 429]}, "prediction_text": "Islamization"}
{"answers": {"text": ["his means of seizing power", "his means of seizing", "seizing power"], "answer_start": [870, 870, 883]}, "prediction_text": "Symbols."}
{"answers": {"text": ["1988", "1988", "1988"], "answer_start": [1094, 1094, 1094]}, "prediction_text": "1988"}
{"answers": {"text": ["Wahhabi/Salafi jihadist extremist militant", "Wahhabi/Salafi jihadist extremist militant group", "extremist militant"], "answer_start": [190, 190, 214]}, "prediction_text": "Wahhabi/Salafi jihadist extremist group."}
{"answers": {"text": ["Sunni Arabs", "Sunni Arabs", "Sunni Arabs"], "answer_start": [278, 278, 278]}, "prediction_text": "Abu Bakr al-Baghdadi"}
{"answers": {"text": ["ten million", "ten million", "ten million"], "answer_start": [506, 506, 506]}, "prediction_text": "Ten million."}
{"answers": {"text": ["recognition", "international recognition", "recognition"], "answer_start": [674, 660, 674]}, "prediction_text": "International recognition."}
{"answers": {"text": ["a caliphate", "a caliphate", "caliphate"], "answer_start": [348, 348, 350]}, "prediction_text": "Caliphate"}
{"answers": {"text": ["2004", "2004", "2004"], "answer_start": [93, 93, 93]}, "prediction_text": "2004"}
{"answers": {"text": ["2003", "March 2003", "March 200"], "answer_start": [160, 154, 154]}, "prediction_text": "March 2003"}
{"answers": {"text": ["notorious intransigence", "its failure to consult and \"notorious intransigence\"", "notorious intransigence"], "answer_start": [362, 334, 362]}, "prediction_text": "To drive Iraqi government forces out of key cities in western Iraq."}
{"answers": {"text": ["March 2011", "March 2011", "March 2011"], "answer_start": [255, 255, 255]}, "prediction_text": "March 2011"}
{"answers": {"text": ["a terrorist organisation", "a terrorist organisation", "terrorist organisation"], "answer_start": [906, 906, 908]}, "prediction_text": "Responsible for human rights abuses and war crimes."}
{"answers": {"text": ["a different view", "Islam's pivotal turning point as occurring not with the death of Ali", "different view"], "answer_start": [47, 139, 49]}, "prediction_text": "HT sees Islam's pivotal turning point as occurring not with the death of Ali, or one of the other four rightly guided Caliphs in the 7th century, but with the abolition of the Ottoman Caliphate in 1924."}
{"answers": {"text": ["7th century", "the 7th century", "7th century"], "answer_start": [264, 260, 264]}, "prediction_text": "1924"}
{"answers": {"text": ["1924", "1924", "1924"], "answer_start": [328, 328, 328]}, "prediction_text": "1924"}
{"answers": {"text": ["true Islamic", "the true Islamic system", "true Islamic"], "answer_start": [369, 365, 369]}, "prediction_text": "True Islamic system."}
{"answers": {"text": ["ended the true Islamic system", "working through Turkish modernist Mustafa Kemal Atat\u00fcrk", "abolition of the Ottoman Caliphate"], "answer_start": [359, 463, 290]}, "prediction_text": "The disbelieving (Kafir) colonial powers."}
{"answers": {"text": ["armed", "armed jihad", "armed"], "answer_start": [22, 22, 22]}, "prediction_text": "armed jihad"}
{"answers": {"text": ["ideological struggle", "ideological struggle", "ideological struggle"], "answer_start": [100, 100, 100]}, "prediction_text": "Through \"ideological struggle\" to change Muslim public opinion."}
{"answers": {"text": ["elites", "government", "elites"], "answer_start": [181, 227, 181]}, "prediction_text": "elites"}
{"answers": {"text": ["Egypt", "Egypt", "Egypt"], "answer_start": [361, 361, 361]}, "prediction_text": "Egypt"}
{"answers": {"text": ["terrorist groups", "terrorist groups", "terrorist groups"], "answer_start": [446, 446, 446]}, "prediction_text": "Terrorist groups."}
{"answers": {"text": ["over 900,000", "900,000", "over 900,000"], "answer_start": [19, 24, 19]}, "prediction_text": "900,000"}
{"answers": {"text": ["strong Islamist", "a strong Islamist outlook", "Islamist"], "answer_start": [192, 190, 199]}, "prediction_text": "Strong Islamist."}
{"answers": {"text": ["2007", "2007", "2007"], "answer_start": [336, 336, 336]}, "prediction_text": "2007"}
{"answers": {"text": ["Londonistan", "Londonistan", "Londonistan"], "answer_start": [419, 419, 419]}, "prediction_text": "Londonistan"}
{"answers": {"text": ["incitement to terrorism", "incitement to terrorism", "incitement to terrorism"], "answer_start": [557, 557, 557]}, "prediction_text": "Incitement to terrorism."}
{"answers": {"text": ["since 2001", "since 2001", "2001"], "answer_start": [85, 85, 91]}, "prediction_text": "2001"}
{"answers": {"text": ["State", "State Department", "State"], "answer_start": [188, 188, 188]}, "prediction_text": "State Department"}
{"answers": {"text": ["Christian Whiton", "George W. Bush", "Christian Whiton"], "answer_start": [334, 371, 334]}, "prediction_text": "Christian Whiton"}
{"answers": {"text": ["U.S. Defense Secretary", "Defense Secretary", "Defense Secretary"], "answer_start": [519, 524, 524]}, "prediction_text": "Defense Secretary"}
{"answers": {"text": ["undermining the communist ideology", "undermining the communist ideology", "undermining the communist ideology"], "answer_start": [660, 660, 660]}, "prediction_text": "Undermining communism."}
{"answers": {"text": ["Latin", "Latin", "Latin", "Latin", "Latin"], "answer_start": [74, 74, 74, 74, 74]}, "prediction_text": "Latin"}
{"answers": {"text": ["military force", "colonization, use of military force, or other means", "colonization, use of military force, or other means", "colonization, use of military force, or other", "colonization"], "answer_start": [240, 219, 219, 219, 219]}, "prediction_text": "Military force."}
{"answers": {"text": ["Japan", "Japan", "Japanese", "Japanese", "Japan"], "answer_start": [450, 450, 450, 450, 450]}, "prediction_text": "Japan"}
{"answers": {"text": ["technologies and ideas", "technologies and ideas", "technologies and ideas", "technologies and ideas", "technologies and ideas."], "answer_start": [372, 372, 372, 372, 372]}, "prediction_text": "technologies and ideas."}
{"answers": {"text": ["influence", "influence", "influence", "influence", "influence"], "answer_start": [71, 71, 71, 71, 71]}, "prediction_text": "influence"}
{"answers": {"text": ["\"Formal imperialism\"", "formal", "\"Formal imperialism", "Formal imperialism", "Formal"], "answer_start": [401, 365, 401, 402, 402]}, "prediction_text": "Formal imperialism."}
{"answers": {"text": ["othering", "othering", "othering", "othering", "othering"], "answer_start": [279, 279, 279, 279, 279]}, "prediction_text": "diplomacy or military force."}
{"answers": {"text": ["direct", "direct", "direct", "direct", "direct"], "answer_start": [517, 517, 517, 517, 517]}, "prediction_text": "less direct."}
{"answers": {"text": ["\"informal\" imperialism", "Informal rule", "informal\" imperialism", "imperialism"], "answer_start": [450, 639, 451, 18]}, "prediction_text": "informal imperialism"}
{"answers": {"text": ["\"formal\"", "formal", "formal", "formal", "formal"], "answer_start": [437, 438, 438, 438, 438]}, "prediction_text": "Full-fledged colonial rule."}
{"answers": {"text": ["aggressiveness", "aggressiveness", "aggressiveness", "aggressiveness", "aggressiveness"], "answer_start": [161, 161, 161, 161, 161]}, "prediction_text": "aggressiveness."}
{"answers": {"text": ["ownership of private industries", "technological superiority", "technological superiority", "ownership of private industries", "technological superiority,"], "answer_start": [888, 796, 796, 888, 796]}, "prediction_text": "technological superiority"}
{"answers": {"text": ["informal", "Informal rule is generally less costly", "Informal rule", "formal", "formal"], "answer_start": [739, 639, 639, 438, 438]}, "prediction_text": "Formal imperialism."}
{"answers": {"text": ["distinction", "distinction", "distinction", "distinction", "distinction"], "answer_start": [167, 167, 167, 167, 167]}, "prediction_text": "The amount of land a country controls is its greatest."}
{"answers": {"text": ["the world systems theory", "world systems theory", "world systems theory.", "world systems theory"], "answer_start": [948, 952, 952, 952]}, "prediction_text": "World systems theory."}
{"answers": {"text": ["Lenin", "Lenin", "Lenin"], "answer_start": [1349, 1349, 1349]}, "prediction_text": "Lenin"}
{"answers": {"text": ["empires", "empires"], "answer_start": [1751, 1751]}, "prediction_text": "Empire"}
{"answers": {"text": ["seaborne", "land", "sea and trade routes", "sea"], "answer_start": [818, 301, 364, 551]}, "prediction_text": "land."}
{"answers": {"text": ["colonialism", "colonialism", "colonialism", "colonialism", "colonialism"], "answer_start": [48, 48, 48, 48, 48]}, "prediction_text": "colonialism"}
{"answers": {"text": ["political focus", "political focus", "political focus", "political focus", "ideological"], "answer_start": [1222, 1222, 1222, 1222, 392]}, "prediction_text": "A political focus."}
{"answers": {"text": ["ideological", "ideological", "ideological", "commercial"], "answer_start": [392, 392, 392, 490]}, "prediction_text": "settlement or commercial intentions."}
{"answers": {"text": ["Ottoman", "Ottoman", "Ottoman", "Ottoman", "Ottoman"], "answer_start": [1003, 1003, 1003, 1003, 1003]}, "prediction_text": "Ottoman."}
{"answers": {"text": ["person or group of people", "a person or group of people", "a person or group of people", "person or group of people"], "answer_start": [257, 255, 255, 257]}, "prediction_text": "distant territory"}
{"answers": {"text": ["Imperialism and colonialism", "Imperialism and colonialism", "Imperialism and colonialism", "Imperialism and colonialism"], "answer_start": [0, 0, 0, 0]}, "prediction_text": "Imperialism and colonialism."}
{"answers": {"text": ["taking physical control of another", "a conquest over an area", "the process of a country taking physical control of another", "dominating areas", "country taking physical control of another"], "answer_start": [347, 700, 322, 536, 339]}, "prediction_text": "Taking physical control of another."}
{"answers": {"text": ["conquering the other state's lands", "conquering the other state's lands and therefore increasing its own dominance", "by conquering the other state's lands", "political and monetary dominance"], "answer_start": [1061, 1061, 1058, 409]}, "prediction_text": "Conquering the other state's lands."}
{"answers": {"text": ["exploitation", "the exploitation of the valuable assets and supplies", "the exploitation of the valuable assets and supplies of the nation that was conquered", "exploitation of the valuable assets and supplies of the nation that was conquered and the conquering nation then gaining the benefits", "exploitation of the valuable assets and supplies of the nation that was conquered"], "answer_start": [845, 841, 841, 845, 845]}, "prediction_text": "The core meaning of colonialism is the exploitation of the valuable assets and supplies of the nation that was conquered and the conquering nation then gaining the benefits from the spoils of the war."}
{"answers": {"text": ["characteristics", "characteristics of the conquering peoples", "characteristics of the conquering peoples", "the characteristics of the conquering peoples", "characteristics"], "answer_start": [1403, 1403, 1403, 1399, 1403]}, "prediction_text": "characteristics of the conquering peoples."}
{"answers": {"text": ["empire-building", "defense and justification of empire-building", "is the defense and justification of empire-building based on seemingly rational grounds", "defense and justification of empire-building based on seemingly rational grounds", "defense and justification of empire-building"], "answer_start": [74, 45, 38, 45, 45]}, "prediction_text": "defense and justification of empire-building based on seemingly rational grounds."}
{"answers": {"text": ["imperialism", "imperialism", "imperialism", "imperialism", "imperialism"], "answer_start": [526, 526, 526, 526, 526]}, "prediction_text": "Imperialism."}
{"answers": {"text": ["highest 'social efficiency'", "highest 'social efficiency'", "the races of highest 'social efficiency'\"", "of highest 'social efficiency", "races of highest 'social efficiency'\""], "answer_start": [352, 352, 339, 349, 343]}, "prediction_text": "The races of highest social efficiency."}
{"answers": {"text": ["theory of races", "Social Darwinism", "Social Darwinism", "Social Darwinism"], "answer_start": [737, 713, 713, 713]}, "prediction_text": "Social Darwinism"}
{"answers": {"text": ["whiteness", "whiteness", "whiteness", "whiteness", "whiteness"], "answer_start": [940, 940, 940, 940, 940]}, "prediction_text": "White"}
{"answers": {"text": ["Germany", "Germany", "Germany", "Germany", "Germany"], "answer_start": [316, 316, 316, 316, 316]}, "prediction_text": "Germany"}
{"answers": {"text": ["Britain", "Britain", "Britain", "Britain", "Britain"], "answer_start": [349, 349, 349, 349, 349]}, "prediction_text": "Britain"}
{"answers": {"text": ["Political", "Political", "Political", "Political", "Political"], "answer_start": [266, 266, 266, 266, 266]}, "prediction_text": "British and German."}
{"answers": {"text": ["geographical societies in Europe", "necessary for a state\u2019s survival", "survival", "necessary", "necessary"], "answer_start": [51, 415, 439, 415, 415]}, "prediction_text": "Survival"}
{"answers": {"text": ["fund", "fund travelers who would come back with tales of their discoveries", "fund travelers", "able to fund travelers", "fund"], "answer_start": [121, 121, 121, 113, 121]}, "prediction_text": "Fund travelers."}
{"answers": {"text": ["environmental determinism", "environmental determinism", "environmental determinism", "environmental determinism", "environmental determinism"], "answer_start": [30, 30, 30, 30, 30]}, "prediction_text": "Environmental determinism"}
{"answers": {"text": ["temperate", "temperate zone", "the temperate zone", "temperate zone", "temperate"], "answer_start": [324, 324, 320, 324, 324]}, "prediction_text": "Temperate."}
{"answers": {"text": ["Orientalism", "Orientalism", "Orientalism", "Orientalism", "Orientalism"], "answer_start": [389, 389, 389, 389, 389]}, "prediction_text": "Orientalism"}
{"answers": {"text": ["uncivilized", "uncivilized", "fully human", "uncivilized people", "uncivilized"], "answer_start": [106, 106, 305, 106, 106]}, "prediction_text": "uncivilized people"}
{"answers": {"text": ["superior", "superior", "the superior and the norm", "superior", "superior"], "answer_start": [529, 529, 525, 529, 529]}, "prediction_text": "The superior and the norm."}
{"answers": {"text": ["Terra nullius", "Terra nullius", "Terra nullius", "Terra nullius"], "answer_start": [247, 247, 247, 247]}, "prediction_text": "Roman law."}
{"answers": {"text": ["the eighteenth century", "eighteenth century", "eighteenth century", "eighteenth century", "eighteenth century,"], "answer_start": [449, 453, 453, 453, 453]}, "prediction_text": "1788"}
{"answers": {"text": ["the British Empire", "Terra nullius", "the British Empire", "British Empire", "British"], "answer_start": [87, 247, 87, 91, 91]}, "prediction_text": "British Empire"}
{"answers": {"text": ["Aboriginal", "Aboriginal", "Aboriginal inhabitants", "Aboriginal"], "answer_start": [562, 562, 562, 562]}, "prediction_text": "Aboriginals"}
{"answers": {"text": ["empty land", "empty land", "empty land", "empty land", "'empty land'"], "answer_start": [315, 315, 315, 315, 314]}, "prediction_text": "Empty land."}
{"answers": {"text": ["an imaginative geography", "imaginative geography", "imaginative geography", "imaginative geography", "imaginative geography"], "answer_start": [75, 78, 78, 78, 78]}, "prediction_text": "imaginative geography"}
{"answers": {"text": ["irrational and backward", "them", "as irrational and backward", "irrational and backward", "irrational and backward"], "answer_start": [605, 404, 602, 605, 605]}, "prediction_text": "irrational and backward."}
{"answers": {"text": ["inferior", "irrational and backward", "its inferior", "irrational and backward", "inferior"], "answer_start": [738, 605, 734, 605, 738]}, "prediction_text": "irrational and backward."}
{"answers": {"text": ["Orientalism", "inferior", "Defining the East as a negative vision of itself", "Orientalism", "Orientalism"], "answer_start": [928, 738, 681, 928, 928]}, "prediction_text": "Orientalism"}
{"answers": {"text": ["progressive", "rational and progressive", "rational and progressive", "rational and progressive"], "answer_start": [663, 650, 650, 650]}, "prediction_text": "irrational and backward."}
{"answers": {"text": ["nineteenth-century maps", "nineteenth-century maps", "the role of nineteenth-century maps", "the role of nineteenth-century maps", "the role of nineteenth-century maps during the \"scramble for Africa\""], "answer_start": [76, 76, 64, 64, 64]}, "prediction_text": "nineteenth-century cartographic techniques"}
{"answers": {"text": ["blank spaces on contemporary maps", "fill in blank spaces on contemporary maps", "maps", "maps", "blank space"], "answer_start": [519, 511, 149, 149, 372]}, "prediction_text": "blank space"}
{"answers": {"text": ["unexplored territory", "unknown or unexplored territory", "unknown or unexplored territory", "to denote unknown or unexplored territory", "unknown or unexplored territory"], "answer_start": [405, 394, 394, 384, 394]}, "prediction_text": "Unknown or unexplored territory."}
{"answers": {"text": ["nineteenth-century cartographic techniques", "nineteenth-century maps", "nineteenth-century cartographic techniques", "nineteenth-century maps", "role of nineteenth-century maps"], "answer_start": [303, 76, 303, 76, 68]}, "prediction_text": "nineteenth-century cartographic techniques"}
{"answers": {"text": ["French", "French", "French", "French", "French"], "answer_start": [236, 236, 236, 236, 236]}, "prediction_text": "French."}
{"answers": {"text": ["the pre-Columbian era", "pre-Columbian", "pre-Columbian era", "pre-Columbian", "pre-Columbian"], "answer_start": [669, 673, 673, 673, 673]}, "prediction_text": "Pre-Columbian era."}
{"answers": {"text": ["Genghis Khan", "Genghis Khan", "Genghis Khan", "Genghis Khan", "Genghis Khan"], "answer_start": [324, 324, 324, 324, 324]}, "prediction_text": "Genghis Khan"}
{"answers": {"text": ["dozens", "dozens", "dozens", "dozens", "dozens"], "answer_start": [485, 440, 440, 440, 440]}, "prediction_text": "dozens"}
{"answers": {"text": ["Ethiopian Empire", "the Ethiopian Empire", "Ethiopian", "Ethiopian"], "answer_start": [559, 555, 559, 559]}, "prediction_text": "British Empire"}
{"answers": {"text": ["Sub-Saharan Africa", "Sub-Saharan Africa", "Sub-Saharan Africa", "Sub-Saharan Africa"], "answer_start": [448, 448, 448, 448]}, "prediction_text": "Africa"}
{"answers": {"text": ["Cultural imperialism", "Cultural imperialism", "Cultural imperialism", "Cultural imperialism", "Cultural imperialism"], "answer_start": [0, 0, 0, 0, 0]}, "prediction_text": "Cultural imperialism."}
{"answers": {"text": ["soft power", "soft power", "soft power", "soft power", "soft power"], "answer_start": [687, 100, 100, 100, 100]}, "prediction_text": "Cultural imperialism is when a country's influence is felt in social and cultural circles, i.e. its soft power, such that it changes the moral, cultural and societal worldview of another."}
{"answers": {"text": ["Dallas", "Dallas", "Dallas", "Dallas", "Dallas"], "answer_start": [510, 510, 510, 510, 510]}, "prediction_text": "Dallas"}
{"answers": {"text": ["Roman", "Roman", "Roman", "Roman", "Roman"], "answer_start": [991, 911, 911, 911, 911]}, "prediction_text": "Rome"}
{"answers": {"text": ["bans", "bans on foreign popular culture, control of the internet and unauthorised satellite dishes", "bans on foreign popular culture, control of the internet and unauthorised satellite dishes", "bans"], "answer_start": [765, 765, 765, 765]}, "prediction_text": "Bans on foreign popular culture, control of the internet, unauthorised satellite dishes."}
{"answers": {"text": ["around 1700", "around 1700", "1700", "around 1700", "1700"], "answer_start": [48, 48, 55, 48, 55]}, "prediction_text": "1700"}
{"answers": {"text": ["colonizing", "process of colonizing, influencing, and annexing other parts of the world", "colonizing, influencing, and annexing other parts of the world in order to gain political power", "colonizing, influencing, and annexing"], "answer_start": [137, 126, 137, 137]}, "prediction_text": "Colonizing, influencing, and annexing other parts of the world."}
{"answers": {"text": ["thousands", "thousands", "thousands", "thousands", "thousands"], "answer_start": [299, 299, 299, 299, 299]}, "prediction_text": "Thousands of years."}
{"answers": {"text": ["middle of the 20th century", "20th century", "20th century", "middle of the 20th century", "20th century,"], "answer_start": [446, 460, 460, 446, 460]}, "prediction_text": "Middle of the 20th century."}
{"answers": {"text": ["Open Door Policy", "Open Door Policy", "Open Door Policy", "Open Door Policy", "Open Door Policy"], "answer_start": [561, 561, 561, 561, 561]}, "prediction_text": "The Open Door Policy"}
{"answers": {"text": ["1919", "1919", "1919", "1919", "1919"], "answer_start": [52, 52, 52, 52, 52]}, "prediction_text": "1919"}
{"answers": {"text": ["1999", "1999", "1999", "1999", "1999"], "answer_start": [89, 89, 89, 89, 89]}, "prediction_text": "1999"}
{"answers": {"text": ["historians", "historians", "historians", "historians", "historians"], "answer_start": [25, 25, 25, 25, 25]}, "prediction_text": "Historians"}
{"answers": {"text": ["the world's economy", "economy", "economy", "world's economy", "world's economy"], "answer_start": [723, 735, 735, 727, 727]}, "prediction_text": "World economy."}
{"answers": {"text": ["many imperial powers", "imperial powers", "imperial powers", "imperial powers"], "answer_start": [844, 849, 849, 849]}, "prediction_text": "British emigration, trade, and capital."}
{"answers": {"text": ["economic growth", "economic growth", "economic growth", "economic growth", "economic growth"], "answer_start": [71, 71, 71, 71, 71]}, "prediction_text": "economic growth"}
{"answers": {"text": ["mid-18th century", "18th century", "mid-18th century", "mid-18th century", "mid-18th century"], "answer_start": [239, 243, 239, 239, 239]}, "prediction_text": "mid-18th century"}
{"answers": {"text": ["colonies", "collecting resources from colonies", "colonies", "colonies", "colonies"], "answer_start": [116, 90, 116, 116, 116]}, "prediction_text": "colonies"}
{"answers": {"text": ["the Mughal state", "Mughal state", "Mughal state", "Mughal state", "Mughal state"], "answer_start": [345, 349, 349, 349, 349]}, "prediction_text": "The British."}
{"answers": {"text": ["communication", "communication", "communication", "communication", "communication"], "answer_start": [27, 27, 27, 27, 27]}, "prediction_text": "Communication advances."}
{"answers": {"text": ["deadly explosives", "explosives", "explosives", "deadly explosives", "deadly explosives"], "answer_start": [122, 129, 129, 122, 122]}, "prediction_text": "deadly explosives"}
{"answers": {"text": ["the machine gun", "machine gun", "machine gun", "machine gun", "machine gun"], "answer_start": [267, 271, 271, 271, 271]}, "prediction_text": "machine gun"}
{"answers": {"text": ["arrows, swords, and leather shields", "arrows, swords, and leather shields", "arrows, swords, and leather shields", "arrows, swords, and leather shields", "arrows, swords, and leather shields"], "answer_start": [462, 462, 462, 462, 462]}, "prediction_text": "Arrows, swords, and leather shields."}
{"answers": {"text": ["European", "European", "European", "Europe"], "answer_start": [348, 348, 348, 42]}, "prediction_text": "Europe"}
{"answers": {"text": ["British", "British", "British", "British"], "answer_start": [84, 84, 84, 84]}, "prediction_text": "Britain"}
{"answers": {"text": ["in the late 1870s", "1870s", "1870s", "late 1870s", "1870s"], "answer_start": [187, 199, 199, 194, 199]}, "prediction_text": "Late 1870s."}
{"answers": {"text": ["philanthropy", "idealism and philanthropy", "philanthropy", "idealism and philanthropy"], "answer_start": [471, 458, 471, 458]}, "prediction_text": "Idealism and philanthropy."}
{"answers": {"text": ["to constantly expand investment", "to constantly expand investment", "to constantly expand investment, material resources and manpower", "need for capitalist economies to constantly expand investment, material resources and manpower", "need for capitalist economies to constantly expand investment, material resources and manpower in such a way that necessitated colonial expansion."], "answer_start": [1009, 1009, 1009, 979, 979]}, "prediction_text": "To constantly expand investment, material resources, and manpower."}
{"answers": {"text": ["aristocracy", "aristocracy", "aristocracy", "aristocracy", "aristocracy"], "answer_start": [36, 36, 36, 36, 36]}, "prediction_text": "aristocracy"}
{"answers": {"text": ["the 1950s", "1950s", "1950s", "1950s", "1950s"], "answer_start": [616, 620, 620, 620, 620]}, "prediction_text": "1950s"}
{"answers": {"text": ["before World War I", "World War I", "before World War I", "before World War I", "before World War I,"], "answer_start": [361, 368, 361, 361, 361]}, "prediction_text": "Before World War I."}
{"answers": {"text": ["disease", "disease", "disease", "disease", "disease"], "answer_start": [699, 699, 699, 699, 699]}, "prediction_text": "disease"}
{"answers": {"text": ["taxation", "removing its economic foundation", "domestic social reforms", "removing its economic foundation", "removing its economic foundation."], "answer_start": [808, 725, 646, 725, 725]}, "prediction_text": "Remove its economic foundation."}
{"answers": {"text": ["environmental determinism", "environmental determinism", "environmental determinism", "environmental determinism", "environmental determinism"], "answer_start": [12, 12, 12, 12, 12]}, "prediction_text": "Environmental determinism."}
{"answers": {"text": ["the environment in which they lived", "environment in which they lived", "environment", "the environment in which they lived", "environment"], "answer_start": [191, 195, 195, 191, 195]}, "prediction_text": "Environment."}
{"answers": {"text": ["less civilized", "less civilized", "less civilized", "less civilized", "less civilized"], "answer_start": [330, 330, 330, 330, 330]}, "prediction_text": "Less civilized."}
{"answers": {"text": ["Africa", "Africa", "Africa", "Africa", "Africa"], "answer_start": [509, 509, 509, 509, 509]}, "prediction_text": "Africa"}
{"answers": {"text": ["orientalism and tropicality", "orientalism and tropicality", "orientalism and tropicality", "orientalism and tropicality", "orientalism and tropicality."], "answer_start": [639, 639, 639, 639, 639]}, "prediction_text": "orientalism and tropicality."}
{"answers": {"text": ["geographic scholars", "geographic scholars", "geographic scholars", "geographic scholars", "geographic scholars"], "answer_start": [13, 13, 13, 13, 13]}, "prediction_text": "Geographic scholars."}
{"answers": {"text": ["Northern Europe and the Mid-Atlantic", "Northern Europe and the Mid-Atlantic", "Northern Europe and the Mid-Atlantic", "Mid-Atlantic", "Northern Europe and the Mid-Atlantic"], "answer_start": [134, 134, 134, 158, 134]}, "prediction_text": "Northern Europe, Mid-Atlantic, tropical climates."}
{"answers": {"text": ["guidance", "guidance and intervention", "guidance and intervention", "guidance and intervention", "guidance"], "answer_start": [424, 424, 424, 424, 424]}, "prediction_text": "Guidance and intervention."}
{"answers": {"text": ["orientalism", "orientalism", "orientalism", "orientalism", "orientalism"], "answer_start": [590, 590, 590, 590, 590]}, "prediction_text": "orientalism"}
{"answers": {"text": ["colonizing empires", "colonizing empires", "colonizing empires", "colonizing empires", "colonizing empires"], "answer_start": [39, 39, 39, 39, 39]}, "prediction_text": "European empires"}
{"answers": {"text": ["the sixteenth century", "sixteenth century", "sixteenth century", "sixteenth century", "sixteenth century"], "answer_start": [56, 60, 60, 60, 60]}, "prediction_text": "Sixteenth century."}
{"answers": {"text": ["1599", "1599", "1599", "1599", "1599"], "answer_start": [82, 82, 82, 82, 82]}, "prediction_text": "1599"}
{"answers": {"text": ["Queen Elizabeth", "Queen Elizabeth", "Queen Elizabeth", "Queen Elizabeth", "Queen Elizabeth"], "answer_start": [155, 155, 155, 155, 155]}, "prediction_text": "Queen Elizabeth"}
{"answers": {"text": ["exploitation", "political activity caused exploitation", "political activity caused exploitation", "exploitation", "exploitation"], "answer_start": [418, 392, 392, 418, 418]}, "prediction_text": "Plundering."}
{"answers": {"text": ["the Portuguese", "Portuguese", "Portuguese", "Portuguese", "Portuguese"], "answer_start": [322, 326, 326, 326, 326]}, "prediction_text": "Portugal"}
{"answers": {"text": ["1830", "1830", "1830", "1830", "1830"], "answer_start": [34, 34, 34, 34, 34]}, "prediction_text": "1830"}
{"answers": {"text": ["1850", "after 1850", "after 1850", "after 1850", "1850"], "answer_start": [98, 92, 92, 92, 98]}, "prediction_text": "1850"}
{"answers": {"text": ["Catholicism", "Catholicism", "Catholicism", "Catholicism", "Catholicism"], "answer_start": [609, 609, 609, 609, 609]}, "prediction_text": "Catholicism"}
{"answers": {"text": ["Africa", "North and West Africa", "North and West Africa", "North and West Africa, as well as South-East Asia, with other conquests in Central and East Africa, as well as the South Pacific", "North and West Africa, as well as South-East Asia,"], "answer_start": [144, 129, 129, 129, 129]}, "prediction_text": "North and West Africa."}
{"answers": {"text": ["when Germany started to build her own", "Germany started to build her own colonial empire", "when Germany started to build her own colonial empire", "when Germany started to build her own colonial empire", "when Germany started to build her own colonial empire."], "answer_start": [323, 328, 323, 323, 323]}, "prediction_text": "1850"}
{"answers": {"text": ["civilize the inferior", "civilize the inferior", "civilize the inferior", "to civilize the inferior", "civilize"], "answer_start": [290, 290, 290, 287, 290]}, "prediction_text": "civilize the inferior."}
{"answers": {"text": ["assimilation", "Full citizenship rights", "assimilation", "assimilation", "Full citizenship rights"], "answer_start": [342, 314, 342, 342, 314]}, "prediction_text": "citizenship rights."}
{"answers": {"text": ["small numbers of settlers", "sent small numbers of settlers to its colonies", "sent small numbers of settlers to its colonies,", "sent small numbers of settlers to its colonies", "small numbers of settlers"], "answer_start": [479, 474, 474, 474, 479]}, "prediction_text": "Small numbers of settlers."}
{"answers": {"text": ["Christianity and French culture", "Christianity", "Christianity and French culture", "Christianity and French culture"], "answer_start": [85, 85, 85, 85]}, "prediction_text": "Christianity and French culture."}
{"answers": {"text": ["Algeria", "Algeria", "Algeria", "Algeria"], "answer_start": [557, 557, 557, 557]}, "prediction_text": "Algeria"}
{"answers": {"text": ["overseas colonies", "overseas colonies", "overseas colonies", "overseas colonies", "overseas colonies"], "answer_start": [64, 64, 64, 64, 64]}, "prediction_text": "Overseas colonies."}
{"answers": {"text": ["anti-colonial movements", "anti-colonial movements", "anti-colonial movements", "anti-colonial movements", "anti-colonial movements"], "answer_start": [153, 153, 153, 153, 153]}, "prediction_text": "Anti-colonial movements."}
{"answers": {"text": ["Vietnam", "Vietnam", "Vietnam", "Vietnam", "Vietnam"], "answer_start": [247, 247, 247, 247, 247]}, "prediction_text": "Vietnam"}
{"answers": {"text": ["Algeria", "Algeria", "Algeria", "Algeria", "Algeria"], "answer_start": [297, 297, 297, 297, 297]}, "prediction_text": "Algeria"}
{"answers": {"text": ["1960", "1960", "1960", "1960", "1960"], "answer_start": [525, 525, 525, 525, 525]}, "prediction_text": "1960"}
{"answers": {"text": ["Scandinavia", "Scandinavia and northern Europe", "Scandinavia and northern Europe", "Scandinavia and northern Europe", "Scandinavia and northern Europe,"], "answer_start": [33, 33, 33, 33, 33]}, "prediction_text": "Scandinavia and northern Europe."}
{"answers": {"text": ["Muslim Iberia", "Muslim Iberia", "Muslim Iberia", "Muslim Iberia", "Muslim Iberia"], "answer_start": [738, 738, 738, 738, 738]}, "prediction_text": "Italy"}
{"answers": {"text": ["middle period of classical antiquity", "the middle period of classical antiquity", "the middle period of classical antiquity", "the middle period of classical antiquity", "middle period of classical antiquity"], "answer_start": [137, 133, 133, 133, 137]}, "prediction_text": "800 CE."}
{"answers": {"text": ["800 CE", "in late antiquity", "late antiquity", "late antiquity", "by 800 CE"], "answer_start": [254, 191, 194, 194, 251]}, "prediction_text": "800 CE."}
{"answers": {"text": ["central Europe", "amorphous area of central Europe", "an amorphous area of central Europe", "central Europe", "amorphous area of central Europe."], "answer_start": [906, 888, 885, 906, 888]}, "prediction_text": "Central Europe"}
{"answers": {"text": ["late 19th century", "late 19th century", "19th century", "late 19th century", "late 19th century."], "answer_start": [149, 149, 154, 149, 149]}, "prediction_text": "late 19th century."}
{"answers": {"text": ["1862", "1862", "1862", "1862", "1862"], "answer_start": [712, 712, 712, 712, 712]}, "prediction_text": "1862"}
{"answers": {"text": ["after the Franco-German War", "after the Franco-German War", "after the Franco-German War", "after the Franco-German War", "after the Franco-German War,"], "answer_start": [638, 638, 638, 638, 638]}, "prediction_text": "1871"}
{"answers": {"text": ["Napoleon", "Napoleon", "the defeat of Napoleon", "Napoleon", "defeat of Napoleon"], "answer_start": [356, 356, 342, 356, 346]}, "prediction_text": "Napoleon"}
{"answers": {"text": ["Europe", "Europe", "Europe", "Europe", "Europe itself."], "answer_start": [1092, 1092, 1092, 1092, 1092]}, "prediction_text": "Europe itself."}
{"answers": {"text": ["the South Pacific", "South Pacific", "South Pacific", "South Pacific", "South Pacific"], "answer_start": [75, 79, 79, 79, 79]}, "prediction_text": "South Pacific."}
{"answers": {"text": ["prestige", "prestige", "prestige", "prestige"], "answer_start": [329, 329, 329, 329]}, "prediction_text": "German prestige."}
{"answers": {"text": ["1884", "1884", "1884", "1884", "1884"], "answer_start": [526, 526, 526, 526, 526]}, "prediction_text": "1884"}
{"answers": {"text": ["New Guinea", "German New Guinea", "German New Guinea", "German New Guinea", "German New Guinea"], "answer_start": [512, 505, 505, 505, 505]}, "prediction_text": "Friedrichsruh"}
{"answers": {"text": ["Hamburg merchants and traders", "Hamburg merchants and traders", "Hamburg merchants and traders", "Hamburg merchants and traders"], "answer_start": [360, 360, 360, 360]}, "prediction_text": "Hamburg merchants and traders."}
{"answers": {"text": ["Japan took part of Sakhalin Island", "Japan took part of Sakhalin Island from Russia", "Japan took part of Sakhalin Island from Russia", "Japan took part of Sakhalin Island from Russia", "Japan took part of Sakhalin Island from Russia"], "answer_start": [114, 114, 114, 114, 114]}, "prediction_text": "Japan took part of Sakhalin Island from Russia. Korea was annexed in 1910. Japan took German-leased territories in China\u2019s Shandong Province. Japan took part of the Mariana, Caroline, and Marshall Islands. Japan also took part of parts of far eastern Russia and parts of eastern Siberia. Japan also took part of parts of far eastern Russia and parts of eastern Siberia as a participant in the Siberian Intervention. Japan also took part of parts of far eastern Russia and parts of eastern Siberia as a participant in the Siberian Intervention. Japan also took part of parts of far eastern Russia and parts of eastern Siberia as a participant in the Siberian Intervention. Japan also took part of parts of"}
{"answers": {"text": ["1894", "1894", "1894", "1894", "1894"], "answer_start": [38, 38, 38, 38, 38]}, "prediction_text": "1894"}
{"answers": {"text": ["Thailand", "Thailand", "Thailand", "Thailand", "Thailand"], "answer_start": [821, 821, 821, 821, 821]}, "prediction_text": "Thailand"}
{"answers": {"text": ["Manchuria", "Manchuria", "Manchuria", "Manchuria", "Manchuria"], "answer_start": [484, 484, 484, 484, 484]}, "prediction_text": "Manchuria"}
{"answers": {"text": ["China", "People\u2019s Republic of China", "the People\u2019s Republic of China", "People\u2019s Republic of China", "People\u2019s Republic of China"], "answer_start": [732, 711, 707, 711, 711]}, "prediction_text": "China"}
{"answers": {"text": ["1932", "1932", "1932", "1932", "1932"], "answer_start": [496, 496, 496, 496, 496]}, "prediction_text": "1923"}
{"answers": {"text": ["Lenin", "Lenin", "Lenin", "Lenin", "Lenin"], "answer_start": [153, 153, 153, 153, 153]}, "prediction_text": "Lenin"}
{"answers": {"text": ["Eastern Europe", "Eastern Europe", "in areas its forces occupied in Eastern Europe", "1919\u201320", "Eastern Europe"], "answer_start": [670, 670, 638, 604, 670]}, "prediction_text": "Eastern Europe."}
{"answers": {"text": ["Bolshevik leaders", "Bolshevik leaders", "Bolshevik leaders", "Bolshevik leaders"], "answer_start": [0, 0, 0, 0]}, "prediction_text": "Bolshevik leaders"}
{"answers": {"text": ["a world revolution", "world revolution", "a world revolution", "a world revolution", "world revolution."], "answer_start": [90, 92, 90, 90, 92]}, "prediction_text": "World revolution."}
{"answers": {"text": ["Lenin", "Lenin", "Lenin", "Lenin", "Lenin"], "answer_start": [110, 110, 110, 110, 110]}, "prediction_text": "Joseph Stalin"}
{"answers": {"text": ["Mao Zedong", "Mao Zedong", "Mao Zedong", "Mao Zedong", "Sultan Galiev and Vasyl Shakhrai"], "answer_start": [1519, 1519, 1519, 1519, 1817]}, "prediction_text": "Mao Zedong"}
{"answers": {"text": ["Nikita Khrushchev", "Nikita Khrushchev", "Nikita Khrushchev", "Nikita Khrushchev", "Khrushchev"], "answer_start": [751, 751, 751, 751, 758]}, "prediction_text": "Nikita Khrushchev"}
{"answers": {"text": ["socialism in one country", "socialism", "socialism", "socialism", "socialism in one country'"], "answer_start": [281, 281, 281, 281, 281]}, "prediction_text": "Socialism in one country."}
{"answers": {"text": ["mercantilism", "mercantilism", "mercantilism", "mercantilism", "mercantilism"], "answer_start": [38, 38, 38, 38, 38]}, "prediction_text": "mercantilism"}
{"answers": {"text": ["1776", "1776", "1776", "1776", "1776"], "answer_start": [205, 205, 205, 205, 205]}, "prediction_text": "1776"}
{"answers": {"text": ["free trade", "free trade", "free trade", "free trade", "free trade"], "answer_start": [500, 500, 500, 500, 500]}, "prediction_text": "Free trade."}
{"answers": {"text": ["about 1820", "1820", "1820", "1820", "1820"], "answer_start": [424, 430, 430, 430, 430]}, "prediction_text": "1820"}
{"answers": {"text": ["1815", "1815", "1815", "1815", "1815"], "answer_start": [735, 735, 735, 735, 735]}, "prediction_text": "1815"}
{"answers": {"text": ["The British Empire", "British Empire", "The British", "British", "British"], "answer_start": [494, 498, 122, 498, 498]}, "prediction_text": "Britain"}
{"answers": {"text": ["pseudo-sciences", "pseudo-sciences", "pseudo-sciences", "British Empire", "pseudo"], "answer_start": [258, 258, 258, 498, 258]}, "prediction_text": "pseudo-sciences"}
{"answers": {"text": ["The British spirit of imperialism", "imperialism", "Social Darwinism", "imperialism"], "answer_start": [122, 144, 277, 144]}, "prediction_text": "British Empire"}
{"answers": {"text": ["Middle East", "Middle East", "the Middle East", "Africa", "Middle East."], "answer_start": [109, 109, 105, 66, 109]}, "prediction_text": "Africa"}
{"answers": {"text": ["the Monroe Doctrine", "through policies", "policies such as the Monroe Doctrine", "policies such as the Monroe Doctrine", "Monroe Doctrine"], "answer_start": [149, 124, 132, 132, 153]}, "prediction_text": "Through military force."}
{"answers": {"text": ["interventionism", ".", "interventionism", "interventionism", "interventionism"], "answer_start": [268, 1200, 268, 268, 268]}, "prediction_text": "interventionism"}
{"answers": {"text": ["a war erupted", "a war erupted", "war", "war", "war"], "answer_start": [727, 727, 729, 729, 729]}, "prediction_text": "The deaths of many Filipinos."}
{"answers": {"text": ["the Philippines", "Philippines", "Philippines", "Philippines", "Philippines"], "answer_start": [685, 689, 689, 689, 689]}, "prediction_text": "Philippines"}
{"answers": {"text": ["a \"racket\"", "racket", "racket", "a \"racket\"", "racket"], "answer_start": [979, 982, 982, 979, 982]}, "prediction_text": "Racket"}
{"answers": {"text": ["Isiah Bowman", "Isiah Bowman", "Isiah Bowman", "Isiah Bowman", "Isiah Bowman"], "answer_start": [103, 103, 103, 103, 103]}, "prediction_text": "Isiah Bowman"}
{"answers": {"text": ["1917", "1917", "1917", "1917", "1917"], "answer_start": [208, 277, 208, 208, 208]}, "prediction_text": "1917"}
{"answers": {"text": ["American delegation from the Paris Peace Conference", "American delegation from the Paris Peace Conference", "the American delegation from the Paris Peace Conference", "American delegation from the Paris Peace Conference", "American delegation from the Paris Peace Conference"], "answer_start": [336, 336, 332, 336, 336]}, "prediction_text": "Isiah Bowman"}
{"answers": {"text": ["U.S authorship of a 'new world'", "allow for U.S authorship of a 'new world' which was to be characterized by geographical order", "allow for U.S authorship of a 'new world' which was to be characterized by geographical order", "allow for U.S authorship of a 'new world'", "U.S authorship of a 'new world'"], "answer_start": [459, 449, 449, 449, 459]}, "prediction_text": "Build a premise that would allow for U.S authorship of a 'new world' which was to be characterized by geographical order."}
{"answers": {"text": ["Wilson's geographer", "Wilson's geographer", "Wilson's geographer", "Wilson's geographer", "Wilson's geographer."], "answer_start": [623, 623, 623, 623, 623]}, "prediction_text": "Wilson's geographer"}
{"answers": {"text": ["internal strife", "internal strife", "internal strife", "internal strife", "internal strife"], "answer_start": [24, 24, 24, 24, 24]}, "prediction_text": "Internal colonialism."}
{"answers": {"text": ["\"internal colonialism\"", "internal colonialism", "internal colonialism", "internal colonialism", "internal"], "answer_start": [560, 561, 561, 561, 116]}, "prediction_text": "internal colonialism"}
{"answers": {"text": ["12 to 15 million", "12 to 15 million", "12 to 15 million", "12 to 15 million", "12 to 15 million"], "answer_start": [661, 661, 661, 661, 661]}, "prediction_text": "12 to 15 million."}
{"answers": {"text": ["the contemporary Orient", "the contemporary Orient", "the contemporary Orient", "contemporary Orient", "contemporary Orient, \""], "answer_start": [1213, 1213, 1213, 1217, 1217]}, "prediction_text": "The contemporary Orient."}
{"answers": {"text": ["1923", "1923", "1923", "1923", "1923"], "answer_start": [66, 66, 66, 66, 66]}, "prediction_text": "1923"}
{"answers": {"text": ["Suleiman the Magnificent", "Suleiman the Magnificent", "Suleiman the Magnificent", "Suleiman the Magnificent", "Suleiman the Magnificent,"], "answer_start": [168, 168, 168, 168, 168]}, "prediction_text": "Suleiman the Magnificent"}
{"answers": {"text": ["32", "32", "32", "32", "32"], "answer_start": [425, 425, 425, 425, 425]}, "prediction_text": "32 provinces."}
{"answers": {"text": ["Europe", "Europe", "Europe", "Europe", "Europe"], "answer_start": [293, 293, 293, 293, 293]}, "prediction_text": "Europe"}
{"answers": {"text": ["During the 16th and 17th centuries", "17th century", "the 16th and 17th centuries", "16th and 17th centuries", "16th and 17th centuries"], "answer_start": [72, 391, 79, 83, 83]}, "prediction_text": "16th century."}
{"answers": {"text": ["Istanbul", "Istanbul", "Istanbul", "Istanbul", "Istanbul"], "answer_start": [5, 5, 5, 5, 5]}, "prediction_text": "Istanbul"}
{"answers": {"text": ["Germany", "Germany", "Germany", "Germany", "Germany"], "answer_start": [358, 358, 358, 358, 358]}, "prediction_text": "Germany"}
{"answers": {"text": ["World War I", "creation of modern Balkan and Middle Eastern states", "the creation of modern Balkan and Middle Eastern states", "creation of modern Balkan and Middle Eastern states", "creation of modern Balkan and Middle Eastern states"], "answer_start": [493, 609, 605, 609, 609]}, "prediction_text": "World War I"}
{"answers": {"text": ["Turkey", "Turkey", "Turkey", "Turkey", "Turkey"], "answer_start": [551, 551, 551, 551, 551]}, "prediction_text": "Germany"}
{"answers": {"text": ["United Methodist Church", "United Methodist Church (UMC)", "United Methodist Church"], "answer_start": [4, 4, 4]}, "prediction_text": "United Methodist Church"}
{"answers": {"text": ["mainline Protestant Methodist denomination", "is a mainline Protestant Methodist denomination", "a mainline Protestant Methodist denomination"], "answer_start": [39, 34, 37]}, "prediction_text": "United Methodist Church"}
{"answers": {"text": ["1968", "mainline Protestant Methodist denomination", "1968"], "answer_start": [167, 39, 167]}, "prediction_text": "1968"}
{"answers": {"text": ["union of the Methodist Church (USA) and the Evangelical United Brethren Church", "John and Charles Wesley in England", "the union of the Methodist Church (USA) and the Evangelical United Brethren Church"], "answer_start": [179, 316, 175]}, "prediction_text": "John and Charles Wesley"}
{"answers": {"text": ["Wesleyan", "orientation is decidedly Wesleyan", "Wesleyan"], "answer_start": [463, 438, 463]}, "prediction_text": "Wesleyan"}
{"answers": {"text": ["United Methodist Church", "The United Methodist Church", "The United Methodist Church"], "answer_start": [4, 0, 0]}, "prediction_text": "United Methodist Church"}
{"answers": {"text": ["80 million", "80 million adherents", "80 million"], "answer_start": [117, 117, 117]}, "prediction_text": "80 million."}
{"answers": {"text": ["mainline Protestant denomination", "Protestant denomination", "mainline Protestant denomination"], "answer_start": [207, 216, 207]}, "prediction_text": "Protestant denomination"}
{"answers": {"text": ["3.6%", "3.6% of the U.S population, or", "3.6%"], "answer_start": [638, 638, 638]}, "prediction_text": "3.6%"}
{"answers": {"text": ["mid-18th century", "began in the mid-18th century within the Church of England.", "mid-18th century"], "answer_start": [73, 60, 73]}, "prediction_text": "mid-18th century"}
{"answers": {"text": ["within the Church of England", "the Church of England.", "the Church of England"], "answer_start": [90, 97, 97]}, "prediction_text": "Oxford University campus."}
{"answers": {"text": ["being methodical and exceptionally detailed in their Bible study", "They focused on Bible study, methodical study of scripture and living a holy life", "being methodical and exceptionally detailed in their Bible study, opinions and disciplined lifestyle."], "answer_start": [411, 245, 411]}, "prediction_text": "Methodical and detailed in their Bible study, opinions, and disciplined lifestyle."}
{"answers": {"text": ["1735", "In 1735", "1735"], "answer_start": [3, 0, 3]}, "prediction_text": "1735"}
{"answers": {"text": ["colony of Georgia", "the colony of Georgia", "America"], "answer_start": [100, 96, 41]}, "prediction_text": "Georgia"}
{"answers": {"text": ["American Indians", "to the American Indians", "American Indians"], "answer_start": [76, 69, 76]}, "prediction_text": "American Indians"}
{"answers": {"text": ["salvation by God's grace", "plain, old, Bible Christianity\"", "salvation by God's grace"], "answer_start": [629, 421, 629]}, "prediction_text": "Salvation by God's grace."}
{"answers": {"text": ["American Revolution", "the American Revolution", "the American Revolution"], "answer_start": [94, 90, 90]}, "prediction_text": "The American Revolution."}
{"answers": {"text": ["1784", "In 1784", "1784"], "answer_start": [231, 228, 231]}, "prediction_text": "1784"}
{"answers": {"text": ["Thomas Coke", "Thomas Coke", "Thomas Coke"], "answer_start": [390, 390, 390]}, "prediction_text": "Thomas Coke"}
{"answers": {"text": ["Lovely Lane Methodist Church", "held at the Lovely Lane Methodist Church", "Lovely Lane Methodist Church"], "answer_start": [736, 724, 736]}, "prediction_text": "Lovely Lane Methodist Church"}
{"answers": {"text": ["Lovely Lane Methodist Church", "Lovely Lane Methodist Church,", "Lovely Lane Methodist Church"], "answer_start": [736, 736, 736]}, "prediction_text": "Lovely Lane Methodist Church"}
{"answers": {"text": ["St. George's United Methodist Church", "St. George's United Methodist Church,", "St. George's United Methodist Church"], "answer_start": [0, 0, 0]}, "prediction_text": "St. George's United Methodist Church"}
{"answers": {"text": ["St. George's United Methodist Church", "St. George's United Methodist Church", "St. George's United Methodist Church"], "answer_start": [0, 0, 0]}, "prediction_text": "St. George's United Methodist Church"}
{"answers": {"text": ["1767", "congregation was founded in 1767,", "1767"], "answer_start": [252, 224, 252]}, "prediction_text": "1767"}
{"answers": {"text": ["sail loft on Dock Street", "meeting initially in a sail loft on Dock Street", "Dock Street"], "answer_start": [281, 258, 294]}, "prediction_text": "Dock Street"}
{"answers": {"text": ["1784", "the Methodist Episcopal Church was not founded until 1784.", "1784"], "answer_start": [553, 500, 553]}, "prediction_text": "1784"}
{"answers": {"text": ["Richard Allen and Absalom Jones", "Richard Allen", "Richard Allen and Absalom Jones"], "answer_start": [0, 0, 0]}, "prediction_text": "Richard Allen"}
{"answers": {"text": ["St. George's Church", "St. George's Church", "St. George's Church"], "answer_start": [123, 123, 123]}, "prediction_text": "St. George's Church"}
{"answers": {"text": ["1784", "1784", "1784"], "answer_start": [146, 146, 146]}, "prediction_text": "1784"}
{"answers": {"text": ["1830", "In 1830", "1830"], "answer_start": [160, 157, 160]}, "prediction_text": "1830"}
{"answers": {"text": ["issue of laity having a voice and vote in the administration of the church", "over the issue of laity having a voice and vote in the administration of the church", "over the issue of laity having a voice and vote in the administration of the church"], "answer_start": [249, 240, 240]}, "prediction_text": "Laity having a voice and vote in the administration of the church."}
{"answers": {"text": ["1844", "In 1844,", "1844"], "answer_start": [442, 439, 442]}, "prediction_text": "1844"}
{"answers": {"text": ["because of tensions over slavery and the power of bishops in the denomination", "because of tensions over slavery and the power of bishops in the denomination.", "tensions over slavery and the power of bishops in the denomination"], "answer_start": [532, 532, 543]}, "prediction_text": "Tensions over slavery and the power of bishops."}
{"answers": {"text": ["April 23, 1968", "On April 23, 1968", "1968"], "answer_start": [3, 0, 13]}, "prediction_text": "1968"}
{"answers": {"text": ["constituting General Conference in Dallas, Texas", "the constituting General Conference in Dallas, Texas.", "Dallas"], "answer_start": [233, 229, 268]}, "prediction_text": "Dallas, Texas"}
{"answers": {"text": ["Bishop Lloyd Christ Wicke", "Bishop Lloyd Christ", "Bishop Reuben H. Mueller"], "answer_start": [186, 186, 119]}, "prediction_text": "Bishop Lloyd Christ Wicke"}
{"answers": {"text": ["holy catholic (or universal) church", "the holy catholic (or universal) church", "holy catholic (or universal) church"], "answer_start": [65, 61, 65]}, "prediction_text": "The United Methodist Church"}
{"answers": {"text": ["The Book of Discipline", ". The Book of Discipline", "Book of Discipline"], "answer_start": [243, 241, 247]}, "prediction_text": "Book of Discipline"}
{"answers": {"text": ["meaning that all who are truly believers in every age belong to the holy Church invisible", "meaning that all who are truly believers in every age belong to the holy Church invisible, while the United Methodist Church is a branch of the Church visible,", "all who are truly believers in every age belong to the holy Church invisible, while the United Methodist Church is a branch of the Church visible,"], "answer_start": [414, 414, 427]}, "prediction_text": "The concept of the visible and invisible Church."}
{"answers": {"text": ["result of the American Revolution", "As a result of the American Revolution,", "the American Revolution"], "answer_start": [130, 125, 140]}, "prediction_text": "To break with standard practice and ordain two of his lay preachers as presbyters."}
{"answers": {"text": ["Dr. Thomas Coke", "Dr. Thomas Coke", "Dr. Thomas Coke"], "answer_start": [317, 317, 317]}, "prediction_text": "Dr. Thomas Coke"}
{"answers": {"text": ["Thomas Vasey and Richard Whatcoat.", "Thomas Vasey and Richard Whatcoat", "Thomas Vasey and Richard Whatcoat"], "answer_start": [282, 282, 282]}, "prediction_text": "Thomas Vasey and Richard Whatcoat."}
{"answers": {"text": ["1968", "1968", "1968"], "answer_start": [529, 529, 529]}, "prediction_text": "1968"}
{"answers": {"text": ["John Wesley and Charles Wesley", "John Wesley and Charles Wesley", "John Wesley and Charles Wesley"], "answer_start": [316, 316, 316]}, "prediction_text": "John Wesley and Charles Wesley."}
{"answers": {"text": ["Albert C. Outler", "theologian Albert C. Outler", "Albert C. Outler"], "answer_start": [546, 535, 546]}, "prediction_text": "Albert C. Outler"}
{"answers": {"text": ["Albert C. Outler", "Albert C. Outler"], "answer_start": [546, 546]}, "prediction_text": "Albert C. Outler"}
{"answers": {"text": ["Prevenient grace", "Prevenient grace,", "Prevenient grace"], "answer_start": [0, 0, 0]}, "prediction_text": "Prevenient grace."}
{"answers": {"text": ["Prevenient grace", "God", "Prevenient grace"], "answer_start": [0, 225, 0]}, "prediction_text": "Jesus Christ"}
{"answers": {"text": ["the grace that \"goes before\" us", "It is that power which enables us to love and motivates us to seek a relationship with God through Jesus Christ.", "that power which enables us to love and motivates us to seek a relationship with God through Jesus Christ."], "answer_start": [21, 78, 84]}, "prediction_text": "Grace."}
{"answers": {"text": ["Prevenient grace", "sin", "sin"], "answer_start": [415, 249, 456]}, "prediction_text": "God's offer of salvation."}
{"answers": {"text": ["Justifying Grace or Accepting Grace", "Justifying Grace or Accepting Grace", "Justifying Grace"], "answer_start": [0, 0, 0]}, "prediction_text": "Justifying Grace"}
{"answers": {"text": ["justifying grace", "The justifying grace", "justifying grace"], "answer_start": [339, 335, 339]}, "prediction_text": "Justifying grace."}
{"answers": {"text": ["conversion", "conversion", "conversion"], "answer_start": [494, 494, 494]}, "prediction_text": "Conversion."}
{"answers": {"text": ["conversion", "conversion, \"accepting Jesus as your personal Lord and Savior,\" or being \"born again", "justifying grace"], "answer_start": [494, 494, 460]}, "prediction_text": "New Birth"}
{"answers": {"text": ["New Birth", "John Wesley originally called this experience the New Birth.", "New Birth"], "answer_start": [631, 581, 631]}, "prediction_text": "New Birth."}
{"answers": {"text": ["grace of God which sustains the believers in the journey toward Christian Perfection", "is that grace of God which sustains the believers in the journey toward Christian Perfection", "that grace of God which sustains the believers in the journey toward Christian Perfection"], "answer_start": [26, 18, 21]}, "prediction_text": "Sanctifying Grace is a state of grace in which the believer is enabled to respond to God by leading a Spirit-filled and Christ-like life aimed toward love."}
{"answers": {"text": ["Sanctifying Grace", "a genuine love of God with heart, soul, mind, and strength,", "Sanctifying Grace"], "answer_start": [0, 112, 0]}, "prediction_text": "Sanctifying Grace"}
{"answers": {"text": ["a genuine love of God with heart, soul, mind, and strength, and a genuine love of our neighbors as ourselves", "leading a Spirit-filled and Christ-like life aimed toward love", "a genuine love of God with heart, soul, mind"], "answer_start": [112, 272, 112]}, "prediction_text": "Christian Perfection is a state of love toward God and toward our neighbors."}
{"answers": {"text": ["Christian Perfection", "Christian Perfection", "Christian Perfection"], "answer_start": [90, 90, 90]}, "prediction_text": "Sanctifying grace."}
{"answers": {"text": ["Wesleyan theology", "Wesleyan theology", "Wesleyan theology"], "answer_start": [0, 0, 0]}, "prediction_text": "Wesleyan Quadrilateral."}
{"answers": {"text": ["prima scriptura", "Wesleyan theology s", "prima scriptura"], "answer_start": [383, 0, 383]}, "prediction_text": "United Methodist Church"}
{"answers": {"text": ["UMC", "the UMC", "the UMC"], "answer_start": [726, 722, 722]}, "prediction_text": "United Methodist Church"}
{"answers": {"text": ["Book of Discipline", "Book of Discipline", "The Book of Discipline"], "answer_start": [619, 619, 615]}, "prediction_text": "The Book of Discipline."}
{"answers": {"text": ["2008", "2008", "2008"], "answer_start": [604, 604, 604]}, "prediction_text": "2008 General Conference."}
{"answers": {"text": ["pro-choice", "pro-choice", "pro-choice"], "answer_start": [502, 502, 502]}, "prediction_text": "Pro-life."}
{"answers": {"text": ["Religious Coalition for Reproductive Choice", "Religious Coalition for Reproductive Choice.", "the Religious Coalition for Reproductive Choice"], "answer_start": [552, 552, 548]}, "prediction_text": "Religious Coalition for Reproductive Choice"}
{"answers": {"text": ["The General Board of Church and Society, and the United Methodist Women", "two official bodies of the United Methodist Church", "The General Board of Church and Society, and the United Methodist Women"], "answer_start": [544, 410, 544]}, "prediction_text": "The General Board of Church and Society, and the United Methodist Women."}
{"answers": {"text": ["all women", "unacceptable pregnancy. In", "women"], "answer_start": [822, 166, 826]}, "prediction_text": "Women"}
{"answers": {"text": ["the mother", "supportive ministry with all women,", "the mother, for whom devastating damage may result from an unacceptable pregnancy"], "answer_start": [107, 797, 107]}, "prediction_text": "Mother."}
{"answers": {"text": ["Taskforce of United Methodists on Abortion and Sexuality (", "United Methodists on Abortion and Sexuality (TUMAS)", "Members of the United Methodist Church"], "answer_start": [103, 116, 0]}, "prediction_text": "TUMAS"}
{"answers": {"text": ["2012", "2012", "2012"], "answer_start": [383, 383, 383]}, "prediction_text": "2012"}
{"answers": {"text": ["Rev. Paul T. Stallsworth", "Rev. Paul T. Stallsworth", "Rev. Paul T. Stallsworth"], "answer_start": [507, 507, 507]}, "prediction_text": "Rev. Paul T. Stallsworth"}
{"answers": {"text": ["temperance movement", "temperance movement.", "the temperance movement"], "answer_start": [53, 53, 49]}, "prediction_text": "Temperance movement"}
{"answers": {"text": ["2011 and 2012", "in 2011 and 2012", "2011 and 2012"], "answer_start": [784, 781, 784]}, "prediction_text": "2011 and 2012."}
{"answers": {"text": ["The Use of Money", "The Use of Money,", "\"The Use of Money,\""], "answer_start": [148, 148, 147]}, "prediction_text": "The Use of Money"}
{"answers": {"text": ["unfermented grape juice", "uses unfermented grape juice", "unfermented grape juice"], "answer_start": [548, 543, 548]}, "prediction_text": "Unfermented grape juice."}
{"answers": {"text": ["capital punishment", "capital punishment", "capital punishment"], "answer_start": [75, 75, 75]}, "prediction_text": "Capital punishment."}
{"answers": {"text": ["John 8:7.", "John 8:7", "John 8:7"], "answer_start": [555, 555, 555]}, "prediction_text": "John 8:7"}
{"answers": {"text": ["Matthew 5:38-39", "Matthew 5:38-39", "Matthew 5:38-39"], "answer_start": [504, 504, 504]}, "prediction_text": "Matthew 5:38-39 and John 8:7."}
{"answers": {"text": ["The General Conference", "The General Conference of the United Methodist Church", "The General Conference"], "answer_start": [565, 565, 565]}, "prediction_text": "General Conference of the United Methodist Church"}
{"answers": {"text": ["same-sex unions", "prohibits the celebration of same-sex unions.", "same-sex"], "answer_start": [70, 41, 70]}, "prediction_text": "Same-sex unions."}
{"answers": {"text": ["1999", "1999", "1999"], "answer_start": [161, 161, 161]}, "prediction_text": "1999"}
{"answers": {"text": ["2016", "2016", "2016"], "answer_start": [781, 781, 781]}, "prediction_text": "2016"}
{"answers": {"text": ["Connectional Table", "the Connectional Table", "the Connectional Table"], "answer_start": [428, 424, 424]}, "prediction_text": "Connectional Table"}
{"answers": {"text": ["LGBT", "LGBT community", "LGBT community"], "answer_start": [380, 380, 380]}, "prediction_text": "LGBT community."}
{"answers": {"text": ["same-gender marriages with resolutions", "permit ministers to officiate same-sex weddings", "same-gender marriages"], "answer_start": [724, 551, 724]}, "prediction_text": "Same-gender marriages."}
{"answers": {"text": ["1987", "1987", "1987"], "answer_start": [3, 3, 3]}, "prediction_text": "1987"}
{"answers": {"text": ["2005", "2005", "2005"], "answer_start": [150, 150, 150]}, "prediction_text": "2005"}
{"answers": {"text": ["Baltimore-Washington Conference of the UMC", "The Baltimore-Washington Conference of the UMC", "The Baltimore-Washington Conference of the UMC"], "answer_start": [953, 949, 949]}, "prediction_text": "Baltimore-Washington Conference of the UMC"}
{"answers": {"text": ["conscription", "conscription", "conscription"], "answer_start": [36, 36, 36]}, "prediction_text": "Conscription."}
{"answers": {"text": ["the way of military action", "persons who conscientiously oppose all war", "military action"], "answer_start": [592, 162, 603]}, "prediction_text": "military action, inaction"}
{"answers": {"text": ["all war", "war", "war"], "answer_start": [197, 201, 201]}, "prediction_text": "War."}
{"answers": {"text": ["Christ's message and teachings", "is incompatible with Christ's message and teachings.", "Christ's message and teachings"], "answer_start": [68, 47, 68]}, "prediction_text": "Christ's message and teachings."}
{"answers": {"text": ["instrument of national foreign policy", "national foreign policy", "war"], "answer_start": [140, 154, 130]}, "prediction_text": "national foreign policy"}
{"answers": {"text": ["general and complete disarmament", "evils as genocide, brutal suppression of human rights, and unprovoked international aggression", "general and complete disarmament"], "answer_start": [845, 242, 845]}, "prediction_text": "General and complete disarmament."}
{"answers": {"text": ["The Sexual Ethics Task Force of The United Methodist Church", "The Sexual Ethics Task Force of The United Methodist Church", "The Sexual Ethics Task Force of The United Methodist Church"], "answer_start": [195, 195, 195]}, "prediction_text": "The Sexual Ethics Task Force of The United Methodist Church."}
{"answers": {"text": ["violence, degradation, exploitation, and coercion", "about violence, degradation, exploitation, and coercion\"", "violence, degradation, exploitation, and coercion"], "answer_start": [63, 57, 63]}, "prediction_text": "Violence, degradation, exploitation, and coercion."}
{"answers": {"text": ["girls and women", "relationships with parishioners and family, and their perceptions of girls and women.\"", "girls and women"], "answer_start": [536, 467, 536]}, "prediction_text": "Girls and women."}
{"answers": {"text": ["IVF", "created for IVF that remain after the procreative efforts have ceased,", "research"], "answer_start": [69, 57, 285]}, "prediction_text": "IVF."}
{"answers": {"text": ["stem cells", "stem cells", "stem cells"], "answer_start": [485, 485, 485]}, "prediction_text": "Stem cells."}
{"answers": {"text": ["research", "sake of research", "research"], "answer_start": [377, 369, 377]}, "prediction_text": "Research."}
{"answers": {"text": ["Sunday Service of the Methodists in North America", "Sunday Service of the Methodists in North America", "the Sunday Service of the Methodists in North America"], "answer_start": [402, 402, 398]}, "prediction_text": "Sunday Service of the Methodists in North America."}
{"answers": {"text": ["When the Methodists in America were separated from the Church of England", "When the Methodists in America were separated from the Church of England,", "When the Methodists in America were separated from the Church of England"], "answer_start": [241, 241, 241]}, "prediction_text": "Sunday Service of the Methodists in North America."}
{"answers": {"text": ["The Book of Common Prayer", "Book of Common Prayer", "The Book of Common Prayer"], "answer_start": [365, 369, 365]}, "prediction_text": "The Book of Common Prayer."}
{"answers": {"text": ["Africa", "The United Methodist Church in Africa.", "Africa"], "answer_start": [346, 315, 346]}, "prediction_text": "Africa"}
{"answers": {"text": ["Book of Common Prayer", "United Methodist Book of Worship", "Book of Common Prayer."], "answer_start": [750, 643, 750]}, "prediction_text": "Book of Common Prayer"}
{"answers": {"text": ["anointing with oil", "anointing with oil.", "anointing with oil"], "answer_start": [404, 404, 404]}, "prediction_text": "anointing with oil."}
{"answers": {"text": ["Methodist institutions", "Methodist institutions", "Methodist institutions"], "answer_start": [0, 0, 0]}, "prediction_text": "Methodist institutions."}
{"answers": {"text": ["William Booth", "William Booth", "William Booth"], "answer_start": [528, 528, 528]}, "prediction_text": "William Booth"}
{"answers": {"text": ["John Wesley", "John Wesley", "John Wesley"], "answer_start": [612, 612, 612]}, "prediction_text": "John Wesley"}
{"answers": {"text": ["United Methodist Church", "The United Methodist Church", "The United Methodist Church"], "answer_start": [4, 0, 0]}, "prediction_text": "United Methodist Church"}
{"answers": {"text": ["General Conference", "General Conference a", "the General Conference"], "answer_start": [91, 91, 87]}, "prediction_text": "General Conference"}
{"answers": {"text": ["The Book of Discipline", "The Book of Discipline", "The Book of Discipline"], "answer_start": [279, 279, 279]}, "prediction_text": "The Book of Discipline."}
{"answers": {"text": ["General Conference", "General Conference", "the General Conference"], "answer_start": [91, 91, 87]}, "prediction_text": "General Conference"}
{"answers": {"text": ["every four years", "every four years (quadrennium).", "every four years"], "answer_start": [211, 211, 211]}, "prediction_text": "Every four years."}
{"answers": {"text": ["five", "The United States is divided into five jurisdictions", "five"], "answer_start": [153, 119, 153]}, "prediction_text": "Five."}
{"answers": {"text": ["seven", "seven central conferences: Africa, Congo, West Africa, Central & Southern Europe, Germany, Northern Europe and the Philippines.", "seven"], "answer_start": [296, 296, 296]}, "prediction_text": "Seven."}
{"answers": {"text": ["to elect and appoint bishops", "to elect and appoint bishops", "elect and appoint bishops"], "answer_start": [489, 489, 492]}, "prediction_text": "Elect and appoint bishops."}
{"answers": {"text": ["bishops", "bishops", "bishops"], "answer_start": [510, 510, 510]}, "prediction_text": "Bishops"}
{"answers": {"text": ["Episcopal Areas", "Episcopal Areas,", "Episcopal Areas"], "answer_start": [586, 586, 586]}, "prediction_text": "Episcopal Areas"}
{"answers": {"text": ["Mission Council", "the Mission Council (usually consisting of church bishops)", "the Mission Council"], "answer_start": [60, 56, 56]}, "prediction_text": "Mission Council"}
{"answers": {"text": ["church bishops", "church bishops)", "church bishops"], "answer_start": [99, 99, 99]}, "prediction_text": "Church bishops"}
{"answers": {"text": ["36", "36 acres", "36"], "answer_start": [314, 314, 314]}, "prediction_text": "36 acres."}
{"answers": {"text": ["for the George W. Bush Presidential Library", "the George W. Bush Presidential Library", "the George W. Bush Presidential Library"], "answer_start": [369, 373, 373]}, "prediction_text": "To the George W. Bush Presidential Library."}
{"answers": {"text": ["Southern Methodist University", "Southern Methodist University"], "answer_start": [339, 339]}, "prediction_text": "Southern Methodist University"}
{"answers": {"text": ["nine", "nine members,", "nine"], "answer_start": [78, 78, 78]}, "prediction_text": "Nine."}
{"answers": {"text": ["Judicial Council", "The Judicial Council", "The Judicial Council"], "answer_start": [4, 0, 0]}, "prediction_text": "The Judicial Council"}
{"answers": {"text": ["eight-year term", "eight-year term", "eight-year"], "answer_start": [156, 156, 156]}, "prediction_text": "Eight years."}
{"answers": {"text": ["twice a year", "meets twice a year at", "every eight years"], "answer_start": [766, 760, 213]}, "prediction_text": "Twice a year."}
{"answers": {"text": ["various locations throughout the world", "at various locations throughout the world.", "various locations throughout the world"], "answer_start": [782, 779, 782]}, "prediction_text": "Various locations throughout the world."}
{"answers": {"text": ["The Annual Conference", "synod", "The Annual Conference"], "answer_start": [0, 120, 0]}, "prediction_text": "Annual Conference"}
{"answers": {"text": ["geographical area it covers as well as the frequency of meeting", "geographical area it covers as well as the frequency of meeting. Clergy are members of their Annual Conference rather than of any local congregation,", "the geographical area it covers"], "answer_start": [316, 316, 312]}, "prediction_text": "The geographical area it covers."}
{"answers": {"text": ["their Annual Conference", "Annual Conference", "their Annual Conference"], "answer_start": [403, 409, 403]}, "prediction_text": "Annual Conference"}
{"answers": {"text": ["The Book of Discipline", "The Book of Discipline", "The Book of Discipline"], "answer_start": [0, 0, 0]}, "prediction_text": "The Book of Discipline."}
{"answers": {"text": ["three", "three members", "three"], "answer_start": [233, 233, 233]}, "prediction_text": "Three."}
{"answers": {"text": ["nine", "no more than nine members", "nine"], "answer_start": [264, 251, 264]}, "prediction_text": "Three."}
{"answers": {"text": ["church conference", "church conference", "The church conference"], "answer_start": [648, 648, 644]}, "prediction_text": "The church conference."}
{"answers": {"text": ["church conference", "church conference", "The church conference"], "answer_start": [648, 648, 644]}, "prediction_text": "Church conference"}
{"answers": {"text": ["one hundred", "one hundred colleges and universities", "around one hundred"], "answer_start": [189, 189, 182]}, "prediction_text": "One hundred."}
{"answers": {"text": ["three hundred sixty", "three hundred sixty schools and institutions overseas.", "three hundred"], "answer_start": [562, 562, 562]}, "prediction_text": "Three hundred sixty."}
{"answers": {"text": ["International Association of Methodist-related Schools, Colleges, and Universities", "members of the International Association of Methodist-related Schools, Colleges, and Universities. The church operates three hundred sixty scho", "the International Association of Methodist-related Schools, Colleges, and Universities"], "answer_start": [458, 443, 454]}, "prediction_text": "International Association of Methodist-related Schools, Colleges, and Universities."}
{"answers": {"text": ["John Wesley", "John Wesley,", "John Wesley"], "answer_start": [44, 44, 44]}, "prediction_text": "John Wesley"}
{"answers": {"text": ["pastors", "appointed to various ministries.", "pastors"], "answer_start": [510, 338, 510]}, "prediction_text": "Pastors"}
{"answers": {"text": ["Annual Conference Order of Elders", "a member of their Annual Conference Order of Elders.", "Annual Conference Order of Elders"], "answer_start": [898, 880, 898]}, "prediction_text": "Annual Conference Order of Elders."}
{"answers": {"text": ["Annual Conference Order of Deacons", "Annual Conference Order of Deacons", "Annual Conference Order of Deacons"], "answer_start": [994, 994, 994]}, "prediction_text": "Their Annual Conference Order of Deacons."}
{"answers": {"text": ["Annual Conference Cabinet", "All clergy appointments a", "the Annual Conference Cabinet"], "answer_start": [96, 0, 92]}, "prediction_text": "Annual Conference Cabinet"}
{"answers": {"text": ["one year at a time", "one year at a time,", "one year"], "answer_start": [526, 526, 526]}, "prediction_text": "One year."}
{"answers": {"text": ["bishop has read the appointments at the session of the Annual Conference", "Until the bishop has read the appointments at the session of the Annual Conference,", "the bishop has read the appointments at the session of the Annual Conference"], "answer_start": [282, 272, 278]}, "prediction_text": "The bishop reads the appointments at the session of the Annual Conference."}
{"answers": {"text": ["Elders", "Elders", "Elders"], "answer_start": [0, 0, 0]}, "prediction_text": "Elders"}
{"answers": {"text": ["the local church", "bishop", "the local church"], "answer_start": [171, 68, 171]}, "prediction_text": "God"}
{"answers": {"text": ["2\u20133 years", "2\u20133 years as provisional Elders prior to their ordination.", "2\u20133 years"], "answer_start": [570, 570, 570]}, "prediction_text": "2-3 years."}
{"answers": {"text": ["District Superintendents", "ordained by a bishop", "District Superintendents"], "answer_start": [467, 54, 467]}, "prediction_text": "Bishop"}
{"answers": {"text": ["2\u20133 years", "Deacons serve a term of 2\u20133 years as provisional deacons prior to their ordination.", "2\u20133 years"], "answer_start": [670, 646, 670]}, "prediction_text": "2-3 years."}
{"answers": {"text": ["Deacons", "Deacons", "Deacons"], "answer_start": [318, 318, 318]}, "prediction_text": "Elders"}
{"answers": {"text": ["Deacons", "Deacons", "Deacons"], "answer_start": [479, 479, 479]}, "prediction_text": "Deacons"}
{"answers": {"text": ["granted sacramental authority", "granted sacramental authority", "sacramental authority"], "answer_start": [561, 561, 569]}, "prediction_text": "Sacramental authority."}
{"answers": {"text": ["1996", "1996", "1996"], "answer_start": [7, 7, 7]}, "prediction_text": "1996"}
{"answers": {"text": ["The provisional elder/deacon", "The provisional elder/deacon", "The provisional elder/deacon"], "answer_start": [227, 227, 227]}, "prediction_text": "Provisional elder"}
{"answers": {"text": ["1996 General Conference", "1996 General Conference the", "the ordination order of transitional deacon was abolished"], "answer_start": [7, 7, 31]}, "prediction_text": "The provisional elder/deacon."}
{"answers": {"text": ["Licensed Local Pastor", "'Licensed Local Pastor", "'Licensed Local Pastor"], "answer_start": [913, 912, 912]}, "prediction_text": "Licensed Local Pastor"}
{"answers": {"text": ["licensed local pastor", "five-year course of s", "licensed local pastor"], "answer_start": [1073, 1352, 1073]}, "prediction_text": "Local Pastors."}
{"answers": {"text": ["five", "five-year course of study at an", "five-year"], "answer_start": [1352, 1352, 1352]}, "prediction_text": "Five years."}
{"answers": {"text": ["Associate Membership", "Associate Membership", "Associate Membership"], "answer_start": [1625, 1625, 1625]}, "prediction_text": "Associate Membership."}
{"answers": {"text": ["Baptized Members", "Baptized Members", "Baptized Members"], "answer_start": [70, 70, 70]}, "prediction_text": "Baptized Members."}
{"answers": {"text": ["confirmation and sometimes the profession of faith", "through confirmation and sometimes the profession of faith.", "confirmation"], "answer_start": [257, 249, 257]}, "prediction_text": "Through confirmation and the profession of faith."}
{"answers": {"text": ["transfer from another Christian denomination", "through confirmation and sometimes the profession of faith.", "the profession of faith"], "answer_start": [511, 249, 284]}, "prediction_text": "Transfer from another Christian denomination."}
{"answers": {"text": ["Baptism", "Baptism", "Baptism"], "answer_start": [45, 45, 45]}, "prediction_text": "Baptism"}
{"answers": {"text": ["confirmation and membership preparation classes", "In confirmation and membership preparation classes,", "confirmation and membership preparation classes"], "answer_start": [591, 588, 591]}, "prediction_text": "Through Church and the Methodist-Christian theological tradition."}
{"answers": {"text": ["The Book of Discipline", "The Book of Discipline", "The Book of Discipline of the United Methodist Church"], "answer_start": [80, 401, 80]}, "prediction_text": "Book of Discipline."}
{"answers": {"text": ["Church and the Methodist-Christian theological tradition", "learn about Church and the Methodist-Christian theological tradition in order to profess their ultimate faith in Christ.", "Church and the Methodist-Christian theological tradition"], "answer_start": [661, 649, 661]}, "prediction_text": "Church and the Methodist-Christian theological tradition."}
{"answers": {"text": ["lay servants", "lay servants"], "answer_start": [270, 270]}, "prediction_text": "Lay servants."}
{"answers": {"text": ["they must be recommended by their pastor and Church Council or Charge Conference, and complete the basic course for lay servant", "they must be recommended by their pastor and Church Council or Charge Conference, and complete the basic course for lay servant", "recommended by their pastor and Church Council or Charge Conference, and complete the basic course for lay servant"], "answer_start": [541, 541, 554]}, "prediction_text": "Recommended by their pastor and Church Council or Charge Conference."}
{"answers": {"text": ["annually", "Each year", "Each year"], "answer_start": [1064, 670, 670]}, "prediction_text": "annually."}
{"answers": {"text": ["at least one advanced course every three years", "complete the basic course and one advanced lay servant course,", "one advanced course every three years"], "answer_start": [1097, 896, 1106]}, "prediction_text": "One."}
{"answers": {"text": ["United Methodist Church", "The United Methodist Church is", "The United Methodist Church"], "answer_start": [4, 0, 0]}, "prediction_text": "The United Methodist Church"}
{"answers": {"text": ["observer status", "it voted to seek observer status", "observer"], "answer_start": [357, 340, 357]}, "prediction_text": "Observer status."}
{"answers": {"text": ["blurring of theological and confessional differences in the interests of unity", "\"blurring of theological and confessional differences in the interests of unity", "the \"blurring of theological and confessional differences in the interests of unity.\""], "answer_start": [564, 563, 559]}, "prediction_text": "Blurring of theological and confessional differences."}
{"answers": {"text": ["2000", "2000", "2000"], "answer_start": [335, 335, 335]}, "prediction_text": "2000"}
{"answers": {"text": ["May 2012", "May 2012", "2012"], "answer_start": [379, 379, 383]}, "prediction_text": "2012"}
{"answers": {"text": ["1985", "since 1985", "1985"], "answer_start": [38, 32, 38]}, "prediction_text": "1985"}
{"answers": {"text": ["11 million", "about 11 million members in nearly 42,000 congregations", "11 million"], "answer_start": [213, 207, 213]}, "prediction_text": "11 million"}
{"answers": {"text": ["42,000", "42,000 congregations", "42,000"], "answer_start": [242, 242, 242]}, "prediction_text": "42,000"}
{"answers": {"text": ["8 million", "8 million", "8 million"], "answer_start": [355, 355, 355]}, "prediction_text": "8 million"}
{"answers": {"text": ["34,000", "34,000 congregations", "34,000"], "answer_start": [381, 381, 381]}, "prediction_text": "34,000"}
{"answers": {"text": ["Texas", "Texas", "Texas"], "answer_start": [473, 473, 473]}, "prediction_text": "Texas"}
{"answers": {"text": ["11.4 million", "11.4 million", "11.4 million"], "answer_start": [85, 85, 85]}, "prediction_text": "11.4 million"}
{"answers": {"text": ["7.9 million", "7.9 million in the U.S", "7.9 million"], "answer_start": [110, 110, 110]}, "prediction_text": "11.4 million"}
{"answers": {"text": ["3.5 million", "3.5 million", "3.5 million"], "answer_start": [138, 138, 138]}, "prediction_text": "3.5 million"}
{"answers": {"text": ["Wesleyan Holiness Consortium", "Wesleyan Holiness Consortium", "Wesleyan Holiness Consortium"], "answer_start": [32, 32, 32]}, "prediction_text": "Wesleyan Holiness Consortium"}
{"answers": {"text": ["World Methodist Council", "World Methodist Council", "World Methodist Council"], "answer_start": [162, 162, 162]}, "prediction_text": "World Methodist Council"}
{"answers": {"text": ["July 18, 2006", "On July 18, 2006", "July 18, 2006"], "answer_start": [324, 321, 324]}, "prediction_text": "July 18, 2006"}
{"answers": {"text": ["1754\u20131763", "1754\u20131763", "1754\u20131763", "1754\u20131763", "1754\u20131763"], "answer_start": [27, 27, 27, 27, 27]}, "prediction_text": "1754-1763"}
{"answers": {"text": ["colonies of British America and New France", "colonies of British America and New France", "British America and New France", "British America and New France", "the colonies of British America and New France"], "answer_start": [135, 135, 147, 147, 131]}, "prediction_text": "British America and New France."}
{"answers": {"text": ["roughly 60,000 European settlers", "60,000", "60,000", "60,000", "60,000 European settlers"], "answer_start": [395, 403, 403, 403, 403]}, "prediction_text": "60,000"}
{"answers": {"text": ["2 million", "2 million", "2 million", "2 million", "2 million"], "answer_start": [443, 443, 443, 443, 443]}, "prediction_text": "2 million"}
{"answers": {"text": ["primarily along the frontiers between New France and the British colonies", "between New France and the British colonies", "frontiers between New France and the British colonies", "along the frontiers", "Virginia in the South to Nova Scotia in the North"], "answer_start": [19, 49, 39, 29, 99]}, "prediction_text": "Virginia"}
{"answers": {"text": ["dispute over control of the confluence of the Allegheny and Monongahela rivers, called the Forks of the Ohio", "dispute over control of the confluence of the Allegheny and Monongahela rivers", "dispute over control of the confluence of the Allegheny and Monongahela rivers", "dispute over control", "a dispute over control of the confluence of the Allegheny and Monongahela rivers"], "answer_start": [166, 166, 166, 166, 164]}, "prediction_text": "The war started with a dispute over control of the confluence of the Allegheny and Monongahela rivers."}
{"answers": {"text": ["Battle of Jumonville Glen in May 1754,", "May 1754", "May 1754", "May 1754", "May 1754"], "answer_start": [400, 429, 429, 429, 429]}, "prediction_text": "May 1754"}
{"answers": {"text": ["1755", "1755", "1755", "1755", "1755"], "answer_start": [3, 3, 3, 3, 3]}, "prediction_text": "1755"}
{"answers": {"text": ["disaster; he was defeated in the Battle of the Monongahela", "disaster", "was a disaster", "he was defeated", "None succeeded"], "answer_start": [223, 223, 217, 233, 170]}, "prediction_text": "Disastrous."}
{"answers": {"text": ["combination of poor management, internal divisions, and effective Canadian scouts, French regular forces, and Indian warrior allies", "poor management, internal divisions, and effective Canadian scouts, French regular forces, and Indian warrior allies", "a combination of poor management, internal divisions, and effective Canadian scouts, French regular forces, and Indian warrior allies", "poor management, internal divisions, and effective Canadian scouts", "a combination of poor management, internal divisions, and effective Canadian scouts"], "answer_start": [439, 454, 437, 454, 437]}, "prediction_text": "Poor management, internal divisions, and effective Canadian scouts."}
{"answers": {"text": ["Fort Beaus\u00e9jour", "Fort Beaus\u00e9jour", "Fort Beaus\u00e9jour", "Fort Beaus\u00e9jour", "Fort Beaus\u00e9jour"], "answer_start": [602, 602, 602, 602, 602]}, "prediction_text": "Fort Beaus\u00e9jour"}
{"answers": {"text": ["expulsion of the Acadians", "expulsion of the Acadians", "deportation", "expulsion", "deportation"], "answer_start": [700, 700, 742, 700, 742]}, "prediction_text": "Expel."}
{"answers": {"text": ["William Pitt", "William Pitt", "William Pitt", "William Pitt", "William Pitt"], "answer_start": [235, 235, 235, 235, 235]}, "prediction_text": "William Pitt"}
{"answers": {"text": ["unwilling to risk large convoys to aid the limited forces it had in New France", "limited forces", "significantly increased", "France was unwilling to risk large convoys to aid the limited forces it had in New France"], "answer_start": [359, 402, 266, 348]}, "prediction_text": "Increased."}
{"answers": {"text": ["against Prussia and its allies in the European theatre of the war.", "Prussia", "against Prussia and its allies", "European theatre", "Prussia"], "answer_start": [470, 478, 470, 508, 478]}, "prediction_text": "Europe."}
{"answers": {"text": ["Sainte Foy in Quebec", "Sainte Foy", "Sainte Foy in Quebec", "Sainte Foy", "Sainte Foy"], "answer_start": [761, 761, 761, 761, 761]}, "prediction_text": "Sainte Foy"}
{"answers": {"text": ["territory east of the Mississippi to Great Britain", "France", "territory east of the Mississippi", "east of the Mississippi", "territory east of the Mississippi"], "answer_start": [113, 96, 113, 123, 113]}, "prediction_text": "French Louisiana."}
{"answers": {"text": ["French Louisiana west of the Mississippi River (including New Orleans) to its ally Spain", "French Louisiana", "French Louisiana west of the Mississippi River (including New Orleans)", "Louisiana west of the Mississippi River", "French Louisiana west of the Mississippi River (including New Orleans)"], "answer_start": [174, 174, 174, 181, 174]}, "prediction_text": "Florida"}
{"answers": {"text": ["confirming Britain's position as the dominant colonial power in eastern North America", "confirming Britain's position as the dominant colonial power in eastern North America", "confirming Britain's position as the dominant colonial power in eastern North America", "dominant colonial power", "confirming Britain's position as the dominant colonial power in eastern North America"], "answer_start": [504, 504, 504, 541, 504]}, "prediction_text": "The significance of British win was that Britain ceded its territory east of the Mississippi to Great Britain."}
{"answers": {"text": ["1740s", "1740s", "1740s", "1740s", "1740s"], "answer_start": [219, 219, 219, 219, 219]}, "prediction_text": "1740s"}
{"answers": {"text": ["Indians fought on both sides of the conflict, and that this was part of the Seven Years' War", "Indians fought on both sides of the conflict", "obscures the fact that Indians fought on both sides of the conflict, and that this was part of the Seven Years' War", "Seven Years' War", "it obscures the fact that Indians fought on both sides of the conflict"], "answer_start": [461, 461, 438, 537, 435]}, "prediction_text": "The French and Indian war."}
{"answers": {"text": ["much larger conflict between France and Great Britain", "conflict between France and Great Britain", "in King George's reign", "conflict between France and Great Britain", "a much larger conflict between France and Great Britain"], "answer_start": [557, 569, 265, 569, 555]}, "prediction_text": "France and Great Britain."}
{"answers": {"text": ["Fourth Intercolonial War and the Great War for the Empire", "Fourth Intercolonial War and the Great War for the Empire", "Fourth Intercolonial War and the Great War for the Empire", "Fourth Intercolonial War and the Great War for the Empire", "the Fourth Intercolonial War and the Great War for the Empire"], "answer_start": [760, 760, 760, 760, 756]}, "prediction_text": "Fourth Intercolonial War, Great War for the Empire."}
{"answers": {"text": ["declaration of war in 1756 to the signing of the peace treaty in 1763", "1756 to the signing of the peace treaty in 1763", "1756 to the signing of the peace treaty in 1763", "1756 to the signing of the peace treaty in 1763", "the official declaration of war in 1756 to the signing of the peace treaty in 1763"], "answer_start": [228, 250, 250, 250, 215]}, "prediction_text": "1756-1763"}
{"answers": {"text": ["six years", "six years", "six years", "six years", "six years"], "answer_start": [450, 450, 450, 450, 450]}, "prediction_text": "Six years."}
{"answers": {"text": ["1760", "1760", "1760", "1760", "1760"], "answer_start": [534, 534, 534, 534, 534]}, "prediction_text": "1760"}
{"answers": {"text": ["Battle of Jumonville Glen", "Battle of Jumonville Glen", "Jumonville Glen", "Battle of Jumonville Glen", "Battle of Jumonville Glen"], "answer_start": [470, 470, 480, 470, 470]}, "prediction_text": "Jumonville Glen"}
{"answers": {"text": ["about 75,000", "75,000", "75,000", "75,000", "75,000"], "answer_start": [31, 37, 37, 37, 37]}, "prediction_text": "75,000"}
{"answers": {"text": ["heavily concentrated along the St. Lawrence River valley, with some also in Acadia", "along the St. Lawrence River valley", "St. Lawrence River valley", "along the St. Lawrence River valley"], "answer_start": [52, 73, 83, 73]}, "prediction_text": "St. Lawrence River valley."}
{"answers": {"text": ["St. Lawrence and Mississippi watersheds, did business with local tribes, and often married Indian women", "St. Lawrence and Mississippi watersheds", "throughout the St. Lawrence and Mississippi watersheds", "St. Lawrence and Mississippi", "the St. Lawrence and Mississippi watersheds"], "answer_start": [480, 480, 465, 480, 476]}, "prediction_text": "St. Lawrence and Mississippi watersheds."}
{"answers": {"text": ["20 to 1", "20 to 1", "20 to 1", "20 to 1", "20 to 1"], "answer_start": [40, 40, 40, 40, 40]}, "prediction_text": "20 to 1."}
{"answers": {"text": ["from Nova Scotia and Newfoundland in the north, to Georgia in the south", "eastern coast of the continent", "eastern coast of the continent,", "eastern coast", "from Nova Scotia and Newfoundland in the north, to Georgia in the south"], "answer_start": [136, 104, 104, 104, 136]}, "prediction_text": "Nova Scotia and Newfoundland."}
{"answers": {"text": ["along the coast, the settlements were growing into the interior", "along the coast", "along the coast", "along the coast", "along the coast"], "answer_start": [426, 426, 426, 426, 426]}, "prediction_text": "Coastline."}
{"answers": {"text": ["native tribes", "native tribes", "native tribes", "native tribes", "native tribes"], "answer_start": [69, 69, 69, 69, 69]}, "prediction_text": "Native tribes."}
{"answers": {"text": ["Mi'kmaq and the Abenaki", "Mi'kmaq and the Abenaki", "the Mi'kmaq and the Abenaki", "Mi'kmaq and the Abenaki", "the Mi'kmaq and the Abenaki"], "answer_start": [102, 102, 98, 102, 98]}, "prediction_text": "Mi'kmaq, Abenaki, Iroquois Confederation."}
{"answers": {"text": ["present-day Upstate New York and the Ohio Country", "present-day Upstate New York and the Ohio Country", "Upstate New York and the Ohio Country", "New York and the Ohio", "Upstate New York and the Ohio Country"], "answer_start": [353, 353, 365, 373, 365]}, "prediction_text": "Nova Scotia, Acadia, and the eastern portions of Canada."}
{"answers": {"text": ["Iroquois rule, and were limited by them in authority to make agreements", "Iroquois", "Iroquois", "Iroquois", "Iroquois"], "answer_start": [565, 565, 565, 565, 565]}, "prediction_text": "Iroquois Confederation"}
{"answers": {"text": ["Catawba, Muskogee-speaking Creek and Choctaw", "Catawba", "Catawba", "Catawba", "Catawba"], "answer_start": [70, 70, 70, 70, 70]}, "prediction_text": "Catawba, Muskogee, Creek, Choctaw, and the Iroquoian-speaking Cherokee tribes."}
{"answers": {"text": ["western portions of the Great Lakes region", "Great Lakes", "tribes in western portions of the Great Lakes region", "western portions of the Great Lakes", "western portions of the Great Lakes region"], "answer_start": [257, 281, 247, 257, 257]}, "prediction_text": "western portions of the Great Lakes region (an area not directly subject to the conflict between the French and British)"}
{"answers": {"text": ["Iroquois Six Nations, and also by the Cherokee", "Iroquois Six Nations, and also by the Cherokee", "Iroquois Six Nations, and also by the Cherokee", "Iroquois", "the Iroquois Six Nations"], "answer_start": [493, 493, 493, 493, 489]}, "prediction_text": "Iroquois Six Nations, Cherokee."}
{"answers": {"text": ["no French regular army troops were stationed in North America", "no French regular army troops were stationed in North America", "no French regular army troops were stationed in North America,", "no French regular army", "no French regular army troops were stationed in North America"], "answer_start": [25, 25, 25, 25, 25]}, "prediction_text": "French regular army troops."}
{"answers": {"text": ["few British troops", "few", "few", "not have any standing forces", "few British troops"], "answer_start": [92, 92, 92, 479, 92]}, "prediction_text": "3,000"}
{"answers": {"text": ["mustered local militia companies, generally ill trained and available only for short periods, to deal with native threats, but did not have any standing forces.", "local militia companies", "local militia companies", "militia support", "local militia companies"], "answer_start": [348, 357, 357, 297, 357]}, "prediction_text": "Local militia companies."}
{"answers": {"text": ["about 3,000 miles (4,800 km) between June and November 1749.", "about 3,000 miles", "3,000 miles", "3,000 miles", "3,000 miles"], "answer_start": [110, 110, 116, 116, 116]}, "prediction_text": "3,000 miles (4,800 km)"}
{"answers": {"text": ["200 Troupes de la marine and 30 Indians", "C\u00e9loron", "200 Troupes de la marine and 30 Indians", "200 Troupes de la marine and 30 Indians", "200 Troupes de la marine and 30 Indians"], "answer_start": [46, 511, 46, 46, 46]}, "prediction_text": "Indians"}
{"answers": {"text": ["British merchants or fur-traders, C\u00e9loron informed them of the French claims on the territory and told them to leave.", "told them to leave", "Whenever he encountered British merchants or fur-traders, C\u00e9loron informed them of the French claims on the territory and told them to leave.", "buried lead plates", "Whenever he encountered British merchants or fur-traders, C\u00e9loron informed them of the French claims on the territory and told them to leave"], "answer_start": [614, 712, 590, 519, 590]}, "prediction_text": "He told British merchants and fur-traders to leave."}
{"answers": {"text": ["informed C\u00e9loron that they owned the Ohio Country and that they would trade with the British regardless of the French", "informed C\u00e9loron that they owned the Ohio Country and that they would trade with the British", "they owned the Ohio Country", "they owned the Ohio Country and that they would trade with the British regardless of the French"], "answer_start": [80, 80, 102, 102]}, "prediction_text": "They informed C\u00e9loron that they owned the Ohio Country and that they would trade with the British regardless of the French."}
{"answers": {"text": ["village of Pickawillany", "village of Pickawillany", "village of Pickawillany", "Pickawillany", "village of Pickawillany"], "answer_start": [329, 329, 329, 340, 329]}, "prediction_text": "Pickawillany"}
{"answers": {"text": ["threatened \"Old Briton\" with severe consequences if he continued to trade with the British", "C\u00e9loron threatened \"Old Briton\" with severe consequences", "threatened \"Old Briton\" with severe consequences", "threatened", "C\u00e9loron threatened \"Old Briton\""], "answer_start": [413, 405, 413, 413, 405]}, "prediction_text": "Threatened \"Old Briton\" with severe consequences."}
{"answers": {"text": ["ignored the warning.", "ignored the warning", "ignored the warning", "ignored the warning", "\"Old Briton\" ignored the warning"], "answer_start": [518, 518, 518, 518, 505]}, "prediction_text": "Ignored."}
{"answers": {"text": ["very badly disposed towards the French, and are entirely devoted to the English", "very badly disposed towards the French", "are very badly disposed towards the French, and are entirely devoted to the English", "what way they could be brought back", "very badly disposed towards the French"], "answer_start": [110, 110, 106, 207, 110]}, "prediction_text": "He felt that the Natives of these localities were very badly disposed towards the French."}
{"answers": {"text": ["proposing that action be taken", "each side proposing that action be taken", "proposing that action be taken", "each side proposing that action be taken"], "answer_start": [379, 369, 379, 369]}, "prediction_text": "Conflicts."}
{"answers": {"text": ["British colonists would not be safe as long as the French were present", "British colonists would not be safe", "British colonists would not be safe as long as the French were present", "British colonists would not be safe", "forceful"], "answer_start": [532, 532, 532, 532, 509]}, "prediction_text": "He felt that French advancement was a threat to British colonial interests."}
{"answers": {"text": ["1749", "1749", "1749", "1749", "1749"], "answer_start": [3, 3, 3, 3, 3]}, "prediction_text": "1749"}
{"answers": {"text": ["Ohio Company of Virginia", "Ohio Company of Virginia", "Ohio Company of Virginia", "Ohio Company of Virginia", "Ohio Company"], "answer_start": [48, 48, 48, 48, 48]}, "prediction_text": "Ohio Company of Virginia"}
{"answers": {"text": ["Christopher Gist", "Christopher Gist", "Christopher Gist", "Christopher Gist", "Christopher Gist"], "answer_start": [393, 393, 393, 393, 393]}, "prediction_text": "Christopher Gist"}
{"answers": {"text": ["Treaty of Logstown", "1752 Treaty of Logstown", "1752 Treaty of Logstown", "Treaty of Logstown", "Treaty of Logstown"], "answer_start": [572, 567, 567, 572, 572]}, "prediction_text": "Treaty of Logstown."}
{"answers": {"text": ["mouth of the Monongahela River (the site of present-day Pittsburgh, Pennsylvania)", "mouth of the Monongahela River", "mouth of the Monongahela River", "Pittsburgh, Pennsylvania", "the mouth of the Monongahela River"], "answer_start": [764, 764, 764, 820, 760]}, "prediction_text": "Logstown."}
{"answers": {"text": ["King George's War", "King George's War", "King George's War", "King George's War"], "answer_start": [77, 77, 77, 77]}, "prediction_text": "King George's War"}
{"answers": {"text": ["1748 with the signing of the Treaty of Aix-la-Chapelle", "signing of the Treaty of Aix-la-Chapelle", "signing of the Treaty of Aix-la-Chapelle", "1748", "the Treaty of Aix-la-Chapelle"], "answer_start": [114, 128, 128, 114, 139]}, "prediction_text": "Treaty of Aix-la-Chapelle."}
{"answers": {"text": ["conflicting territorial claims between British and French", "conflicting territorial claims between British and French colonies in North America", "conflicting territorial claims between British and French colonies in North America", "conflicting territorial claims", "The issues of conflicting territorial claims between British and French colonies"], "answer_start": [248, 248, 248, 248, 234]}, "prediction_text": "The Treaty of Aix-la-Chapelle did not address issues in Europe."}
{"answers": {"text": ["Frontiers from between Nova Scotia and Acadia in the north, to the Ohio Country in the south, were claimed by both sides", "Frontiers from between Nova Scotia and Acadia in the north, to the Ohio Country in the south, were claimed by both sides", "Frontiers from between Nova Scotia and Acadia in the north, to the Ohio Country in the south, were claimed by both sides.", "claimed by both sides", "Frontiers from between Nova Scotia and Acadia in the north, to the Ohio Country in the south, were claimed by both sides"], "answer_start": [405, 405, 405, 504, 405]}, "prediction_text": "The Treaty of Aix-la-Chapelle."}
{"answers": {"text": ["Marquis de la Jonqui\u00e8re", "Marquis de la Jonqui\u00e8re", "Marquis de la Jonqui\u00e8re", "Marquis de la Jonqui\u00e8re", "Marquis de la Jonqui\u00e8re"], "answer_start": [55, 55, 55, 55, 55]}, "prediction_text": "Marquis de la Jonqui\u00e8re"}
{"answers": {"text": ["300 men, including French-Canadians and warriors of the Ottawa", "300", "300 men", "300", "300 men"], "answer_start": [486, 486, 486, 486, 486]}, "prediction_text": "300"}
{"answers": {"text": ["punish the Miami people of Pickawillany for not following C\u00e9loron's orders to cease trading with the British", "punish the Miami people of Pickawillany", "punish the Miami people of Pickawillany for not following C\u00e9loron's orders", "punish the Miami people", "to punish the Miami people of Pickawillany"], "answer_start": [571, 571, 571, 571, 568]}, "prediction_text": "To punish the Miami people of Pickawillany."}
{"answers": {"text": ["capturing three traders and killing 14 people of the Miami nation, including Old Briton", "capturing three traders and killing 14 people of the Miami nation", "capturing three traders and killing 14 people of the Miami nation, including Old Briton", "capturing three traders and killing 14 people", "capturing three traders and killing 14 people of the Miami nation"], "answer_start": [759, 759, 759, 759, 759]}, "prediction_text": "Captured traders."}
{"answers": {"text": ["Paul Marin de la Malgue", "Paul Marin de la Malgue", "Paul Marin de la Malgue", "Paul Marin de la Malgue", "Paul Marin de la Malgue"], "answer_start": [23, 23, 23, 23, 23]}, "prediction_text": "Paul Marin de la Malgue"}
{"answers": {"text": ["Fort Presque Isle (near present-day Erie, Pennsylvania", "Fort Presque Isle", "near present-day Erie, Pennsylvania", "Fort Presque Isle", "near present-day Erie, Pennsylvania"], "answer_start": [425, 425, 444, 425, 444]}, "prediction_text": "Fort Presque Isle."}
{"answers": {"text": ["Fort Le Boeuf (present-day Waterford, Pennsylvania", "Fort Le Boeuf", "present-day Waterford, Pennsylvania", "Fort Le Boeuf", "present-day Waterford, Pennsylvania"], "answer_start": [600, 600, 615, 600, 615]}, "prediction_text": "Fort Le Boeuf."}
{"answers": {"text": ["protect the King's land in the Ohio Valley from the British", "protect the King's land in the Ohio Valley from the British", "to protect the King's land in the Ohio Valley from the British", "protect the King's land in the Ohio Valley", "he moved south, he drove off or captured British traders"], "answer_start": [142, 142, 139, 142, 707]}, "prediction_text": "To protect the King's land in the Ohio Valley from the British."}
{"answers": {"text": ["Tanaghrisson", "Tanaghrisson", "Tanaghrisson", "Tanaghrisson", "the Mingo"], "answer_start": [809, 809, 809, 809, 834]}, "prediction_text": "Tanaghrisson"}
{"answers": {"text": ["British Superintendent for Indian Affairs in the New York region and beyond", "British Superintendent for Indian Affairs", "British Superintendent for Indian Affairs in the New York region and beyond", "British Superintendent for Indian Affairs", "British Superintendent for Indian Affairs"], "answer_start": [83, 83, 83, 83, 83]}, "prediction_text": "British Superintendent for Indian Affairs."}
{"answers": {"text": ["Warraghiggey, meaning \"He who does great things.\"", "Warraghiggey", "Warraghiggey", "Warraghiggey", "Warraghiggey"], "answer_start": [197, 197, 197, 197, 197]}, "prediction_text": "Warraghiggey"}
{"answers": {"text": ["colonel of the Iroquois", "He who does great things", "He who does great things.", "honorary member of the Iroquois Confederacy", "Warraghiggey"], "answer_start": [384, 220, 220, 299, 197]}, "prediction_text": "Warraghiggey"}
{"answers": {"text": ["Mohawk Chief Hendrick", "Mohawk Chief Hendrick", "Mohawk Chief Hendrick", "Chief Hendrick", "Chief Hendrick"], "answer_start": [588, 588, 588, 595, 595]}, "prediction_text": "Mohawk Chief Hendrick"}
{"answers": {"text": ["Ohio Company", "Ohio Company", "Ohio Company,", "Ohio Company", "the Ohio Company"], "answer_start": [61, 61, 61, 61, 57]}, "prediction_text": "Ohio Company"}
{"answers": {"text": ["Major George Washington", "George Washington", "Major George Washington", "George Washington", "Major George Washington"], "answer_start": [232, 238, 232, 238, 232]}, "prediction_text": "Major George Washington"}
{"answers": {"text": ["Jacob Van Braam as an interpreter; Christopher Gist, a company surveyor working in the area; and a few Mingo led by Tanaghrisson", "Jacob Van Braam", "Jacob Van Braam", "Jacob Van Braam"], "answer_start": [440, 440, 440, 440]}, "prediction_text": "Jacob Van Braam"}
{"answers": {"text": ["December 12", "December 12", "December 12", "December 12", "December 12"], "answer_start": [573, 573, 573, 573, 573]}, "prediction_text": "December 12."}
{"answers": {"text": ["Jacques Legardeur de Saint-Pierre", "Jacques Legardeur de Saint-Pierre", "Jacques Legardeur de Saint-Pierre", "Jacques Legardeur de Saint-Pierre", "Jacques Legardeur de Saint-Pierre"], "answer_start": [0, 0, 0, 0, 0]}, "prediction_text": "Jacques Legardeur de Saint-Pierre"}
{"answers": {"text": ["Dinwiddie demanding an immediate French withdrawal from the Ohio Country", "Dinwiddie", "from Dinwiddie demanding an immediate French withdrawal from the Ohio Country", "immediate French withdrawal", "the letter from Dinwiddie"], "answer_start": [231, 231, 226, 254, 215]}, "prediction_text": "Dinwiddie's letter."}
{"answers": {"text": ["As to the Summons you send me to retire, I do not think myself obliged to obey it.", "said, \"As to the Summons you send me to retire, I do not think myself obliged to obey it.\"", "\"As to the Summons you send me to retire, I do not think myself obliged to obey it.\"", "I do not think myself obliged to obey", "\"As to the Summons you send me to retire, I do not think myself obliged to obey it.\""], "answer_start": [325, 318, 324, 366, 324]}, "prediction_text": "Saint-Pierre said, \"As to the Summons you send me to retire, I do not think myself obliged to obey it.\""}
{"answers": {"text": ["France's claim to the region was superior to that of the British", "Sieur de La Salle had explored the Ohio Country nearly a century earlier", "Ren\u00e9-Robert Cavelier, Sieur de La Salle had explored the Ohio Country nearly a century earlier.", "Ren\u00e9-Robert Cavelier, Sieur de La Salle had explored the Ohio Country", "Ren\u00e9-Robert Cavelier, Sieur de La Salle had explored the Ohio Country nearly a century earlier"], "answer_start": [433, 527, 505, 505, 505]}, "prediction_text": "superior"}
{"answers": {"text": ["Contrec\u0153ur led 500 men south from Fort Venango on April 5, 1754", "40", "40", "additional French forces", "40 men"], "answer_start": [334, 65, 65, 215, 65]}, "prediction_text": "500"}
{"answers": {"text": ["early months of 1754", "1754", "1754", "1754", "1754"], "answer_start": [120, 136, 136, 136, 136]}, "prediction_text": "April 5, 1754"}
{"answers": {"text": ["Fort Duquesne.", "Fort Duquesne", "Fort Duquesne", "Fort Duquesne", "Fort Duquesne"], "answer_start": [586, 586, 586, 586, 586]}, "prediction_text": "Fort Duquesne"}
{"answers": {"text": ["with Tanaghrisson and his party, surprised the Canadians on May 28 in what became known as the Battle of Jumonville Glen", "killed many of the Canadians", "surprised the Canadians on May 28", "Battle of Jumonville Glen", "surprised the Canadians on May 28"], "answer_start": [366, 493, 399, 461, 399]}, "prediction_text": "He continued toward Fort Duquesne and met with the Mingo leader."}
{"answers": {"text": ["killed many of the Canadians, including their commanding officer, Joseph Coulon de Jumonville", "many of the Canadians, including their commanding officer, Joseph Coulon de Jumonville", "Canadians, including their commanding officer", "many of the Canadians, including their commanding officer, Joseph Coulon de Jumonville"], "answer_start": [493, 500, 512, 500]}, "prediction_text": "Joseph Coulon de Jumonville."}
{"answers": {"text": ["regain authority over his own people. They had been inclined to support the French, with whom they had long trading relationships", "gain the support of the British and regain authority over his own people", "had promised", "regain authority over his own people", "to gain the support of the British and regain authority over his own people"], "answer_start": [763, 727, 195, 763, 724]}, "prediction_text": "To regain authority over his own people."}
{"answers": {"text": ["dislodge the French", "dislodge the French", "dislodge the French", "dislodge the French"], "answer_start": [186, 186, 186, 186]}, "prediction_text": "Dislodge the French."}
{"answers": {"text": ["plans leaked to France well before Braddock's departure", "before Braddock's departure", "well before Braddock's departure for North America", "before Braddock's departure", "well before Braddock's departure for North America"], "answer_start": [301, 329, 324, 329, 324]}, "prediction_text": "February 1755."}
{"answers": {"text": ["dispatched six regiments to New France under the command of Baron Dieskau in 1755.", "dispatched six regiments to New France", "dispatched six regiments to New France under the command of Baron Dieskau in 1755", "six regiments to New France", "dispatched six regiments to New France"], "answer_start": [403, 403, 403, 414, 403]}, "prediction_text": "Dispatched six regiments to New France under the command of Baron Dieskau."}
{"answers": {"text": ["blockade French ports, sent out their fleet in February 1755", "blockade French ports", "blockade French ports", "blockade French ports", "blockade French ports"], "answer_start": [512, 512, 512, 512, 512]}, "prediction_text": "Blockading French ports."}
{"answers": {"text": ["Albany Congress", "Albany Congress", "the opening of hostilities", "Albany Congress", "Albany Congress"], "answer_start": [93, 93, 41, 93, 93]}, "prediction_text": "Albany Congress"}
{"answers": {"text": ["formalize a unified front in trade and negotiations with various Indians, since allegiance of the various tribes and nations was seen to be pivotal", "formalize a unified front in trade and negotiations with various Indians", "formalize a unified front in trade and negotiations with various Indians", "unified front in trade and negotiations with various Indians", "to formalize a unified front in trade and negotiations with various Indians"], "answer_start": [165, 165, 165, 177, 162]}, "prediction_text": "To formalize a unified front in trade and negotiations with various Indians."}
{"answers": {"text": ["The plan that the delegates agreed to was never ratified by the colonial legislatures nor approved of by the crown", "was never ratified", "never ratified", "never ratified", "The plan that the delegates agreed to was never ratified"], "answer_start": [359, 397, 401, 401, 359]}, "prediction_text": "No."}
{"answers": {"text": ["format of the congress and many specifics of the plan became the prototype for confederation during the War of Independence", "became the prototype for confederation during the War of Independence", "became the prototype for confederation during the War of Independence", "prototype for confederation", "became the prototype for confederation during the War of Independence"], "answer_start": [493, 547, 547, 558, 547]}, "prediction_text": "To formalize a unified front in trade and negotiations with various Indians."}
{"answers": {"text": ["Braddock (with George Washington as one of his aides) led about 1,500 army troops", "Braddock", "Braddock (with George Washington", "Braddock", "Braddock"], "answer_start": [0, 0, 0, 0, 0]}, "prediction_text": "George Washington"}
{"answers": {"text": ["The expedition was a disaster", "disaster", "was a disaster", "disaster", "The expedition was a disaster"], "answer_start": [158, 179, 173, 179, 158]}, "prediction_text": "1,500"}
{"answers": {"text": ["Approximately 1,000 British soldiers were killed or injured.", "Approximately 1,000", "1,000 British soldiers", "1,000", "1,000 British soldiers"], "answer_start": [330, 330, 344, 344, 344]}, "prediction_text": "500"}
{"answers": {"text": ["Washington and Thomas Gage", "George Washington", "Washington and Thomas Gage", "Washington and Thomas Gage", "George Washington"], "answer_start": [530, 432, 530, 530, 432]}, "prediction_text": "Washington and Thomas Gage."}
{"answers": {"text": ["Shirley and Johnson.", "British", "British war plans", "British war plans", "Shirley and Johnson"], "answer_start": [81, 34, 34, 34, 81]}, "prediction_text": "Shirley"}
{"answers": {"text": ["efforts to fortify Oswego were bogged down in logistical difficulties, exacerbated by Shirley's inexperience", "logistical", "logistical difficulties", "logistical", "logistical"], "answer_start": [112, 158, 158, 158, 158]}, "prediction_text": "logistical difficulties"}
{"answers": {"text": ["planned to attack Fort Niagara", "Fort Niagara", "Fort Niagara", "Fort Niagara"], "answer_start": [372, 390, 390, 390]}, "prediction_text": "Fort Niagara."}
{"answers": {"text": ["garrisons", "garrisons", "garrisons", "garrisons"], "answer_start": [432, 432, 432, 432]}, "prediction_text": "Garrisons."}
{"answers": {"text": ["Marquis de Vaudreuil.", "Marquis de Vaudreuil", "the Marquis de Vaudreuil", "Marquis de Vaudreuil", "Marquis de Vaudreuil"], "answer_start": [106, 106, 102, 106, 106]}, "prediction_text": "Marquis de Vaudreuil"}
{"answers": {"text": ["sent Dieskau to Fort St. Fr\u00e9d\u00e9ric to meet that threat", "sent Dieskau to Fort St. Fr\u00e9d\u00e9ric", "sent Dieskau to Fort St. Fr\u00e9d\u00e9ric", "sent Dieskau to Fort St. Fr\u00e9d\u00e9ric", "sent Dieskau to Fort St. Fr\u00e9d\u00e9ric"], "answer_start": [367, 367, 367, 367, 367]}, "prediction_text": "Vaudreuil sent Dieskau to Fort St. Fr\u00e9d\u00e9ric to meet the threat."}
{"answers": {"text": ["inconclusively, with both sides withdrawing from the field", "The battle ended inconclusively", "ended inconclusively", "both sides withdrawing from the field", "The battle ended inconclusively"], "answer_start": [755, 738, 749, 776, 738]}, "prediction_text": "Johnson"}
{"answers": {"text": ["Fort William Henry", "Fort William Henry", "Fort William Henry", "Fort William Henry", "Fort William Henry"], "answer_start": [844, 844, 844, 844, 844]}, "prediction_text": "Fort William Henry"}
{"answers": {"text": ["Ticonderoga Point,", "Ticonderoga Point", "Ticonderoga Point", "Ticonderoga Point", "Ticonderoga Point"], "answer_start": [891, 891, 891, 891, 891]}, "prediction_text": "Ticonderoga Point."}
{"answers": {"text": ["Colonel Monckton", "Colonel Monckton", "Colonel Monckton", "Colonel Monckton", "British"], "answer_start": [0, 0, 0, 0, 30]}, "prediction_text": "Colonel Monckton"}
{"answers": {"text": ["deportation of the French-speaking Acadian population from the area.", "deportation of the French-speaking Acadian population from the area", "captured Fort Beaus\u00e9jour", "captured Fort Beaus\u00e9jour", "the deportation of the French-speaking Acadian population"], "answer_start": [263, 263, 57, 57, 259]}, "prediction_text": "By deporting the French-speaking Acadian population."}
{"answers": {"text": ["Petitcodiac in 1755 and at Bloody Creek near Annapolis Royal in 1757", "Petitcodiac in 1755 and at Bloody Creek near Annapolis Royal in 1757", "deportation of the French-speaking Acadian population from the area", "Petitcodiac in 1755 and at Bloody Creek", "Petitcodiac in 1755 and at Bloody Creek near Annapolis Royal in 1757"], "answer_start": [949, 949, 263, 949, 949]}, "prediction_text": "Dartmouth and Lunenburg."}
{"answers": {"text": ["William Shirley", "William Shirley", "William Shirley", "William Shirley", "William Shirley"], "answer_start": [33, 33, 33, 33, 33]}, "prediction_text": "William Shirley"}
{"answers": {"text": ["Albany", "meeting in Albany in December 1755", "Albany", "Albany in December 1755", "a meeting in Albany"], "answer_start": [117, 106, 117, 117, 104]}, "prediction_text": "Albany in December 1755."}
{"answers": {"text": ["capture Niagara, Crown Point and Duquesne, he proposed attacks on Fort Frontenac on the north shore of Lake Ontario", "Fort Frontenac", "Fort Frontenac", "Fort Frontenac", "Fort Frontenac"], "answer_start": [213, 279, 279, 279, 279]}, "prediction_text": "Fort Frontenac, Niagara, Crown Point, Duquesne."}
{"answers": {"text": ["through the wilderness of the Maine district and down the Chaudi\u00e8re River to attack the city of Quebec", "wilderness of the Maine district and down the Chaudi\u00e8re River", "the wilderness of the Maine district", "Maine", "the wilderness of the Maine district and down the Chaudi\u00e8re River"], "answer_start": [347, 359, 355, 377, 355]}, "prediction_text": "Lake Ontario."}
{"answers": {"text": ["Major General James Abercrombie", "Lord Loudoun", "Major General James Abercrombie", "Major General James Abercrombie", "Major General James Abercrombie"], "answer_start": [63, 44, 63, 63, 63]}, "prediction_text": "James Abercrombie"}
{"answers": {"text": ["Major General Louis-Joseph de Montcalm", "Major General Louis-Joseph de Montcalm", "Lord Loudoun", "Major General Louis-Joseph de Montcalm", "Major General Louis-Joseph de Montcalm"], "answer_start": [305, 305, 44, 305, 305]}, "prediction_text": "Major General James Abercrombie"}
{"answers": {"text": ["May 18, 1756", "May 18, 1756", "May 18, 1756", "May 18, 1756", "May 18, 1756"], "answer_start": [525, 525, 525, 525, 525]}, "prediction_text": "May 18, 1756"}
{"answers": {"text": ["Oneida Carry", "Oneida Carry", "forts Shirley had erected at the Oneida Carry", "Oneida Carry", "Oneida Carry"], "answer_start": [323, 323, 290, 323, 323]}, "prediction_text": "Fort Shirley."}
{"answers": {"text": ["Battle of Fort Bull", "Battle of Fort Bull", "Battle of Fort Bull", "Battle of Fort Bull", "March Battle of Fort Bull"], "answer_start": [350, 350, 350, 350, 344]}, "prediction_text": "Fort Shirley."}
{"answers": {"text": ["45,000 pounds", "45,000 pounds", "45,000 pounds", "45,000 pounds", "45,000 pounds"], "answer_start": [448, 448, 448, 448, 448]}, "prediction_text": "45,000 pounds."}
{"answers": {"text": ["hopes for campaigns on Lake Ontario, and endangered the Oswego garrison", "campaigns on Lake Ontario, and endangered the Oswego garrison", "campaigns on Lake Ontario", "campaigns on Lake Ontario", "hopes for campaigns on Lake Ontario"], "answer_start": [502, 512, 512, 512, 502]}, "prediction_text": "British hopes for campaigns on Lake Ontario."}
{"answers": {"text": ["Abercrombie", "Abercrombie", "Abercrombie", "Abercrombie", "Abercrombie"], "answer_start": [80, 80, 80, 80, 80]}, "prediction_text": "Abercrombie"}
{"answers": {"text": ["Ticonderoga", "Ticonderoga", "Ticonderoga", "Ticonderoga", "Ticonderoga"], "answer_start": [334, 334, 334, 334, 334]}, "prediction_text": "Ticonderoga"}
{"answers": {"text": ["Oswego", "Oswego", "Oswego", "Oswego", "Oswego"], "answer_start": [493, 493, 493, 493, 493]}, "prediction_text": "Ticonderoga"}
{"answers": {"text": ["disposition of prisoners' personal effects", "the disposition of prisoners' personal effects", "about the disposition of prisoners' personal effects", "disposition of prisoners' personal effects", "the disposition of prisoners' personal effects"], "answer_start": [592, 588, 582, 592, 588]}, "prediction_text": "Personal effects."}
{"answers": {"text": ["attack on New France's capital, Quebec", "an attack on New France's capital, Quebec", "an attack on New France's capital, Quebec", "one major operation", "an attack on New France's capital, Quebec"], "answer_start": [106, 103, 103, 73, 103]}, "prediction_text": "Attack New France's capital, Quebec."}
{"answers": {"text": ["to distract Montcalm", "distract Montcalm", "distract Montcalm", "distract Montcalm", "to distract Montcalm"], "answer_start": [192, 195, 195, 195, 192]}, "prediction_text": "To distract Montcalm."}
{"answers": {"text": ["William Pitt", "William Pitt", "William Pitt", "William Pitt", "William Pitt"], "answer_start": [287, 287, 287, 287, 287]}, "prediction_text": "William Pitt"}
{"answers": {"text": ["returned to New York amid news that a massacre had occurred at Fort William Henry.", "returned to New York", "returned to New York", "returned to New York", "returned to New York"], "answer_start": [685, 685, 685, 685, 685]}, "prediction_text": "Returned to New York."}
{"answers": {"text": ["French irregular forces (Canadian scouts and Indians)", "French irregular forces", "French irregular forces", "French irregular forces", "French irregular forces"], "answer_start": [0, 0, 0, 0, 0]}, "prediction_text": "British rangers."}
{"answers": {"text": ["Lake George", "Lake George", "Lake George", "Lake George", "Lake George"], "answer_start": [255, 255, 255, 255, 255]}, "prediction_text": "Lake George"}
{"answers": {"text": ["attacked the British column, killing and capturing several hundred men, women, children, and slaves.", "attacked the British column", "attacked the British column", "attacked the British", "attacked the British column"], "answer_start": [564, 564, 564, 564, 564]}, "prediction_text": "Attacked British rangers."}
{"answers": {"text": ["British blockade of the French coastline limited French shipping.", "British blockade of the French coastline", "British blockade of the French coastline", "British blockade of the French coastline", "British blockade"], "answer_start": [65, 65, 65, 65, 65]}, "prediction_text": "British blockade of French coastline."}
{"answers": {"text": ["poor harvest", "allegedly corrupt machinations of Fran\u00e7ois Bigot", "poor harvest in 1757", "poor harvest in 1757", "a poor harvest"], "answer_start": [188, 238, 188, 188, 186]}, "prediction_text": "Poor harvest."}
{"answers": {"text": ["St. Lawrence, with primary defenses at Carillon, Quebec, and Louisbourg,", "St. Lawrence", "St. Lawrence", "St. Lawrence", "the defense of the St. Lawrence"], "answer_start": [873, 873, 873, 873, 854]}, "prediction_text": "St. Lawrence"}
{"answers": {"text": ["British failures in North America, combined with other failures in the European theater", "British failures in North America, combined with other failures in the European theater", "British failures in North America, combined with other failures in the Europe", "failures in North America", "British failures in North America"], "answer_start": [4, 4, 4, 12, 4]}, "prediction_text": "The British failures in North America."}
{"answers": {"text": ["Loudoun", "Duke of Cumberland", "Loudoun", "Pitt", "Newcastle"], "answer_start": [363, 173, 363, 207, 123]}, "prediction_text": "Pitt"}
{"answers": {"text": ["three major offensive actions involving large numbers of regular troops", "three major offensive actions", "three major offensive actions", "three major offensive actions", "three major offensive actions"], "answer_start": [481, 481, 481, 481, 481]}, "prediction_text": "Three major offensive actions."}
{"answers": {"text": ["Two of the expeditions were successful, with Fort Duquesne and Louisbourg", "Two", "Two", "Two", "Two"], "answer_start": [641, 641, 641, 641, 641]}, "prediction_text": "Two."}
{"answers": {"text": ["3,600", "3,60", "3,600", "3,600", "3,600"], "answer_start": [102, 102, 102, 102, 102]}, "prediction_text": "3,600"}
{"answers": {"text": ["18,000 regulars, militia and Native American allies", "18,000", "18,000", "18,000", "18,000"], "answer_start": [174, 174, 174, 174, 174]}, "prediction_text": "3,600"}
{"answers": {"text": ["sent John Bradstreet on an expedition that successfully destroyed Fort Frontenac", "successfully destroyed Fort Frontenac", "destroyed Fort Frontenac", "destroyed Fort Frontenac", "destroyed Fort Frontenac"], "answer_start": [360, 403, 416, 416, 416]}, "prediction_text": "Destroyed Fort Frontenac."}
{"answers": {"text": ["recalled and replaced by Jeffery Amherst, victor at Louisbourg.", "Abercrombie was recalled and replaced", "was recalled and replaced by Jeffery Amherst", "recalled and replaced", "Abercrombie was recalled and replaced by Jeffery Amherst,"], "answer_start": [557, 541, 553, 557, 541]}, "prediction_text": "He was recalled and replaced by Jeffery Amherst."}
{"answers": {"text": ["invasion of Britain, to draw British resources away from North America and the European mainland", "invasion of Britain", "an invasion of Britain", "invasion of Britain", "an invasion of Britain"], "answer_start": [175, 175, 172, 175, 172]}, "prediction_text": "Britain."}
{"answers": {"text": ["The invasion failed both militarily and politically, as Pitt again planned significant campaigns against New France", "failed", "invasion failed", "failed", "The invasion failed both militarily and politically"], "answer_start": [273, 286, 277, 286, 273]}, "prediction_text": "Not successful."}
{"answers": {"text": ["Lagos and Quiberon Bay.", "battles at Lagos and Quiberon Bay", "Lagos and Quiberon Bay", "Lagos and Quiberon Bay", "Lagos and Quiberon Bay"], "answer_start": [505, 494, 505, 505, 505]}, "prediction_text": "Lagos and Quiberon Bay."}
{"answers": {"text": ["James Wolfe", "James Wolfe", "James Wolfe", "James Wolfe", "James Wolfe"], "answer_start": [116, 116, 116, 116, 116]}, "prediction_text": "James Wolfe"}
{"answers": {"text": ["cut off the French frontier forts further to the west and south", "successfully cut off the French frontier forts further to the west and south", "cut off the French frontier forts", "cut off the French frontier forts", "successfully cut off the French frontier forts"], "answer_start": [254, 241, 254, 254, 241]}, "prediction_text": "The significance of victory at Forth Niagara for British was that it cut off the French frontier forts further to the west and south."}
{"answers": {"text": ["Battle of Sainte-Foy", "Battle of Sainte-Foy", "Sainte-Foy", "Battle of Sainte-Foy", "Battle of Sainte-Foy"], "answer_start": [405, 405, 415, 405, 405]}, "prediction_text": "Battle of Sainte-Foy"}
{"answers": {"text": ["naval Battle of the Restigouche", "Battle of the Restigouche", "Battle of the Restigouche", "Battle of the Restigouche", "Battle of the Restigouche"], "answer_start": [502, 508, 508, 508, 508]}, "prediction_text": "Fort Niagara"}
{"answers": {"text": ["Governor Vaudreuil", "Governor Vaudreuil", "Governor Vaudreuil", "Governor Vaudreuil", "Governor Vaudreuil"], "answer_start": [55, 55, 55, 55, 55]}, "prediction_text": "General Amherst"}
{"answers": {"text": ["freedom to continue worshiping in their Roman Catholic tradition, continued ownership of their property,", "French residents who chose to remain in the colony would be given freedom", "continue worshiping in their Roman Catholic tradition, continued ownership of their property, and the right to remain undisturbed", "French residents who chose to remain in the colony would be given freedom"], "answer_start": [247, 181, 258, 181]}, "prediction_text": "Medical treatment for the sick and wounded French soldiers and French regular troops."}
{"answers": {"text": ["General Amherst.", "General Amherst", "General Amherst", "Amherst", "Amherst"], "answer_start": [119, 119, 119, 127, 127]}, "prediction_text": "General Amherst"}
{"answers": {"text": ["signing of the Treaty of Paris on 10 February 1763", "10 February 1763", "10 February 1763", "10 February 1763", "10 February 1763"], "answer_start": [51, 85, 85, 85, 85]}, "prediction_text": "10 February 1763."}
{"answers": {"text": ["Treaty of Hubertusburg on 15 February 1763", "15 February 1763", "15 February 1763", "15 February 1763", "15 February 1763"], "answer_start": [178, 204, 204, 204, 204]}, "prediction_text": "15 February 1763."}
{"answers": {"text": ["continental North American possessions east of the Mississippi or the Caribbean islands of Guadeloupe and Martinique", "surrendering either its continental North American possessions east of the Mississippi or the Caribbean islands of Guadeloupe and Martinique", "either its continental North American possessions east of the Mississippi or the Caribbean islands of Guadeloupe and Martinique,", "its continental North American possessions east of the Mississippi or the Caribbean islands", "surrendering either its continental North American possessions east of the Mississippi or the Caribbean islands of Guadeloupe and Martinique"], "answer_start": [287, 263, 276, 283, 263]}, "prediction_text": "The French chose to surrender their continental North American possessions east of the Mississippi."}
{"answers": {"text": ["value of the Caribbean islands' sugar cane to be greater and easier to defend than the furs from the continent", "They viewed the economic value of the Caribbean islands' sugar cane to be greater and easier to defend than the furs from the continent", "They viewed the economic value of the Caribbean islands' sugar cane to be greater", "value of the Caribbean islands' sugar", "They viewed the economic value of the Caribbean islands' sugar cane to be greater and easier to defend than the furs from the continent"], "answer_start": [659, 634, 634, 659, 634]}, "prediction_text": "To cede the former."}
{"answers": {"text": ["80,000", "80,000", "80,000", "80,000", "80,000"], "answer_start": [86, 86, 86, 86, 86]}, "prediction_text": "80,000"}
{"answers": {"text": ["1755", "1755", "1755", "1755", "1755"], "answer_start": [186, 186, 186, 186, 186]}, "prediction_text": "1755"}
{"answers": {"text": ["throughout its North American provinces", "throughout its North American provinces", "North American provinces", "North American provinces", "its North American provinces"], "answer_start": [315, 315, 330, 330, 326]}, "prediction_text": "France."}
{"answers": {"text": ["New Orleans", "New Orleans", "New Orleans", "New Orleans", "New Orleans"], "answer_start": [640, 398, 398, 640, 398]}, "prediction_text": "New Orleans"}
{"answers": {"text": ["King George III", "King George III", "King George III", "King George III", "King George III"], "answer_start": [22, 22, 22, 22, 22]}, "prediction_text": "King George III"}
{"answers": {"text": ["outlined the division and administration of the newly conquered territory", "outlined the division and administration of the newly conquered territory", "division and administration of the newly conquered territory", "division and administration", "the division and administration of the newly conquered territory"], "answer_start": [102, 102, 115, 115, 111]}, "prediction_text": "To divide and administer the newly conquered territory."}
{"answers": {"text": ["west of the Appalachian Mountains", "lands west of the Appalachian Mountains", "west of the Appalachian Mountains", "west of the Appalachian Mountains", "west of the Appalachian Mountains"], "answer_start": [345, 339, 345, 345, 345]}, "prediction_text": "West of the Appalachian Mountains."}
{"answers": {"text": ["Most went to Cuba,", "Most went to Cuba", "Cuba", "Cuba", "Cuba"], "answer_start": [831, 831, 844, 844, 844]}, "prediction_text": "Cuba"}
{"answers": {"text": ["military roads to the area by Braddock and Forbes", "construction of military roads to the area by Braddock and Forbes", "the construction of military roads to the area", "military roads to the area", "the construction of military roads"], "answer_start": [304, 288, 284, 304, 284]}, "prediction_text": "Military roads."}
{"answers": {"text": ["1769", "1769", "1769", "1769", "1769"], "answer_start": [443, 443, 443, 443, 443]}, "prediction_text": "1769"}
{"answers": {"text": ["Choctaw and the Creek", "tribes that did not want to do business with the British", "Choctaw and the Creek", "Choctaw and the Creek"], "answer_start": [646, 553, 646, 646]}, "prediction_text": "Choctaw and Creek."}
{"answers": {"text": ["disappearance of a strong ally and counterweight to British expansion, leading to their ultimate dispossession", "disappearance of a strong ally and counterweight to British expansion", "elimination of French power", "."], "answer_start": [88, 88, 33, 985]}, "prediction_text": "The loss of French power in North America meant the disappearance of a strong ally and counterweight to British expansion."}
{"answers": {"text": ["force", "force", "the concept of force", "the concept of force", "force", "force"], "answer_start": [46, 46, 31, 31, 46, 46]}, "prediction_text": "Force"}
{"answers": {"text": ["fundamental error", "A fundamental error", "A fundamental error", "A fundamental error", "A fundamental error", "A fundamental error"], "answer_start": [387, 385, 385, 385, 385, 385]}, "prediction_text": "Aristotle and Archimedes."}
{"answers": {"text": ["Sir Isaac Newton", "Sir Isaac Newton", "Sir Isaac Newton", "Sir Isaac Newton", "Sir Isaac Newton", "Sir Isaac Newton"], "answer_start": [654, 654, 654, 654, 654, 654]}, "prediction_text": "Sir Isaac Newton"}
{"answers": {"text": ["nearly three hundred years", "nearly three hundred years", "nearly three hundred years", "nearly three hundred years", "nearly three hundred years", "three hundred years"], "answer_start": [727, 727, 727, 727, 727, 734]}, "prediction_text": "Nearly three hundred years."}
{"answers": {"text": ["Einstein", "Einstein", "Einstein", "Einstein", "Einstein", "Einstein"], "answer_start": [782, 782, 782, 782, 782, 782]}, "prediction_text": "Einstein"}
{"answers": {"text": ["Standard Model", "Standard Model", "Standard Model", "Standard Model", "a Standard Model", "a Standard Model"], "answer_start": [149, 149, 149, 149, 147, 147]}, "prediction_text": "Standard Model"}
{"answers": {"text": ["gauge bosons", "gauge bosons", "gauge bosons", "gauge bosons", "gauge bosons", "gauge bosons"], "answer_start": [281, 281, 281, 281, 281, 281]}, "prediction_text": "gauge bosons"}
{"answers": {"text": ["strong", "strong", "strong,", "strong", "strong", "strong, electromagnetic"], "answer_start": [444, 444, 444, 444, 444, 444]}, "prediction_text": "Strongest main interaction: electromagnetic."}
{"answers": {"text": ["gravitational", "gravitational", "gravitational", "gravitational", "gravitational", "gravitational"], "answer_start": [479, 479, 479, 479, 479, 479]}, "prediction_text": "Electromagnetic force."}
{"answers": {"text": ["electroweak interaction", "a more fundamental electroweak interaction", "more fundamental electroweak interaction.", "a more fundamental electroweak interaction", "fundamental electroweak interaction."], "answer_start": [665, 646, 648, 646, 653]}, "prediction_text": "Electroweak interaction."}
{"answers": {"text": ["Aristotle", "Aristotle", "Aristotle", "Aristotle", "Aristotle", "Aristotle"], "answer_start": [0, 0, 0, 0, 0, 0]}, "prediction_text": "Aristotle"}
{"answers": {"text": ["Aristotelian cosmology", "Aristotelian cosmology", "Aristotelian cosmology", "Aristotelian cosmology", "cosmology", "Aristotelian cosmology"], "answer_start": [95, 95, 95, 95, 108, 95]}, "prediction_text": "Aristotle's cosmology."}
{"answers": {"text": ["four", "four", "four", "four", "four", "four"], "answer_start": [173, 173, 173, 173, 173, 173]}, "prediction_text": "Earth and water."}
{"answers": {"text": ["on the ground", "ground", "heavy bodies to fall", "on the ground", "on the ground", "on the ground"], "answer_start": [381, 388, 540, 381, 381, 381]}, "prediction_text": "Earth and water elements."}
{"answers": {"text": ["unnatural", "unnatural", "natural motion", "unnatural", "continued application of a force", "unnatural"], "answer_start": [598, 598, 577, 598, 641, 598]}, "prediction_text": "unnatural or forced motion."}
{"answers": {"text": ["17th century", "17th century", "17th century", "the 17th century", "17th century", "17th century"], "answer_start": [80, 80, 80, 76, 80, 80]}, "prediction_text": "17th century."}
{"answers": {"text": ["Galileo Galilei", "Galileo Galilei", "Galileo Galilei", "Galileo Galilei,", "Galileo Galilei", "Galileo Galilei"], "answer_start": [101, 101, 101, 101, 101, 101]}, "prediction_text": "Galileo Galilei"}
{"answers": {"text": ["impetus", "innate force of impetus", "gravity", "an innate force of impetus", "innate force of impetus", "innate force of impetus"], "answer_start": [220, 204, 447, 201, 204, 204]}, "prediction_text": "innate force of impetus."}
{"answers": {"text": ["Galileo", "Galileo", "Galileo Galilei", "Galileo", "Galileo", "Galileo"], "answer_start": [229, 229, 101, 229, 229, 229]}, "prediction_text": "Galileo Galilei"}
{"answers": {"text": ["friction", "friction", "gravity", "friction", "friction", "force"], "answer_start": [589, 589, 447, 589, 589, 570]}, "prediction_text": "Friction."}
{"answers": {"text": ["Newton", "Newton's", "Newton's", "Newton's", "Newton's"], "answer_start": [0, 0, 0, 0, 0]}, "prediction_text": "Newton"}
{"answers": {"text": ["lack of net force", "lack of net force", "constant velocity was associated with a lack of net force", "constant velocity was associated with a lack of net force", "lack of net force"], "answer_start": [252, 252, 212, 212, 252]}, "prediction_text": "Lack of net force."}
{"answers": {"text": ["Newton", "Newton", "Newton", "Newton", "Newton"], "answer_start": [319, 319, 319, 319, 319]}, "prediction_text": "Newton"}
{"answers": {"text": ["Newton's First", "first law", "Newton's First Law", "First Law", "Newton's First Law"], "answer_start": [737, 526, 737, 746, 737]}, "prediction_text": "Newton's First Law."}
{"answers": {"text": ["the same", "a Galilean transformation", "Galilean transformation", "the laws of physics are the same in every inertial frame of reference", "the same in every inertial frame of reference"], "answer_start": [1065, 1146, 1148, 1041, 1065]}, "prediction_text": "Newton's First Law of Motion."}
{"answers": {"text": ["laws of physics", "the laws of physics", "the laws of physics", "the laws of physics", "the laws of physics"], "answer_start": [78, 74, 74, 74, 74]}, "prediction_text": "Laws of physics."}
{"answers": {"text": ["parabolic", "parabolic path", "a curving parabolic path", "curving parabolic path", "curving parabolic path"], "answer_start": [411, 411, 401, 403, 403]}, "prediction_text": "Curving parabolic path."}
{"answers": {"text": ["at rest", "at rest", "rest", "the vehicle and everything inside of it is at rest:"], "answer_start": [770, 770, 773, 727]}, "prediction_text": "At rest."}
{"answers": {"text": ["Inertia", "Inertia", "Inertia", "Inertia", "Inertia"], "answer_start": [1067, 1067, 1067, 1067, 1067]}, "prediction_text": "Inertia."}
{"answers": {"text": ["inertia", "inertia", "inertia", "inertia", "inertia"], "answer_start": [15, 15, 15, 15, 15]}, "prediction_text": "Inertia"}
{"answers": {"text": ["rotational inertia of planet", "rotational inertia of planet Earth", "rotational inertia", "rotational inertia", "The rotational inertia of planet Earth"], "answer_start": [197, 197, 197, 197, 193]}, "prediction_text": "Rotational inertia."}
{"answers": {"text": ["Albert Einstein", "Albert Einstein", "Albert Einstein", "Albert Einstein", "Albert Einstein"], "answer_start": [309, 309, 309, 309, 309]}, "prediction_text": "Albert Einstein"}
{"answers": {"text": ["weightlessness", "weightlessness", "weightlessness", "weightlessness", "weightlessness"], "answer_start": [602, 602, 602, 602, 602]}, "prediction_text": "Weightlessness."}
{"answers": {"text": ["principle of equivalence", "principle of equivalence", "principle of equivalence", "principle of equivalence", "principle of equivalence"], "answer_start": [1062, 1062, 1062, 1062, 1062]}, "prediction_text": "Albert Einstein."}
{"answers": {"text": ["Newton's Second Law", "Newton's Second Law", "Newton's Second Law", "Newton's Second Law", "Newton's Second Law"], "answer_start": [0, 0, 0, 0, 0]}, "prediction_text": "Newton's Second Law."}
{"answers": {"text": ["kinematic", "kinematic measurements", "kinematic", "kinematic", "kinematic"], "answer_start": [174, 174, 174, 174, 174]}, "prediction_text": "Kinematic measurements."}
{"answers": {"text": ["General relativity", "General relativity", "General relativity", "General relativity", "General relativity"], "answer_start": [385, 385, 385, 385, 385]}, "prediction_text": "General relativity."}
{"answers": {"text": ["General relativity", "coherent theory of quantum gravity", "General relativity", "General relativity"], "answer_start": [385, 469, 385, 385]}, "prediction_text": "Newton's second law."}
{"answers": {"text": ["fixed", "an equality", "fixed", "fixed", "unclear"], "answer_start": [760, 701, 760, 760, 511]}, "prediction_text": "Relative units of force and mass."}
{"answers": {"text": ["Newton's Third", "Newton's Third Law", "Newton's Third Law", "Newton's Third Law", "Newton's Third Law"], "answer_start": [0, 0, 0, 0, 0]}, "prediction_text": "Newton's Third Law."}
{"answers": {"text": ["Newton's Third", "The third law", "The third law", "Newton's Third Law", "Newton's Third Law"], "answer_start": [0, 135, 135, 0, 0]}, "prediction_text": "Newton's Third Law."}
{"answers": {"text": ["unidirectional", "unidirectional force", "unidirectional force", "unidirectional", "unidirectional force"], "answer_start": [264, 264, 264, 264, 264]}, "prediction_text": "Unidirectional force."}
{"answers": {"text": ["magnitude", "action-reaction", "the action-reaction law", "equal in magnitude"], "answer_start": [456, 534, 530, 447]}, "prediction_text": "F = \u2212F"}
{"answers": {"text": ["center of mass", "center of mass", "the center of mass", "the center of mass", "the center of mass"], "answer_start": [421, 421, 417, 417, 417]}, "prediction_text": "The center of mass."}
{"answers": {"text": ["closed", "closed system", "closed system of particles", "a closed system of particles", "a closed system"], "answer_start": [21, 21, 21, 19, 19]}, "prediction_text": "Closed system."}
{"answers": {"text": ["mass of the system", "mass of the system", "the mass of the system", "the mass of the system", "mass of the system"], "answer_start": [535, 535, 531, 531, 535]}, "prediction_text": "Mass of system."}
{"answers": {"text": ["intuitive understanding", "an intuitive understanding", "an intuitive understanding", "intuitive understanding", "an intuitive understanding"], "answer_start": [67, 64, 64, 67, 64]}, "prediction_text": "Intuitive understanding."}
{"answers": {"text": ["standard measurement scale", "precise operational definitions", "precise operational definitions", "precise operational definitions", "precise operational definitions"], "answer_start": [319, 224, 224, 224, 224]}, "prediction_text": "Precise operational definitions."}
{"answers": {"text": ["Newtonian mechanics", "Newtonian mechanics", "Newtonian mechanics", "Newtonian mechanics", "Newtonian mechanics"], "answer_start": [500, 500, 500, 500, 500]}, "prediction_text": "Newtonian mechanics"}
{"answers": {"text": ["experimentation", "Through experimentation", "Through experimentation", "Through experimentation", "experimentation"], "answer_start": [355, 347, 347, 347, 355]}, "prediction_text": "Through experimentation."}
{"answers": {"text": ["vector quantities", "vector quantities", "vector quantities", "\"vector quantities\"", "dependent upon how strong"], "answer_start": [159, 159, 159, 158, 52]}, "prediction_text": "Forces follow a different set of mathematical rules than physical quantities that do not have direction."}
{"answers": {"text": ["denoted scalar quantities", "scalar quantities", "denoted scalar quantities", "scalar quantities", "scalar quantities"], "answer_start": [300, 308, 300, 308, 308]}, "prediction_text": "Scalar quantities."}
{"answers": {"text": ["Associating forces with vectors", "Associating forces with vectors", "Associating forces with vectors", "Associating forces with vectors", "know both the magnitude and the direction of both forces to calculate the result"], "answer_start": [1188, 1188, 1188, 1188, 430]}, "prediction_text": "By associating forces with vectors."}
{"answers": {"text": ["ambiguous", "ambiguous", "ambiguous", "ambiguous", "ambiguous"], "answer_start": [598, 598, 598, 598, 598]}, "prediction_text": "Ambiguous."}
{"answers": {"text": ["Associating forces with vectors", "Associating forces with vectors", "adding the two force magnitudes or subtracting one from the other", "knowing the direction of the forces", "it is impossible"], "answer_start": [1188, 1188, 1121, 1019, 772]}, "prediction_text": "You know the magnitude and direction of both forces."}
{"answers": {"text": ["static equilibrium", "static equilibrium", "conditions of static equilibrium", "static equilibrium", "static equilibrium"], "answer_start": [77, 77, 63, 77, 77]}, "prediction_text": "Static equilibrium."}
{"answers": {"text": ["magnitude and direction", "magnitude and direction", "magnitude and direction", "magnitude and direction", "magnitude and direction"], "answer_start": [248, 248, 248, 248, 248]}, "prediction_text": "magnitude and direction."}
{"answers": {"text": ["net force", "net force", "the resultant (also called the net force)", "net force", "net force"], "answer_start": [366, 366, 335, 366, 366]}, "prediction_text": "net force"}
{"answers": {"text": ["respective lines of application", "respective lines of application", "their respective lines of application", "their respective lines of application", "their effects on the motion of the body"], "answer_start": [863, 863, 857, 857, 942]}, "prediction_text": "Lines of application."}
{"answers": {"text": ["parallelogram", "parallelogram", "parallelogram", "parallelogram", "parallelogram"], "answer_start": [506, 506, 413, 625, 413]}, "prediction_text": "parallelogram rule of vector addition."}
{"answers": {"text": ["independent components", "independent components", "independent components", "independent components"], "answer_start": [57, 57, 57, 57]}, "prediction_text": "components of a set of basis vectors."}
{"answers": {"text": ["two", "two", "two", "two"], "answer_start": [177, 177, 177, 177]}, "prediction_text": "2."}
{"answers": {"text": ["the original force", "the original force", "the original force", "the original force"], "answer_start": [292, 292, 292, 292]}, "prediction_text": "The original force."}
{"answers": {"text": ["orthogonal", "uniquely determined", "orthogonal components", "orthogonal components"], "answer_start": [497, 557, 497, 497]}, "prediction_text": "The independant components of a vector sum that has been determined by scalar addition of individual vectors."}
{"answers": {"text": ["three-dimensional", "three-dimensional", "three-dimensional", "three-dimensional"], "answer_start": [1134, 1134, 1134, 1134]}, "prediction_text": "Three-dimensional."}
{"answers": {"text": ["static friction", "static friction", "friction", "static friction", "applied force"], "answer_start": [147, 147, 154, 147, 119]}, "prediction_text": "Static friction."}
{"answers": {"text": ["static friction", "static friction", "static friction", "static friction", "static friction"], "answer_start": [147, 147, 147, 147, 147]}, "prediction_text": "Static friction."}
{"answers": {"text": ["applied", "applied force", "applied force", "applied force", "applied"], "answer_start": [297, 119, 297, 297, 297]}, "prediction_text": "applied force"}
{"answers": {"text": ["applied force", "applied force", "applied force", "applied force", "applied force"], "answer_start": [403, 403, 403, 403, 403]}, "prediction_text": "The applied force."}
{"answers": {"text": ["forces", "static equilibrium", "forces", "force of gravity", "forces"], "answer_start": [75, 2, 75, 225, 75]}, "prediction_text": "Forces."}
{"answers": {"text": ["spring reaction force", "spring reaction force", "the force of gravity", "spring reaction", "spring reaction force"], "answer_start": [299, 299, 221, 299, 299]}, "prediction_text": "Spring reaction force."}
{"answers": {"text": ["gravity", "object's weight", "spring reaction force", "the object's weight", "the object's weight"], "answer_start": [234, 340, 299, 336, 336]}, "prediction_text": "The weight of the object."}
{"answers": {"text": ["gravity", "gravity", "the force of gravity", "force of gravity", "the force of gravity"], "answer_start": [439, 439, 426, 430, 426]}, "prediction_text": "gravity"}
{"answers": {"text": ["Isaac Newton", "Isaac Newton", "Isaac Newton", "Isaac Newton", "Isaac Newton"], "answer_start": [757, 757, 757, 757, 757]}, "prediction_text": "Isaac Newton"}
{"answers": {"text": ["Galileo", "Galileo", "Galileo", "Galileo", "Galileo"], "answer_start": [43, 43, 43, 43, 43]}, "prediction_text": "Galileo"}
{"answers": {"text": ["rest", "rest", "rest", "rest", "rest"], "answer_start": [354, 354, 354, 354, 354]}, "prediction_text": "rest"}
{"answers": {"text": ["Galileo", "Aristotle's", "Aristotle", "Aristotle", "Aristotle's"], "answer_start": [272, 381, 381, 381, 381]}, "prediction_text": "Aristotle"}
{"answers": {"text": ["behind the foot of the mast", "straight down", "behind the foot of the mast", "behind the foot of the mast", "behind the foot of the mast of a moving ship"], "answer_start": [869, 753, 869, 869, 869]}, "prediction_text": "Behind the foot of the mast."}
{"answers": {"text": ["foot of the mast", "the foot of the mast", "at the foot of the mast", "at the foot of the mast", "at the foot of the mast"], "answer_start": [1003, 999, 996, 996, 996]}, "prediction_text": "Behind the foot of the mast."}
{"answers": {"text": ["dynamic equilibrium", "dynamic equilibrium", "dynamic equilibrium", "dynamic equilibrium", "dynamic equilibrium"], "answer_start": [17, 17, 17, 17, 17]}, "prediction_text": "Constant velocity motion across a surface."}
{"answers": {"text": ["kinetic friction force", "kinetic friction force", "kinetic friction force", "kinetic friction", "kinetic friction force"], "answer_start": [189, 189, 189, 189, 189]}, "prediction_text": "Kinetic friction."}
{"answers": {"text": ["kinetic friction", "object started with a non-zero velocity", "kinetic friction", "kinetic friction"], "answer_start": [470, 293, 470, 470]}, "prediction_text": "Kinetic friction."}
{"answers": {"text": ["Aristotle", "Aristotle", "Aristotle", "Aristotle", "Aristotle"], "answer_start": [381, 381, 381, 381, 381]}, "prediction_text": "Aristotle"}
{"answers": {"text": ["Schr\u00f6dinger", "Newtonian equations", "the Schr\u00f6dinger equation", "Schr\u00f6dinger", "Schr\u00f6dinger equation"], "answer_start": [178, 210, 174, 178, 178]}, "prediction_text": "Schr\u00f6dinger equation"}
{"answers": {"text": ["Newtonian", "Newtonian equations", "Newtonian equations.", "Newtonian", "Newtonian equations"], "answer_start": [210, 210, 210, 210, 210]}, "prediction_text": "Schr\u00f6dinger equation"}
{"answers": {"text": ["classical position variables", "classical position variables", "classical position variables", "classical position variables", "classical position variables"], "answer_start": [544, 544, 544, 544, 544]}, "prediction_text": "V(x,y,z) or fields."}
{"answers": {"text": ["quantized", "operators", "quantized", "quantized", "quantized"], "answer_start": [309, 90, 309, 309, 309]}, "prediction_text": "discrete portions."}
{"answers": {"text": ["force", "force", "force", "force", "The notion \"force\""], "answer_start": [12, 12, 12, 12, 0]}, "prediction_text": "\"force\""}
{"answers": {"text": ["spin", "spin", "spin", "spin", "the \"spin"], "answer_start": [220, 220, 220, 220, 215]}, "prediction_text": "spin"}
{"answers": {"text": ["Pauli", "Pauli principle", "Pauli principle", "Pauli", "Pauli"], "answer_start": [244, 244, 244, 244, 244]}, "prediction_text": "Pauli principle."}
{"answers": {"text": ["spin", "value of the spin", "the value of the spin", "the spin", "value of the spin,"], "answer_start": [333, 320, 316, 329, 320]}, "prediction_text": "The value of the spin."}
{"answers": {"text": ["antiparallel", "antiparallel spins", "antisymmetric", "antiparallel", "antiparallel"], "answer_start": [668, 668, 540, 668, 668]}, "prediction_text": "Parallel spins."}
{"answers": {"text": ["parallel", "antisymmetric", "symmetric", "parallel", "symmetric"], "answer_start": [494, 540, 464, 494, 464]}, "prediction_text": "Parallel spins."}
{"answers": {"text": ["mathematical by-product", "mathematical by-product of exchange", "a mathematical by-product of exchange of momentum", "mathematical by-product of exchange"], "answer_start": [88, 88, 86, 88]}, "prediction_text": "Through the use of Feynman diagrams."}
{"answers": {"text": ["force", "4-momentum in relativity and momentum of virtual particles in quantum electrodynamics", "force"], "answer_start": [249, 317, 249]}, "prediction_text": "Force."}
{"answers": {"text": ["conservation of momentum", "conservation of momentum", "conservation of momentum", "the currently known fundamental forces"], "answer_start": [409, 409, 409, 580]}, "prediction_text": "momentum conservation."}
{"answers": {"text": ["Feynman", "Feynman diagrams", "Feynman"], "answer_start": [1199, 1199, 1199]}, "prediction_text": "Feynman diagrams."}
{"answers": {"text": ["straight", "straight line", "straight", "straight"], "answer_start": [1280, 1280, 1280, 1280]}, "prediction_text": "Straight lines."}
{"answers": {"text": ["four", "four", "four", "four"], "answer_start": [47, 47, 47, 47]}, "prediction_text": "Four."}
{"answers": {"text": ["strong and weak", "strong and weak forces", "strong and weak", "strong and weak forces"], "answer_start": [82, 82, 82, 82]}, "prediction_text": "Strong and weak forces."}
{"answers": {"text": ["electromagnetic", "electromagnetic force", "electromagnetic"], "answer_start": [277, 277, 277]}, "prediction_text": "Electromagnetic force."}
{"answers": {"text": ["masses", "masses", "masses", "masses"], "answer_start": [371, 371, 371, 371]}, "prediction_text": "masses"}
{"answers": {"text": ["Pauli exclusion principle", "the Pauli exclusion principle", "the Pauli exclusion principle", "Pauli exclusion principle"], "answer_start": [575, 571, 571, 575]}, "prediction_text": "Pauli exclusion principle."}
{"answers": {"text": ["Isaac Newton", "Isaac Newton", "Isaac Newton", "Isaac Newton"], "answer_start": [125, 125, 125, 125]}, "prediction_text": "Isaac Newton"}
{"answers": {"text": ["20th", "the 20th century", "20th", "20th"], "answer_start": [479, 475, 479, 479]}, "prediction_text": "20th century"}
{"answers": {"text": ["unification", "unification models", "unification", "unification"], "answer_start": [1457, 1457, 1457, 1457]}, "prediction_text": "String theory."}
{"answers": {"text": ["self-consistent unification", "self-consistent unification models", "self-consistent unification", "self-consistent unification models that would combine all four fundamental interactions"], "answer_start": [1441, 1441, 1441, 1441]}, "prediction_text": "Grand unified theory."}
{"answers": {"text": ["Isaac Newton", "Isaac Newton", "Isaac Newton", "Isaac Newton"], "answer_start": [83, 83, 83, 83]}, "prediction_text": "Isaac Newton"}
{"answers": {"text": ["Galileo", "Galileo", "Galileo", "Galileo"], "answer_start": [233, 233, 233, 233]}, "prediction_text": "Isaac Newton"}
{"answers": {"text": ["about 9.81 meters per second squared", "about 9.81 meters per second squared", "about 9.81 meters per second", "9.81 meters per second"], "answer_start": [555, 555, 555, 561]}, "prediction_text": "9.81 meters per second squared."}
{"answers": {"text": ["sea level", "sea level", "sea level", "from sea level"], "answer_start": [624, 624, 624, 619]}, "prediction_text": "Sea level."}
{"answers": {"text": ["force of gravity", "the force of gravity on an object", "force of gravity", "force of gravity"], "answer_start": [746, 742, 746, 746]}, "prediction_text": "9.81 meters per second squared."}
{"answers": {"text": ["at larger distances.", "at larger distances", "at larger distances", "at larger distances"], "answer_start": [87, 87, 87, 87]}, "prediction_text": "At larger distances."}
{"answers": {"text": ["the Moon", "the mass () and the radius () of the Earth", "force of gravity", "the same force of gravity if the acceleration due to gravity decreased as an inverse square law."], "answer_start": [166, 475, 222, 213]}, "prediction_text": "Mass of the attracting body."}
{"answers": {"text": ["mass", "the mass of the attracting body", "the mass of the attracting body", "the mass of the attracting body"], "answer_start": [395, 391, 391, 391]}, "prediction_text": "Mass of the attracting body."}
{"answers": {"text": ["radius () of the Earth", "the radius () of the Earth", "radius"], "answer_start": [495, 491, 495]}, "prediction_text": "Mass of the Earth."}
{"answers": {"text": ["Newton's Universal Gravitation Constant,", "dimensional constant", "a dimensional constant", "a dimensional constant"], "answer_start": [134, 20, 18, 18]}, "prediction_text": "Newton's Universal Gravitation Constant"}
{"answers": {"text": ["Henry Cavendish", "Henry Cavendish", "Henry Cavendish", "Henry Cavendish"], "answer_start": [245, 245, 245, 245]}, "prediction_text": "Henry Cavendish"}
{"answers": {"text": ["1798", "1798", "1798", "1798"], "answer_start": [236, 236, 236, 236]}, "prediction_text": "1798"}
{"answers": {"text": ["Newton", "Newton", "Newton", "Newton"], "answer_start": [492, 643, 492, 492]}, "prediction_text": "Henry Cavendish"}
{"answers": {"text": ["Mercury", "Mercury", "Mercury", "Mercury"], "answer_start": [36, 36, 36, 36]}, "prediction_text": "Mercury"}
{"answers": {"text": ["Vulcan", "Vulcan", "Vulcan", "Vulcan"], "answer_start": [170, 170, 170, 170]}, "prediction_text": "Vulcan"}
{"answers": {"text": ["theory of general relativity", "theory of general relativity (GR)", "general relativity", "general relativity"], "answer_start": [324, 324, 334, 334]}, "prediction_text": "GR"}
{"answers": {"text": ["Albert Einstein", "Albert Einstein", "Albert Einstein", "Albert Einstein"], "answer_start": [293, 293, 293, 293]}, "prediction_text": "Albert Einstein"}
{"answers": {"text": ["Albert Einstein", "Albert Einstein", "Albert Einstein", "Albert Einstein"], "answer_start": [293, 293, 293, 293]}, "prediction_text": "Albert Einstein"}
{"answers": {"text": ["general relativity", "general relativity", "general relativity", "general relativity"], "answer_start": [24, 24, 24, 24]}, "prediction_text": "General relativity."}
{"answers": {"text": ["ballistic trajectory", "path between two space-time events", "straight lines", "the shortest space-time path between two space-time events."], "answer_start": [716, 322, 244, 298]}, "prediction_text": "Ballistic trajectory of the object."}
{"answers": {"text": ["gravitational force", "gravitational force", "gravitational force", "gravitational force"], "answer_start": [1117, 1117, 1117, 1117]}, "prediction_text": "gravitational force"}
{"answers": {"text": ["global", "a global sense", "in space", "the perspective of the object"], "answer_start": [498, 496, 685, 363]}, "prediction_text": "From the perspective of the object, all motion occurs as if there were no gravitation whatsoever."}
{"answers": {"text": ["electric current", "electric current", "electric current", "electric current"], "answer_start": [36, 36, 36, 36]}, "prediction_text": "electric current"}
{"answers": {"text": ["unified electromagnetic", "unified electromagnetic force", "electromagnetic", "unified electromagnetic force"], "answer_start": [294, 294, 302, 294]}, "prediction_text": "Magnetic field."}
{"answers": {"text": ["Lorentz's Law", "Lorentz's Law", "Lorentz's Law", "Lorentz's Law"], "answer_start": [139, 139, 139, 139]}, "prediction_text": "Lorentz's Law"}
{"answers": {"text": ["electrostatic force", "the electrostatic force (due to the electric field) and the magnetic force (due to the magnetic field).", "electrostatic force (due to the electric field) and the magnetic force", "the electrostatic force (due to the electric field) and the magnetic force"], "answer_start": [389, 385, 389, 385]}, "prediction_text": "electrostatic force and magnetic force."}
{"answers": {"text": ["James Clerk Maxwell", "James Clerk Maxwell", "James Clerk Maxwell", "James Clerk Maxwell"], "answer_start": [88, 88, 88, 88]}, "prediction_text": "James Clerk Maxwell"}
{"answers": {"text": ["1864", "1864", "1864", "1864"], "answer_start": [78, 78, 78, 78]}, "prediction_text": "1864"}
{"answers": {"text": ["20", "20", "20", "20"], "answer_start": [159, 159, 159, 159]}, "prediction_text": "4"}
{"answers": {"text": ["4", "4", "4", "4"], "answer_start": [215, 215, 215, 215]}, "prediction_text": "4 vector equations."}
{"answers": {"text": ["Maxwell", "James Clerk Maxwell", "Maxwell", "Maxwell"], "answer_start": [444, 88, 444, 444]}, "prediction_text": "Josiah Willard Gibbs"}
{"answers": {"text": ["electromagnetic theory", "electromagnetic theory", "electromagnetic theory", "electromagnetic theory"], "answer_start": [33, 33, 33, 33]}, "prediction_text": "Attempting to reconcile electromagnetic theory with two observations."}
{"answers": {"text": ["quantum mechanics", "quantum mechanics", "quantum mechanics", "the work of leading theoretical physicists"], "answer_start": [283, 283, 283, 186]}, "prediction_text": "Quantum mechanics."}
{"answers": {"text": ["quantum electrodynamics", "quantum electrodynamics (or QED)", "quantum electrodynamics", "quantum electrodynamics"], "answer_start": [370, 370, 370, 370]}, "prediction_text": "QED."}
{"answers": {"text": ["photons", "photons", "photons", "photons"], "answer_start": [501, 501, 501, 501]}, "prediction_text": "photons"}
{"answers": {"text": ["quantum electrodynamics", "quantum electrodynamics", "quantum electrodynamics", "quantum electrodynamics"], "answer_start": [370, 370, 370, 370]}, "prediction_text": "Quantum electrodynamics (or QED)"}
{"answers": {"text": ["repulsion of like charges", "repulsion of like charges under the influence of the electromagnetic force", "repulsion of like charges", "the repulsion of like charges under the influence of the electromagnetic force"], "answer_start": [90, 90, 90, 86]}, "prediction_text": "Electromagnetic force."}
{"answers": {"text": ["the Pauli exclusion principle", "the Pauli exclusion principle", "Pauli exclusion principle", "Pauli exclusion principle"], "answer_start": [218, 218, 222, 222]}, "prediction_text": "Electromagnetic force."}
{"answers": {"text": ["energy", "energy", "energy", "energy"], "answer_start": [579, 579, 579, 579]}, "prediction_text": "energy"}
{"answers": {"text": ["as a structural force", "as a structural force", "a structural force", "macroscopically as a structural force"], "answer_start": [657, 657, 660, 641]}, "prediction_text": "The Pauli exclusion principle manifests in the macro world as a finite set of electron states."}
{"answers": {"text": ["repulsion of like charges", "repulsion of like charges under the influence of the electromagnetic force", "repulsion of like charges", "the repulsion of like charges under the influence of the electromagnetic force"], "answer_start": [90, 90, 90, 86]}, "prediction_text": "Electromagnetic force."}
{"answers": {"text": ["the Pauli exclusion principle", "the Pauli exclusion principle", "Pauli exclusion principle", "Pauli exclusion principle"], "answer_start": [218, 218, 222, 222]}, "prediction_text": "Electromagnetic force."}
{"answers": {"text": ["energy", "energy", "energy", "energy"], "answer_start": [579, 539, 579, 579]}, "prediction_text": "energy"}
{"answers": {"text": ["as a structural force", "as a structural force", "a structural force", "macroscopically as a structural force"], "answer_start": [657, 657, 660, 641]}, "prediction_text": "The Pauli exclusion principle manifests in the macro world as a finite set of electron states."}
{"answers": {"text": ["elementary particles", "elementary particles", "elementary particles", "elementary particles"], "answer_start": [41, 41, 41, 41]}, "prediction_text": "elementary particles"}
{"answers": {"text": ["residual of the force", "residual of the force", "nucleons in atomic nuclei", "a residual of the force"], "answer_start": [74, 74, 182, 72]}, "prediction_text": "gluons"}
{"answers": {"text": ["nuclear", "nuclear force.", "nuclear", "nuclear force"], "answer_start": [216, 216, 216, 216]}, "prediction_text": "gluons"}
{"answers": {"text": ["as gluons", "as gluons", "as gluons", "as gluons"], "answer_start": [282, 282, 282, 282]}, "prediction_text": "Gluons."}
{"answers": {"text": ["color confinement", "color confinement", "color confinement", "color confinement"], "answer_start": [564, 564, 564, 564]}, "prediction_text": "Color confinement."}
{"answers": {"text": ["weak force", "weak force", "weak force", "weak force"], "answer_start": [4, 4, 4, 4]}, "prediction_text": "The weak force."}
{"answers": {"text": ["beta decay", "beta decay (of neutrons in atomic nuclei)", "beta decay", "beta decay"], "answer_start": [95, 95, 95, 95]}, "prediction_text": "beta decay."}
{"answers": {"text": ["radioactivity", "radioactivity", "radioactivity", "radioactivity"], "answer_start": [156, 156, 156, 156]}, "prediction_text": "Radioactivity."}
{"answers": {"text": ["1013", "1013", "1013", "1013"], "answer_start": [241, 241, 241, 241]}, "prediction_text": "1013 times."}
{"answers": {"text": ["approximately 1015 kelvins", "in excess of approximately 1015 kelvins", "1015 kelvins", "in excess of approximately 1015 kelvins"], "answer_start": [514, 501, 528, 501]}, "prediction_text": "1015 kelvins."}
{"answers": {"text": ["normal force", "normal force", "normal force", "normal force"], "answer_start": [4, 4, 4, 4]}, "prediction_text": "Pauli repulsion"}
{"answers": {"text": ["Pauli repulsion", "Pauli repulsion", "Pauli repulsion", "Pauli repulsion"], "answer_start": [127, 127, 127, 127]}, "prediction_text": "Pauli repulsion."}
{"answers": {"text": ["fermionic nature of electrons", "fermionic nature of electrons", "fermionic nature of electrons", "fermionic nature of electrons"], "answer_start": [151, 151, 151, 151]}, "prediction_text": "Electron clouds."}
{"answers": {"text": ["normal", "normal force", "normal force", "normal force"], "answer_start": [298, 298, 298, 298]}, "prediction_text": "Pauli repulsion."}
{"answers": {"text": ["ideal strings", "ideal strings that are massless", "ideal strings that are massless", "ideal strings that are massless, frictionless, unbreakable, and unstretchable"], "answer_start": [36, 36, 36, 36]}, "prediction_text": "Ideal strings."}
{"answers": {"text": ["ideal pulleys", "ideal pulleys", "ideal pulleys", "ideal pulleys"], "answer_start": [141, 141, 141, 141]}, "prediction_text": "movable pulleys"}
{"answers": {"text": ["action-reaction pairs", "instantaneously in action-reaction pairs", "in action-reaction pairs", "instantaneously in action-reaction pairs"], "answer_start": [269, 250, 266, 250]}, "prediction_text": "instantaneously in action-reaction pairs."}
{"answers": {"text": ["conservation of mechanical energy", "conservation of mechanical energy", "the tension force on a load can be multiplied", "tension force on a load can be multiplied"], "answer_start": [997, 997, 623, 627]}, "prediction_text": "The final effect of adding more and more idea strings to a load is the conservation of mechanical energy."}
{"answers": {"text": ["movable pulleys", "connecting the same string multiple times to the same object through the use of a set-up that uses movable pulleys,", "every string", "connecting the same string multiple times to the same object through the use of a set-up that uses movable pulleys"], "answer_start": [606, 507, 674, 507]}, "prediction_text": "movable pulleys"}
{"answers": {"text": ["idealized point particles", "idealized point particles rather than three-dimensional objects", "idealized point particles", "idealized point particles"], "answer_start": [100, 100, 100, 100]}, "prediction_text": "Idealized point particles."}
{"answers": {"text": ["three-dimensional objects", "three-dimensional objects", "three-dimensional objects"], "answer_start": [138, 138, 138]}, "prediction_text": "Three-dimensional objects."}
{"answers": {"text": ["extended", "extended", "extended"], "answer_start": [530, 530, 530]}, "prediction_text": "Extended fluids."}
{"answers": {"text": ["other parts", "other parts of an object", "other parts of an object", "other parts of an object"], "answer_start": [276, 276, 276, 276]}, "prediction_text": "Other parts of the object."}
{"answers": {"text": ["extended structure", "extended structure", "extended structure", "extended structure and forces that act on one part of an object might affect other parts of an object"], "answer_start": [199, 199, 199, 199]}, "prediction_text": "Structure."}
{"answers": {"text": ["stress tensor", "stress tensor", "deformations", "The stress tensor"], "answer_start": [376, 376, 434, 372]}, "prediction_text": "Deformations."}
{"answers": {"text": ["pressure terms", "stress tensor", "pressure terms associated with forces that act normal to the cross-sectional area (the matrix diagonals of the tensor) as well as shear terms"], "answer_start": [132, 376, 132]}, "prediction_text": "The cross-sectional area."}
{"answers": {"text": ["pressure terms", "matrix diagonals of the tensor)", "pressure terms"], "answer_start": [132, 219, 132]}, "prediction_text": "matrix diagonals"}
{"answers": {"text": ["formalism", "the relevant cross-sectional area for the volume for which the stress-tensor is being calculated", "formalism", "This formalism"], "answer_start": [113, 10, 113, 108]}, "prediction_text": "matrix diagonals"}
{"answers": {"text": ["rotational equivalent for position", "rotation", "rotational inertia", "angle is the rotational equivalent for position"], "answer_start": [77, 14, 242, 64]}, "prediction_text": "Angular momentum."}
{"answers": {"text": ["unbalanced torque", "unbalanced torque", "unbalanced torque", "an unbalanced torque"], "answer_start": [346, 346, 346, 343]}, "prediction_text": "Unbalanced torque."}
{"answers": {"text": ["Newton's Second Law of Motion", "Newton's Second Law of Motion", "Newton's Second Law of Motion", "Newton's Second Law of Motion"], "answer_start": [375, 375, 375, 375]}, "prediction_text": "Newton's Second Law of Motion."}
{"answers": {"text": ["toward the center of the curving path", "center of the curving path.", "the center of the curving path", "directed toward the center of the curving path"], "answer_start": [291, 302, 298, 282]}, "prediction_text": "The unbalanced centripetal force goes to the center of the curving path."}
{"answers": {"text": ["perpendicular", "perpendicular", "perpendicular", "perpendicular"], "answer_start": [346, 346, 346, 346]}, "prediction_text": "They change the direction of the velocity vector."}
{"answers": {"text": ["centripetal", "unbalanced centripetal force", "unbalanced centripetal force", "centripetal"], "answer_start": [837, 224, 224, 837]}, "prediction_text": "Centripetal force."}
{"answers": {"text": ["radial", "radial (centripetal) force", "radial", "radial"], "answer_start": [829, 829, 829, 829]}, "prediction_text": "tangential force"}
{"answers": {"text": ["tangential force", "tangential force", "tangential force", "tangential force"], "answer_start": [729, 729, 729, 729]}, "prediction_text": "The unbalanced centripetal force."}
{"answers": {"text": ["kinetic", "kinetic", "kinetic", "kinetic"], "answer_start": [127, 127, 127, 127]}, "prediction_text": "kinetic energy"}
{"answers": {"text": ["potential", "potential", "potential", "potential"], "answer_start": [138, 138, 138, 138]}, "prediction_text": "Potential energy."}
{"answers": {"text": ["net mechanical energy", "net mechanical energy", "net mechanical energy", "net mechanical energy"], "answer_start": [196, 196, 196, 196]}, "prediction_text": "The net mechanical energy."}
{"answers": {"text": ["difference in potential energy", "the difference in potential energy", "the difference in potential energy", "the difference in potential energy between two different locations in space"], "answer_start": [330, 326, 326, 326]}, "prediction_text": "Potential energy."}
{"answers": {"text": ["artifact", "artifact of the potential field", "an artifact"], "answer_start": [434, 434, 431]}, "prediction_text": "The force called rgarding a potential field."}
{"answers": {"text": ["forces", "forces as being due to gradient of potentials", "forces", "forces as being due to gradient of potentials"], "answer_start": [58, 58, 58, 58]}, "prediction_text": "forces due to gradient of potentials"}
{"answers": {"text": ["gradient of potentials", "macrophysical considerations that yield forces as arising from a macroscopic statistical average of microstates", "gradient of potentials.", "gradient of potentials"], "answer_start": [81, 126, 81, 81]}, "prediction_text": "Macroscopic statistical average of microstates."}
{"answers": {"text": ["friction", "friction", "friction", "friction"], "answer_start": [252, 252, 252, 252]}, "prediction_text": "Forces."}
{"answers": {"text": ["Nonconservative", "Nonconservative forces other than friction", "Nonconservative", "Nonconservative forces"], "answer_start": [430, 430, 430, 430]}, "prediction_text": "Nonconservative forces."}
{"answers": {"text": ["statistical mechanics", "statistical mechanics", "detailed", "detailed treatment with statistical mechanics"], "answer_start": [134, 134, 110, 110]}, "prediction_text": "Statistical mechanics."}
{"answers": {"text": ["nonconservative forces", "internal energies of the system", "nonconservative forces", "nonconservative forces"], "answer_start": [188, 229, 188, 188]}, "prediction_text": "Internal energies."}
{"answers": {"text": ["nonconservative forces", "nonconservative forces", "nonconservative forces", "nonconservative forces"], "answer_start": [188, 188, 188, 188]}, "prediction_text": "Nonconservative forces."}
{"answers": {"text": ["Second", "Second law of thermodynamics", "Second law", "Second"], "answer_start": [331, 331, 331, 331]}, "prediction_text": "Second law of thermodynamics."}
{"answers": {"text": ["nonconservative forces", "nonconservative forces", "nonconservative forces", "nonconservative forces"], "answer_start": [361, 361, 361, 361]}, "prediction_text": "Nonconservative forces."}
{"answers": {"text": ["kilogram-force", "pound-force", "kilogram-force (kgf)", "kilogram-force", "the kilogram-force ("], "answer_start": [82, 4, 82, 82, 78]}, "prediction_text": "kilogram-force (kgf)"}
{"answers": {"text": ["kilopond", "kilopond", "kilopond", "kilopond", "kilopond"], "answer_start": [114, 114, 114, 114, 114]}, "prediction_text": "kgf"}
{"answers": {"text": ["slug", "metric slug", "metric slug", "metric slug", "the metric slug"], "answer_start": [274, 267, 267, 267, 263]}, "prediction_text": "Slug (mug or hyl)"}
{"answers": {"text": ["kip", "kip", "kip", "kip", "kip"], "answer_start": [712, 712, 712, 712, 712]}, "prediction_text": "kilogram-force (kgf)"}
{"answers": {"text": ["sth\u00e8ne", "sth\u00e8ne", "sth\u00e8ne", "sth\u00e8ne", "sth\u00e8ne"], "answer_start": [665, 665, 665, 665, 665]}, "prediction_text": "kilogram-force (kgf)"}
