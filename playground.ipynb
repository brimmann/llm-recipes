{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a63dbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.data_utils import get_distillation_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab19dfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs import dataset as DATA_CONFIG\n",
    "from configs import fsdp_config as FSDP_CONFIG\n",
    "from configs import train_config as TRAIN_CONFIG\n",
    "from configs import distillation_config as DISTIL_CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1e491c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config, fsdp_config, distil_config, data_config = TRAIN_CONFIG(), FSDP_CONFIG(), DISTIL_CONFIG(), DATA_CONFIG()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12ac7602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_config(project_name=None, model_name='meta-llama/Llama-2-7b-hf', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=8, batching_strategy='padding', context_length=None, gradient_accumulation_steps=1, num_epochs=1, num_workers_dataloader=2, lr=1e-06, weight_decay=0.1, seed=42, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='', freeze_layers=False, num_freeze_layers=1, quantization=False, save_model=True, save_step=1000, save_optimizer=False, use_fast_kernels=False, distillation=False, save_all=False, training_size=1, encoder_decoder=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f977da22",
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict = {'model_name': 'EleutherAI/pythia-410m-deduped', 'dataset.file': 'datasets-m/loader/squad.py', 'lr': 1e-06, 'num_epochs': 5, 'batch_size_training': 1, 'val_batch_size': 1, 'output_dir': 'train/output/path', 'distillation_config.model_name': 'mistralai/Mistral-7B-Instruct-v0.2', 'distillation': True, 'distillation_config.enable_fsdp': False, 'distillation_config.pure_bf16': True, 'distillation_config.distil_factor': 1.5, 'save_step': 100, }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60e878a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_config(config, update_dict, isSubmodule=False):\n",
    "    if isinstance(config, (tuple, list)):\n",
    "        for c in config:\n",
    "            update_config(c, update_dict, isSubmodule)\n",
    "    else:\n",
    "        for k, v in update_dict.items():\n",
    "            if \".\" in k:\n",
    "                config_name, param_name = k.split(\".\", 1)\n",
    "                if type(config).__name__ == config_name:\n",
    "                    if hasattr(config, param_name):\n",
    "                        setattr(config, param_name, v)\n",
    "            elif not isSubmodule and hasattr(config, k):\n",
    "                setattr(config, k, v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3aa47240",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_config((train_config, fsdp_config, data_config), args_dict)\n",
    "update_config((distil_config), args_dict, isSubmodule=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fb15ea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_config(project_name=None, model_name='EleutherAI/pythia-410m-deduped', enable_fsdp=False, low_cpu_fsdp=False, run_validation=True, batch_size_training=1, batching_strategy='padding', context_length=None, gradient_accumulation_steps=1, num_epochs=5, num_workers_dataloader=2, lr=1e-06, weight_decay=0.1, seed=42, use_fp16=False, mixed_precision=True, val_batch_size=1, peft_method='lora', use_peft=False, output_dir='train/output/path', freeze_layers=False, num_freeze_layers=1, quantization=False, save_model=True, save_step=100, save_optimizer=False, use_fast_kernels=False, distillation=True, save_all=False, training_size=1, encoder_decoder=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad654a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brimmann/works/llm-recipes/models/checkpoint_handler.py:7: DeprecationWarning: `torch.distributed._shard.checkpoint` will be deprecated, use `torch.distributed.checkpoint` instead\n",
      "  import torch.distributed._shard.checkpoint as dist_cp\n",
      "/home/brimmann/works/llm-recipes/models/tools.py:5: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import packaging\n"
     ]
    }
   ],
   "source": [
    "from models.models_utils import get_distillation_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d16c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3e35bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `GPTNeoXSdpaAttention` class is deprecated in favor of simply modifying the `config._attn_implementation`attribute of the `GPTNeoXAttention` class! It will be removed in v4.48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Model EleutherAI/pythia-410m-deduped\n",
      "\n",
      "--> EleutherAI/pythia-410m-deduped has 405.334016 Million params\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97afa6f451a0401399e60c8512e5610c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Model mistralai/Mistral-7B-Instruct-v0.2\n",
      "\n",
      "--> mistralai/Mistral-7B-Instruct-v0.2 has 7241.732096 Million params\n",
      "\n"
     ]
    }
   ],
   "source": [
    "student_tokenizer, teacher_tokenizer, model = get_distillation_models(train_config, distil_config, fsdp_config, 0, args_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfb04474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dl_kwargs:  {'batch_sampler': <data.sampler.LengthBasedBatchSampler object at 0x746821432b80>, 'collate_fn': DataCollatorForSeq2Seq(tokenizer=GPTNeoXTokenizerFast(name_or_path='EleutherAI/pythia-410m-deduped', vocab_size=50254, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<|padding|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t50254: AddedToken(\"                        \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50255: AddedToken(\"                       \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50256: AddedToken(\"                      \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50257: AddedToken(\"                     \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50258: AddedToken(\"                    \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50259: AddedToken(\"                   \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50260: AddedToken(\"                  \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50261: AddedToken(\"                 \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50262: AddedToken(\"                \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50263: AddedToken(\"               \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50264: AddedToken(\"              \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50265: AddedToken(\"             \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50266: AddedToken(\"            \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50267: AddedToken(\"           \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50268: AddedToken(\"          \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50269: AddedToken(\"         \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50270: AddedToken(\"        \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50271: AddedToken(\"       \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50272: AddedToken(\"      \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50273: AddedToken(\"     \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50274: AddedToken(\"    \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50275: AddedToken(\"   \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "\t50276: AddedToken(\"  \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
      "}\n",
      "), model=None, padding=True, max_length=None, pad_to_multiple_of=None, label_pad_token_id=-100, return_tensors='pt')}\n",
      "train_dataloder in utils:  3\n",
      "--> Training Set Length = 3\n",
      "--> Validation Set Length = 1\n",
      "Inside data_utils, s_train_len and s_eval_len:  3 1\n",
      "train_dl_kwargs:  {'batch_sampler': <data.sampler.LengthBasedBatchSampler object at 0x74681cba6400>, 'collate_fn': DataCollatorForSeq2Seq(tokenizer=LlamaTokenizerFast(name_or_path='mistralai/Mistral-7B-Instruct-v0.2', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "), model=None, padding=True, max_length=None, pad_to_multiple_of=None, label_pad_token_id=-100, return_tensors='pt')}\n",
      "train_dataloder in utils:  3\n",
      "--> Training Set Length = 3\n",
      "--> Validation Set Length = 1\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, teacher_train_dataloader, eval_dataloader, teacher_eval_dataloader = get_distillation_dataloader(data_config, train_config, distil_config, student_tokenizer, teacher_tokenizer, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb8c760a",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_el1 = next(iter(train_dataloader))\n",
    "teacher_ele1 = next(iter(teacher_train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60ff904d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> [INST] You are an agent answering questions as part of a reading comprehension activity. You must read and understand the context text step by step. Answers consist of 0 to 5 words, exclusively continuous words taken from the contextual text provided.\\n\\nTitle: Christine\\'s boyfriend\\nContext: Patrick Harris (Tim DeKay), Old Christine\\'s new boyfriend, who she meets in a video store and starts dating.\\nQuestion: Who played patrick on new adventures of old christine? [/INST] Answer: Tim DeKay</s> [INST] Title: June 14, 2018: Death Row Inmates\\nContext: As of June 14, 2018, there were 2,718 death row inmates in the United States.\\nQuestion: Total number of death row inmates in the us? [/INST] Answer: 2,718</s> [INST] Title: Modern Communism\\nContext: Most modern forms of communism are grounded at least nominally in Marxism, an ideology conceived by noted sociologist Karl Marx during the mid nineteenth century.\\nQuestion: Who came up with the idea of communism ? [/INST] Answer: Karl Marx</s> [INST] Title: Time\\nContext: Time has long been a major subject of study in religion, philosophy, and science, but defining it in a manner applicable to all fields without circularity has consistently eluded scholars. Nevertheless, diverse fields such as business, industry, sports, the sciences, and the performing arts all incorporate some notion of time into their respective measuring systems. Some simple definitions of time include \"time is what clocks measure\", which is a problematically vague and self-referential definition that utilizes the device used to measure the subject as the definition of the subject, and \"time is what keeps everything from happening at once\", which is without substantive meaning in the absence of the definition of simultaneity in the context of the limitations of human sensation, observation of events, and the perception of such events.\\nQuestion: Fields such as business, industry, sports, science, and performing arts incorporate some notion of what into their measuring systems? [/INST] Answer: The notion of time.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_tokenizer.decode(teacher_ele1[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa22ab17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>Context: Time has long been a major subject of study in religion, philosophy, and science, but defining it in a manner applicable to all fields without circularity has consistently eluded scholars. Nevertheless, diverse fields such as business, industry, sports, the sciences, and the performing arts all incorporate some notion of time into their respective measuring systems. Some simple definitions of time include \"time is what clocks measure\", which is a problematically vague and self-referential definition that utilizes the device used to measure the subject as the definition of the subject, and \"time is what keeps everything from happening at once\", which is without substantive meaning in the absence of the definition of simultaneity in the context of the limitations of human sensation, observation of events, and the perception of such events.\\nQuestion: Fields such as business, industry, sports, science, and performing arts incorporate some notion of what into their measuring systems?\\nAnswer: The notion of time.<|endoftext|>'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_tokenizer.decode(student_el1[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987eebce",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv run python -m datasets-m.generator --model_id \"mistralai/Mistral-7B-Instruct-v0.2\" --dataset_id \"GEM/squad_v2\" --num_workers 1 --bfloat --batch_size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f27626",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "d = load_from_disk(\"/home/brimmann/works/llm-recipes/datasets/hf/Mistral-7B-Instruct-v0.2-squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5a1dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62286b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = DatasetDict({\"train\": d})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9993a3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e847775c",
   "metadata": {},
   "outputs": [],
   "source": [
    "d.save_to_disk(\"/home/brimmann/works/llm-recipes/datasets/hf/Mistral-7B-Instruct-v0.2-squad\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
